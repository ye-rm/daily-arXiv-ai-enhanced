<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs](https://arxiv.org/abs/2602.15995)
*Xiang Fu,Shiman Meng,Weiping Zhang,Luanzheng Guo,Kento Sato,Dong H. Ahn,Ignacio Laguna,Gregory L. Lee,Martin Schulz*

Main category: cs.DC

TL;DR: Novel record-and-replay techniques (Distributed Clock and Distributed Epoch) for OpenMP programs that reduce synchronization overhead by 2-5x compared to traditional approaches, enabling scalable replay of MPI+OpenMP applications.


<details>
  <summary>Details</summary>
Motivation: OpenMP remains the most popular shared memory programming framework despite its non-deterministic execution making debugging/testing challenging. Scalable replay of OpenMP programs is still an unresolved problem.

Method: Proposed two novel techniques: Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Implemented in ReOMP tool and integrated with ReMPI for MPI+OpenMP applications.

Result: Evaluation shows 2-5x more efficiency than traditional approaches that synchronize on every shared-memory access. Successfully integrated with ReMPI for MPI+OpenMP replay with small MPI-scale-independent runtime overhead.

Conclusion: The proposed DC and DE techniques provide scalable record-and-replay solutions for OpenMP programs, addressing the debugging/testing challenges of non-deterministic execution while maintaining efficiency and compatibility with MPI-level tools.

Abstract: After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.

</details>


### [2] [Scrutinizing Variables for Checkpoint Using Automatic Differentiation](https://arxiv.org/abs/2602.16010)
*Xin Huang,Weiping Zhang,Shiman Meng,Wubiao Xu,Xiang Fu,Luanzheng Guo,Kento Sato*

Main category: cs.DC

TL;DR: Using automatic differentiation to identify and exclude non-critical data elements from checkpointing in HPC applications, reducing storage needs by up to 20%.


<details>
  <summary>Details</summary>
Motivation: Checkpoint/Restart consumes significant system resources in HPC applications, but not all data elements are actually used in computation. Excluding unused data from checkpointing could improve storage and compute efficiency.

Method: Proposes a systematic approach using automatic differentiation (AD) to analyze every element within variables (like arrays) for checkpointing. Each element is inspected to determine if it impacts application output, allowing identification of critical vs. uncritical elements that can be excluded from checkpointing.

Result: Successfully visualized critical/uncritical elements/regions within variables across eight NAS Parallel Benchmark (NPB) suites. Found interesting patterns/distributions that follow the physical formulation/logic of algorithms. Achieved up to 20% storage savings for checkpointing.

Conclusion: The AD-based approach effectively identifies non-critical data elements that can be excluded from checkpointing, significantly reducing storage requirements while maintaining application correctness, with patterns reflecting the underlying algorithm logic.

Abstract: Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.

</details>


### [3] [LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum](https://arxiv.org/abs/2602.16100)
*Zijie Su,Muhammed Tawfiqul Islam,Mohammad Goudarzi,Adel N. Toosi*

Main category: cs.DC

TL;DR: Dynamic pipeline reconfiguration for LLM serving in serverless environments with minimal downtime and performance impact.


<details>
  <summary>Details</summary>
Motivation: Efficient LLM serving under limited GPU resources is challenging due to diverse workloads and heterogeneous GPU clusters. Serverless computing offers resource utilization benefits but requires dynamic adaptation to elastic environments, which is difficult due to LLM's stateful nature and large model parameters.

Method: Proposes a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. The system can select optimal pipeline configurations in response to changing workloads.

Result: Experimental results on heterogeneous GPU platforms (NVIDIA A100 and L40s) show the migration mechanism incurs less than 50 ms service downtime, with under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

Conclusion: The proposed dynamic pipeline reconfiguration approach effectively addresses the challenge of adapting LLM serving to elastic serverless environments while maintaining low latency and high performance.

Abstract: With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

</details>


### [4] [Near-optimal population protocols on bounded-degree trees](https://arxiv.org/abs/2602.16222)
*Joel Rybicki,Jakob Solnerzik,Robin Vacus*

Main category: cs.DC

TL;DR: Population protocols on bounded-degree trees don't show significant space-time trade-offs for leader election and exact majority, unlike complete graphs. Constant-space protocols achieve near-optimal stabilization time with linear speed-up.


<details>
  <summary>Details</summary>
Motivation: While optimal space-time trade-offs are known for population protocols in complete graphs, it was unknown whether similar trade-offs exist for sparse graphs like bounded-degree trees. Existing lower bound techniques don't extend beyond dense graphs.

Method: Developed two novel protocols: 1) fast self-stabilizing 2-hop coloring protocol for general graphs with stabilization time bounded using stochastic drift, and 2) self-stabilizing tree orientation algorithm that builds rooted trees in optimal time. These enable using simple constant-state protocols designed for directed trees.

Result: Constant-space protocols achieve near-optimal worst-case expected stabilization time for leader election and exact majority on bounded-degree trees. Directed annihilation dynamics solve exact majority in O(n² log n) steps on directed trees, showing linear speed-up over state-of-the-art.

Conclusion: Bounded-degree trees don't exhibit significant asymptotic space-time trade-offs for fundamental problems like leader election and exact majority, unlike complete graphs. The new protocols provide efficient constant-space solutions with optimal time complexity.

Abstract: We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.
  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.
  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \log n)$ steps on directed trees.

</details>


### [5] [DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting](https://arxiv.org/abs/2602.16233)
*Prabhjot Singh,Adel N. Toosi,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Circuit cutting decomposes large quantum circuits into smaller subcircuits for classical reconstruction, but its end-to-end impact on training pipelines needs better measurement. The paper proposes a cut-aware execution pipeline and quantifies overheads, showing reconstruction dominates runtime but accuracy is preserved.


<details>
  <summary>Details</summary>
Motivation: Prior work focuses on subcircuit counts and sampling complexity, but the end-to-end impact of circuit cutting on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. Understanding the practical scaling limits and overheads is crucial for real-world applications.

Method: Proposes a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload with four phases: partitioning, subexperiment generation, parallel execution, and classical reconstruction. Uses logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST) to quantify cutting overheads, scaling limits, and sensitivity to stragglers.

Result: Cutting introduces substantial end-to-end overheads that grow with number of cuts, with reconstruction constituting a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Test accuracy and robustness are preserved in measured regimes, with configuration-dependent improvements observed in some cut settings.

Conclusion: Practical scaling of circuit cutting for learning workloads depends on reducing and overlapping reconstruction, and on scheduling policies that account for barrier-dominated critical paths. Despite systems costs, accuracy and robustness can be preserved with proper configuration.

Abstract: Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.

</details>


### [6] [push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation](https://arxiv.org/abs/2602.16338)
*Mohsen Ahmadvand,Rok Pajnič,Ching-Lun Chiu*

Main category: cs.DC

TL;DR: push0 is a cloud-native proof orchestration system that manages zero-knowledge proof generation for blockchain systems, addressing strict timing constraints and fault tolerance requirements.


<details>
  <summary>Details</summary>
Motivation: Zero-knowledge proof generation has strict timing constraints in blockchain systems: delayed proofs cause finality lag and economic loss in ZK-rollups, and Ethereum's L1 zkEVM requires proofs within 12-second slot windows. Current systems lack a principled orchestration framework that addresses head-of-chain ordering, sub-slot latency bounds, fault tolerance, and prover-agnostic workflow composition.

Method: push0 uses a cloud-native architecture that decouples prover binaries from scheduling infrastructure. It employs an event-driven dispatcher-collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. The system formalizes requirements from production ZK-rollup operations and Ethereum's real-time proving specification.

Result: Production Kubernetes cluster experiments show push0 achieves 5 ms median orchestration overhead with 99-100% scaling efficiency at 32 dispatchers. This overhead is negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results (3-10 ms P50). Production deployment on Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity.

Conclusion: push0 provides an effective orchestration framework for zero-knowledge proof generation that meets the stringent requirements of blockchain systems. It enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides scheduling primitives suitable for both centralized rollup operators and decentralized multi-prover networks.

Abstract: Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.

</details>


### [7] [Load Balanced Parallel Node Generation for Meshless Numerical Methods](https://arxiv.org/abs/2602.16347)
*Jon Vehovar,Miha Rot,Matjaž Depolli,Gregor Kosec*

Main category: cs.DC

TL;DR: Parallel Poisson disc sampling method for meshless analysis using coupled spatial indexing and work distribution hypertrees to enable efficient parallel node generation without locking.


<details>
  <summary>Details</summary>
Motivation: Meshless methods require efficient node generation for solving PDEs, particularly for complex geometries with variable node density. Existing methods need parallelization for better performance, but current parallel approaches face synchronization and locking challenges.

Method: Modified n-dimensional Poisson disc sampling with coupled spatial indexing and prebuilt work distribution hypertrees. Threads advance separate fronts and claim work hypertree leaves while avoiding neighboring leaves claimed by other threads. Combines node placement constraints with partially prebuilt spatial hypertree to eliminate tree locking during modifications.

Result: The algorithm reduces mutex acquisitions for point insertion collision checks through work hypertree leaf-level thread collision handling. Enables efficient parallel execution while maintaining node placement quality for meshless analysis.

Conclusion: Proposed parallel Poisson disc sampling algorithm successfully addresses synchronization challenges, improves performance over existing parallelization attempts, and provides foundations for adaptation to distributed systems.

Abstract: Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.

</details>


### [8] [How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability](https://arxiv.org/abs/2602.16362)
*MHD Saria Allahham,Hossam S. Hassanein*

Main category: cs.DC

TL;DR: Analytical framework for computational reliability in Extreme Edge Computing (XEC) that quantifies probability of meeting streaming service requirements despite volatile device availability, with closed-form reliability expressions and optimal workload allocation rules.


<details>
  <summary>Details</summary>
Motivation: Consumer devices in XEC exhibit volatile computational availability due to competing applications and unpredictable usage patterns, creating fundamental challenges for distributed inference streaming services that need to meet strict throughput and latency requirements.

Method: Developed analytical framework for computational reliability defined as probability that instantaneous capacity meets demand at specified QoS threshold. Derived closed-form reliability expressions under two information regimes: Minimal Information (MI) using declared operational bounds, and historical data using Maximum Likelihood Estimation. Extended framework to multi-device deployments with reliability expressions for series, parallel, and partitioned workload configurations.

Result: Framework provides optimal workload allocation rules and analytical bounds for device selection, enabling orchestrators to evaluate deployment feasibility and configure distributed streaming systems. Validation using real-time object detection with YOLO11m model in emulated XEC environments showed close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

Conclusion: The analytical framework provides tractable tools for quantifying computational reliability in XEC, addressing the fundamental challenge of volatile device availability in distributed inference streaming services, with practical applications for orchestrators in deployment planning and system configuration.

Abstract: Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

</details>


### [9] [FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving](https://arxiv.org/abs/2602.16603)
*Chia-chi Hsieh,Zan Zong,Xinyang Chen,Jianjiang Li,Jidong Zhai,Lijie Wen*

Main category: cs.DC

TL;DR: FlowPrefill is a serving system for LLMs that optimizes TTFT and goodput by decoupling preemption granularity from scheduling frequency using operator-level preemption and event-driven scheduling.


<details>
  <summary>Details</summary>
Motivation: The growing demand for LLMs requires serving systems to handle concurrent requests with diverse SLOs, but compute-intensive prefill phases cause head-of-line blocking where long requests delay higher-priority ones, leading to TTFT SLO violations. While chunked prefill enables interruptibility, it creates a trade-off between responsiveness and throughput.

Method: FlowPrefill introduces two key innovations: 1) Operator-Level Preemption that leverages operator boundaries for fine-grained execution interruption without efficiency loss, and 2) Event-Driven Scheduling that triggers decisions only on request arrival/completion events to minimize overhead while supporting efficient preemption.

Result: Evaluation on real-world production traces shows FlowPrefill improves maximum goodput by up to 5.6× compared to state-of-the-art systems while satisfying heterogeneous SLOs.

Conclusion: FlowPrefill resolves the conflict between responsiveness and throughput in LLM serving by decoupling preemption granularity from scheduling frequency, achieving adaptive prefill scheduling that optimizes both TTFT and goodput.

Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [Decomposing Large-Scale Ising Problems on FPGAs: A Hybrid Hardware Approach](https://arxiv.org/abs/2602.15985)
*Ruihong Yin,Yue Zheng,Chaohui Li,Ahmet Efe,Abhimanyu Kumar,Ziqing Zeng,Ulya R. Karpuzcu,Sachin S. Sapatnekar,Chris H. Kim*

Main category: cs.ET

TL;DR: FPGA-accelerated decomposition for analog Ising machines achieves 2× speedup and 100× energy efficiency over CPU baselines


<details>
  <summary>Details</summary>
Motivation: Analog Ising machines offer fast optimization but suffer from scalability limitations requiring decomposition, which introduces latency when done on CPUs, preventing full utilization of the high-speed analog solver

Method: Heterogeneous system with FPGA offloading decomposition workload, tightly integrated with custom 28nm Ising solver, migrating decomposition logic to reconfigurable hardware with parallel processing elements to minimize communication latency

Result: Nearly 2× speedup and energy efficiency improvement of over two orders of magnitude compared to optimized software baselines on modern CPUs

Conclusion: Co-design approach effectively bridges speed gap between digital preprocessing and analog solving, enabling high-speed analog Ising machines to tackle large-scale real-world problems

Abstract: Emerging analog computing substrates, such as oscillator-based Ising machines, offer rapid convergence times for combinatorial optimization but often suffer from limited scalability due to physical implementation constraints. To tackle real-world problems involving thousands of variables, problem decomposition is required; however, performing this step on standard CPUs introduces significant latency, preventing the high-speed solver from operating at full capacity. This work presents a heterogeneous system that offloads the decomposition workload to an FPGA, tightly integrated with a custom 28nm Ising solver. By migrating the decomposition logic to reconfigurable hardware and utilizing parallel processing elements, the system minimizes the communication latency typically associated with host-device interactions. Our evaluation demonstrates that this co-design approach effectively bridges the speed gap between digital preprocessing and analog solving, achieving nearly 2$\times$ speedup and an energy efficiency improvement of over two orders of magnitude compared to optimized software baselines running on modern CPUs.

</details>


### [11] [Bibby AI -- AI Latex Editor writing assistant for researchers vs Overleaf Alternative vs OpenAI Prism. (Bibby AI Latex Editor)](https://arxiv.org/abs/2602.16432)
*Nilesh jain,Rohit Yadav,Andrej Karpathy*

Main category: cs.ET

TL;DR: Bibby AI is an AI-first LaTeX editor that integrates multiple AI tools for academic writing, outperforming existing solutions in LaTeX error detection and fixing.


<details>
  <summary>Details</summary>
Motivation: Current LaTeX editors lack native AI integration, forcing researchers to leave their editing environment for AI assistance, which fragments document context and interrupts writing flow.

Method: Developed Bibby AI as a native, AI-first LaTeX editor with embedded AI writing assistant, smart citation search, AI table/equation generation, paper reviewer, abstract generator, literature review drafting, research assistant, and real-time LaTeX error detection/auto-fix. Created LaTeXBench-500 benchmark with 500 real-world compilation errors across six categories for evaluation.

Result: Bibby achieved 91.4% detection accuracy and 83.7% one-click fix accuracy on LaTeXBench-500, significantly outperforming Overleaf's native diagnostics (61.2%) and OpenAI Prism (78.3%/64.1%).

Conclusion: Bibby AI demonstrates that a privacy-preserving, research-first AI editor can meaningfully accelerate every stage of academic manuscript preparation and is superior to existing solutions like Overleaf and OpenAI Prism.

Abstract: Large language models are increasingly integrated into academic writing workflows; however, the most widely used \LaTeX\ editors remain AI-peripheral -- offering compilation and collaboration, but no native intelligence. This separation forces researchers to leave their editing environment for AI assistance, fragmenting document context and interrupting writing flow. We present Bibby AI (trybibby.com), a native, AI-first \LaTeX\ editor that unifies the complete research writing lifecycle within a single interface. Bibby embeds an AI writing assistant, smart citation search, AI table and equation generation, an AI paper reviewer, abstract generator, literature review drafting, a deep research assistant, and real-time \LaTeX\ error detection and auto-fix -- all natively, without plugins or copy-paste workflows. We introduce LaTeXBench-500, a benchmark of 500 real-world compilation errors across six categories. Bibby achieves 91.4\% detection accuracy and 83.7\% one-click fix accuracy, outperforming Overleaf's native diagnostics (61.2\%) and OpenAI Prism (78.3 / 64.1\%) by large margins. Bibby demonstrates that a privacy-preserving, research-first AI editor can meaningfully accelerate every stage of academic manuscript preparation. We found that Bibby AI is a far superior alternative to overleaf latex and better than OpenAI Prism functionalities and AI.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware](https://arxiv.org/abs/2602.16024)
*R. Kanda,H. L. Blevec,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: Real-time few-shot learning implementation on tiny FPGA SoCs with arbitrary fixed-point bit-widths using FINN framework, achieving 2x throughput on CIFAR-10 while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional Tensil-based design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits, restricting optimization opportunities for resource-constrained FPGA SoCs.

Method: Adopted FINN framework for arbitrary bit-width implementations, with customizations including: 1) Optimization of Transpose nodes to resolve data format mismatches, 2) Addition of handling for converting final reduce mean operation to Global Average Pooling (GAP).

Result: Achieved approximately twice the throughput while maintaining the same accuracy as conventional realization in evaluations using CIFAR-10 dataset, with reduced bit-width.

Conclusion: The proposed methodology enables efficient real-time few-shot learning on resource-constrained FPGA SoCs like PYNQ-Z1 by allowing arbitrary fixed-point bit-width optimization through FINN framework with custom adjustments.

Abstract: In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.

</details>


### [13] [DARTH-PUM: A Hybrid Processing-Using-Memory Architecture](https://arxiv.org/abs/2602.16075)
*Ryan Wong,Ben Feinberg,Saugata Ghose*

Main category: cs.AR

TL;DR: DARTH-PUM is a general-purpose hybrid PUM architecture that integrates analog and digital processing-using-memory to enable complete kernel execution in memory, achieving significant speedups for various applications.


<details>
  <summary>Details</summary>
Motivation: Analog PUM is efficient for matrix-vector multiplication but cannot perform non-MVM operations, limiting it to ML inference. Recent work shows memory arrays can also perform Boolean PUM operations, creating an opportunity to combine both for general-purpose computation.

Method: Proposes DARTH-PUM with optimized peripheral circuitry, coordinating hardware to manage analog/digital PUM interfaces, easy-to-use programming interface, and flexible data width support. Enables full kernel execution in memory across embedded to large-scale applications.

Result: Achieves speedups of 59.4x for AES encryption, 14.8x for convolutional neural networks, and 40.8x for large-language models over analog+CPU baseline.

Conclusion: DARTH-PUM successfully integrates analog and digital PUM to create a practical, scalable architecture for general-purpose computation that can execute complete kernels in memory with significant performance improvements.

Abstract: Analog processing-using-memory (PUM; a.k.a. in-memory computing) makes use of electrical interactions inside memory arrays to perform bulk matrix-vector multiplication (MVM) operations. However, many popular matrix-based kernels need to execute non-MVM operations, which analog PUM cannot directly perform. To retain its energy efficiency, analog PUM architectures augment memory arrays with CMOS-based domain-specific fixed-function hardware to provide complete kernel functionality, but the difficulty of integrating such specialized CMOS logic with memory arrays has largely limited analog PUM to being an accelerator for machine learning inference, or for closely related kernels. An opportunity exists to harness analog PUM for general-purpose computation: recent works have shown that memory arrays can also perform Boolean PUM operations, albeit with very different supporting hardware and electrical signals than analog PUM.
  We propose DARTH-PUM, a general-purpose hybrid PUM architecture that tackles key hardware and software challenges to integrating analog PUM and digital PUM. We propose optimized peripheral circuitry, coordinating hardware to manage and interface between both types of PUM, an easy-to-use programming interface, and low-cost support for flexible data widths. These design elements allow us to build a practical PUM architecture that can execute kernels fully in memory, and can scale easily to cater to domains ranging from embedded applications to large-scale data-driven computing. We show how three popular applications (AES encryption, convolutional neural networks, large-language models) can map to and benefit from DARTH-PUM, with speedups of 59.4x, 14.8x, and 40.8x over an analog+CPU baseline.

</details>


### [14] [Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture](https://arxiv.org/abs/2602.16143)
*Naoya Onizawa,Taiga Kubuta,Duckgyu Shin,Takahiro Hanyu*

Main category: cs.AR

TL;DR: FPGA-based stochastic simulated quantum annealing architecture for scalable, energy-efficient combinatorial optimization using p-bits, achieving 50% energy reduction and 90% logic resource reduction compared to prior work.


<details>
  <summary>Details</summary>
Motivation: Existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead, requiring more efficient hardware solutions.

Method: Combines spin-serial and replica-parallel update schedule with dual-BRAM delay-line architecture to enable scalable support for fully connected Ising models while eliminating fan-out growth in logic resources.

Result: Implemented on Xilinx ZC706 FPGA, solves 800-node MAX-CUT benchmark with up to 50% reduction in energy consumption and over 90% reduction in logic resources compared to prior FPGA-based p-bit annealing architectures.

Conclusion: Demonstrates practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

Abstract: Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

</details>
