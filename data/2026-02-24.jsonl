{"id": "2602.19433", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.19433", "abs": "https://arxiv.org/abs/2602.19433", "authors": ["Paul Borrill"], "title": "Why iCloud Fails: The Category Mistake of Cloud Synchronization", "comment": "18 pages, 3 figures, 37 references", "summary": "iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality."}
{"id": "2602.18568", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18568", "abs": "https://arxiv.org/abs/2602.18568", "authors": ["Matthew Adiletta", "Gu-Yeon Wei", "David Brooks"], "title": "RPU -- A Reasoning Processing Unit", "comment": null, "summary": "Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.\n  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B."}
{"id": "2602.19031", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19031", "abs": "https://arxiv.org/abs/2602.19031", "authors": ["Meng Zhang", "Ziang Yin", "Nicholas Gangi", "Alexander Chen", "Brett Bamfo", "Tianle Xu", "Jiaqi Gu", "Zhaoran Rena Huang"], "title": "SKYLIGHT: A Scalable Hundred-Channel 3D Photonic In-Memory Tensor Core Architecture for Real-time AI Inference", "comment": null, "summary": "The growing computational demands of artificial intelligence (AI) are challenging conventional electronics, making photonic computing a promising alternative. However, existing photonic architectures face fundamental scalability and reliability barriers. This paper introduces SKYLIGHT, a scalable 3D photonic in-memory tensor core architecture designed for real-time AI inference. By co-designing its topology, wavelength routing, accumulation, and programming in a 3D stack, SKYLIGHT overcomes key limitations. Its innovations include a low-loss 3D Si/SiN crossbar topology, a thermally robust non-micro-ring resonator (MRR)-based wavelength-division multiplexing (WDM) component, a hierarchical signal accumulation using a multi-port photodetector (PD), and optically programmed non-volatile phase-change material (PCM) weights. Importantly, SKYLIGHT enables in-situ weight updates that support label-free, layer-local learning (e.g., forward-forward local updates) in addition to inference. Using SimPhony for system-level modeling, we show that a single 144 x 256 SKYLIGHT core is feasible within a single reticle and delivers 342.1 TOPS at 23.7 TOPS/W, enabling ResNet-50 inference at 1212 FPS with 27 mJ per image, and achieves 84.17 FPS/W end-to-end (1.61 x higher than an NVIDIA RTX PRO 6000 Blackwell GPU) under the same workload in real-time measurements. System-level evaluations on four representative machine learning tasks, including unsupervised local self-learning, demonstrate SKYLIGHT's robustness to realistic hardware non-idealities (low-bit quantization and signal-proportional analog noise capturing modulation, PCM programming, and readout variations). With noise-aware training, SKYLIGHT maintains high task accuracy, validating its potential as a comprehensive solution for energy-efficient, large-scale photonic AI accelerators."}
{"id": "2602.18641", "categories": ["cs.DC", "physics.hist-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18641", "abs": "https://arxiv.org/abs/2602.18641", "authors": ["Paul Borrill"], "title": "The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist", "comment": "13 pages, no figures", "summary": "In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats \"synchronized time\" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum \"mysteries\" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution."}
{"id": "2602.18750", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.18750", "abs": "https://arxiv.org/abs/2602.18750", "authors": ["He Sun", "Li Li", "Mingjun Xiao"], "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD", "comment": "12 pages, 12 figures, under review", "summary": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy."}
{"id": "2602.19312", "categories": ["cs.ET", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.19312", "abs": "https://arxiv.org/abs/2602.19312", "authors": ["Kyriakos Stylianopoulos", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "George C. Alexandropoulos"], "title": "Metasurfaces-Integrated Wireless Neural Networks for Lightweight Over-The-Air Edge Inference", "comment": "9 pages, 6 figures, submitted for magazine publication", "summary": "The upcoming sixth Generation (6G) of wireless networks envisions ultra-low latency and energy efficient Edge Inference (EI) for diverse Internet of Things (IoT) applications. However, traditional digital hardware for machine learning is power intensive, motivating the need for alternative computation paradigms. Over-The-Air (OTA) computation is regarded as an emerging transformative approach assigning the wireless channel to actively perform computational tasks. This article introduces the concept of Metasurfaces-Integrated Neural Networks (MINNs), a physical-layer-enabled deep learning framework that leverages programmable multi-layer metasurface structures and Multiple-Input Multiple-Output (MIMO) channels to realize computational layers in the wave propagation domain. The MINN system is conceptualized as three modules: Encoder, Channel (uncontrollable propagation features and metasurfaces), and Decoder. The first and last modules, realized respectively at the multi-antenna transmitter and receiver, consist of conventional digital or purposely designed analog Deep Neural Network (DNN) layers, and the metasurfaces responses of the Channel module are optimized alongside all modules as trainable weights. This architecture enables computation offloading into the end-to-end physical layer, flexibly among its constituent modules, achieving performance comparable to fully digital DNNs while significantly reducing power consumption. The training of the MINN framework, two representative variations, and performance results for indicative applications are presented, highlighting the potential of MINNs as a lightweight and sustainable solution for future EI-enabled wireless systems. The article is concluded with a list of open challenges and promising research directions."}
{"id": "2602.18723", "categories": ["cs.DC", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18723", "abs": "https://arxiv.org/abs/2602.18723", "authors": ["Paul Borrill"], "title": "What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature", "comment": "15 pages, no figures", "summary": "The foundational impossibility results of distributed computing -- the Fischer-Lynch-Paterson theorem, the Two Generals Problem, the CAP theorem -- are widely understood as discoveries about the physical limits of coordination. This paper argues that they are nothing of the sort. They are consequences of a category mistake: treating Forward-In-Time-Only (FITO) information flow as a law of nature rather than recognizing it as a design choice inherited from Shannon's channel model and Lamport's happened-before relation. We develop this argument in six steps. First, we introduce the category mistake framework from Ryle through Spekkens' ontic/epistemic distinction in quantum foundations. Second, we identify FITO as the hidden axiom that unifies the classical impossibility results. Third, we apply Spekkens' Leibnizian principle to show that FITO-based models contain surplus ontological structure. Fourth, we develop the counterfactual: what changes when FITO is dropped. Fifth, we demonstrate that the impossibility theorems are theorems about FITO systems, not about physics. Sixth, we sketch the transactional alternative -- bilateral interactions that dissolve the apparent impossibilities by replacing unidirectional message passing with atomic bilateral transactions. The implication is that distributed computing has spent fifty years optimizing within the wrong design space."}
{"id": "2602.19007", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19007", "abs": "https://arxiv.org/abs/2602.19007", "authors": ["Md Rownak Hossain Chowdhury", "Mostafizur Rahman"], "title": "A Logic-Reuse Approach to Nibble-based Multiplier Design for Low Power Vector Computing", "comment": null, "summary": "Vector multiplication is a fundamental operation for AI acceleration, responsible for over 85% of computational load in convolution tasks. While essential, these operations are primary drivers of area, power, and delay in modern datapath designs. Conventional multiplier architectures often force a compromise between latency and complexity: high-speed array multipliers demand significant power, whereas sequential designs offer efficiency at the cost of throughput. This paper presents a precompute-reuse nibble multiplier architecture that bridges this gap by reformulating multiplication as a structured composition of reusable nibble-level precomputed values. The proposed design treats each operand as an independent low-precision element, decomposes it into fixed-width nibbles, and generates scaled multiples of a broadcast operand using compact shift-add logic. By replacing wide lookup tables and multiway multiplexers with logic-based precomputation and regular accumulation, the architecture decouples cycle complexity from gate delay. The design completes each 8-bit multiplication in two deterministic cycles with a short critical path, scales efficiently across vector lanes, and significantly reduces area and energy consumption. RTL implementations synthesized in TSMC 28 nm technology demonstrate up to 1.69x area reduction and 1.63x power improvement over shift-add, and nearly 2.6x area and 2.7x power savings compared to LUT-based array multipliers at 128 bit scale."}
{"id": "2602.19341", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments."}
{"id": "2602.18755", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18755", "abs": "https://arxiv.org/abs/2602.18755", "authors": ["Omar Basit", "Yunzhao Liu", "Z. Jonny Kong", "Y. Charlie Hu"], "title": "BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS", "comment": null, "summary": "Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.\n  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.\n  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe."}
{"id": "2602.19242", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19242", "abs": "https://arxiv.org/abs/2602.19242", "authors": ["Zheng Li", "Guangyi Zeng", "Paul Delestrac", "Enyi Yao", "Simei Yang"], "title": "pHNSW: PCA-Based Filtering to Accelerate HNSW Approximate Nearest Neighbor Search", "comment": "6 pages for ASPDAC2026 conference", "summary": "Hierarchical Navigable Small World (HNSW) has demonstrated impressive accuracy and low latency for high-dimensional nearest neighbor searches. However, its high computational demands and irregular, large-volume data access patterns present significant challenges to search efficiency. To address these challenges, we introduce pHNSW, an algorithm-hardware co-optimized solution that accelerates HNSW through Principal Component Analysis (PCA) filtering. On the algorithm side, we apply PCA filtering to reduce the dimensionality of the dataset, thereby lowering the volume of neighbor access and decreasing the computational load for distance calculations. On the hardware side, we design the pHNSW processor with custom instructions to optimize search throughput and energy efficiency. In the experiments, we synthesized the pHNSW processor RTL design with a 65nm technology node and evaluated it using DDR4 and HBM1.0 DRAM standards. The results show that pHNSW boosts Queries per Second (QPS) by 14.47x-21.37x on a CPU and 5.37x-8.46x on a GPU, while reducing energy consumption by up to 57.4% compared to standard HNSW implementation."}
{"id": "2602.19694", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.19694", "abs": "https://arxiv.org/abs/2602.19694", "authors": ["Bo Liu", "Tong Li", "Zhu Xiao", "Ruihui Li", "Geyong Min", "Zhuo Tang", "Kenli Li"], "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs", "comment": "under review", "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks."}
{"id": "2602.18797", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18797", "abs": "https://arxiv.org/abs/2602.18797", "authors": ["Mubshra Zulfiqar", "Muhammad Ayzed Mirza", "Basit Qureshi"], "title": "Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning", "comment": null, "summary": "Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-user interference in multi-antenna (MIMO) uplinks complicate real-time resource management. Traditional centralized optimization and off-policy reinforcement learning struggle with scalability and signaling overhead in dense networks. This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework based on multi-agent proximal policy optimization. The multi-user MIMO-MEC system is modeled as a Decentralized Partially Observable Markov Decision Process (DEC-POMDP) to jointly minimize carbon emissions and buffer latency and energy wastage. A scalable architecture utilizes decentralized execution with parameter sharing (DEPS), which enables autonomous IoT agents to make fine-grained power control and offloading decisions based solely on local observations. Additionally, a carbon-first reward structure adaptively prioritizes green time slots for data transmission to decouple system throughput from grid-dependent carbon footprints. Finally, experimental results demonstrate CADDTO-PPO outperforms deep deterministic policy gradient (DDPG) and lyapunov-based baselines. The framework achieves the lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling validates the framework to demonstrate a constant $O(1)$ inference complexity and theoretical lightweight feasibility for future generation sustainable IoT deployments."}
{"id": "2602.19268", "categories": ["cs.AR", "cs.AI", "cs.CV", "cs.NE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.19268", "abs": "https://arxiv.org/abs/2602.19268", "authors": ["Sonu Kumar", "Mohd Faisal Khan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications", "comment": null, "summary": "This brief presents a runtime-adaptive, performance-enhanced vector engine featuring a low-resource, iterative CORDIC-based MAC unit for edge AI acceleration. The proposed design enables dynamic reconfiguration between approximate and accurate modes, exploiting the latency-accuracy trade-off for a wide range of workloads. Its resource-efficient approach further enables up to 4x throughput improvement within the same hardware resources by leveraging vectorised, time-multiplexed execution and flexible precision scaling. With a time-multiplexed multi-AF block and a lightweight pooling and normalisation unit, the proposed vector engine supports flexible precision (4/8/16-bit) and high MAC density. The ASIC implementation results show that each MAC stage can save up to 33% of time and 21% of power, with a 256-PE configuration that achieves higher compute density (4.83 TOPS/mm2 ) and energy efficiency (11.67 TOPS/W) than previous state-of-the-art work. A detailed hardware-software co-design methodology for object detection and classification tasks on Pynq-Z2 is discussed to assess the proposed architecture, demonstrating a scalable, energy-efficient solution for edge AI applications."}
{"id": "2602.20083", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20083", "abs": "https://arxiv.org/abs/2602.20083", "authors": ["Xinzhao Li", "Alptekin Vardar", "Franz Müller", "Navya Goli", "Umamaheswara Tida", "Kai Ni", "X. Sharon Hu", "Thomas Kämpfe", "Ruiyang Qin"], "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval", "comment": "Accepted by DAC'26", "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG."}
{"id": "2602.18931", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18931", "abs": "https://arxiv.org/abs/2602.18931", "authors": ["Noah Martin", "Fahad Dogar"], "title": "WANSpec: Leveraging Global Compute Capacity for LLM Inference", "comment": null, "summary": "Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%."}
{"id": "2602.19305", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19305", "abs": "https://arxiv.org/abs/2602.19305", "authors": ["Irisha M. Goswami", "D. G. Perera"], "title": "Closed-Loop Environmental Control System on Embedded Systems", "comment": "19 pages, 6 figures", "summary": "In this paper, our objective is to design, build, and verify a closed-loop environmental control system tailored for small-scale agriculture applications. This project aims to develop a low-cost, safety-critical embedded solution using the Nuvoton NUC140 microcontroller to automate temperature regulation. The goal was to mitigate crop yield losses caused by environmental fluctuations in a greenhouse. Our final implemented system successfully meets all design specifications, demonstrating robust temperature regulation through a PID control loop and ensuring hardware safety through galvanic isolation"}
{"id": "2602.19084", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19084", "abs": "https://arxiv.org/abs/2602.19084", "authors": ["Emir Gencer", "Mohammad Kefah Taha Issa", "Ilyas Turimbetov", "James D. Trotter", "Didem Unat"], "title": "ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication", "comment": "11 pages, 8 figures. To appear in the 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2026)", "summary": "UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.\n  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace."}
{"id": "2602.19720", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19720", "abs": "https://arxiv.org/abs/2602.19720", "authors": ["Xiaoke Wang", "Raveena Raikar", "Markus Rein", "Ruiqi Chen", "Chang Meng", "Dirk Stroobandt"], "title": "Interconnect-Aware Logic Resynthesis for Multi-Die FPGAs", "comment": null, "summary": "Multi-die FPGAs enable device scaling beyond reticle limits but introduce severe interconnect overhead across die boundaries. Inter-die connections, commonly referred to as super-long lines (SLLs), incur high delay and consume scarce interposer interconnect resources, often dominating critical paths and complicating physical design. To address this, this work proposes an interconnect-aware logic resynthesis method that restructures the LUT-level netlist to reduce the number of SLLs. The resynthesis engine uses die partitioning information to apply logic resubstitutions, which simplifies local circuit structures and eliminates SLLs. By reducing the number of SLLs early in the design flow, prior to physical implementation, the proposed method shortens critical paths, alleviates pressure on scarce interposer interconnect resources, and improves overall physical design flexibility. We further build a tool flow for multi-die FPGAs by integrating the proposed resynthesis method with packing and placement. Experimental results on the EPFL benchmarks show that, compared with a state-of-the-art framework, the proposed method reduces the number of SLLs by up to 24.8% for a 2-die FPGA and up to 27.38% for a 3-die FPGA. On MCNC benchmarks, our tool flow achieves an average SLL reduction of 1.65% while preserving placement quality. On Koios benchmarks, where fewer removable SLLs exist, several designs still exhibit considerable inter-die edge reductions. Overall, the results confirm that reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs."}
{"id": "2602.19088", "categories": ["cs.DC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.19088", "abs": "https://arxiv.org/abs/2602.19088", "authors": ["Ziwei Zhou", "Si Liu", "Zhou Zhou", "Peixin Wang", "MIn Zhang"], "title": "A Formal Framework for Predicting Distributed System Performance under Faults", "comment": "32 pages, 3 figures. Accepted by FM 2026", "summary": "Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments."}
{"id": "2602.19884", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19884", "abs": "https://arxiv.org/abs/2602.19884", "authors": ["Harry Fitchett", "Jasmine Ritchie", "Charles Fox"], "title": "Extending CPU-less parallel execution of lambda calculus in digital logic with lists and arithmetic", "comment": null, "summary": "Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic. This work uses lambda-calculus as a hyper simple functional language to prove the concept, but is impractical for real-world programming due to the well-known inefficiencies of pure lambda$-calculus. It is common in language design to extend basic lambda-calculus with additional primitives to short-cut common tasks such as arithmetic and lists. In this work, we build upon our previous research to examine how such extensions may be applied to CPU-less functional execution in digital logic, with the objective of advancing the approach toward practical implementation. We present a set of structures and algorithms for representing new primitives, describe a systematic process for selecting, implementing, and evaluating them, and demonstrate substantial reductions in execution time and node usage. These improvements are implemented in an open-source system, which is shown to correctly evaluate a range of representative lambda expressions."}
{"id": "2602.19121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19121", "abs": "https://arxiv.org/abs/2602.19121", "authors": ["Matthias Függer", "Thomas Nowak"], "title": "Asymptotic Subspace Consensus in Dynamic Networks", "comment": null, "summary": "We introduce the problem of asymptotic subspace consensus, which requires the outputs of processes to converge onto a common subspace while remaining inside the convex hull of initial vectors.This is a relaxation of asymptotic consensus in which outputs have to converge to a single point, i.e., a zero-dimensional affine subspace.\n  We give a complete characterization of the solvability of asymptotic subspace consensus in oblivious message adversaries. In particular, we show that a large class of algorithms used for asymptotic consensus gracefully degrades to asymptotic subspace consensus in distributed systems with weaker assumptions on the communication network. We also present bounds on the rate by which a lower-than-initial dimension is reached."}
{"id": "2602.19031", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19031", "abs": "https://arxiv.org/abs/2602.19031", "authors": ["Meng Zhang", "Ziang Yin", "Nicholas Gangi", "Alexander Chen", "Brett Bamfo", "Tianle Xu", "Jiaqi Gu", "Zhaoran Rena Huang"], "title": "SKYLIGHT: A Scalable Hundred-Channel 3D Photonic In-Memory Tensor Core Architecture for Real-time AI Inference", "comment": null, "summary": "The growing computational demands of artificial intelligence (AI) are challenging conventional electronics, making photonic computing a promising alternative. However, existing photonic architectures face fundamental scalability and reliability barriers. This paper introduces SKYLIGHT, a scalable 3D photonic in-memory tensor core architecture designed for real-time AI inference. By co-designing its topology, wavelength routing, accumulation, and programming in a 3D stack, SKYLIGHT overcomes key limitations. Its innovations include a low-loss 3D Si/SiN crossbar topology, a thermally robust non-micro-ring resonator (MRR)-based wavelength-division multiplexing (WDM) component, a hierarchical signal accumulation using a multi-port photodetector (PD), and optically programmed non-volatile phase-change material (PCM) weights. Importantly, SKYLIGHT enables in-situ weight updates that support label-free, layer-local learning (e.g., forward-forward local updates) in addition to inference. Using SimPhony for system-level modeling, we show that a single 144 x 256 SKYLIGHT core is feasible within a single reticle and delivers 342.1 TOPS at 23.7 TOPS/W, enabling ResNet-50 inference at 1212 FPS with 27 mJ per image, and achieves 84.17 FPS/W end-to-end (1.61 x higher than an NVIDIA RTX PRO 6000 Blackwell GPU) under the same workload in real-time measurements. System-level evaluations on four representative machine learning tasks, including unsupervised local self-learning, demonstrate SKYLIGHT's robustness to realistic hardware non-idealities (low-bit quantization and signal-proportional analog noise capturing modulation, PCM programming, and readout variations). With noise-aware training, SKYLIGHT maintains high task accuracy, validating its potential as a comprehensive solution for energy-efficient, large-scale photonic AI accelerators."}
{"id": "2602.19231", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19231", "abs": "https://arxiv.org/abs/2602.19231", "authors": ["Georgii Semenov", "Vitaly Aksenov"], "title": "Semantic Conflict Model for Collaborative Data Structures", "comment": "6 pages, 7 figures, submitted to PaPoC 2026", "summary": "Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation."}
{"id": "2602.20083", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20083", "abs": "https://arxiv.org/abs/2602.20083", "authors": ["Xinzhao Li", "Alptekin Vardar", "Franz Müller", "Navya Goli", "Umamaheswara Tida", "Kai Ni", "X. Sharon Hu", "Thomas Kämpfe", "Ruiyang Qin"], "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval", "comment": "Accepted by DAC'26", "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG."}
{"id": "2602.19338", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19338", "abs": "https://arxiv.org/abs/2602.19338", "authors": ["Halit Uyanık", "Tolga Ovatman"], "title": "Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement", "comment": null, "summary": "The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations."}
{"id": "2602.19433", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.19433", "abs": "https://arxiv.org/abs/2602.19433", "authors": ["Paul Borrill"], "title": "Why iCloud Fails: The Category Mistake of Cloud Synchronization", "comment": "18 pages, 3 figures, 37 references", "summary": "iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality."}
{"id": "2602.19683", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19683", "abs": "https://arxiv.org/abs/2602.19683", "authors": ["Henrik Möllmann", "Dirk Pflüger", "Alexander Strack"], "title": "GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX", "comment": "13 pages, 7 figures, Workshop on Asynchronous Many-Task Systems and Applications 2026", "summary": "Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.\n  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent."}
{"id": "2602.19742", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19742", "abs": "https://arxiv.org/abs/2602.19742", "authors": ["Yulun Huang", "Zhiyu Wang", "Rajkumar Buyya"], "title": "A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response", "comment": null, "summary": "Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations."}
{"id": "2602.19802", "categories": ["cs.DC", "cs.NE", "math.CV", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.19802", "abs": "https://arxiv.org/abs/2602.19802", "authors": ["Romain de Coudenhove", "Yannis Bendi-Ouis", "Anthony Strock", "Xavier Hinaut"], "title": "Linear Reservoir: A Diagonalization-Based Optimization", "comment": null, "summary": "We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues."}
{"id": "2602.20097", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20097", "abs": "https://arxiv.org/abs/2602.20097", "authors": ["Pu Jiao", "Sheng Di", "Jiannan Tian", "Mingze Xia", "Xuan Wu", "Yang Zhang", "Xin Liang", "Franck Cappello"], "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation", "comment": null, "summary": "Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput."}
