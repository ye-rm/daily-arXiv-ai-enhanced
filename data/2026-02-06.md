<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization](https://arxiv.org/abs/2602.04900)
*Sai Sindhur Malleni,Raúl Sevilla,Aleksei Vasilevskii,José Castillo Lema,André Bauer*

Main category: cs.ET

TL;DR: Kubernetes-native tools (Kueue, DAS, GAIE) form a cohesive platform for GenAI workloads, improving batch inference efficiency and online inference latency.


<details>
  <summary>Details</summary>
Motivation: As Generative AI inference becomes a dominant workload, Kubernetes needs to natively support its unique demands for scalability and resource efficiency in complex AI workflows.

Method: Implemented multi-stage ASR and summarization use case: 1) Batch inference with Kueue for job management and Dynamic Accelerator Slicer for parallel execution, 2) Online inference with llm-d using Kubernetes Gateway API Inference Extension for optimized routing.

Result: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; GAIE improved Time to First Token by 82%.

Conclusion: Kubernetes-native components form a cohesive, high-performance platform proving Kubernetes' capability as a unified foundation for demanding GenAI workloads.

Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\%.

</details>


### [2] [Task-Adaptive Physical Reservoir Computing via Tunable Molecular Communication Dynamics](https://arxiv.org/abs/2602.05931)
*Saad Yousuf,Kaan Burak Ikiz,Murat Kuscu*

Main category: cs.ET

TL;DR: Molecular Communication channels can be reconfigured as versatile physical reservoir computers by tuning biophysical parameters, with different regimes optimized for memory-intensive vs nonlinear tasks.


<details>
  <summary>Details</summary>
Motivation: Most physical reservoir computing implementations are static and limited to narrow task ranges, creating a need for more versatile, task-adaptive systems that can be optimized for different computational requirements.

Method: Used dual-simulation approach with deterministic mean-field model and stochastic particle-based model (Smoldyn), tuning biophysical parameters like ligand-receptor kinetics and diffusion dynamics, employing Bayesian optimization to navigate parameter space.

Result: Found clear trade-off: parameter sets with rich channel memory excel at chaotic time-series forecasting (Mackey Glass), while regimes promoting strong receptor nonlinearity are superior for nonlinear data transformation. Post-processing methods improved stochastic reservoir performance by mitigating molecular noise.

Conclusion: Molecular Communication channels serve as design blueprints for tunable, bioinspired computing systems, providing an optimization framework for future wetware AI implementations beyond just computational substrates.

Abstract: Physical Reservoir Computing (PRC) offers an efficient paradigm for processing temporal data, yet most physical implementations are static, limiting their performance to a narrow range of tasks. In this work, we demonstrate in silico that a canonical Molecular Communication (MC) channel can function as a highly versatile and task-adaptive PRC whose computational properties are reconfigurable. Using a dual-simulation approach -- a computationally efficient deterministic mean-field model and a high-fidelity particle-based stochastic model (Smoldyn) -- we show that tuning the channel's underlying biophysical parameters, such as ligand-receptor kinetics and diffusion dynamics, allows the reservoir to be optimized for distinct classes of computation. We employ Bayesian optimization to efficiently navigate this high-dimensional parameter space, identifying discrete operational regimes. Our results reveal a clear trade-off: parameter sets rich in channel memory excel at chaotic time-series forecasting tasks (e.g., Mackey Glass), while regimes that promote strong receptor nonlinearity are superior for nonlinear data transformation. We further demonstrate that post-processing methods improve the performance of the stochastic reservoir by mitigating intrinsic molecular noise. These findings establish the MC channel not merely as a computational substrate, but as a design blueprint for tunable, bioinspired computing systems, providing a clear optimization framework for future wetware AI implementations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: First implementation and evaluation of RISC-V CFI extensions (Zicfiss and Zicfilp) for control-flow integrity protection using shadow stack and landing pad mechanisms in CVA6 core.


<details>
  <summary>Details</summary>
Motivation: To protect vulnerable programs from control-flow hijacking attacks by implementing standardized RISC-V extensions for Control-Flow Integrity (CFI) that provide hardware-based security mechanisms.

Method: Designed and integrated two independent configurable hardware units implementing forward-edge (landing pads) and backward-edge (shadow stack) control-flow protection into the open-source CVA6 RISC-V core. The extensions follow the Zicfiss and Zicfilp specifications.

Result: Achieved only 1.0% area overhead when synthesized in 22 nm FDX technology, with performance overhead up to 15.6% based on evaluation with MiBench automotive benchmark subset. The complete implementation is released as open source.

Conclusion: Successfully demonstrated the feasibility and efficiency of implementing standardized RISC-V CFI extensions in hardware, providing practical control-flow integrity protection with minimal area overhead and reasonable performance impact.

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [4] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE is the first carbon modeling framework for HZO-based FeFET non-volatile memories that quantifies both embodied (manufacturing) and operational (use-phase) carbon across the entire lifecycle.


<details>
  <summary>Details</summary>
Motivation: ICT's growing environmental impact requires understanding hardware lifecycle footprints. Emerging non-volatile memories like HZO-FeFETs promise energy efficiency but lack comprehensive carbon footprint analysis across manufacturing and use phases.

Method: Developed COFFEE framework using real semiconductor fab data for embodied carbon estimation and architecture-level design space exploration tools for operational carbon analysis. Applied to HZO-FeFET devices and tested on edge ML accelerator case study.

Result: HZO-FeFETs show 11% higher embodied carbon per unit area than CMOS baseline at 2MB capacity, but 4.3x lower embodied carbon per MB than SRAM. In edge ML accelerator case, replacing SRAM with HZO-FeFETs reduced embodied carbon by 42.3% and operational carbon by up to 70%.

Conclusion: COFFEE enables comprehensive carbon footprint analysis of emerging memory technologies. HZO-FeFET eNVMs offer significant carbon reduction potential, especially for edge computing applications, demonstrating the importance of lifecycle carbon assessment for sustainable computing.

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [5] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: A flexible FP8 digital compute-in-memory accelerator with dynamic precision adaptation for Transformer workloads, achieving 2.8× higher efficiency than previous work while supporting all FP8 formats.


<details>
  <summary>Details</summary>
Motivation: Existing digital compute-in-memory architectures struggle to support variable FP8 aligned-mantissa bitwidths due to unified alignment strategies and fixed-precision MAC units that cannot handle input data with diverse distributions in Transformer inference and training.

Method: Three key innovations: (1) Dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction for adaptive weight (2/4/6/8b) and input (2∼12b) aligned-mantissa precision; (2) FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; (3) Precision-scalable INT MAC array enabling flexible weight precision with minimal overhead.

Result: Implemented in 28nm CMOS with 64×96 CIM array, achieves 20.4 TFLOPS/W for fixed E5M7 format, demonstrating 2.8× higher FP8 efficiency than previous work while supporting all FP8 formats. DSBP achieves higher efficiency than fixed bitwidth mode at same accuracy level on Llama-7b for both BoolQ and Winogrande datasets.

Conclusion: The proposed flexible FP8 DCIM accelerator successfully addresses the challenge of variable FP8 precision support in Transformer workloads through dynamic precision adaptation, achieving significant efficiency improvements while maintaining accuracy through configurable accuracy-efficiency trade-offs.

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [6] [Wasure: A Modular Toolkit for Comprehensive WebAssembly Benchmarking](https://arxiv.org/abs/2602.05488)
*Riccardo Carissimi,Ben L. Titzer*

Main category: cs.PF

TL;DR: Wasure is a modular CLI toolkit for benchmarking WebAssembly performance across multiple dimensions including runtime engines, hardware, applications, and configurations, with analysis showing significant benchmark diversity needs.


<details>
  <summary>Details</summary>
Motivation: WebAssembly benchmarking is complex and multi-dimensional, depending on runtime engines, hardware architectures, application domains, source languages, benchmark suites, and runtime configurations. There's a need for systematic tools to simplify execution and comparison of WebAssembly benchmarks.

Method: Developed Wasure, a modular and extensible command-line toolkit for executing and comparing WebAssembly benchmarks. Conducted dynamic analysis of benchmark suites included with Wasure to examine code coverage, control flow, and execution patterns.

Result: The analysis revealed substantial differences in code coverage, control flow, and execution patterns among benchmark suites, emphasizing the need for benchmark diversity. Wasure provides a tool for systematic, transparent, and insightful evaluation of WebAssembly engines.

Conclusion: Wasure addresses the multi-dimensional challenge of WebAssembly benchmarking by providing a modular toolkit that supports researchers and developers in conducting more comprehensive evaluations, with analysis highlighting the importance of diverse benchmark suites for meaningful performance assessment.

Abstract: WebAssembly (Wasm) has become a key compilation target for portable and efficient execution across diverse platforms. Benchmarking its performance, however, is a multi-dimensional challenge: it depends not only on the choice of runtime engines, but also on hardware architectures, application domains, source languages, benchmark suites, and runtime configurations. This paper introduces Wasure, a modular and extensible command-line toolkit that simplifies the execution and comparison of WebAssembly benchmarks. To complement performance evaluation, we also conducted a dynamic analysis of the benchmark suites included with Wasure. Our analysis reveals substantial differences in code coverage, control flow, and execution patterns, emphasizing the need for benchmark diversity. Wasure aims to support researchers and developers in conducting more systematic, transparent, and insightful evaluations of WebAssembly engines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: A scalable HPC solution using Biological Finite Volume Method (BioFVM) achieves 200x speedup and 36% memory reduction for tumor simulation, enabling digital twin models.


<details>
  <summary>Details</summary>
Motivation: Scaling cellular resolution models to real-scale tumor simulations is critical for digital twin models but computationally challenging due to trillions of operations per time step requiring HPC solutions.

Method: Developed a scalable HPC solution using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks, specifically creating a novel scalable Biological Finite Volume Method (BioFVM) library.

Result: The HPC proposal achieves almost 200x speedup and up to 36% reduction in memory usage compared to current state-of-the-art solutions.

Conclusion: The scalable BioFVM library enables efficient computation of next-generation biological problems, particularly for digital twin models of diseases like tumors.

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [8] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: The Workflows Community Summit 2025 identified key barriers to scientific workflow adoption and proposed action lines across technology, policy, and community dimensions to advance workflow-centric scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Scientific workflows are essential for orchestrating complex computational processes, managing large datasets, and ensuring reproducibility in modern research, but face significant adoption barriers that need to be addressed.

Method: The Workflows Community Summit 2025 convened international experts to examine emerging challenges and opportunities, identify key barriers to workflow adoption, and propose actionable solutions through collaborative discussion and analysis.

Result: Identified key barriers including tensions between system generality vs domain-specific utility, sustainability concerns, insufficient recognition for workflow developers, and gaps in standardization, funding, training, and cross-disciplinary collaboration.

Conclusion: The summit proposed action lines: shift evaluation metrics toward scientific impact, formalize workflow patterns and benchmarks, cultivate an international workflows community, and invest in human capital through dedicated roles, career pathways, and educational integration.

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [9] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL is an LLM-based framework for autoscaling in microservices that uses chain-of-thought reasoning to diagnose performance issues and allocate resources without deployment-specific training.


<details>
  <summary>Details</summary>
Motivation: Current autoscaling approaches for microservices are either opaque learned models requiring extensive per-deployment training or brittle hand-tuned rules that don't generalize well across rapidly evolving deployments.

Method: ORACL transforms runtime telemetry (pods, replicas, CPU/memory usage, latency, SLOs, fault signals) into natural-language state descriptions, then uses LLM chain-of-thought reasoning to identify root causes, prune action space, and make safe allocation decisions under policy constraints.

Result: Experiments on open-source microservice workloads show ORACL improves root-cause identification accuracy by 15%, accelerates training by up to 24x, and improves QoS by 6% in short-term scenarios without deployment-specific retraining.

Conclusion: Large language models can serve as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments, providing interpretable reasoning and effective autoscaling without extensive per-deployment training.

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [10] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus is a distributed consensus protocol that combines CFT and BFT approaches to maintain integrity even when TEEs are compromised, achieving performance comparable to regular TEE-enabled protocols.


<details>
  <summary>Details</summary>
Motivation: Distributed ledgers rely on TEEs for enhanced resiliency, but hardware TEEs can be vulnerable to attacks, undermining the integrity guarantees that distributed ledgers are designed to provide.

Method: Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside a crash-fault-tolerant (CFT) protocol with no additional messages, achieved through careful refactoring to align their structures.

Result: Proteus achieves performance in line with regular TEE-enabled consensus protocols while guaranteeing integrity in the face of TEE platform compromises.

Conclusion: Proteus provides a cautious approach to trusting TEE guarantees by combining CFT and BFT protocols, offering robust integrity protection even when hardware TEEs are compromised.

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [11] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: The paper shows that reaching univalency (decision point) in Byzantine Agreement doesn't require quadratic communication - the Dolev-Reischuk lower bound's quadratic cost is only for disseminating the outcome, not for reaching agreement itself.


<details>
  <summary>Details</summary>
Motivation: To understand what exactly the Dolev-Reischuk lower bound's quadratic communication cost pays for - whether it's about reaching univalency (the point where protocol outcome is determined) or merely about disseminating the outcome to all processors.

Method: Introduces ε-BA, a relaxation allowing ε-fraction of correct processors to output incorrectly, and proves it can be solved deterministically with O(n log n) communication when f < n(1/3 - ε). Shows any ε-BA protocol can serve as first phase of full BA protocol, followed by single all-to-all exchange and majority vote. Also defines Extractable BA for authenticated settings.

Result: Reaching univalency does not require quadratic communication - ε-BA can be solved with O(n log n) communication. The quadratic cost in Dolev-Reischuk bound stems entirely from dissemination, not from reaching univalency. Extractable BA in authenticated settings can be solved with O(f log f) communication.

Conclusion: The Dolev-Reischuk lower bound's quadratic communication cost is about disseminating the outcome to all processors, not about reaching the decision point (univalency). This clarifies the fundamental nature of communication complexity in Byzantine Agreement protocols.

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [12] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze optimizes pipeline parallelism by adaptively freezing parameters to minimize pipeline bubbles while maintaining accuracy, achieving up to 40% throughput improvement on LLaMA-8B.


<details>
  <summary>Details</summary>
Motivation: Pipeline parallelism enables training large models beyond single-device memory, but practical throughput is limited by pipeline bubbles. Existing parameter freezing methods often over-freeze parameters, causing unnecessary accuracy degradation.

Method: Models pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints.

Result: Achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Enables faster large-scale model training without compromising convergence.

Conclusion: TimelyFreeze effectively balances training throughput and accuracy in pipeline-parallel settings, generalizing across diverse configurations for efficient large-scale model training.

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [13] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: Introduces Location-Aware Dispersion, a generalization of the classic Dispersion problem where robots must relocate to distinct nodes of matching colors, with deterministic algorithms, impossibility results, and lower bounds.


<details>
  <summary>Details</summary>
Motivation: Extends the fundamental Dispersion coordination problem by incorporating location awareness through color matching between robots and nodes, creating a more realistic scenario where robots need to settle at specific types of locations rather than just any available node.

Method: Develops several deterministic algorithms for Location-Aware Dispersion with guaranteed bounds on both time and memory requirements, while also establishing impossibility results and lower bounds for any deterministic algorithm solving this problem.

Result: Presents algorithmic feasibility of Location-Aware Dispersion in anonymous networks, with specific time and memory bounds, while highlighting the increased complexity compared to classic Dispersion solutions.

Conclusion: Location-Aware Dispersion is algorithmically feasible but presents greater challenges than classic Dispersion, requiring more sophisticated solutions due to the color matching constraint between robots and nodes.

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>
