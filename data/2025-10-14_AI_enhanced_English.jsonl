{"id": "2510.10484", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10484", "abs": "https://arxiv.org/abs/2510.10484", "authors": ["Buqing Xu", "Jianfeng Zhu", "Yichi Zhang", "Qinyi Cai", "Guanhua Li", "Shaojun Wei", "Leibo Liu"], "title": "CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor", "comment": null, "summary": "CPU simulators are vital for computer architecture research, primarily for\nestimating performance under different programs. This poses challenges for fast\nand accurate simulation of modern CPUs, especially in multi-core systems.\nModern CPU peformance simulators such as GEM5 adopt the cycle-accurate and\nevent-driven approach, which is timeconsuming to simulate the extensive\nmicroarchitectural behavior of a real benchmark running on out-of-order CPUs.\nRecently, machine leaning based approach has been proposed to improve\nsimulation speed, but they are currently limited to estimating the cycles of\nbasic blocks rather than the complete benchmark program. This paper introduces\na novel ML-based CPU simulator named CAPSim, which uses an attention-based\nneural network performance predictor and instruction trace sampling method\nannotated with context. The attention mechanism effectively captures long-range\ninfluence within the instruction trace, emphasizing critical context\ninformation. This allows the model to improve performance prediction accuracy\nby focusing on important code instruction. CAPSim can predict the execution\ntime of unseen benchmarks at a significantly fast speed compared with an\naccurate O3 simulator built with gem5. Our evaluation on a commercial Intel\nXeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to\nusing gem5 built simulator, which is superior to the cutting-edge deep learning\napproach", "AI": {"tldr": "CAPSim is a novel ML-based CPU simulator that uses attention-based neural networks to predict execution time of benchmarks, achieving 2.2-8.3x speedup over gem5 simulators while maintaining accuracy.", "motivation": "Traditional CPU simulators like gem5 are time-consuming for modern multi-core systems, and existing ML approaches only estimate basic block cycles rather than complete program execution time.", "method": "Uses attention-based neural network performance predictor with instruction trace sampling annotated with context, capturing long-range influence within instruction traces and emphasizing critical context information.", "result": "CAPSim achieves 2.2-8.3x speedup compared to gem5 built simulator while maintaining prediction accuracy for unseen benchmarks, superior to state-of-the-art deep learning approaches.", "conclusion": "The attention mechanism effectively improves performance prediction accuracy by focusing on important code instructions, enabling fast and accurate simulation of complete benchmark programs."}}
{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.", "AI": {"tldr": "The paper proposes THEAS algorithm for dynamic resource adaptation in heterogeneous systems to balance performance and power consumption, especially for fluctuating workloads and non-pinning tasks.", "motivation": "To enhance energy efficiency while maintaining computational resources in systems with significantly fluctuating workloads and non-uniform workload distribution in heterogeneous systems.", "method": "Deployed THEAS algorithm that dynamically adapts resource levels and ensures balance between performance and power consumption. Compared with CFS, EAS, HeteroSched, and Utility-Based Scheduling.", "result": "Comparative analysis shows THEAS's effectiveness across adaptability, core selection, performance scaling, cache awareness, overhead, and real-time suitability metrics.", "conclusion": "THEAS algorithm is suitable for wide range of real-time applications by effectively balancing performance and power consumption in dynamic workload scenarios."}}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs).", "AI": {"tldr": "The paper challenges the conventional wisdom of using CPU limits for cloud-native applications, arguing that CPU limits often harm performance and increase costs for latency-sensitive applications, and calls for rethinking autoscaling and billing paradigms.", "motivation": "The motivation stems from practical experiences showing that CPU limits negatively impact application performance and costs, contradicting academic research and industry best practices that assume CPU limits are essential for operational safety.", "method": "The authors provide empirical evidence against the indiscriminate use of CPU limits and analyze scenarios where CPU limits can be beneficial when used judiciously.", "result": "The research demonstrates that CPU limits are detrimental for latency-sensitive applications and should be completely avoided for such workloads, while being selectively useful only for specific cases like background jobs.", "conclusion": "The paper concludes that the widespread adoption of CPU limits is based on erroneous beliefs, calls for fundamental rethinking of autoscaling and billing approaches, and opens new research directions for resource management without CPU limits."}}
{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper.", "AI": {"tldr": "ISAAC is an LLM-aided CPU verification framework that uses multi-agent stimulus generation with micro-architectural awareness and FPGA-parallel simulation to dramatically improve verification efficiency and bug detection.", "motivation": "CPU verification is a major bottleneck in IC development, with current industrial practices suffering from inefficient stimulus generation and slow simulation infrastructure that delays debugging cycles.", "method": "ISAAC employs a multi-agent stimulus engine with micro-architectural knowledge, plus a forward-snapshot mechanism and decoupled co-simulation architecture enabling single ISS to drive multiple DUTs in parallel on FPGAs.", "result": "Achieved up to 17,536x speed-up over software RTL simulation and detected several previously unknown bugs in a mature CPU that had undergone multiple successful tape-outs.", "conclusion": "ISAAC effectively addresses CPU verification bottlenecks through LLM-aided stimulus generation and FPGA parallelism, significantly accelerating verification while improving bug detection capabilities."}}
{"id": "2510.09851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09851", "abs": "https://arxiv.org/abs/2510.09851", "authors": ["Haci Ismail Aslan", "Syed Muhammad Mahmudul Haque", "Joel Witzke", "Odej Kao"], "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters", "comment": "Accepted at the International Conference on Service-Oriented\n  Computing (ICSOC) 2025", "summary": "Modern applications increasingly span across cloud, fog, and edge\nenvironments, demanding orchestration systems that can adapt to diverse\ndeployment contexts while meeting Quality-of-Service (QoS) requirements.\nStandard Kubernetes schedulers do not account for user-defined objectives such\nas energy efficiency, cost optimization, and global performance, often leaving\noperators to make manual, cluster-by-cluster placement decisions. To address\nthis need, we present QONNECT, a vendor-agnostic orchestration framework that\nenables declarative, QoS-driven application deployment across heterogeneous\nKubernetes and K3s clusters. QONNECT introduces a distributed architecture\ncomposed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and\nlightweight Resource Agents in each cluster. Through a minimal YAML-based\ninterface, users specify high-level QoS goals, which the system translates into\nconcrete placement and migration actions. Our implementation is evaluated on a\nfederated testbed of up to nine cloud-fog-edge clusters using the Istio\nBookinfo microservice application. The system demonstrates dynamic,\npolicy-driven microservice placement, automated failover, QoS-compliant\nrescheduling, and leader re-election after node failure, all without manual\nintervention. By bridging the gap between declarative deployment models and\noperational QoS goals, QONNECT transforms the cloud-edge continuum into a\nunified, self-optimizing platform.", "AI": {"tldr": "QONNECT is a vendor-agnostic orchestration framework that enables QoS-driven application deployment across heterogeneous Kubernetes clusters in cloud-fog-edge environments, addressing limitations of standard Kubernetes schedulers.", "motivation": "Standard Kubernetes schedulers lack support for user-defined objectives like energy efficiency, cost optimization, and global performance, forcing operators to make manual placement decisions across multiple clusters.", "method": "QONNECT uses a distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster. Users specify QoS goals via YAML interface, which the system translates into placement and migration actions.", "result": "Evaluation on a federated testbed with up to nine cloud-fog-edge clusters using Istio Bookinfo microservice application demonstrated dynamic policy-driven placement, automated failover, QoS-compliant rescheduling, and leader re-election without manual intervention.", "conclusion": "QONNECT bridges the gap between declarative deployment models and operational QoS goals, transforming the cloud-edge continuum into a unified, self-optimizing platform."}}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "AI": {"tldr": "ADiP is an adaptive-precision systolic array architecture that enhances matrix multiplication efficiency for transformers by supporting multiple precision modes (8x8, 8x4, 8x2) and computation modes, achieving up to 4x higher throughput and significant latency/energy improvements.", "motivation": "Transformers require efficient acceleration due to substantial memory and computational demands. Quantization reduces memory usage, and adaptive precision architectures can further optimize matrix multiplication by dynamically adjusting precision.", "method": "Proposes ADiP architecture with NxN adaptive-precision PEs and shared accumulators, supporting symmetric single-matrix and asymmetric multi-matrix multiplication with shared input matrix. Includes analytical models for latency/throughput and hardware design space exploration using 22nm technology.", "result": "Achieves up to 4x higher computational throughput, 53.6% latency improvement, and 24.4% energy improvement for BitNet-1.58B. Peak throughput of 8.192 TOPS (8x8), 16.384 TOPS (8x4), and 32.768 TOPS (8x2) with 4096 PEs.", "conclusion": "ADiP effectively addresses transformer acceleration needs through adaptive precision and flexible computation modes, demonstrating significant performance and efficiency gains across various transformer workloads."}}
{"id": "2510.10126", "categories": ["cs.DC", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10126", "abs": "https://arxiv.org/abs/2510.10126", "authors": ["Sehar Zehra", "Hassan Jamil Syed", "Ummay Faseeha"], "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments", "comment": "7 pages , 6 figures , 1 table and it is a conference paper", "summary": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving\nanomaly detection. Existing eBPF-based monitors provide low-overhead system and\nnetwork visibility but are limited to single clusters, while centralized\napproaches incur bandwidth, privacy, and heterogeneity challenges. We propose\nFedMon, a federated eBPF framework that unifies kernel-level telemetry with\nfederated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF\nagents capture syscalls and network events, extract local statistical and\nsequence features, and share only model updates with a global server. A hybrid\ndetection engine combining Variational Autoencoders (VAEs) with Isolation\nForests enables both temporal pattern modeling and outlier detection. Deployed\nacross three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,\nand an F1-score of 0.92, while cutting bandwidth usage by 60% relative to\ncentralized baselines. Results demonstrate that FedMon enhances accuracy,\nscalability, and privacy, providing an effective defense for large-scale,\nmulti-tenant cloud-native environments.", "AI": {"tldr": "FedMon is a federated eBPF framework that combines kernel-level telemetry with federated learning for scalable, privacy-preserving anomaly detection across multiple Kubernetes clusters.", "motivation": "Kubernetes multi-cluster deployments need scalable and privacy-preserving anomaly detection, but existing eBPF monitors are limited to single clusters while centralized approaches face bandwidth, privacy, and heterogeneity challenges.", "method": "Uses lightweight eBPF agents to capture syscalls and network events, extracts local statistical and sequence features, and shares only model updates via federated learning. Combines Variational Autoencoders with Isolation Forests for hybrid temporal pattern modeling and outlier detection.", "result": "Achieved 94% precision, 91% recall, and 0.92 F1-score across three Kubernetes clusters, while reducing bandwidth usage by 60% compared to centralized approaches.", "conclusion": "FedMon enhances accuracy, scalability, and privacy, providing effective defense for large-scale, multi-tenant cloud-native environments."}}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "AI": {"tldr": "Bhasha-Rupantarika is a lightweight multilingual translation system using algorithm-hardware codesign for resource-limited settings, achieving 4.1x model size reduction and 4.2x inference speedup through ultra-low precision quantization on FPGA accelerators.", "motivation": "To develop an efficient multilingual translation system suitable for deployment in resource-constrained environments like IoT devices, addressing the need for real-time performance with limited computational resources.", "method": "Algorithm-hardware codesign approach with sub-octet precision quantization (FP8, INT8, INT4, FP4) deployed on FPGA accelerators, focusing on bidirectional translation between Indian and international languages.", "result": "Achieved 4.1x model size reduction (FP4), 4.2x inference speedup, 66 tokens/s throughput (4.8x improvement), 1.96x LUT reduction, 1.65x FF decrease, and 2.2x throughput enhancement compared to OPU.", "conclusion": "The system provides a viable solution for deployable multilingual AI systems through quantization-aware translation and hardware efficiency, with publicly available code and datasets for reproducibility and further development."}}
{"id": "2510.10166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10166", "abs": "https://arxiv.org/abs/2510.10166", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing", "comment": null, "summary": "Edge computing allows for the decentralization of computing resources. This\ndecentralization is achieved through implementing microservice architectures,\nwhich require low latencies to meet stringent service level agreements (SLA)\nsuch as performance, reliability, and availability metrics. While cloud\ncomputing offers the large data storage and computation resources necessary to\nhandle peak demands, a hybrid cloud and edge environment is required to ensure\nSLA compliance. Several auto-scaling algorithms have been proposed to try to\nachieve these compliance challenges, but they suffer from performance issues\nand configuration complexity. This chapter provides a brief overview of edge\ncomputing architecture, its uses, benefits, and challenges for resource\nscaling. We then introduce Service Level Agreements, and existing research on\ndevising algorithms used in edge computing environments to meet these\nagreements, along with their benefits and drawbacks.", "AI": {"tldr": "This chapter overviews edge computing architecture, its uses, benefits, and challenges for resource scaling, focusing on meeting Service Level Agreements (SLAs) through auto-scaling algorithms in hybrid cloud-edge environments.", "motivation": "Edge computing enables decentralization via microservices requiring low latencies to meet SLAs for performance, reliability, and availability. Hybrid cloud-edge environments are needed to handle peak demands while ensuring SLA compliance, but existing auto-scaling algorithms face performance issues and configuration complexity.", "method": "The chapter provides a brief overview of edge computing architecture, its uses, benefits, and scaling challenges. It introduces Service Level Agreements and reviews existing research on algorithms used in edge computing to meet these agreements.", "result": "The analysis covers the benefits and drawbacks of various auto-scaling algorithms proposed for edge computing environments to achieve SLA compliance.", "conclusion": "The chapter synthesizes current research on edge computing resource scaling and SLA compliance algorithms, highlighting both their advantages and limitations in hybrid cloud-edge environments."}}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy.", "AI": {"tldr": "This paper presents an in-storage computing architecture using 3D Ferroelectric NAND (FeNAND) with hyperdimensional computing to accelerate mass spectrometry library searches, achieving 43x speedup and 21x energy efficiency improvement.", "motivation": "The massive growth of mass spectrometry data (hundreds of terabytes) creates computational bottlenecks for drug discovery applications, and traditional processors struggle with this data volume efficiently.", "method": "Combines 3D FeNAND structure with hyperdimensional computing (HDC) and a dual-bound approximate matching (D-BAM) distance metric to parallelize vector computations for spectral library search.", "result": "Achieved 43x speedup and 21x higher energy efficiency compared to state-of-the-art 3D NAND methods while maintaining comparable accuracy.", "conclusion": "The proposed ISP architecture with FeNAND and HDC successfully addresses throughput limitations of traditional NAND structures and enables efficient large-scale mass spectrometry data processing."}}
{"id": "2510.10302", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10302", "abs": "https://arxiv.org/abs/2510.10302", "authors": ["Liangkun Chen", "Zijian Wen", "Tian Wu", "Xiaoxi Zhang", "Chuan Wu"], "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large\nlanguage models (LLMs) to reduce computation cost through model sparsity.\nEmploying speculative decoding (SD) can further accelerate MoE inference by\ndrafting multiple tokens per step and verifying them in parallel. However,\ncombining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth\ncontention during multi-token verification. Existing MoE offloading systems are\nSD-agnostic and do not address this bottleneck. We present SP-MoE, the first\nSD-aware expert-offloading and compute-communication pipelining framework.\nSP-MoE introduces: (1) speculative expert prefetching that exploits structural\ncorrespondence between the draft and target models to prefetch likely experts\nahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch\ndepth based on empirical profiles and an analytical latency model, guaranteeing\njust-in-time availability without overfetch; and (3) a pipelined runtime with\nasynchronous prefetch threads and batched I/O to hide loading latency.\nExtensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT\nspeedup over state-of-the-art methods across diverse datasets, environments,\nand MoE-based models.", "AI": {"tldr": "SP-MoE is a speculative decoding-aware expert-offloading framework that accelerates Mixture-of-Experts (MoE) inference through speculative expert prefetching, cutoff-layer policy, and pipelined runtime.", "motivation": "Combining MoE with speculative decoding (SD) inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification, which existing MoE offloading systems don't address.", "method": "SP-MoE introduces speculative expert prefetching using structural correspondence between draft and target models, a cutoff-layer policy to bound prefetch depth, and a pipelined runtime with asynchronous prefetch threads and batched I/O.", "result": "SP-MoE achieves 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.", "conclusion": "SP-MoE is the first SD-aware expert-offloading framework that effectively addresses the memory and bandwidth bottlenecks in MoE inference with speculative decoding."}}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["Jo\u00e3o Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.", "AI": {"tldr": "An automated framework that improves CIM accelerator efficiency for sparse LLM inference through novel mapping and scheduling strategies, achieving over 50% better array utilization and 4x reduction in memory footprint and FLOPs.", "motivation": "Structured sparsity reduces LLM size but conventional architectures struggle with memory-bound decode stages. CIM architectures help but naive sparse matrix mapping leads to poor array utilization and computational inefficiency.", "method": "Developed an automated framework with novel mapping and scheduling strategies that exploit block-diagonal sparsity patterns to optimize CIM array usage.", "result": "Achieved over 50% improvement in CIM array utilization, more than 4x reduction in memory footprint, and more than 4x reduction in floating-point operations required.", "conclusion": "The proposed framework effectively accelerates sparse LLM inference on CIM accelerators by optimizing sparse matrix mapping, significantly improving computational efficiency and reducing resource requirements."}}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines.", "AI": {"tldr": "FLAMMABLE is a comprehensive Multi-Model Federated Learning framework that optimizes training by intelligently adapting client batch sizes and assigning multiple models based on client capabilities, achieving significant performance improvements over existing baselines.", "motivation": "Existing FL solutions and naive extensions don't efficiently address the amplified challenges in MMFL settings, which include data heterogeneity, system heterogeneity, and additional heterogeneity across multiple models being trained in parallel.", "method": "FLAMMABLE optimizes model training by intelligently adapting client batch sizes while engaging clients to train multiple carefully chosen models depending on their system capabilities in each training round.", "result": "Extensive evaluations show FLAMMABLE boosts MMFL time-to-accuracy performance by 1.1-10.0\u00d7 while improving final model accuracy by 1.3-5.4% compared to several known baselines.", "conclusion": "FLAMMABLE effectively addresses the challenges of Multi-Model Federated Learning and the authors also developed the first benchmark platform for MMFL to enable future reproducible research in this area."}}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks.", "AI": {"tldr": "DCP is a dynamic context parallel training framework that uses fine-grained blockwise partitioning to adapt to varying sequence lengths and attention patterns, reducing communication overhead and improving computation balance compared to static methods.", "motivation": "Existing context parallel methods use static configurations that don't account for the dynamic nature of training data, leading to unnecessary communication overhead and imbalanced computation due to variability in sequence lengths and token relationships.", "method": "Introduces fine-grained blockwise partitioning of both data and computation, enabling flexible mapping of data and computation blocks to devices to adapt to varying sequence characteristics.", "result": "DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns, with end-to-end training speed-up of 0.94x~1.16x for causal masks and 1.00x~1.46x for sparse masks.", "conclusion": "DCP effectively addresses the limitations of static context parallel methods by dynamically adapting to data characteristics, significantly reducing communication overhead and improving training efficiency."}}
{"id": "2510.10818", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10818", "abs": "https://arxiv.org/abs/2510.10818", "authors": ["Kevin Chalmers", "Jan B\u00e6kgaard Pedersen"], "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes", "comment": null, "summary": "We present the first spin-free, kernel-lock-free mutex that cooperates with\nuser-mode schedulers and is formally proven FIFO-fair and linearizable using\nCSP/FDR. Our fairness oracle and stability-based proof method are reusable\nacross coroutine runtime designs. We designed the claim/release protocol for a\nprocess-oriented language -- ProcessJ -- to manage the race for claiming shared\ninter-process communication channels. Internally, we use a lock-free queue to\npark waiting processes for gaining access to a shared object, such as exclusive\naccess to a shared channel to read from or write to. The queue ensures control\nand fairness for processes wishing to access a shared resource, as the protocol\nhandles claim requests in the order they are inserted into the queue. We\nproduce CSP models of our protocol and a mutex specification, demonstrating\nwith FDR that our protocol behaves as a locking mutex.", "AI": {"tldr": "A spin-free, kernel-lock-free mutex that cooperates with user-mode schedulers, formally proven FIFO-fair and linearizable using CSP/FDR, designed for ProcessJ language to manage shared channel access.", "motivation": "To create a fair mutex that works with user-mode schedulers without kernel involvement, providing formal guarantees for process-oriented languages managing shared communication channels.", "method": "Uses a lock-free queue to park waiting processes, ensuring FIFO ordering for access to shared resources. Protocol designed for ProcessJ language with formal CSP modeling and FDR verification.", "result": "Successfully demonstrated that the protocol behaves as a locking mutex with FIFO fairness and linearizability, with reusable proof methods for coroutine runtime designs.", "conclusion": "The presented mutex provides formally verified fairness and correctness for shared resource access in process-oriented programming, with reusable verification techniques for similar systems."}}
{"id": "2510.10833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10833", "abs": "https://arxiv.org/abs/2510.10833", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed and\nreliable systems. In the proposed framework we have used three parts to\nincrease Satisfaction and Performance of this framework. At first we analyze\nprevious frameworks related to integrated systems, then represent new proposed\nframework in order to improving previous framework, and we discuss its\ndifferent phases. Finally we compare the results of simulation of the new\nframework with previous ones. In FIDRS framework, the technique of\nheterogeneous distributed data base is used to improve Performance and speed in\nresponding to users and in this way we can improve dependability and\nreliability of framework simultaneously. In extraction phase of the new\nframework we have used RMSD algorithm that decreases responding time in big\ndatabase. Finally by using FDIRS framework we succeeded to increase Efficiency,\nPerformance and reliability of integrated systems and remove some of previous\nframeworks problems.", "AI": {"tldr": "A new framework called FIDRS for integrated distributed and reliable systems that improves performance, speed, and reliability using heterogeneous distributed databases and RMSD algorithm.", "motivation": "To address limitations in previous integrated systems frameworks by improving satisfaction, performance, and reliability while removing existing problems.", "method": "Uses three parts: analysis of previous frameworks, new framework design with heterogeneous distributed database technique, and RMSD algorithm for faster response times in big databases.", "result": "Simulation results show improved efficiency, performance, and reliability compared to previous frameworks, with reduced response times.", "conclusion": "The FIDRS framework successfully increases efficiency, performance, and reliability of integrated systems while solving problems from previous frameworks."}}
{"id": "2510.11189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11189", "abs": "https://arxiv.org/abs/2510.11189", "authors": ["Yangyang Wen", "Paul Townend", "Per-Olov \u00d6stberg", "Abel Souza", "Cl\u00e9ment Courageux-Sudan"], "title": "A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems", "comment": "9 pages, 4 figures. Accepted at the 2025 IEEE Joint Cloud Computing\n  (JCC) track of IEEE CISOSE 2025. Conference: IEEE JCC 2025, 16th IEEE\n  International Conference on Joint Cloud Computing, Tucson, Arizona, USA, from\n  July 21 to 24, 2025", "summary": "As microservice-based systems scale across the cloud-edge continuum,\ntraditional centralized scheduling mechanisms increasingly struggle with\nlatency, coordination overhead, and fault tolerance. This paper presents a new\narchitectural direction: leveraging service mesh sidecar proxies as\ndecentralized, in-situ schedulers to enable scalable, low-latency coordination\nin large-scale, cloud-native environments. We propose embedding lightweight,\nautonomous scheduling logic into each sidecar, allowing scheduling decisions to\nbe made locally without centralized control. This approach leverages the\ngrowing maturity of service mesh infrastructures, which support programmable\ndistributed traffic management. We describe the design of such an architecture\nand present initial results demonstrating its scalability potential in terms of\nresponse time and latency under varying request rates. Rather than delivering a\nfinalized scheduling algorithm, this paper presents a system-level\narchitectural direction and preliminary evidence to support its scalability\npotential.", "AI": {"tldr": "A decentralized scheduling approach using service mesh sidecar proxies as in-situ schedulers for scalable cloud-edge microservices.", "motivation": "Traditional centralized scheduling struggles with latency, coordination overhead, and fault tolerance as microservices scale across cloud-edge continuum.", "method": "Embed lightweight autonomous scheduling logic into service mesh sidecar proxies, enabling local scheduling decisions without centralized control.", "result": "Initial results demonstrate scalability potential with improved response time and latency under varying request rates.", "conclusion": "This paper presents a system-level architectural direction using service mesh infrastructure for decentralized scheduling rather than a finalized algorithm."}}
{"id": "2510.11211", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11211", "abs": "https://arxiv.org/abs/2510.11211", "authors": ["Sheikh Azizul Hakim", "Saem Hasan"], "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models", "comment": null, "summary": "Large language models (LLM) are advanced AI systems trained on extensive\ntextual data, leveraging deep learning techniques to understand and generate\nhuman-like language. Today's LLMs with billions of parameters are so huge that\nhardly any single computing node can train, fine-tune, or infer from them.\nTherefore, several distributed computing techniques are being introduced in the\nliterature to properly utilize LLMs. We have explored the application of\ndistributed computing techniques in LLMs from two angles.\n  \\begin{itemize}\n  \\item We study the techniques that democratize the LLM, that is, how large\nmodels can be run on consumer-grade computers. Here, we also implement a novel\nmetaheuristics-based modification to an existing system.\n  \\item We perform a comparative study on three state-of-the-art LLM serving\ntechniques. \\end{itemize}", "AI": {"tldr": "This paper explores distributed computing techniques for large language models (LLMs) from two perspectives: democratizing LLMs to run on consumer-grade computers with metaheuristics-based modifications, and comparing three state-of-the-art LLM serving techniques.", "motivation": "Current LLMs with billions of parameters are too large for single computing nodes to handle training, fine-tuning, or inference, necessitating distributed computing approaches to properly utilize these models.", "method": "The research examines distributed computing techniques from two angles: 1) Techniques to run large models on consumer-grade computers with novel metaheuristics-based modifications to existing systems, and 2) A comparative study of three state-of-the-art LLM serving techniques.", "result": "The paper presents both a novel metaheuristics-based modification for running LLMs on consumer hardware and a comparative analysis of three advanced LLM serving techniques, though specific performance metrics are not detailed in the abstract.", "conclusion": "Distributed computing techniques are essential for effectively utilizing large language models, and the research demonstrates approaches for both democratizing access to LLMs on consumer hardware and evaluating modern serving systems."}}
{"id": "2510.11513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11513", "abs": "https://arxiv.org/abs/2510.11513", "authors": ["Alex Elwood", "Tom Deakin", "Justin Lovegrove", "Chris Nelson"], "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems", "comment": null, "summary": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a\nchallenge to scale due to complex data dependencies, memory access patterns and\na high-dimensional domain. In this paper, we review the performance bottlenecks\nwithin the shared memory parallelization scheme of an existing transport solver\non modern many-core architectures with high core counts. With this analysis, we\nthen survey the performance of this solver across a variety of compute\nhardware. We then present a new Asynchronous Many-Task (AMT) algorithm for\nshared memory parallelism, present results showing an increase in computational\nperformance over the existing method, and evaluate why performance is improved.", "AI": {"tldr": "The paper analyzes performance bottlenecks in S_N transport solvers on modern many-core architectures and proposes an Asynchronous Many-Task (AMT) algorithm that improves computational performance over existing methods.", "motivation": "Discrete ordinates S_N transport solvers on unstructured meshes face scaling challenges due to complex data dependencies, memory access patterns, and high-dimensional domains, particularly on modern many-core architectures with high core counts.", "method": "The authors review performance bottlenecks in existing transport solvers, survey performance across various compute hardware, and develop a new Asynchronous Many-Task (AMT) algorithm for shared memory parallelism.", "result": "The proposed AMT algorithm demonstrates increased computational performance compared to the existing method, with analysis explaining why performance improvements are achieved.", "conclusion": "The new Asynchronous Many-Task approach effectively addresses scaling limitations in S_N transport solvers and provides better performance on modern many-core architectures."}}
{"id": "2510.11697", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11697", "abs": "https://arxiv.org/abs/2510.11697", "authors": ["Matteo Mordacchini", "Emanuele Carlini", "Patrizio Dazzi"], "title": "A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem", "comment": null, "summary": "We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in\na decentralized network. MWVC, a classical NP-hard problem, is foundational in\napplications such as network monitoring and resource placement. We propose a\nfully decentralized protocol where each node makes decisions using only local\nknowledge and communicates with its neighbors. The method is adaptive,\ncommunication-efficient, and avoids centralized coordination. We evaluate the\nprotocol on real-world and synthetic graphs, comparing it to both centralized\nand decentralized baselines. Our results demonstrate competitive solution\nquality with reduced communication overhead, highlighting the feasibility of\nMWVC computation in decentralized environments.", "AI": {"tldr": "A decentralized protocol for computing Minimum Weighted Vertex Cover (MWVC) using only local knowledge and neighbor communication, achieving competitive solution quality with low communication overhead.", "motivation": "MWVC is a fundamental NP-hard problem with applications in network monitoring and resource placement, but existing approaches often require centralized coordination which is impractical in decentralized networks.", "method": "Proposed a fully decentralized protocol where each node makes decisions based only on local information and communicates with immediate neighbors, avoiding centralized coordination while being adaptive and communication-efficient.", "result": "Evaluation on real-world and synthetic graphs shows competitive solution quality compared to both centralized and decentralized baselines, with significantly reduced communication overhead.", "conclusion": "The study demonstrates the feasibility of computing MWVC in decentralized environments through local decision-making and neighbor communication, offering practical solutions for distributed network applications."}}
