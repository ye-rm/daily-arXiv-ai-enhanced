{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.", "AI": {"tldr": "The paper proposes THEAS algorithm for dynamic resource adaptation in heterogeneous systems to improve energy efficiency while maintaining computational performance, especially for fluctuating workloads and non-pinning tasks.", "motivation": "To enhance energy efficiency in heterogeneous systems where workloads fluctuate significantly and are not uniformly distributed, requiring dynamic adaptation of resource levels.", "method": "Deployed THEAS algorithm that balances performance and power consumption through dynamic resource adaptation, with comparative analysis against CFS, EAS, HeteroSched, and Utility-Based Scheduling.", "result": "THEAS algorithm ensures balance between performance and power consumption, making it suitable for real-time applications. Comparative analysis shows its advantages in adaptability, core selection, performance scaling, cache awareness, overhead, and real-time suitability.", "conclusion": "THEAS algorithm provides an effective solution for dynamic resource adaptation in heterogeneous systems, achieving energy efficiency while maintaining necessary computational resources for fluctuating workloads."}}
{"id": "2510.09851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09851", "abs": "https://arxiv.org/abs/2510.09851", "authors": ["Haci Ismail Aslan", "Syed Muhammad Mahmudul Haque", "Joel Witzke", "Odej Kao"], "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters", "comment": "Accepted at the International Conference on Service-Oriented\n  Computing (ICSOC) 2025", "summary": "Modern applications increasingly span across cloud, fog, and edge\nenvironments, demanding orchestration systems that can adapt to diverse\ndeployment contexts while meeting Quality-of-Service (QoS) requirements.\nStandard Kubernetes schedulers do not account for user-defined objectives such\nas energy efficiency, cost optimization, and global performance, often leaving\noperators to make manual, cluster-by-cluster placement decisions. To address\nthis need, we present QONNECT, a vendor-agnostic orchestration framework that\nenables declarative, QoS-driven application deployment across heterogeneous\nKubernetes and K3s clusters. QONNECT introduces a distributed architecture\ncomposed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and\nlightweight Resource Agents in each cluster. Through a minimal YAML-based\ninterface, users specify high-level QoS goals, which the system translates into\nconcrete placement and migration actions. Our implementation is evaluated on a\nfederated testbed of up to nine cloud-fog-edge clusters using the Istio\nBookinfo microservice application. The system demonstrates dynamic,\npolicy-driven microservice placement, automated failover, QoS-compliant\nrescheduling, and leader re-election after node failure, all without manual\nintervention. By bridging the gap between declarative deployment models and\noperational QoS goals, QONNECT transforms the cloud-edge continuum into a\nunified, self-optimizing platform.", "AI": {"tldr": "QONNECT is a vendor-agnostic orchestration framework that enables QoS-driven application deployment across heterogeneous Kubernetes clusters in cloud-fog-edge environments.", "motivation": "Standard Kubernetes schedulers lack support for user-defined objectives like energy efficiency, cost optimization, and global performance, forcing manual cluster-by-cluster placement decisions.", "method": "Distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster, using YAML-based interface for QoS specification.", "result": "Demonstrated dynamic policy-driven microservice placement, automated failover, QoS-compliant rescheduling, and leader re-election on federated testbed with up to nine clusters using Istio Bookinfo application.", "conclusion": "QONNECT bridges declarative deployment models with operational QoS goals, transforming the cloud-edge continuum into a unified, self-optimizing platform."}}
{"id": "2510.10126", "categories": ["cs.DC", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10126", "abs": "https://arxiv.org/abs/2510.10126", "authors": ["Sehar Zehra", "Hassan Jamil Syed", "Ummay Faseeha"], "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments", "comment": "7 pages , 6 figures , 1 table and it is a conference paper", "summary": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving\nanomaly detection. Existing eBPF-based monitors provide low-overhead system and\nnetwork visibility but are limited to single clusters, while centralized\napproaches incur bandwidth, privacy, and heterogeneity challenges. We propose\nFedMon, a federated eBPF framework that unifies kernel-level telemetry with\nfederated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF\nagents capture syscalls and network events, extract local statistical and\nsequence features, and share only model updates with a global server. A hybrid\ndetection engine combining Variational Autoencoders (VAEs) with Isolation\nForests enables both temporal pattern modeling and outlier detection. Deployed\nacross three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,\nand an F1-score of 0.92, while cutting bandwidth usage by 60% relative to\ncentralized baselines. Results demonstrate that FedMon enhances accuracy,\nscalability, and privacy, providing an effective defense for large-scale,\nmulti-tenant cloud-native environments.", "AI": {"tldr": "FedMon is a federated eBPF framework that combines kernel-level telemetry with federated learning for scalable, privacy-preserving anomaly detection across multiple Kubernetes clusters, achieving 94% precision with 60% bandwidth reduction.", "motivation": "Kubernetes multi-cluster deployments need scalable and privacy-preserving anomaly detection. Existing eBPF monitors are limited to single clusters, while centralized approaches face bandwidth, privacy, and heterogeneity challenges.", "method": "Lightweight eBPF agents capture syscalls and network events, extract local features, and share only model updates with a global server. A hybrid detection engine combines Variational Autoencoders with Isolation Forests for temporal pattern modeling and outlier detection.", "result": "Deployed across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall, and F1-score of 0.92, while reducing bandwidth usage by 60% compared to centralized baselines.", "conclusion": "FedMon enhances accuracy, scalability, and privacy, providing an effective defense for large-scale, multi-tenant cloud-native environments."}}
{"id": "2510.10166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10166", "abs": "https://arxiv.org/abs/2510.10166", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing", "comment": null, "summary": "Edge computing allows for the decentralization of computing resources. This\ndecentralization is achieved through implementing microservice architectures,\nwhich require low latencies to meet stringent service level agreements (SLA)\nsuch as performance, reliability, and availability metrics. While cloud\ncomputing offers the large data storage and computation resources necessary to\nhandle peak demands, a hybrid cloud and edge environment is required to ensure\nSLA compliance. Several auto-scaling algorithms have been proposed to try to\nachieve these compliance challenges, but they suffer from performance issues\nand configuration complexity. This chapter provides a brief overview of edge\ncomputing architecture, its uses, benefits, and challenges for resource\nscaling. We then introduce Service Level Agreements, and existing research on\ndevising algorithms used in edge computing environments to meet these\nagreements, along with their benefits and drawbacks.", "AI": {"tldr": "This chapter overviews edge computing architecture, SLAs, and auto-scaling algorithms for hybrid cloud-edge environments, highlighting performance and configuration challenges in meeting service level agreements.", "motivation": "Edge computing's decentralization through microservices requires low latencies to meet strict SLAs, but existing auto-scaling algorithms face performance issues and complexity in hybrid cloud-edge setups.", "method": "Provides an overview of edge computing architecture, SLAs, and analyzes existing research on auto-scaling algorithms for edge environments, examining their benefits and drawbacks.", "result": "Identifies that while cloud computing handles peak demands, hybrid cloud-edge environments are necessary for SLA compliance, but current scaling solutions have limitations.", "conclusion": "The chapter synthesizes edge computing challenges and existing algorithmic approaches for SLA compliance, highlighting the need for improved scaling solutions in hybrid environments."}}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs).", "AI": {"tldr": "The paper challenges the conventional wisdom of using CPU limits for cloud-native applications, arguing that CPU limits harm performance and increase costs for latency-sensitive applications, and calls for rethinking autoscaling and billing paradigms.", "motivation": "The motivation stems from practical experiences showing CPU limits negatively impact application performance and costs, contradicting academic research and industry best practices that assume CPU limits are essential for operational safety.", "method": "The authors provide empirical evidence against CPU limits and analyze the erroneous beliefs driving their indiscriminate adoption, while also identifying specific scenarios where CPU limits can be beneficial.", "result": "The research demonstrates that CPU limits are detrimental for latency-sensitive applications and should be completely avoided in such cases, prompting a need to reconsider fundamental cloud resource management approaches.", "conclusion": "CPU limits should be eschewed for latency-sensitive applications, requiring a fundamental rethinking of auto-scaling and billing paradigms, though they can still be beneficial in specific scenarios like background jobs when used judiciously."}}
{"id": "2510.10484", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10484", "abs": "https://arxiv.org/abs/2510.10484", "authors": ["Buqing Xu", "Jianfeng Zhu", "Yichi Zhang", "Qinyi Cai", "Guanhua Li", "Shaojun Wei", "Leibo Liu"], "title": "CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor", "comment": null, "summary": "CPU simulators are vital for computer architecture research, primarily for\nestimating performance under different programs. This poses challenges for fast\nand accurate simulation of modern CPUs, especially in multi-core systems.\nModern CPU peformance simulators such as GEM5 adopt the cycle-accurate and\nevent-driven approach, which is timeconsuming to simulate the extensive\nmicroarchitectural behavior of a real benchmark running on out-of-order CPUs.\nRecently, machine leaning based approach has been proposed to improve\nsimulation speed, but they are currently limited to estimating the cycles of\nbasic blocks rather than the complete benchmark program. This paper introduces\na novel ML-based CPU simulator named CAPSim, which uses an attention-based\nneural network performance predictor and instruction trace sampling method\nannotated with context. The attention mechanism effectively captures long-range\ninfluence within the instruction trace, emphasizing critical context\ninformation. This allows the model to improve performance prediction accuracy\nby focusing on important code instruction. CAPSim can predict the execution\ntime of unseen benchmarks at a significantly fast speed compared with an\naccurate O3 simulator built with gem5. Our evaluation on a commercial Intel\nXeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to\nusing gem5 built simulator, which is superior to the cutting-edge deep learning\napproach", "AI": {"tldr": "CAPSim is a novel ML-based CPU simulator that uses attention-based neural networks and instruction trace sampling to predict execution time of benchmarks much faster than traditional cycle-accurate simulators like GEM5.", "motivation": "Traditional CPU simulators like GEM5 are slow for simulating modern multi-core CPUs, and existing ML approaches only estimate cycles for basic blocks rather than complete programs.", "method": "Uses attention-based neural network performance predictor and instruction trace sampling with context annotation. The attention mechanism captures long-range influences in instruction traces and focuses on critical context information.", "result": "CAPSim achieves 2.2-8.3x speedup compared to GEM5's O3 simulator while maintaining accuracy, and outperforms state-of-the-art deep learning approaches.", "conclusion": "CAPSim demonstrates that ML-based approaches can effectively accelerate CPU simulation while maintaining prediction accuracy for complete benchmark programs."}}
{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper.", "AI": {"tldr": "ISAAC is an LLM-aided CPU verification framework that uses multi-agent stimulus generation with micro-architectural knowledge and FPGA-parallel simulation to achieve massive speed-ups and detect previously unknown bugs.", "motivation": "CPU verification is a critical bottleneck in IC development, with current differential testing methods facing issues in stimulus generation (low-quality, redundant tests) and simulation infrastructure (slow feedback, limited visibility).", "method": "ISAAC uses a multi-agent stimulus engine infused with micro-architectural knowledge and historical bug patterns for targeted test generation, plus a lightweight forward-snapshot mechanism and decoupled co-simulation architecture enabling single ISS to drive multiple DUTs in parallel on FPGAs.", "result": "Achieved up to 17,536x speed-up over software RTL simulation and detected several previously unknown bugs, with two specific bugs reported in the paper.", "conclusion": "ISAAC successfully addresses CPU verification bottlenecks through LLM-aided stimulus generation and FPGA-parallel simulation, demonstrating significant performance improvements and bug detection capabilities on mature CPU designs."}}
{"id": "2510.10302", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10302", "abs": "https://arxiv.org/abs/2510.10302", "authors": ["Liangkun Chen", "Zijian Wen", "Tian Wu", "Xiaoxi Zhang", "Chuan Wu"], "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large\nlanguage models (LLMs) to reduce computation cost through model sparsity.\nEmploying speculative decoding (SD) can further accelerate MoE inference by\ndrafting multiple tokens per step and verifying them in parallel. However,\ncombining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth\ncontention during multi-token verification. Existing MoE offloading systems are\nSD-agnostic and do not address this bottleneck. We present SP-MoE, the first\nSD-aware expert-offloading and compute-communication pipelining framework.\nSP-MoE introduces: (1) speculative expert prefetching that exploits structural\ncorrespondence between the draft and target models to prefetch likely experts\nahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch\ndepth based on empirical profiles and an analytical latency model, guaranteeing\njust-in-time availability without overfetch; and (3) a pipelined runtime with\nasynchronous prefetch threads and batched I/O to hide loading latency.\nExtensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT\nspeedup over state-of-the-art methods across diverse datasets, environments,\nand MoE-based models.", "AI": {"tldr": "SP-MoE is a speculative decoding-aware expert-offloading framework that accelerates Mixture-of-Experts (MoE) inference through speculative expert prefetching, cutoff-layer policies, and pipelined runtime, achieving 1.07-3.5\u00d7 speedup over state-of-the-art methods.", "motivation": "Combining MoE with speculative decoding (SD) inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification, but existing MoE offloading systems are SD-agnostic and don't address this bottleneck.", "method": "SP-MoE introduces: (1) speculative expert prefetching using structural correspondence between draft and target models, (2) cutoff-layer policy based on empirical profiles and analytical latency model, and (3) pipelined runtime with asynchronous prefetch threads and batched I/O.", "result": "Extensive experiments show SP-MoE achieves 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.", "conclusion": "SP-MoE successfully addresses the memory and bandwidth bottlenecks in SD-MoE inference through SD-aware expert-offloading and compute-communication pipelining."}}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "AI": {"tldr": "ADiP is a novel adaptive-precision systolic array architecture for efficient matrix multiplication acceleration in transformers, supporting multiple precision modes and achieving significant performance improvements.", "motivation": "Transformers require efficient acceleration due to substantial memory and computational requirements, and quantization plays a vital role in reducing memory usage while enabling dynamic precision adjustment for enhanced matrix multiplication.", "method": "The architecture consists of NxN adaptive-precision processing elements and shared accumulators, supporting multiple computation modes including symmetric single-matrix multiplication and asymmetric multi-matrix multiplication with shared input matrix. It adapts to different precisions (8bitx8bit, 8bitx4bit, 8bitx2bit) and uses analytical models for latency and throughput analysis.", "result": "ADiP achieves up to 4x higher computational throughput, latency improvement up to 53.6%, and energy improvement up to 24.4% for BitNet-1.58B MHA workloads. At 64x64 size with 4096 PEs, it achieves peak throughput of 8.192 TOPS (8bitx8bit), 16.384 TOPS (8bitx4bit), and 32.768 TOPS (8bitx2bit).", "conclusion": "ADiP provides an efficient adaptive-precision systolic array architecture that significantly enhances matrix multiplication performance for transformer workloads through dynamic precision adjustment and improved data reuse."}}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines.", "AI": {"tldr": "FLAMMABLE is a multi-model federated learning framework that optimizes training by adapting client batch sizes and assigning multiple models based on system capabilities, achieving 1.1-10\u00d7 faster time-to-accuracy and 1.3-5.4% higher accuracy than baselines.", "motivation": "Multi-model FL faces amplified challenges from data, system, and model heterogeneity that existing single-model FL solutions cannot efficiently address, requiring a specialized framework.", "method": "FLAMMABLE intelligently adapts client batch sizes and engages clients to train multiple carefully chosen models per round based on their system capabilities.", "result": "Extensive evaluations show FLAMMABLE boosts time-to-accuracy performance by 1.1-10\u00d7 and improves final model accuracy by 1.3-5.4% compared to baselines.", "conclusion": "FLAMMABLE effectively addresses multi-model FL challenges and the authors developed the first benchmark platform for reproducible MMFL research."}}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "AI": {"tldr": "Bhasha-Rupantarika is a lightweight multilingual translation system using algorithm-hardware codesign for resource-limited settings, achieving 4.1x model size reduction and 4.2x inference speedup through ultra-low precision quantization on FPGA accelerators.", "motivation": "To develop an efficient multilingual translation system suitable for deployment in resource-limited IoT devices, addressing the need for real-time translation in low-resource linguistic contexts.", "method": "Uses algorithm-hardware codesign with sub-octet precision quantization (FP8, INT8, INT4, FP4) and FPGA accelerators for deployment, focusing on bidirectional translation between Indian and international languages.", "result": "Achieved 4.1x model size reduction (FP4), 4.2x inference speedup, 66 tokens/s throughput (4.8x improvement), 1.96x reduction in LUTs, 1.65x decrease in FFs, and 2.2x throughput enhancement compared to OPU.", "conclusion": "The system provides a viable solution for deployable multilingual AI systems through quantization-aware translation and hardware efficiency, with publicly available code and datasets for reproducibility."}}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks.", "AI": {"tldr": "DCP is a dynamic context parallel training framework that uses fine-grained blockwise partitioning to adapt to varying sequence lengths and attention patterns, reducing communication overhead and improving computation balance compared to static parallelization methods.", "motivation": "Existing context parallel methods use static configurations that don't account for the dynamic nature of training data, particularly variations in sequence lengths and attention patterns, leading to unnecessary communication overhead and imbalanced computation.", "method": "DCP introduces fine-grained blockwise partitioning of both data and computation, enabling flexible mapping of data and computation blocks to devices to adapt to varying sequence characteristics.", "result": "DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns, with end-to-end training speed-ups of 0.94x~1.16x for causal masks and 1.00x~1.46x for sparse masks.", "conclusion": "DCP effectively addresses the limitations of static context parallel methods by dynamically adapting to data characteristics, significantly reducing communication overhead and improving training efficiency for long-context generative AI models."}}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy.", "AI": {"tldr": "This paper presents an in-storage computing architecture using 3D Ferroelectric NAND (FeNAND) combined with hyperdimensional computing to enable efficient mass spectrometry library search, achieving 43x speedup and 21x energy efficiency improvement over state-of-the-art methods.", "motivation": "The rapid growth of mass spectrometry data (exceeding hundreds of terabytes) creates challenges for efficient large-scale library search in drug discovery, as traditional processors struggle with this data volume, making in-storage computing a promising alternative.", "method": "The authors propose an ISP architecture using 3D FeNAND structure with higher density and speed, integrated with hyperdimensional computing (HDC) for parallel processing and dual-bound approximate matching (D-BAM) distance metric tailored to FeNAND structure to parallelize vector computations.", "result": "The proposed method achieves 43x speedup and 21x higher energy efficiency compared to state-of-the-art 3D NAND methods while maintaining comparable accuracy for mass spectrometry spectral library search.", "conclusion": "The combination of FeNAND-based in-storage computing with hyperdimensional computing and specialized distance metrics enables efficient processing of large-scale mass spectrometry data, overcoming throughput limitations of traditional NAND structures for computational applications."}}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["Jo\u00e3o Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.", "AI": {"tldr": "An automated framework for accelerating sparse LLM inference on compute-in-memory accelerators using novel mapping and scheduling strategies that exploit block-diagonal sparsity.", "motivation": "Structured sparsity reduces LLM size but conventional architectures struggle with memory-bound decode stages. CIM architectures help but naive sparse matrix mapping leads to poor array utilization and computational inefficiency.", "method": "Developed an automated framework with novel mapping and scheduling strategies that specifically exploit block-diagonal sparsity patterns in sparse LLMs to optimize CIM array utilization.", "result": "Achieved over 50% improvement in CIM array utilization, with more than 4x reduction in both memory footprint and number of required floating-point operations.", "conclusion": "The proposed framework effectively bridges the gap between sparse LLMs and CIM accelerators, significantly improving computational efficiency and enabling more practical deployment of large language models on resource-constrained systems."}}
{"id": "2510.10818", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10818", "abs": "https://arxiv.org/abs/2510.10818", "authors": ["Kevin Chalmers", "Jan B\u00e6kgaard Pedersen"], "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes", "comment": null, "summary": "We present the first spin-free, kernel-lock-free mutex that cooperates with\nuser-mode schedulers and is formally proven FIFO-fair and linearizable using\nCSP/FDR. Our fairness oracle and stability-based proof method are reusable\nacross coroutine runtime designs. We designed the claim/release protocol for a\nprocess-oriented language -- ProcessJ -- to manage the race for claiming shared\ninter-process communication channels. Internally, we use a lock-free queue to\npark waiting processes for gaining access to a shared object, such as exclusive\naccess to a shared channel to read from or write to. The queue ensures control\nand fairness for processes wishing to access a shared resource, as the protocol\nhandles claim requests in the order they are inserted into the queue. We\nproduce CSP models of our protocol and a mutex specification, demonstrating\nwith FDR that our protocol behaves as a locking mutex.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.10833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10833", "abs": "https://arxiv.org/abs/2510.10833", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed and\nreliable systems. In the proposed framework we have used three parts to\nincrease Satisfaction and Performance of this framework. At first we analyze\nprevious frameworks related to integrated systems, then represent new proposed\nframework in order to improving previous framework, and we discuss its\ndifferent phases. Finally we compare the results of simulation of the new\nframework with previous ones. In FIDRS framework, the technique of\nheterogeneous distributed data base is used to improve Performance and speed in\nresponding to users and in this way we can improve dependability and\nreliability of framework simultaneously. In extraction phase of the new\nframework we have used RMSD algorithm that decreases responding time in big\ndatabase. Finally by using FDIRS framework we succeeded to increase Efficiency,\nPerformance and reliability of integrated systems and remove some of previous\nframeworks problems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.11189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11189", "abs": "https://arxiv.org/abs/2510.11189", "authors": ["Yangyang Wen", "Paul Townend", "Per-Olov \u00d6stberg", "Abel Souza", "Cl\u00e9ment Courageux-Sudan"], "title": "A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems", "comment": "9 pages, 4 figures. Accepted at the 2025 IEEE Joint Cloud Computing\n  (JCC) track of IEEE CISOSE 2025. Conference: IEEE JCC 2025, 16th IEEE\n  International Conference on Joint Cloud Computing, Tucson, Arizona, USA, from\n  July 21 to 24, 2025", "summary": "As microservice-based systems scale across the cloud-edge continuum,\ntraditional centralized scheduling mechanisms increasingly struggle with\nlatency, coordination overhead, and fault tolerance. This paper presents a new\narchitectural direction: leveraging service mesh sidecar proxies as\ndecentralized, in-situ schedulers to enable scalable, low-latency coordination\nin large-scale, cloud-native environments. We propose embedding lightweight,\nautonomous scheduling logic into each sidecar, allowing scheduling decisions to\nbe made locally without centralized control. This approach leverages the\ngrowing maturity of service mesh infrastructures, which support programmable\ndistributed traffic management. We describe the design of such an architecture\nand present initial results demonstrating its scalability potential in terms of\nresponse time and latency under varying request rates. Rather than delivering a\nfinalized scheduling algorithm, this paper presents a system-level\narchitectural direction and preliminary evidence to support its scalability\npotential.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.11211", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11211", "abs": "https://arxiv.org/abs/2510.11211", "authors": ["Sheikh Azizul Hakim", "Saem Hasan"], "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models", "comment": null, "summary": "Large language models (LLM) are advanced AI systems trained on extensive\ntextual data, leveraging deep learning techniques to understand and generate\nhuman-like language. Today's LLMs with billions of parameters are so huge that\nhardly any single computing node can train, fine-tune, or infer from them.\nTherefore, several distributed computing techniques are being introduced in the\nliterature to properly utilize LLMs. We have explored the application of\ndistributed computing techniques in LLMs from two angles.\n  \\begin{itemize}\n  \\item We study the techniques that democratize the LLM, that is, how large\nmodels can be run on consumer-grade computers. Here, we also implement a novel\nmetaheuristics-based modification to an existing system.\n  \\item We perform a comparative study on three state-of-the-art LLM serving\ntechniques. \\end{itemize}", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.11513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11513", "abs": "https://arxiv.org/abs/2510.11513", "authors": ["Alex Elwood", "Tom Deakin", "Justin Lovegrove", "Chris Nelson"], "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems", "comment": null, "summary": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a\nchallenge to scale due to complex data dependencies, memory access patterns and\na high-dimensional domain. In this paper, we review the performance bottlenecks\nwithin the shared memory parallelization scheme of an existing transport solver\non modern many-core architectures with high core counts. With this analysis, we\nthen survey the performance of this solver across a variety of compute\nhardware. We then present a new Asynchronous Many-Task (AMT) algorithm for\nshared memory parallelism, present results showing an increase in computational\nperformance over the existing method, and evaluate why performance is improved.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.11697", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11697", "abs": "https://arxiv.org/abs/2510.11697", "authors": ["Matteo Mordacchini", "Emanuele Carlini", "Patrizio Dazzi"], "title": "A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem", "comment": null, "summary": "We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in\na decentralized network. MWVC, a classical NP-hard problem, is foundational in\napplications such as network monitoring and resource placement. We propose a\nfully decentralized protocol where each node makes decisions using only local\nknowledge and communicates with its neighbors. The method is adaptive,\ncommunication-efficient, and avoids centralized coordination. We evaluate the\nprotocol on real-world and synthetic graphs, comparing it to both centralized\nand decentralized baselines. Our results demonstrate competitive solution\nquality with reduced communication overhead, highlighting the feasibility of\nMWVC computation in decentralized environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
