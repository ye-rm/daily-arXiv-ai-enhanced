<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Anchor-and-Connect: Robotic Aerial Base Stations Transforming 6G Infrastructure](https://arxiv.org/abs/2511.15341)
*Wen Shang,Yuan Liao,Vasilis Friderikos,Halim Yanikomeroglu*

Main category: cs.ET

TL;DR: The paper proposes Robotic Aerial Base Stations (RABS) that use energy-neutral anchoring to perch on tall structures, enabling hours of wireless connectivity instead of minutes compared to hovering drones.


<details>
  <summary>Details</summary>
Motivation: Current aerial base stations (ABSs) have limited endurance due to battery constraints, which severely limits their practical implementation and wider adoption in wireless networks.

Method: RABSs are equipped with energy-neutral anchoring end-effectors that autonomously grasp or perch on tall urban structures, eliminating the need for energy-intensive hovering and enabling extended operation.

Result: RABSs can provide seamless wireless connectivity for multiple hours compared to minutes offered by typical hovering-based ABSs, allowing integration into radio access networks to augment capacity as needed.

Conclusion: The proposed RABS concept overcomes fundamental battery limitations of aerial base stations, enabling prolonged service capabilities that can significantly enhance wireless network capacity and flexibility.

Abstract: Despite the significant attention that aerial base stations (ABSs) have received recently, their practical implementation is severely weakened by their limited endurance due to the battery constraints of drones. To overcome this fundamental limitation and barrier for wider adoption, we propose the concept of robotic aerial base stations (RABSs) that are equipped with energy-neutral anchoring end-effectors able to autonomously grasp or perch on tall urban landforms. Thanks to the energy-efficient anchoring operation, RABSs could offer seamless wireless connectivity for multiple hours compared to minutes of the typical hovering-based ABSs. Therefore, the prolonged service capabilities of RABSs allowing them to integrate into the radio access network and augment the network capacity where and when needed. To set the scene, we discuss the key components of the proposed RABS concept including hardware, workflow, communication considerations, and regulation issues. Then, the advantages of RABSs are highlighted which is followed by case studies that compare RABSs with terrestrial micro BSs and other types of non-terrestrial communication infrastructure, such as hovering-based, tethered, and laser-powered ABSs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU is a hardware-software co-designed system that uses optimized coroutines and hardware support to hide memory latency in disaggregated memory systems, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Modern data-intensive applications face memory latency challenges in disaggregated memory systems, and existing coroutine approaches struggle to balance latency-hiding efficiency with runtime overhead.

Method: Combines compiler procedures for optimized coroutine code generation, context minimization, and request coalescing with hardware support including decoupled memory operations, coroutine-specific memory operations, and a memory-guided branch prediction mechanism.

Result: Achieves 1.51x speedup over state-of-the-art coroutine methods on Intel processors, and 3.39x-4.87x performance improvements over baseline processor on FPGA-emulated disaggregated systems under different latency conditions.

Conclusion: CoroAMU effectively addresses memory latency challenges in disaggregated systems through hardware-software co-design, demonstrating substantial performance gains across different platforms and latency scenarios.

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [3] [DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution](https://arxiv.org/abs/2511.15367)
*Xin Yang,Xin Fan,Zengshi Wang,Jun Han*

Main category: cs.AR

TL;DR: DARE is an irregularity-tolerant Matrix Processing Unit (MPU) that addresses performance issues in sparse DNN workloads through a Densifying ISA and filtered Runahead Execution, achieving significant performance and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Current hardware-algorithm co-optimization for DNN workloads is insufficient, leading to suboptimal performance. Sparse DNNs cause irregular memory access patterns with high cache miss rates, and existing runahead execution suffers from prefetch redundancy. Matrix ISAs have stride constraints that prevent efficient densification of sparse operations.

Method: DARE extends the ISA to support densifying sparse operations and implements a lightweight runahead mechanism with filtering capability to reduce prefetch redundancy and improve memory access patterns.

Result: DARE improves performance by 1.04× to 4.44× and increases energy efficiency by 1.00× to 22.8× over baseline, with 3.91× lower hardware overhead than NVR.

Conclusion: DARE effectively addresses irregularities in sparse DNN workloads through hardware-algorithm co-optimization, demonstrating significant performance and efficiency gains with minimal hardware overhead.

Abstract: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.
  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.
  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

</details>


### [4] [Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism](https://arxiv.org/abs/2511.15397)
*Cong Wang,Zexin Fu,Jiayi Huang,Shanshi Huang*

Main category: cs.AR

TL;DR: Hemlet is a heterogeneous CIM chiplet system designed to accelerate Vision Transformers, addressing scalability and communication challenges through flexible integration of analog CIM, digital CIM, and data processing chiplets.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high memory and computational demands that challenge hardware deployment. Monolithic CIM accelerators face scalability issues, while chiplet designs introduce expensive communication overhead through network-on-package.

Method: Hemlet integrates heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets to enable flexible resource scaling and reduce communication overhead.

Result: The system improves throughput while reducing communication costs compared to traditional monolithic CIM designs and standard chiplet approaches.

Conclusion: Hemlet provides a scalable and efficient solution for accelerating Vision Transformers by leveraging heterogeneous CIM chiplets with optimized communication architecture.

Abstract: Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.
  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove

</details>


### [5] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC is a data-centric ML compiler for PIM systems that jointly optimizes data rearrangements and compute code, achieving significant speedups over GPU-only execution for ML kernels and LLM inference.


<details>
  <summary>Details</summary>
Motivation: Processing-In-Memory (PIM) devices can accelerate memory-intensive ML kernels but face performance challenges due to different data layout requirements between Host processors and PIM cores, necessitating data rearrangements that current compilation approaches don't systematically optimize.

Method: DCC integrates a multi-layer PIM abstraction, maps data partitioning strategies to compute loop partitions, applies PIM-specific code optimizations, and uses a fast performance prediction model to select optimal configurations in a unified tuning process.

Result: DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution for ML kernels, and accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) in LLM inference.

Conclusion: Joint optimization of data rearrangements and compute code is essential for PIM systems, and DCC provides an effective compiler solution that significantly outperforms GPU-only execution across diverse PIM backends and ML workloads.

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [6] [Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference](https://arxiv.org/abs/2511.15505)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: An instruction-based coordination architecture for FPGA systems with multiple processing units to accelerate DNN inference, enabling programmable synchronization and flexible model partitioning with high efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient coordination and synchronization in FPGA-based multi-PU systems for DNN inference acceleration, overcoming limitations of prior approaches that lack programmability and dynamic adaptability.

Method: Instruction-based coordination architecture with controller units and peer-to-peer synchronization units, organized into load/compute/store functional groups. Includes compilation framework to transform DNN models into executable programs with flexible partitioning and multiple deployment strategies supporting pipeline and batch-level parallelism.

Result: Experimental results on ResNet-50 show up to 98% compute efficiency and 2.7× throughput efficiency gains over prior works across different configurations, with runtime switching between deployment strategies without FPGA reconfiguration.

Conclusion: The proposed architecture enables effective design space exploration and dynamic trade-offs between single-batch and multi-batch performance, demonstrating significant efficiency improvements for multi-PU FPGA systems in DNN inference acceleration.

Abstract: This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.

</details>


### [7] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: A roadmap for open-source chiplet-based RISC-V systems targeting HPC and AI, starting from Occamy (first open dual-chiplet RISC-V manycore) to Ramora (mesh-NoC dual-chiplet) and Ogopogo (7nm quad-chiplet), with extensions into simulation, EDA, PDKs, and PHYs.


<details>
  <summary>Details</summary>
Motivation: To close the performance gap between open-source RISC-V designs and proprietary systems in high-performance computing and artificial intelligence applications.

Method: Developed a progressive roadmap: 1) Occamy - first open silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, 2) Ramora - mesh-NoC-based dual-chiplet system, 3) Ogopogo - 7nm quad-chiplet concept architecture achieving state-of-the-art compute density.

Result: Created a scalable chiplet-based RISC-V system roadmap that progresses from dual-chiplet to quad-chiplet designs with increasing performance and compute density.

Conclusion: The roadmap successfully demonstrates the potential for open-source chiplet-based RISC-V systems to achieve competitive performance while exploring extensions of openness beyond logic-core RTL into broader design ecosystem components.

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)
*Mingkun Yu,Heming Zhong,Dan Huang,Yutong Lu,Jiazhi Jiang*

Main category: cs.DC

TL;DR: PolyKAN is a GPU-accelerated library that optimizes Kolmogorov-Arnold Networks (KANs) through four key techniques, achieving 1.2-10x faster inference and 1.4-12x faster training compared to baseline implementations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing KAN implementations suffer from low GPU utilization, hindering practical adoption despite KANs' promise of higher expressive capability and interpretability in AI for Science applications.

Method: PolyKAN fuses forward and backward passes into optimized CUDA kernels using four techniques: lookup-table with linear interpolation, 2D tiling for parallelism, two-stage reduction for atomic updates, and coefficient-layout reordering for memory efficiency.

Result: PolyKAN delivers 1.2-10x faster inference and 1.4-12x faster training than Triton + cuBLAS baseline with identical accuracy across speech, audio-enhancement, and tabular-regression workloads on both high-end and consumer-grade GPUs.

Conclusion: PolyKAN successfully addresses the GPU utilization bottleneck in KAN implementations, making KANs more practical for real-world applications while maintaining their theoretical advantages.

Abstract: Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.

</details>


### [9] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: The paper introduces RemoteOptiGraph, a distributed optimization modeling abstraction that extends Plasmo.jl's OptiGraph model to handle distributed memory environments through InterWorkerEdges, enabling scalable decomposition algorithms like Benders decomposition.


<details>
  <summary>Details</summary>
Motivation: To provide a unified modeling approach for distributed optimization problems that avoids bespoke solutions and enables exploitation of distributed memory structure through general-purpose meta-algorithms.

Method: Extends the OptiGraph hypergraph model with InterWorkerEdges that manage linking constraints across workers in distributed memory environments, implemented in the Plasmo.jl package.

Result: Applied to a large-scale mixed integer capacity expansion model for the western US with over 12 million variables and constraints, achieving 7.5x speedup using Benders decomposition compared to solving without decomposition.

Conclusion: RemoteOptiGraph provides an effective abstraction for distributed optimization modeling that enables significant performance improvements through decomposition algorithms in distributed memory environments.

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [10] [GPU-Initiated Networking for NCCL](https://arxiv.org/abs/2511.15076)
*Khaled Hamidouche,John Bachan,Pak Markthub,Peter-Jan Gootzen,Elena Agostini,Sylvain Jeaugey,Aamir Shafi,Georgios Theodorakis,Manjunath Gorentla Venkata*

Main category: cs.DC

TL;DR: NCCL 2.28 introduces Device API with GPU-Initiated Networking (GIN) for direct GPU-to-GPU communication, enabling device-side control to eliminate CPU coordination overhead in modern AI workloads like Mixture-of-Experts architectures.


<details>
  <summary>Details</summary>
Motivation: Modern AI workloads, especially Mixture-of-Experts architectures, require low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional host-initiated GPU communication models introduce CPU coordination overhead that limits performance for applications needing tight integration of computation and communication.

Method: GIN uses a three-layer architecture: NCCL Core host-side APIs for setup, Device-side APIs for remote memory operations from CUDA kernels, and a network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy backends) for broad hardware support. The GPUDirect backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend uses lock-free GPU-to-CPU queues over standard RDMA networks.

Result: Integration with DeepEP MoE communication library demonstrates GIN's practicality. Comprehensive benchmarking shows GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

Conclusion: GIN enables efficient device-initiated communication that eliminates CPU coordination overhead, providing significant performance benefits for modern AI workloads like Mixture-of-Experts architectures while maintaining compatibility with NCCL's established collective algorithms and infrastructure.

Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

</details>


### [11] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle is a two-layer blockchain consensus architecture that optimizes the trade-off between security, latency, and decentralization. BB-Core provides fast finality with reduced fault tolerance, while BB-Guard ensures security through decentralized timestamping and recovery mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the blockchain consensus trilemma where high-throughput systems sacrifice decentralization or security, and decentralized secure systems have poor performance. The goal is to achieve sub-second finality at high throughput while maintaining strong safety and liveness.

Method: Two-layer architecture: BB-Core (n=5f+1 protocol with medium-sized validator set for low latency) and BB-Guard (provides decentralized timestamping, proactive misbehavior detection, and synchronous recovery path). Guard validators detect equivocations/liveness failures and can restart core protocol or select canonical fork.

Result: BB-Core reduces latency by 20-25% compared to Mysticeti. The system achieves optimistic sub-second finality at high throughput while maintaining strong safety and liveness under mild synchrony assumption.

Conclusion: BlueBottle successfully balances the consensus trilemma by combining fast core consensus with robust guard mechanisms, enabling high-performance blockchain systems without compromising security or decentralization.

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


### [12] [Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies](https://arxiv.org/abs/2511.15388)
*Lucianna Kiffer,Lioba Heimbach,Dennis Trautwein,Yann Vonlanthen,Oliver Gasser*

Main category: cs.DC

TL;DR: First comprehensive measurement study of 36 blockchain networks revealing dramatic variations in network size (10-10,000+ nodes), IPv4/IPv6 usage patterns, geographic concentration, and discovery protocol effectiveness.


<details>
  <summary>Details</summary>
Motivation: Blockchain peer-to-peer networks remain largely opaque despite being fundamental infrastructure, with limited understanding of their actual decentralization, resilience, and operational characteristics across different ecosystems.

Method: 9-month longitudinal study using 15 active crawlers, community data sources, hourly connectivity probes, Ethereum discovery protocol metadata inference, and Internet-wide scanning with network-specific payloads to identify responsive peers.

Result: Found significant network size variation (10-10,000+ nodes), quantified IPv4/IPv6 usage trends, analyzed AS and geographic concentration, characterized churn and diurnal behavior, and assessed discovery protocol coverage and redundancy.

Conclusion: The study exposes critical differences in network resilience, decentralization, and observability across blockchain ecosystems, while providing a general framework for large-scale measurement of decentralized networks to enable continued monitoring and transparent assessment.

Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.

</details>


### [13] [When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit](https://arxiv.org/abs/2511.15421)
*Ethan Hicks,Joseph Oglio,Mikhail Nesterenko,Gokarna Sharma*

Main category: cs.DC

TL;DR: Analysis of Bitcoin transaction finality based on transaction amount and user risk tolerance, using fork probability modeling and prospect theory to determine optimal confirmation depth.


<details>
  <summary>Details</summary>
Motivation: Current Bitcoin transaction confirmation uses fixed block depths (e.g., 6 blocks) without considering transaction amount or user risk preferences, potentially leading to inefficient or insecure confirmations.

Method: Simulated fork analysis under varying network delays and actual Bitcoin data analysis to model confirmation revocation probability, combined with prospect theory to relate confirmation probability to transaction value and risk tolerance.

Result: Established relationship between block depth and probability of confirmation revocation, enabling personalized confirmation strategies based on transaction amount and individual risk preferences.

Conclusion: Transaction confirmation depth should be dynamically determined based on transaction amount and user risk tolerance rather than using fixed block depths, improving both security and efficiency.

Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.

</details>


### [14] [Proving there is a leader without naming it](https://arxiv.org/abs/2511.15491)
*Laurent Feuilloley,Josef Erik Sedláček,Martin Slávik*

Main category: cs.DC

TL;DR: The paper explores whether sublogarithmic leader certification is possible in graph classes that don't contain cycle graphs, and whether identifiers are necessary in such topologies.


<details>
  <summary>Details</summary>
Motivation: To understand how network structure affects local certification complexity, particularly for leader election in graphs without cycle structures.

Method: Analyzes local certification mechanisms for unique leader verification across different graph classes including small diameter graphs, chordal graphs, grids, and dense graphs.

Result: Shows that sublogarithmic leader certification is achievable in certain graph classes that don't contain cycle graphs, and explores the necessity of identifiers in such topologies.

Conclusion: Network structure significantly impacts local certification complexity, with sublogarithmic certification possible in specific graph classes without cycle structures, and identifiers may not always be necessary.

Abstract: Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.
  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).
  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]

</details>
