<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Annotated PIM Bibliography](https://arxiv.org/abs/2601.09002)
*Peter M. Kogge*

Main category: cs.AR

TL;DR: Historical survey of Processing in Memory (PIM) technologies spanning over 60 years, challenging the notion that PIM is a recent revolutionary development.


<details>
  <summary>Details</summary>
Motivation: To provide historical context and correct the misconception that Processing in Memory is a new revolutionary technology, showing that many examples date back over 60 years.

Method: Creation of an annotated bibliography covering the entire historical timeframe of PIM technology, organized to support an upcoming article.

Result: A comprehensive historical survey document that traces PIM concepts through various implementations and terminology (CIM, LIM, IMC, NMC) over six decades.

Conclusion: PIM technology has deep historical roots spanning over 60 years, and understanding this history is important for proper context and future development of memory-centric computing approaches.

Abstract: Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [2] [Probabilistic Computers for MIMO Detection: From Sparsification to 2D Parallel Tempering](https://arxiv.org/abs/2601.09037)
*M Mahmudul Hasan Sajeeb,Corentin Delacour,Kevin Callahan-Coray,Sanjay Seshan,Tathagata Srimani,Kerem Y. Camsari*

Main category: cs.ET

TL;DR: FPGA-based probabilistic computer using p-bits with graph sparsification solves dense NP-hard MIMO detection problems with 4.7ms end-to-end solution time, achieving better bit error rates than conventional linear detectors.


<details>
  <summary>Details</summary>
Motivation: Real-world combinatorial optimization problems require dense connectivity that scales poorly in hardware. The paper addresses this challenge for probabilistic computers built from p-bits, specifically targeting MIMO detection - a dense, NP-hard problem central to wireless communications.

Method: Uses graph sparsification with auxiliary copy variables to reduce hardware connectivity requirements. Implements fully on-chip parallel tempering solver on FPGA with 15 temperature replicas of a 128-node sparsified system (1,920 p-bits). Employs Two-Dimensional Parallel Tempering (2D-PT) that exchanges replicas across both temperature and constraint dimensions for faster convergence.

Result: Achieves bit error rates significantly below conventional linear detectors with complete end-to-end solution times of 4.7 ms per instance. Demonstrates over 10X faster convergence with 2D-PT without manual parameter tuning. ASIC projections in 7 nm technology indicate about 90 MHz operation with less than 200 mW power dissipation.

Conclusion: Establishes an on-chip p-bit architecture and scalable algorithmic framework for dense combinatorial optimization, suggesting that massive parallelism across multiple chips could approach the throughput demands of next-generation wireless systems.

Abstract: Probabilistic computers built from p-bits offer a promising path for combinatorial optimization, but the dense connectivity required by real-world problems scales poorly in hardware. Here, we address this through graph sparsification with auxiliary copy variables and demonstrate a fully on-chip parallel tempering solver on an FPGA. Targeting MIMO detection, a dense, NP-hard problem central to wireless communications, we fit 15 temperature replicas of a 128-node sparsified system (1,920 p-bits) entirely on-chip and achieve bit error rates significantly below conventional linear detectors. We report complete end-to-end solution times of 4.7 ms per instance, with all loading, sampling, readout, and verification overheads included. ASIC projections in 7 nm technology indicate about 90 MHz operation with less than 200 mW power dissipation, suggesting that massive parallelism across multiple chips could approach the throughput demands of next-generation wireless systems. However, sparsification introduces sensitivity to the copy-constraint strength. Employing Two-Dimensional Parallel Tempering (2D-PT), which exchanges replicas across both temperature and constraint dimensions, we demonstrate over 10X faster convergence without manual parameter tuning. These results establish an on-chip p-bit architecture and a scalable algorithmic framework for dense combinatorial optimization.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [3] [Revisiting Disaggregated Large Language Model Serving for Performance and Energy Implications](https://arxiv.org/abs/2601.08833)
*Jiaxi Li,Yue Zhu,Eun Kyung Lee,Klara Nahrstedt*

Main category: cs.PF

TL;DR: This paper systematically benchmarks disaggregated LLM serving architectures that separate prefill and decode stages across different GPUs, evaluating various KV cache transfer paths and optimization strategies for performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing works propose various KV cache transfer paths for disaggregated LLM serving but lack systematic benchmarking of their performance and energy efficiency. Optimization techniques like KV cache reuse and frequency scaling have been used but their performance and energy implications haven't been rigorously benchmarked.

Method: The paper re-evaluates prefill-decode disaggregation under different KV transfer mediums and optimization strategies, including a new colocated serving baseline. It evaluates disaggregated setups under different KV cache transfer paths and uses GPU profiling with dynamic voltage and frequency scaling (DVFS) to identify and compare performance-energy Pareto frontiers.

Result: Performance benefits from prefill-decode disaggregation are not guaranteed and depend on request load and KV transfer mediums. Stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.

Conclusion: The research fills the gap in systematic benchmarking of disaggregated LLM serving architectures, providing insights into when disaggregation provides performance benefits and revealing that energy savings from stage-wise frequency scaling are offset by the higher baseline energy consumption of disaggregated setups.

Abstract: Different from traditional Large Language Model (LLM) serving that colocates the prefill and decode stages on the same GPU, disaggregated serving dedicates distinct GPUs to prefill and decode workload. Once the prefill GPU completes its task, the KV cache must be transferred to the decode GPU. While existing works have proposed various KV cache transfer paths across different memory and storage tiers, there remains a lack of systematic benchmarking that compares their performance and energy efficiency. Meanwhile, although optimization techniques such as KV cache reuse and frequency scaling have been utilized for disaggregated serving, their performance and energy implications have not been rigorously benchmarked. In this paper, we fill this research gap by re-evaluating prefill-decode disaggregation under different KV transfer mediums and optimization strategies. Specifically, we include a new colocated serving baseline and evaluate disaggregated setups under different KV cache transfer paths. Through GPU profiling using dynamic voltage and frequency scaling (DVFS), we identify and compare the performance-energy Pareto frontiers across all setups to evaluate the potential energy savings enabled by disaggregation. Our results show that performance benefits from prefill-decode disaggregation are not guaranteed and depend on the request load and KV transfer mediums. In addition, stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.

</details>


### [4] [LookAhead: The Optimal Non-decreasing Index Policy for a Time-Varying Holding Cost problem](https://arxiv.org/abs/2601.08960)
*Keerthana Gurushankar,Zhouzi Li,Mor Harchol-Balter,Alan Scheller-Wolf*

Main category: cs.PF

TL;DR: The paper introduces LookAhead, the first optimal index policy for a two-class M/M/1 queue with time-varying holding costs for class 1 jobs and constant holding costs for class 2 jobs.


<details>
  <summary>Details</summary>
Motivation: In practice, job delay costs often increase over time, modeled by the Time-Varying Holding Cost (TVHC) problem. However, no optimality results exist for TVHC outside asymptotic regimes, motivating study of a simpler two-class M/M/1 queue special case.

Method: The authors propose a novel scheduling policy called LookAhead that considers jobs' future holding costs rather than current costs. The key innovation is determining the optimal "lookahead amount" X - how far into the future to consider costs when making scheduling decisions.

Result: The paper derives the first optimal (non-decreasing) index policy for this TVHC special case. The LookAhead policy with the derived optimal lookahead amount X provides optimal scheduling for the two-class M/M/1 queue with time-varying class 1 costs and constant class 2 costs.

Conclusion: The LookAhead policy represents a significant theoretical advance, providing the first optimal scheduling solution for a non-asymptotic TVHC problem. The approach of looking ahead to future costs rather than current costs offers a promising direction for more general TVHC problems.

Abstract: In practice, the cost of delaying a job can grow as the job waits. Such behavior is modeled by the Time-Varying Holding Cost (TVHC) problem, where each job's instantaneous holding cost increases with its current age (a job's age is the time since it arrived). The goal of the TVHC problem is to find a scheduling policy that minimizes the time-average total holding cost across all jobs.
  However, no optimality results are known for the TVHC problem outside of the asymptotic regime. In this paper, we study a simple yet still challenging special case: A two-class M/M/1 queue in which class 1 jobs incur a non-decreasing, time-varying holding cost and class 2 jobs incur a constant holding cost.
  Our main contribution is deriving the first optimal (non-decreasing) index policy for this special case of the TVHC problem. Our optimal policy, called LookAhead, stems from the following idea: Rather than considering each job's current holding cost when making scheduling decisions, we should look at their cost some $X$ time into the future, where this $X$ is intuitively called the ``lookahead amount." This paper derives that optimal lookahead amount.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: Machine learning approach to automatically select optimal thread count for GEMM operations, achieving 25-40% speedup on modern HPC architectures.


<details>
  <summary>Details</summary>
Motivation: Modern multi-core shared memory systems make it challenging to determine the optimal number of threads for multi-thread GEMM runtime minimization, despite single-thread GEMM being well-optimized.

Method: Developed an Architecture and Data-Structure Aware Linear Algebra (ADSALA) library using machine learning to optimize BLAS routines. Specifically, uses ML models on-the-fly to automatically select optimal thread count for GEMM tasks based on collected training data.

Result: Achieved 25-40% speedup compared to traditional GEMM implementations in BLAS when using GEMM with memory usage within 100 MB, tested on two HPC node architectures (Intel Cascade Lake and AMD Zen 3).

Conclusion: Machine learning can effectively optimize thread selection for GEMM operations, demonstrating significant performance improvements on modern multi-core architectures through the ADSALA approach.

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [6] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: LatencyPrism is a zero-intrusion multi-platform latency sculpting system for LLM inference that provides real-time monitoring, anomaly detection, and SLO adherence without requiring code changes or service restarts.


<details>
  <summary>Details</summary>
Motivation: LLM inference latency critically impacts user experience and operational costs, with latency spikes degrading service quality despite acceptable average performance. Existing AI profiling methods are inadequate due to intrusive designs requiring service restarts and hardware-bound implementations that fail to adapt to heterogeneous inference environments with diverse software frameworks and XPU architectures.

Method: LatencyPrism is designed as a zero-intrusion multi-platform latency sculpting system that breaks down inference latency across pipelines, proactively alerts on latency anomalies, and guarantees SLO adherence without code modifications or service restarts. It enables low-overhead real-time monitoring at batch level with millisecond alert triggering.

Result: Deployed across thousands of XPUs for over six months, LatencyPrism achieves an F1-score of 0.98 for distinguishing between workload-driven latency variations and anomalies indicating underlying issues. It provides real-time monitoring with alerts triggered in milliseconds.

Conclusion: LatencyPrism successfully addresses the challenges of latency analysis in distributed inference environments by providing a non-intrusive, multi-platform solution for real-time monitoring, anomaly detection, and SLO adherence, demonstrating practical deployment effectiveness over extended periods.

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


### [7] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: PDCC protocol enables dynamic reconfiguration in payment systems using Byzantine Consistent Broadcast to avoid global ordering, achieving high performance with smooth configuration changes.


<details>
  <summary>Details</summary>
Motivation: Modern payment systems need dynamic reconfiguration capabilities without performance degradation, requiring a solution that avoids global transaction ordering bottlenecks.

Method: PDCC combines user nonce-based transaction ordering with periodic system-wide consensus, using Byzantine Consistent Broadcast to enable transaction-driven dynamic reconfiguration.

Result: The protocol achieves high performance by avoiding global transaction ordering and enables smooth reconfiguration without impacting original system performance.

Conclusion: PDCC provides an effective dynamic reconfiguration solution for modern payment systems, balancing performance and flexibility through innovative transaction ordering and consensus mechanisms.

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [8] [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)
*Yifei Xie,Btissam Er-Rahmadi,Xiao Chen,Tiejun Ma,Jane Hillston*

Main category: cs.DC

TL;DR: A View Change Optimization (VCO) model using mixed integer programming improves parallel BFT performance by optimizing leader selection and follower reassignment across committees, addressing view change bottlenecks in permissioned blockchains.


<details>
  <summary>Details</summary>
Motivation: Parallel BFT protocols face performance bottlenecks during view changes when leaders fail. Existing approaches use passive view change with blind leader rotation, often selecting unavailable or slow nodes, degrading system performance.

Method: Proposed a VCO model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees considering communication delays and failure scenarios. Used decomposition method with efficient subproblems and improved benders cuts to solve the model, then developed an iterative backup leader selection algorithm.

Result: Experiments in Microsoft Azure cloud show VCO-driven parallel BFT outperforms existing configuration methods under both normal and faulty conditions. The model remains effective as network size increases.

Conclusion: The VCO model provides an effective solution for high-performance parallel BFT systems by optimizing view change processes, making it suitable for scalable permissioned blockchain applications.

Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.

</details>


### [9] [High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data](https://arxiv.org/abs/2601.09334)
*Valerio Besozzi,Matteo Della Bartola,Patrizio Dazzi,Marco Danelutto*

Main category: cs.DC

TL;DR: Systematic review of 122 papers (2018-2025) on using serverless computing for HPC/AI workloads, proposing taxonomy of 8 research directions and 9 use case domains.


<details>
  <summary>Details</summary>
Motivation: Convergence of cloud and HPC infrastructures creates need for scalable execution models; serverless computing shows promise for dynamic, parallel, distributed workloads in compute-intensive applications.

Method: Comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, with taxonomy development and analysis of publication trends and collaboration networks.

Result: Proposed taxonomy with 8 primary research directions and 9 targeted use case domains; identified growing research interest and interconnections in serverless computing for HPC/AI applications.

Conclusion: Serverless computing offers promising foundation for next-generation solutions for parallel compute-intensive applications across cloud, HPC, and hybrid environments.

Abstract: The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.

</details>
