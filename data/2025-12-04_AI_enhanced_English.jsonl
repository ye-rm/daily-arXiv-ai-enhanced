{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.", "AI": {"tldr": "TokenScale is an autoscaling framework for disaggregated LLM serving that uses Token Velocity as a predictive metric and Convertible Decoders to dynamically handle bursty workloads, improving SLO attainment and reducing costs.", "motivation": "Prefill/decode disaggregation improves resource utilization but struggles with bursty workloads. Existing autoscaling policies rely on lagging indicators (GPU utilization, request counts), causing slow reactions to load spikes, SLO violations, and costly over-provisioning.", "method": "Two innovations: 1) Token Velocity - a novel metric that unifies prefill, network, and decode stages by quantifying their rate of work as a leading indicator of system backpressure; 2) Convertible Decoders - decoder GPUs that can dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer.", "result": "Evaluation on GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems (DistServe, BlitzScale, AIBrix).", "conclusion": "By combining predictive metrics with flexible system design, TokenScale significantly boosts performance and efficiency of disaggregated LLM serving infrastructure, solving the performance mismatch in existing autoscaling approaches."}}
{"id": "2512.03487", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.03487", "abs": "https://arxiv.org/abs/2512.03487", "authors": ["Zhen Wang", "Bin Lin", "Qiang", "Ye"], "title": "Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks", "comment": null, "summary": "In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.", "AI": {"tldr": "Proposed double-edge-assisted computation offloading and resource allocation scheme for space-air-marine integrated networks using UAVs and LEO satellites to serve maritime autonomous ships.", "motivation": "To address the computing needs of maritime autonomous surface ships in remote ocean areas by leveraging both aerial (UAV) and space (LEO satellite) edge computing resources to minimize energy consumption while meeting latency requirements.", "method": "Formulated optimization problem for joint optimization of offloading mode, offloading volume, and computing resource allocation. Used alternating optimization method and layered approach to decompose the problem and develop energy-efficient algorithms.", "result": "Conducted simulations showing the proposed scheme is effective and efficient compared to benchmark algorithms, achieving energy minimization under latency constraints.", "conclusion": "The double-edge-assisted scheme successfully optimizes computation offloading and resource allocation in SAMINs, providing energy-efficient computing services for maritime autonomous ships through coordinated UAV and satellite edge resources."}}
{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus M\u00fchlh\u00e4u\u00dfer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.", "AI": {"tldr": "This paper explores SIMD vectorization techniques for molecular dynamics force calculations in AutoPas, focusing on particle loading order optimization and dynamic runtime tuning for performance improvement.", "motivation": "Molecular Dynamics simulations provide atomic-scale insights but require efficient computation. The paper aims to improve pairwise force calculation performance in AutoPas through optimized SIMD vectorization, addressing runtime algorithm changes and particle density variations.", "method": "The study investigates various SIMD vectorization techniques for particle interaction calculations, focusing on particle value loading order into vector registers. It extends AutoPas' dynamic tuning mechanism to select optimal vectorization order during runtime based on simulation parameters like particle density and neighbor identification algorithms.", "result": "Benchmarks show that considering different particle interaction orders during runtime leads to considerable performance improvement for force calculations compared to AutoPas' previous approach, with optimization for both execution time and energy consumption.", "conclusion": "Dynamic tuning of SIMD vectorization order based on runtime conditions significantly enhances molecular dynamics simulation performance in AutoPas, demonstrating the importance of adaptive optimization for particle interaction calculations."}}
{"id": "2512.03644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03644", "abs": "https://arxiv.org/abs/2512.03644", "authors": ["Bohan Zhao", "Yuanhong Wang", "Chenglin Liu", "Jiagi Pan", "Guang Yang", "Ruitao Liu", "Tingrui Zhang", "Kai Luo", "Wei Xu"], "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management", "comment": null, "summary": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.", "AI": {"tldr": "FFTrainer is a system that uses surplus network capacity to save and load LLM training states quickly, reducing recovery time by up to 98% and GPU utilization loss by up to 68% compared to traditional checkpointing approaches.", "motivation": "As LLM clusters scale, traditional checkpointing approaches face challenges: node failures cause lengthy recoveries, bulky checkpoints erode efficiency, infrequent checkpoints trigger costly rollbacks, while frequent checkpoints add prohibitive overhead.", "method": "FFTrainer leverages surplus network capacity to quickly save and load training states, preventing rollbacks and accelerating recovery without hindering normal training operations.", "result": "FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% compared to prior checkpointing approaches.", "conclusion": "FFTrainer provides an efficient and robust solution for LLM training by utilizing surplus network capacity to address the challenges of node failures, recovery delays, and checkpoint overhead in large-scale training clusters."}}
{"id": "2512.03594", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03594", "abs": "https://arxiv.org/abs/2512.03594", "authors": ["Afsara Khan", "Austin Rovinski"], "title": "Accelerating Detailed Routing Convergence through Offline Reinforcement Learning", "comment": "To be published in the Design, Automation and Test in Europe (DATE) 2026 Conference", "summary": "Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.\n  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.", "AI": {"tldr": "RL-based detailed router uses CQL to dynamically select routing cost weights, achieving 1.56x average speedup on ISPD19 benchmarks while maintaining or improving DRV counts.", "motivation": "Detailed routing is complex and time-consuming due to shrinking feature sizes and strict design rules. Existing routers use static cost weight scheduling, which doesn't adapt to design or technology variations, leading to slow convergence and high runtime costs.", "method": "Proposes reinforcement learning (RL) approach using Conservative Q-Learning (CQL) model to dynamically select routing cost weights that minimize algorithm iterations. The system learns from previous designs to enable rapid convergence.", "result": "Achieves 1.56x average and up to 3.01x faster runtime on ISPD19 benchmarks compared to baseline router while maintaining or improving DRV counts in all cases. Shows signs of generalization across technologies.", "conclusion": "RL-based dynamic cost weight selection significantly improves detailed routing efficiency, enabling faster convergence without compromising design rule compliance, with promising cross-technology generalization capabilities."}}
{"id": "2512.03279", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2512.03279", "abs": "https://arxiv.org/abs/2512.03279", "authors": ["Kaiwei Tu", "Kan Wu", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau"], "title": "Getting the MOST out of your Storage Hierarchy with Mirror-Optimized Storage Tiering", "comment": "18 pages, to be published in 24th USENIX Conference on File and Storage Technologies (FAST '26)", "summary": "We present Mirror-Optimized Storage Tiering (MOST), a novel tiering-based approach optimized for modern storage hierarchies. The key idea of MOST is to combine the load balancing advantages of mirroring with the space-efficiency advantages of tiering. Specifically, MOST dynamically mirrors a small amount of hot data across storage tiers to efficiently balance load, avoiding costly migrations. As a result, MOST is as space-efficient as classic tiering while achieving better bandwidth utilization under I/O-intensive workloads. We implement MOST in Cerberus, a user-level storage management layer based on CacheLib. We show the efficacy of Cerberus through a comprehensive empirical study: across a range of static and dynamic workloads, Cerberus achieves better throughput than competing approaches on modern storage hierarchies especially under I/O-intensive and dynamic workloads.", "AI": {"tldr": "MOST combines mirroring's load balancing with tiering's space efficiency by dynamically mirroring hot data across storage tiers, avoiding costly migrations while maintaining space efficiency and improving bandwidth utilization.", "motivation": "Modern storage hierarchies need efficient management approaches that balance load while maintaining space efficiency. Traditional approaches either sacrifice space efficiency for load balancing (mirroring) or suffer from costly migrations (tiering).", "method": "MOST dynamically mirrors a small amount of hot data across storage tiers to balance load without costly migrations. Implemented in Cerberus, a user-level storage management layer based on CacheLib.", "result": "Cerberus achieves better throughput than competing approaches on modern storage hierarchies, especially under I/O-intensive and dynamic workloads, while maintaining space efficiency comparable to classic tiering.", "conclusion": "MOST successfully combines the advantages of mirroring and tiering, providing an effective solution for modern storage hierarchy management that outperforms existing approaches in throughput and bandwidth utilization."}}
{"id": "2512.03728", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03728", "abs": "https://arxiv.org/abs/2512.03728", "authors": ["Pradnya Taksande", "Shwetha Kiran", "Pranav Jha", "Prasanna Chaporkar"], "title": "AI/ML in 3GPP 5G Advanced - Services and Architecture", "comment": null, "summary": "The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.", "AI": {"tldr": "3GPP Release 19 integrates AI/ML into 5G advanced systems through two paradigms: AI for network optimization and network support for AI applications.", "motivation": "AI/ML represents a paradigm shift in technology being adopted across industries, and 3GPP has been integrating it into 5G advanced systems since Release 18. The paper aims to document the AI/ML advancements in Release 19 standardization.", "method": "The paper focuses on analyzing AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP, examining two key paradigms.", "result": "Release 19 introduces AI/ML advancements in two areas: (1) AI for network - enhancements that AI/ML brings to 5G advanced systems (e.g., resource optimization), and (2) Network for AI - enhancements made to the 5G system to support AI/ML applications (e.g., image recognition).", "conclusion": "3GPP Release 19 represents significant progress in integrating AI/ML into mobile network standards, addressing both how AI can optimize networks and how networks can better support AI applications, positioning 5G advanced systems for continued AI/ML integration in future releases."}}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.", "AI": {"tldr": "Analysis of energy efficiency challenges for synthetic benchmarks and Gromacs on HPC clusters using Intel CPUs and Nvidia GPUs, with profiling tools and best practices recommendations.", "motivation": "To address challenges in analyzing energy efficiency of computational workloads on modern HPC systems, identify pitfalls in measurement methodologies, and establish best practices for accurate energy efficiency assessment.", "method": "Conducted experiments using MPI parallelism on Intel Ice Lake and Sapphire Rapids CPUs, and Nvidia A40/A100 GPUs on Fritz and Alex HPC clusters. Used Likwid and Nvidia profiling tools for metrics and measurements.", "result": "Presented energy efficiency metrics and measurements, revealed challenges and pitfalls encountered during experimentation and analysis, and provided comparative results across different hardware configurations.", "conclusion": "Identified key challenges in energy efficiency analysis, discussed measurement pitfalls, and suggested best practices for future energy efficiency studies on HPC systems."}}
{"id": "2512.03608", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03608", "abs": "https://arxiv.org/abs/2512.03608", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "comment": null, "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "AI": {"tldr": "KVNAND: A DRAM-free architecture that stores both LLM weights and KV cache entirely in compute-enabled 3D NAND flash, enabling efficient edge deployment with long-context support.", "motivation": "Deploying large language models on edge devices faces challenges with DRAM capacity for KV cache storage as context length grows, leading to prohibitive costs and performance penalties when offloading to flash.", "method": "KVNAND leverages in-flash computing for all memory-bound operations, introduces head-group parallelism to boost throughput, employs page-level KV cache mapping to align token access patterns with flash organization, and uses a design space exploration framework to optimize weight and KV placement.", "result": "KVNAND achieves 1.98\u00d7/1.94\u00d7/2.05\u00d7 geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "conclusion": "KVNAND successfully turns flash into a practical medium for long-context KV storage by mitigating latency, energy, and reliability concerns, enabling DRAM-free edge deployment of large language models."}}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.", "AI": {"tldr": "Parallel implementation of Metropolis-Hastings with Parallel Tempering using OpenMP and CUDA achieves up to 52x speedup on CPUs and 986x speedup on GPUs.", "motivation": "MCMC methods struggle with complex configuration spaces, and Parallel Tempering improves accuracy but increases computational cost. Parallelization can counteract this slowdown and enable study of larger models.", "method": "Developed parallel implementation of Metropolis-Hastings with Parallel Tempering using OpenMP for CPU parallelization and CUDA for GPU parallelization.", "result": "Achieved maximum speed-up of 52x using OpenMP with 48 cores, and 986x speed-up with the CUDA version on GPUs.", "conclusion": "Parallel implementations enable more effective MCMC/PT techniques and serve as a benchmark for future quantum implementations of the same algorithm."}}
{"id": "2512.03616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03616", "abs": "https://arxiv.org/abs/2512.03616", "authors": ["Christian Ewert", "Amrit Sharma Poudel", "Mouadh Ayache", "Andrija Neskovic", "Rainer Buchty", "Mladen Berekovic", "Sebastian Berndt", "Saleh Mulhem"], "title": "Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State", "comment": "-", "summary": "Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.", "AI": {"tldr": "A unified hash engine supporting SHA-3 and SHAKE with byte-wise in-place partitioning and fault detection using 2D parity checks on Keccak state, achieving 100% detection of 3 faults and near 100% for more, with 3.7x better area overhead than state-of-the-art.", "motivation": "Hash functions like SHA-3 and SHAKE are key to post-quantum cryptography but need lightweight, fault-resilient implementations for reliable PQC systems, especially in resource-constrained applications.", "method": "Proposes: 1) Unified hash engine supporting SHA-3/SHAKE using byte-wise in-place partitioning of Keccak state; 2) Fault detection exploiting Keccak's cube structure with two-dimensional parity checks for state protection.", "result": "Achieves 100% detection of three Keccak state faults and near 100% for higher numbers; 3.7x improvement in area overhead; 4.5x smaller fault-resilient engine design; less than 8% area overhead when integrated into RISC-V environment.", "conclusion": "Provides a robust, lightweight fault-detection solution for protecting hash functions in resource-constrained PQC applications, outperforming state-of-the-art in area efficiency while covering all standard hash configurations."}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "OD-MoE is a distributed MoE inference framework that enables efficient deployment on memory-constrained edge devices by eliminating expert caches through on-demand expert loading with parallel loading/computation and accurate prediction.", "motivation": "MoE architectures face deployment challenges on low-cost edge devices with tight memory constraints. Existing expert offloading approaches underutilize GPU memory reserved for caching compared to dense LLMs, limiting practical deployment on resource-constrained IoT devices.", "method": "OD-MoE uses two key mechanisms: 1) parallelizing expert loading and computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead during ongoing computation. This enables just-in-time expert loading and eviction.", "result": "OD-MoE achieves 99.94% expert activation prediction accuracy (surpassing existing methods) and delivers ~75% of the decoding speed of fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. Enables MoE inference on edge nodes with <1GB GPU memory.", "conclusion": "OD-MoE eliminates the need for expert caches through on-demand expert loading, enabling practical MoE deployment on low-cost IoT devices at the edge, paving the way for LLM deployment in resource-constrained environments."}}
{"id": "2512.03781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03781", "abs": "https://arxiv.org/abs/2512.03781", "authors": ["Joscha Ilmberger", "Johannes Schemmel"], "title": "The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates", "comment": null, "summary": "The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$\u03bc$s are achieved within each backplane.", "AI": {"tldr": "The paper describes scaling the BrainScaleS-2 neuromorphic computing system using FPGA-based interconnection via an Aggregator unit to connect multiple ASICs in a rack-mounted system.", "motivation": "To scale up the BrainScaleS-2 neuromorphic computing substrate by interconnecting multiple ASICs with analog neuron/synapse circuits and digital CPUs, enabling larger neural network simulations.", "method": "Uses FPGA-based interconnection with an Aggregator unit providing up to 12 transceiver links to Node-FPGAs, plus 4 lanes for extension. Two interconnected backplanes are integrated into a 4U rack case with Ethernet switch, controller, and power supplies.", "result": "Achieved chip-to-chip latencies below 1.3\u03bcs for all spike rates within each backplane, consisting of four hops across three FPGAs.", "conclusion": "Successfully scaled the BrainScaleS-2 compute substrate through FPGA-based interconnection, demonstrating low-latency communication suitable for neuromorphic computing applications."}}
{"id": "2512.04054", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.04054", "abs": "https://arxiv.org/abs/2512.04054", "authors": ["Olasupo Ajayi", "Ryan Grant"], "title": "A Chronological Analysis of the Evolution of SmartNICs", "comment": "8 pages, 13 figures, 2 tables, Southern Africa Telecommunication Networks and Applications Conference (SATNAC) 2025", "summary": "Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.", "AI": {"tldr": "A chronological analysis of Smart NICs (SNICs) from 2010-2024 using 370 articles to examine their evolution, manufacturers, use cases, and application domains.", "motivation": "The growing demand for faster network access and the pervasive nature of Internet-enabled devices have driven the enhancement of traditional NICs to Smart NICs. However, there's ongoing debate about their exact use and applicability, especially with the incorporation of accelerators that relieve host CPUs of various tasks.", "method": "Conducted a chronological analysis using 370 articles published between 2010 and 2024 to gain insights into SNICs, examining their evolution, manufacturers, use cases, and application domains.", "result": "The analysis provides insights into SNIC evolution over 15 years, identifies key manufacturers, documents various use cases, and maps application domains where SNICs have been deployed.", "conclusion": "The study sheds light on the development trajectory of Smart NICs, helping to clarify their role and applicability in modern networking infrastructure despite ongoing debates about their exact use cases."}}
