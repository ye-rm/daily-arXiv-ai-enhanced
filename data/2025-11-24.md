<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [An Introductory Study on the Power Consumption Overhead of ERC-4337 Bundlers](https://arxiv.org/abs/2511.16890)
*Andrei Arusoaie,Claudiu-Nicu Bărbieru,Oana-Otilia Captarencu,Paul-Flavian Diac,Emanuel Onica,Cosmin-Nicolae Vârlan*

Main category: cs.PF

TL;DR: This paper analyzes the power consumption overhead of Ethereum bundlers (ERC-4337 middleware) using empirical measurements with SmartWatts monitoring system.


<details>
  <summary>Details</summary>
Motivation: To address concerns about energy consumption in blockchain networks, particularly with Ethereum's expansion via additional layers like ERC-4337 bundlers, which serve millions of requests but have limited providers.

Method: Used SmartWatts monitoring system leveraging RAPL hardware interfaces to empirically measure correlations between bundler workload and active power consumption.

Result: The study provides the first empirical analysis of bundler power consumption overhead, establishing workload-power consumption correlations.

Conclusion: Bundlers add measurable power consumption overhead to Ethereum access services, which is important for understanding the environmental impact of blockchain network expansions.

Abstract: Ethereum is currently the main blockchain ecosystem providing decentralised trust guarantees for applications ranging from finance to e-government. A common criticism of blockchain networks has been their energy consumption and operational costs. The switch from Proof-of-Work (PoW) protocol to Proof-of-Stake (PoS) protocol has significantly reduced this issue, though concerns remain, especially with network expansions via additional layers. The ERC-4337 standard is a recent proposal that facilitates end-user access to Ethereum-backed applications. It introduces a middleware called a bundler, operated as a third-party service, where part of its operational cost is represented by its power consumption. While bundlers have served over 500 million requests in the past two years, fewer than 15 official bundler providers exist, compared to over 100 regular Ethereum access providers. In this paper, we provide a first look at the active power consumption overhead that a bundler would add to an Ethereum access service. Using SmartWatts, a monitoring system leveraging Running Average Power Limit (RAPL) hardware interfaces, we empirically determine correlations between the bundler workload and its active power consumption.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: MicroEP is a novel parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts (MoE) systems through efficient token scheduling across GPUs, implemented in MicroMoE system which improves training throughput by up to 47.6% compared to state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: The dynamic nature of MoE causes load imbalance among experts, severely impacting training efficiency. Existing solutions either compromise model accuracy or introduce additional system overhead, failing to achieve fine-grained load balancing crucial for optimizing training efficiency.

Method: Proposed MicroEP, a parallelization strategy that achieves optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Implemented MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities.

Result: MicroMoE improves end-to-end training throughput by up to 47.6% compared with state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

Conclusion: MicroEP and MicroMoE successfully address the load balancing challenge in MoE systems, achieving significant performance improvements while maintaining optimal load distribution across GPUs.

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [3] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: This paper analyzes how precision, recall, and inspection frequency of performance anomaly detectors affect the trade-off between latency and resource consumption in Cloud services.


<details>
  <summary>Details</summary>
Motivation: Performance anomaly detectors in Cloud services make mistakes, and understanding which detector characteristics optimize the performance-cost trade-off is crucial for efficient resource management.

Method: The authors use Stochastic Reward Nets to model a Cloud service monitored by a performance anomaly detector, analyzing the impact of detector precision, recall, and inspection frequency.

Result: Results show that high precision and recall are not always necessary - with frequent detection, high precision alone suffices for good performance-cost trade-off, while with infrequent detection, recall becomes most important.

Conclusion: The optimal detector characteristics depend on inspection frequency: precision is key for frequent detection, while recall dominates for infrequent detection scenarios.

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)
*Jiaxun Fang,Li Zhang,Shaoyi Huang*

Main category: cs.AR

TL;DR: The paper proposes an energy-aware layer-wise compression framework for CNN accelerators that reduces MAC unit energy by 58.6% with minimal accuracy loss, outperforming existing power-aware methods.


<details>
  <summary>Details</summary>
Motivation: Current CNN accelerator compression methods use global activation models, coarse energy proxies, or layer-agnostic policies, limiting their effectiveness on real hardware due to inefficient energy optimization.

Method: Developed a layer-aware MAC energy model using per-layer activation statistics and MSB-Hamming distance grouping, combined with energy-accuracy co-optimized weight selection in quantization aware training and energy-prioritized layer-wise compression scheduling.

Result: Achieved up to 58.6% energy reduction in CNN models with only 2-3% accuracy drop, significantly outperforming state-of-the-art power-aware baselines.

Conclusion: The proposed energy-aware layer-wise compression framework effectively reduces CNN accelerator energy consumption while maintaining accuracy, demonstrating superior performance over existing methods through targeted layer-specific optimization.

Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.

</details>


### [5] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: NX-CGRA is a programmable CGRA accelerator for transformer inference that balances performance, energy efficiency, and flexibility for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Transformer workloads at the edge are increasingly diverse and complex, creating challenges in balancing performance, energy efficiency, and architectural flexibility.

Method: Uses a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability to efficiently execute both linear and non-linear transformer functions across varied kernel patterns.

Result: Evaluation with real-world transformer benchmarks shows high overall efficiency and favorable energy-area tradeoffs across different operation classes.

Conclusion: NX-CGRA demonstrates potential as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [6] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: DISCA is a novel digital in-memory stochastic computing architecture that uses compressed quasi-stochastic Bent-Pyramid data format to achieve high energy efficiency (3.59 TOPS/W per bit) for AI matrix multiplication workloads at the edge.


<details>
  <summary>Details</summary>
Motivation: AI applications are moving to edge devices with hardware constraints, while conventional architectures face memory wall issues and Moore's Law limitations. Existing in-memory computing solutions suffer from degraded benefits due to design limitations.

Method: Proposed DISCA architecture using compressed quasi-stochastic Bent-Pyramid data format, combining computational simplicity of analog computing with scalability and reliability of digital systems.

Result: Post-layout modeling shows 3.59 TOPS/W per bit energy efficiency at 500 MHz using 180nm CMOS technology, significantly improving energy efficiency for matrix multiplication by orders of magnitude compared to counterparts.

Conclusion: DISCA provides a promising solution for energy-efficient AI computing at the edge, overcoming limitations of both analog and digital in-memory computing architectures while maintaining computational efficiency and system reliability.

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [7] [MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing](https://arxiv.org/abs/2511.17418)
*Houji Zhou,Ling Yang,Zhiwei Zhou,Yi Li,Xiangshui Miao*

Main category: cs.AR

TL;DR: MemIntelli is an end-to-end simulation framework for memristive in-memory computing that enables pre-verification of intelligent applications through device/circuit modeling and flexible variable-precision computing compatible with NumPy and PyTorch.


<details>
  <summary>Details</summary>
Motivation: To address the reliability challenges in memristive in-memory computing caused by non-ideal effects in devices and peripheral circuits, and to enable efficient software-hardware co-simulation for algorithm development.

Method: Uses mathematical functions for device/circuit abstraction through equivalent circuit modeling, implements flexible variable-precision IMC supporting integer and floating data with bit-slicing, and integrates with NumPy and PyTorch for application compatibility.

Result: Successfully demonstrated diverse intelligent algorithms including equation solving, data clustering, wavelet transformation, and neural network training/inference, showcasing robust processing capabilities.

Conclusion: MemIntelli provides a comprehensive simulation tool that facilitates co-design of IMC systems from device to application level, enabling reliable pre-verification of intelligent applications on memristive devices.

Abstract: Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.

</details>
