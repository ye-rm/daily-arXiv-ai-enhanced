<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 4]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Efficient Data-Driven Production Scheduling in Pharmaceutical Manufacturing](https://arxiv.org/abs/2602.13668)
*Ioannis Balatsos,Athanasios Liakos,Panagiotis Karakostas,Tao Song,Vassilios Pantazopoulos,Christos Papalitsas*

Main category: cs.PF

TL;DR: A constraint-based optimization framework for pharmaceutical job shop scheduling that incorporates site-specific rules (resource calendars, sequence-dependent cleaning) and achieves significant improvements over simplified reference plans.


<details>
  <summary>Details</summary>
Motivation: Industrial pharmaceutical scheduling needs to handle complex site-specific constraints like resource calendars, maintenance schedules, and sequence-dependent cleaning times that are often ignored in simplified models, leading to impractical schedules.

Method: Data-driven constraint programming framework using open source solver, capturing fixed routings, designated machines, resource calendars with weekends/maintenance, and campaign sequencing with sequence-dependent cleaning times from site tables.

Result: Significant improvements over reference plans: makespan reductions of 88.1%, 77.6%, 54.9% and total tardiness reductions of 72.1%, 58.7%, 18.2% across three instances (10, 30, 84 jobs). Composite objectives further reduced late jobs with minimal makespan impact.

Conclusion: Compact constraint programming can deliver feasible, transparent schedules respecting site rules while improving due date adherence on real industrial data, proving optimality for small cases and achieving low gaps for larger instances.

Abstract: This paper develops a data-driven, constraint-based optimization framework for a complex industrial job shop scheduling problem variant in pharmaceutical manufacturing. The formulation captures fixed routings and designated machines, explicit resource calendars with weekends and planned maintenance, and campaign sequencing through sequence-dependent cleaning times derived from site tables. The model is implemented with an open source constraint solver and evaluated on deterministic snapshots from a solid oral dosage facility under three objective formulations: makespan, makespan plus total tardiness, and makespan plus average tardiness. On three industrial instances of increasing size (10, 30, and 84 jobs) the proposed schedules dominate reference plans that solve a simplified variant without the added site rules. Makespan reductions reach \(88.1\%\), \(77.6\%\), and \(54.9\%\) and total tardiness reductions reach \(72.1\%\), \(58.7\%\), and \(18.2\%\), respectively. The composite objectives further decrease late job counts with negligible makespan change on the smaller instances and a modest increase on the largest instance. Optimality is proven on the small case, with relative gaps of \(0.77\%\) and \(14.92\%\) on the medium and large cases under a fixed time limit. The results show that a compact constraint programming formulation can deliver feasible, transparent schedules that respect site rules while improving adherence to due dates on real industrial data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics](https://arxiv.org/abs/2602.13789)
*Zhengyan Chu*

Main category: cs.DC

TL;DR: TEG proposes a thermodynamic governance paradigm for Exascale computing, replacing centralized orchestration with decentralized Langevin agents and holographic potential fields to manage stochastic AI workloads.


<details>
  <summary>Details</summary>
Motivation: Current cloud orchestration (like Kubernetes) faces fundamental physical limits at Exascale due to O(N) latency scaling, head-of-line blocking, and inability to handle stochastic chaos of next-gen AI workloads.

Method: Models compute cluster as dissipative structure far from equilibrium; introduces TEG with decentralized Langevin Agents executing Brownian motion on Holographic Potential Field; uses Landau Phase Transition for stability; implements Token Evaporation for entropy dissipation; employs Dual-Number Damping, OS-level Airlock Mutex, and High-Order Control Barrier Functions.

Result: Reduces decision complexity to O(1); converts OOM catastrophic failures to manageable Glassy States; mathematically guarantees safety under high inertia; enables emergent order rather than deterministic control for Exascale scalability.

Conclusion: Emergent order, not deterministic control, is necessary for Exascale scalability; TEG demonstrates thermodynamic governance as viable paradigm shift for managing stochastic chaos in next-generation AI workloads at massive scale.

Abstract: As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing "Newtonian" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, "Head-of-Line" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability.

</details>


### [3] [ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies](https://arxiv.org/abs/2602.14107)
*Yuze Liu,Shibo Chu,Tiehua Zhang,Hao Zhou,Zhishu Shen,Jinze Wang,Jianzhong Qi,Feng Xia*

Main category: cs.DC

TL;DR: ML-ECS is a collaborative multimodal learning framework for edge-cloud environments that addresses modality and model heterogeneity through cross-modal alignment, adaptive tuning, robust aggregation, and efficient knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Real-world edge environments face challenges in collaborative multimodal learning due to modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different encoders/fusion modules), which hinder effective privacy-preserving deployment of foundation models.

Method: Four-component framework: 1) Cross-modal contrastive learning (CCL) to align modality representations in shared latent space, 2) Adaptive multimodal tuning (AMT) to preserve domain-specific knowledge, 3) Modality-aware model aggregation (MMA) for robust aggregation while mitigating missing modality noise, 4) SLM-enhanced CCL (SE-CCL) for bidirectional cloud-edge knowledge transfer.

Result: Outperforms state-of-the-art baselines under varying modality availability, achieving 5.44% to 12.08% improvements in Rouge-LSum, improves both client- and server-side performance, and achieves high communication efficiency requiring only 0.65% of total parameter volume by communicating only low-rank LoRA parameters and fused representations.

Conclusion: ML-ECS effectively addresses heterogeneity challenges in edge-cloud multimodal learning, enabling efficient, privacy-preserving collaborative training while maintaining high performance across diverse modality combinations and model architectures.

Abstract: Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.

</details>


### [4] [Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302)
*Chunlin Tian,Kahou Tam,Yebo Wu,Shuaihang Zhong,Li Li,Nicholas D. Lane,Chengzhong Xu*

Main category: cs.DC

TL;DR: Floe: A hybrid federated learning framework combining cloud LLMs with edge SLMs for low-latency, privacy-preserving inference in real-time systems.


<details>
  <summary>Details</summary>
Motivation: Address challenges of deploying LLMs in real-time systems due to high computational demands and privacy concerns, enabling efficient edge deployment while preserving user privacy.

Method: Hybrid federated learning framework with cloud-based black-box LLM and lightweight SLMs on edge devices, using heterogeneity-aware LoRA adaptation for efficient deployment and logit-level fusion for real-time coordination.

Result: Extensive experiments show Floe enhances privacy and personalization, improves model performance, and reduces inference latency on edge devices under real-time constraints compared to baselines.

Conclusion: Floe provides an effective solution for deploying LLMs in latency-sensitive, resource-constrained environments by balancing cloud knowledge with edge privacy and efficiency.

Abstract: Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.

</details>


### [5] [Efficient Multi-round LLM Inference over Disaggregated Serving](https://arxiv.org/abs/2602.14516)
*Wenhao He,Youhe Jiang,Penghao Zhao,Quanqing Xu,Eiko Yoneki,Bin Cui,Fangcheng Fu*

Main category: cs.DC

TL;DR: AMPD is a new disaggregated serving framework for multi-round LLM inference that optimizes resource allocation and scheduling for prefill-decode workloads to maximize SLO attainment.


<details>
  <summary>Details</summary>
Motivation: Multi-round LLM workflows (autonomous agents, iterative retrieval) create challenges for existing prefill-decode disaggregation systems, which fail to handle the interleaved workload patterns and incremental prefill workloads effectively.

Method: AMPD coordinates prefill workloads based on real-time workloads by adaptively determining where to execute them and how to schedule them. It includes a tailored planning algorithm for optimal resource allocation and parallel strategies for prefill and decode phases.

Result: Empirical results show AMPD substantially improves Service Level Objective (SLO) attainment compared to state-of-the-art baselines.

Conclusion: AMPD provides an effective solution for serving LLMs in multi-round inference scenarios by addressing the limitations of existing prefill-decode disaggregation systems through adaptive workload coordination and optimized resource planning.

Abstract: With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.
  In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.

</details>


### [6] [Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement](https://arxiv.org/abs/2602.14704)
*Zong Yu Lee,Xueyan Tang*

Main category: cs.DC

TL;DR: This paper evaluates state-of-the-art algorithms for virtual machine placement formulated as a MinUsageTime Dynamic Vector Bin Packing problem, testing them in non-clairvoyant, clairvoyant, and learning-augmented online settings using real-world Azure datasets.


<details>
  <summary>Details</summary>
Motivation: Virtual machine placement is critical for efficient resource utilization in cloud data centers. The MinUsageTime DVBP formulation aims to minimize total physical machine usage time, but existing algorithms need comprehensive evaluation across different information settings (unknown, known, and predicted VM lifetimes).

Method: The paper evaluates existing MinUsageTime DVBP algorithms and develops new algorithms/enhancements. Empirical experimentation is conducted using real-world Microsoft Azure datasets across three settings: non-clairvoyant (unknown durations), clairvoyant (known durations), and learning-augmented (predicted durations).

Result: Experimental results from Azure datasets provide insights into algorithm structures and identify promising design elements that perform well in practice. The comparison reveals how different approaches fare across the three information settings.

Conclusion: The study provides valuable insights into effective virtual machine placement algorithms, highlighting successful design elements and offering guidance for practical implementation in cloud computing environments.

Abstract: Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory](https://arxiv.org/abs/2602.13434)
*Maccoy Merrell,Daniel Puckett,Gino Chacon,Jeffrey Stuecheli,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: ORAP prefetcher reduces DRAM activation rates by caching rowbuffer contents to minimize Rowhammer mitigation overheads, achieving 4.6% speedup over existing prefetchers in RFM systems.


<details>
  <summary>Details</summary>
Motivation: Rowhammer mitigations in DDR5 (PRAC/RFM) interact negatively with hardware prefetchers, reducing prefetching benefits due to increased DRAM activations that trigger mitigation overheads.

Method: Proposes Optimized Row Access Prefetcher (ORAP) that caches large portions of DRAM rowbuffer contents in last-level cache to reduce future activations, working with Berti prefetcher.

Result: ORAP reduces DRAM activation rates by 51.3%, achieves 4.6% speedup over Berti+SPP-PPF in RFM systems, and reduces energy overheads by 11.8% under PRAC mitigations.

Conclusion: ORAP effectively addresses the cross-interaction between hardware prefetchers and Rowhammer mitigations by reducing activation rates, improving performance and energy efficiency in modern memory systems.

Abstract: Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.

</details>


### [8] [Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits](https://arxiv.org/abs/2602.13825)
*Paras Tiwari,Narendra Singh Dhakad,Shalu Rani,Sanjay Kumar,Themis Prodromakis*

Main category: cs.AR

TL;DR: Memristor-based logic gates and sequential circuits implemented with 90nm CMOS show significant performance improvements over state-of-the-art designs.


<details>
  <summary>Details</summary>
Motivation: To develop more area-efficient, power-efficient, and faster sequential logic circuits using memristor technology integrated with CMOS.

Method: Implemented memristor-driven fundamental logic gates (NOT, AND, NAND, OR, NOR, XOR) and sequential circuits (D, T, JK, SR flip-flops) using SPECTRE in Cadence Virtuoso with 90nm CMOS technology. Used experimentally validated Y2O3-based memristor framework with low variability.

Result: Performance metrics show reductions of ~24% in area, 60% in power, and 58% in delay compared to other state-of-the-art sequential circuits.

Conclusion: Memristor-based designs significantly improve logic circuit performance, demonstrating potential for low-power, low-cost, ultrafast, and compact circuit applications.

Abstract: In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.

</details>


### [9] [ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute](https://arxiv.org/abs/2602.14262)
*Siddhartha Raman Sundara Raman,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: A near-memory GPU architecture achieves 6-16× speedup and 6-13× energy savings across diverse workloads compared to MIAOW GPU, with custom circuits for sparsity and softmax, reconfigurable INT16 compute, and 4.5× speedup on ABI-enabled MI300/Blackwell systems.


<details>
  <summary>Details</summary>
Motivation: To address the computational and energy efficiency challenges of diverse modern workloads (CNNs, GCNs, Linear Programming, LLMs, Ising models) by developing a tightly integrated near-memory GPU architecture that reduces data movement and improves energy efficiency.

Method: Developed a unified near-memory GPU architecture with custom sparsity-aware near-memory circuits and lightweight softmax circuits, supporting reconfigurable compute up to INT16 with dynamic resolution updates, and scalable across problem sizes.

Result: Achieved 6-16× speedup and 6-13× energy savings across various workloads compared to MIAOW GPU, with custom circuits providing 1.5× and 1.6× energy savings respectively. ABI-enabled MI300 and Blackwell systems showed about 4.5× speedup over baseline systems.

Conclusion: The tightly integrated near-memory GPU architecture demonstrates significant performance and energy efficiency improvements across diverse computational workloads, with custom circuits and reconfigurable compute capabilities enabling substantial gains over existing GPU designs.

Abstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.

</details>


### [10] [Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators](https://arxiv.org/abs/2602.14393)
*Zongle Huang,Hongyang Jia,Kaiwei Zou,Yongpan Liu*

Main category: cs.AR

TL;DR: Scope is a merged pipeline framework for neural network accelerators with multi-chip-module architectures that addresses computation underutilization and communication overheads by jointly optimizing multiple layers rather than deploying them separately.


<details>
  <summary>Details</summary>
Motivation: Traditional parallelization schemes for NN inference on MCM architectures (intra-layer parallelism and inter-layer pipelining) fail to address both computation resource underutilization and off-chip communication overheads simultaneously. Existing works deploy layers separately rather than considering them jointly, leading to compromises between computation and communication that limit scalability.

Method: Scope introduces a merged pipeline framework that incorporates a multi-layer dimension, relaxing tradeoffs between computation, communication and memory costs. To handle the increased complexity of design space exploration, the authors develop search algorithms that achieve exponential-to-linear complexity reduction while identifying top-performing solutions.

Result: Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches. The search algorithms identify solutions ranking in the top 0.05% of performance.

Conclusion: The proposed Scope framework successfully addresses the limitations of traditional MCM parallelization schemes by jointly optimizing multiple layers, achieving significant throughput improvements and better scalability through efficient design space exploration.

Abstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.
  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [11] [Robustness Verification of Binary Neural Networks: An Ising and Quantum-Inspired Framework](https://arxiv.org/abs/2602.13536)
*Rahul Singh,Seyran Saeedi,Zheng Zhang*

Main category: cs.ET

TL;DR: BNN robustness verification is computationally hard; paper proposes Ising/quantum-inspired framework to formulate it as QUBO problem solvable by quantum/Ising machines.


<details>
  <summary>Details</summary>
Motivation: Binary neural networks are important for edge computing but verifying their robustness against adversarial attacks is computationally challenging due to combinatorial nature of the decision problem.

Method: Propose Ising- and quantum-inspired framework that formulates BNN robustness verification as Quadratic Constrained Boolean Optimization (QCBO) problem, then transforms it into Quadratic Unconstrained Boolean Optimization (QUBO) instance solvable by Ising/quantum-inspired solvers.

Result: Demonstrate feasibility on binarized MNIST using free energy machine solver and simulated annealing; show deployment on quantum annealing and digital annealing platforms.

Conclusion: Quantum-inspired computing and Ising computing offer promising pathway toward trustworthy AI systems by enabling efficient BNN robustness verification.

Abstract: Binary neural networks (BNNs) are increasingly deployed in edge computing applications due to their low hardware complexity and high energy efficiency. However, verifying the robustness of BNNs against input perturbations, including adversarial attacks, remains computationally challenging because the underlying decision problem is inherently combinatorial. In this paper, we propose an Ising- and quantum-inspired framework for BNN robustness verification. We show that, for a broad class of BNN architectures, robustness verification can be formulated as a Quadratic Constrained Boolean Optimization (QCBO) problem and subsequently transformed into a Quadratic Unconstrained Boolean Optimization (QUBO) instance amenable to Ising and quantum-inspired solvers. We demonstrate the feasibility of this formulation on binarized MNIST by solving the resulting QUBOs with a free energy machine (FEM) solver and simulated annealing. We also show the deployment of this framework on quantum annealing and digital annealing platforms. Our results highlight the potential of quantum-inspired computing and Ising computing as a pathway toward trustworthy AI systems.

</details>


### [12] [From Snapshot Sensing to Persistent EM World Modeling: A Generative-Space Perspective for ISAC](https://arxiv.org/abs/2602.13554)
*Pin-Han Ho,Haoran Mei,Limei Peng,Yiming Miao,Kairan Liang,Yan Jiao*

Main category: cs.ET

TL;DR: The paper introduces a generative-space framework for mmWave sensing that enables persistent electromagnetic world modeling through controlled traversal of low-dimensional excitation spaces, decoupling spatial observability from rigid antenna arrays.


<details>
  <summary>Details</summary>
Motivation: Current mmWave sensing solutions are limited to snapshot-based parameter estimation and rely on hardware-intensive architectures, making scalable and persistent world modeling difficult to achieve. There's a need for more flexible and scalable sensing approaches.

Method: The paper proposes a generative-space framework where sensing is realized through controlled traversal of a low-dimensional excitation space spanning frequency, waveform, and physical embodiment. This decouples spatial observability from rigid antenna arrays. A specific implementation called MRC-FaA-CAF uses multiple FMCW sources coordinating frequency-selective modules along guided-wave backbones.

Result: The generative-space-driven sensing achieves update rates comparable to phased arrays while avoiding dense RF replication and latency penalties of TDM-MIMO systems. It enables interference-free excitation, preserves beat-frequency separability, and maintains low calibration overhead.

Conclusion: Generative-space-driven sensing provides a practical architectural foundation for mmWave systems that can move beyond snapshot sensing toward persistent electromagnetic world modeling, enabling flexible and scalable sensing-by-design radios.

Abstract: Electromagnetic (EM) world modeling is emerging as a foundational capability for environment-aware and embodiment-enabled wireless systems. However, most existing mmWave sensing solutions are designed for snapshot-based parameter estimation and rely on hardware-intensive architectures, making scalable and persistent world modeling difficult to achieve. This article rethinks mmWave sensing from a system-level perspective and introduces a generative-space framework, in which sensing is realized through controlled traversal of a low-dimensional excitation space spanning frequency, waveform, and physical embodiment. This perspective decouples spatial observability from rigid antenna arrays and transmit-time multiplexing, enabling flexible and scalable sensing-by-design radios. To illustrate the practicality of this framework, we present a representative realization called Multi-RF Chain Frequency-as-Aperture Clip-on Aperture Fabric (MRC-FaA-CAF), where multiple FMCW sources coordinate frequency-selective modules distributed along guided-wave backbones. This architecture enables interference-free excitation, preserves beat-frequency separability, and maintains low calibration overhead. Case studies show that generative-space-driven sensing can achieve update rates comparable to phased arrays while avoiding dense RF replication and the latency penalties of TDM-MIMO systems. Overall, this work positions generative-space-driven sensing as a practical architectural foundation for mmWave systems that move beyond snapshot sensing toward persistent EM world modeling.

</details>


### [13] [Probabilistic approximate optimization using single-photon avalanche diode arrays](https://arxiv.org/abs/2602.13943)
*Ziyad Alsawidan,Abdelrahman S. Abdelrahman,Md Sakibur Sajal,Shuvro Chowdhury,Kai-Chun Lin,Hunter Guthrie,Sanjay Seshan,Shawn Blanton,Flaviano Morone,Marc Dandin,Kerem Y. Camsari,Tathagata Srimani*

Main category: cs.ET

TL;DR: PAOA algorithm implemented on CMOS pgSPAD array learns around device variations for combinatorial optimization, achieving high approximation ratios on 26-spin problems.


<details>
  <summary>Details</summary>
Motivation: Specialized hardware for combinatorial optimization typically samples from fixed energy landscapes, but PAOA takes a variational approach that can accommodate device non-idealities in nanoscale systems.

Method: Implemented PAOA on 64×64 perimeter-gated SPAD array in 0.35μm CMOS, using devices with asymmetric Gompertz-type activation functions. Algorithm learns variational parameters from samples rather than calibrating devices, absorbing device variations and mismatches.

Result: Achieved high approximation ratios on canonical 26-spin Sherrington-Kirkpatrick instances with 2p parameters (p up to 17 layers). pgSPAD-based inference closely tracks CPU simulations.

Conclusion: Variational learning can accommodate non-idealities in nanoscale devices, providing a practical path toward larger-scale, CMOS-compatible probabilistic computers.

Abstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $μ$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.

</details>


### [14] [Introduction to Digital Twins for the Smart Grid](https://arxiv.org/abs/2602.14256)
*Xiaoran Liu,Istvan David*

Main category: cs.ET

TL;DR: Digital twins are essential for smart grids as high-fidelity virtual replicas that enable safe design/testing and real-time operational optimization.


<details>
  <summary>Details</summary>
Motivation: Smart grids are complex engineered systems combining physical and software components, creating unique design and operation challenges that require advanced technological solutions.

Method: Digital twins serve as unified technological platforms providing high-fidelity in-silico replicas of physical components for design, testing, operation, and maintenance.

Result: Digital twins enable safe, cost-efficient experimentation in design/verification phases and automated load balancing through real-time simulation in operation phases.

Conclusion: Digital twins are positioned as crucial technological components in smart grids due to their comprehensive benefits across the entire system lifecycle.

Abstract: This chapter provides an introduction to the foundations of digital twins and makes the case for employing them in smart grids. As engineered systems become more complex and autonomous, digital twin technology gains importance as the unified technological platform for design, testing, operation, and maintenance. Smart grids are prime examples of such complex systems, in which unique design and operation challenges arise from the combination of physical and software components. As high-fidelity in-silico replicas of physical components, digital twins provide safe and cost-efficient experimentation facilities in the design and verification phase of smart grids. In the operation phase of smart grids, digital twins enable automated load balancing of grids through real-time simulation and decision-making. These, and an array of similar benefits, position digital twins as crucial technological components in smart grids.

</details>
