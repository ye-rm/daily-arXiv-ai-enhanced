<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: RL-based routing achieves 94% reachability in faulty EJ networks, outperforming greedy routing (10%) and approaching Dijkstra's theoretical optimum (54%) while maintaining high throughput through implicit load balancing.


<details>
  <summary>Details</summary>
Motivation: As many-core architectures increase in density, interconnection networks need both high performance and fault resilience. EJ networks offer superior topological properties but challenge traditional routing heuristics under fault conditions, requiring adaptive solutions that can handle clustered failures.

Method: The paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. The RL agent uses a multi-objective reward function that penalizes fault proximity and rewards path efficiency to learn navigation around clustered failures.

Result: With nine faulty nodes, greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, and sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing.

Conclusion: RL-based adaptive policies bridge the gap between greedy routing's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring global topology knowledge or the computational overhead of optimal algorithms.

Abstract: The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.

</details>


### [2] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: maxwait is a coordination mechanism that makes explicit tradeoffs between timing requirements and consistency in distributed time-sensitive systems, subsuming classical methods while enabling real-time behavior.


<details>
  <summary>Details</summary>
Motivation: Distributed time-sensitive systems need to balance timing requirements (availability) and consistency while dealing with communication delays and synchronization uncertainty. Existing approaches don't provide explicit, configurable tradeoffs between these competing concerns.

Method: The paper presents maxwait, a simple coordination mechanism implemented as an extension of the Lingua Franca coordination language. It enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

Result: maxwait subsumes classical distributed system methods (PTIDES, Chandy-and-Misra, Jefferson's Time-Warp, Lamport's fault detection) and can realize many distributed system patterns (LET, publish-subscribe, actors, CRDTs, RPC with futures). It adds better timing control, bounded time fault detection, and determinism options.

Conclusion: maxwait provides a unified semantic framework for distributed time-sensitive systems that makes timing-consistency tradeoffs explicit and configurable, while offering better control over timing, bounded fault detection, and determinism options compared to existing approaches.

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [3] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: ZipMoE is an efficient on-device MoE serving system that reduces memory footprint and improves inference performance on edge devices through caching-scheduling co-design.


<details>
  <summary>Details</summary>
Motivation: MoE architectures enhance LLM expressive power but have prohibitive memory requirements that hinder deployment on resource-constrained edge devices, especially when model behavior must be preserved without lossy quantization.

Method: ZipMoE uses a caching-scheduling co-design that exploits synergy between edge device hardware properties and statistical redundancy in MoE parameters. It shifts on-device MoE inference from I/O-bound to compute-centric workflow enabling efficient parallelization.

Result: ZipMoE achieves up to 72.77% inference latency reduction and up to 6.76× higher throughput than state-of-the-art systems on representative edge computing platforms using popular open-source MoE models and real-world workloads.

Conclusion: ZipMoE provides an efficient and semantically lossless on-device MoE serving system that addresses memory constraints of edge devices while maintaining model behavior without quantization, significantly improving inference performance.

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [4] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: Ira is a framework that accelerates backup replay in primary-backup replication by transmitting compact hints about future access patterns from the primary to backups, achieving 25× median speedup for Ethereum block execution.


<details>
  <summary>Details</summary>
Motivation: In primary-backup replication, consensus latency is bounded by backup replay time. The primary already knows future access patterns from executing transactions, which could optimize backup replay if shared efficiently.

Method: Ira transmits compact hints alongside transaction batches. Ira-L is a concrete protocol for Ethereum that provides hints containing working set keys and metadata per key indicating which table to read from, enabling efficient prefetching during block replay.

Result: Hints add only 47 KB median compressed per block (~5% overhead). Primary overhead is 28.6% wall-time (10.9% direct hint cost). Backup achieves 25× median per-block speedup, reducing aggregate replay from 6.5 hours to 16 minutes (23.6× wall-time speedup) with 16 prefetch threads.

Conclusion: Ira demonstrates that transmitting compact hints about future access patterns from primary to backups can dramatically accelerate backup replay in primary-backup replication systems, with practical benefits shown for Ethereum block execution.

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [5] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: EWSJF is an adaptive request-level scheduler for LLM serving that improves throughput by 30% and reduces short request latency by 4x compared to FCFS by learning workload structure in real-time.


<details>
  <summary>Details</summary>
Motivation: Standard FCFS scheduling suffers from head-of-line blocking when serving mixed workloads (short interactive queries + long batch requests), leading to high tail latency and underutilized hardware in LLM serving.

Method: EWSJF integrates four components: 1) Refine-and-Prune unsupervised partitioning to discover performance-homogeneous request groups, 2) Dynamic Queue Routing for request assignment, 3) Density-Weighted Scoring for context-aware prioritization balancing urgency and fairness, and 4) Bayesian Meta-Optimization for continuous parameter tuning based on live performance feedback.

Result: Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS.

Conclusion: Adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving, and EWSJF demonstrates significant improvements in both throughput and latency for mixed workloads.

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [6] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: SA-PSKY is a self-adaptive framework for distributed edge-cloud systems that uses reinforcement learning to dynamically optimize probabilistic skyline query processing, reducing communication overhead by 60% and response time by 40% compared to static threshold methods.


<details>
  <summary>Details</summary>
Motivation: Traditional distributed PSKY methods use static thresholds that are ill-suited for volatile edge computing environments, causing communication bottlenecks or excessive computational latency. The exponential growth of sensor data at network edges requires more adaptive solutions.

Method: SA-PSKY formalizes dynamic threshold adjustment as a continuous Markov Decision Process and uses a Deep Deterministic Policy Gradient agent to autonomously optimize filtering intensities in real-time by analyzing multi-dimensional system states (data arrival rates, uncertainty distributions, resource availability).

Result: SA-PSKY outperforms state-of-the-art static and heuristic baselines, achieving up to 60% reduction in communication overhead and 40% reduction in total response time, while maintaining robust scalability across diverse data distributions.

Conclusion: The SA-PSKY framework successfully addresses the resource conflict in edge-cloud systems by providing self-adaptive threshold optimization, making it suitable for the volatile and heterogeneous nature of IoE environments.

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>


### [7] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: The paper provides theoretical justification for using Gaussian Belief Propagation in non-Gaussian problems by proving that variable beliefs converge to Gaussian distributions under certain conditions.


<details>
  <summary>Details</summary>
Motivation: Gaussian Belief Propagation (GBP) is widely used in practice for its efficiency, even in non-Gaussian problems, but lacks theoretical guarantees for when Gaussian approximations are valid in such scenarios.

Method: The authors leverage the Central Limit Theorem to mathematically prove that variable beliefs under BP converge to Gaussian distributions in complex, loopy factor graphs that obey four key assumptions, and validate this experimentally in a stereo depth estimation task.

Result: Theoretical proof shows convergence to Gaussian distributions under specified conditions, and experimental results confirm that variable beliefs become increasingly Gaussian after just a few BP iterations in stereo depth estimation.

Conclusion: The paper provides theoretical justification for the empirical success of GBP in non-Gaussian problems, establishing conditions under which Gaussian approximations are valid in sparsely-connected factor graphs common in spatial AI applications.

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control](https://arxiv.org/abs/2601.21222)
*Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: FireFly-P is an FPGA-based hardware accelerator implementing a novel plasticity algorithm for Spiking Neural Networks, enabling real-time adaptive control for robotics with ultra-low latency and power consumption.


<details>
  <summary>Details</summary>
Motivation: To harness the biologically plausible learning capability of SNNs for robotics, enabling unsupervised adaptation without backpropagation overhead, and creating adaptive control systems suitable for dynamic, unstructured environments.

Method: Developed FireFly-P, an FPGA-based hardware accelerator implementing a novel plasticity algorithm with on-chip plasticity processing, achieving end-to-end latency of 8μs for both inference and plasticity updates.

Result: Implemented on Cmod A7-35T FPGA consuming only 0.713W and ~10K LUTs, demonstrating robust performance in dynamic environments with rapid adaptation to unseen scenarios.

Conclusion: Hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems for resource-constrained embedded robotic platforms.

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$μ$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.

</details>


### [9] [Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios](https://arxiv.org/abs/2601.21584)
*Pin-Han Ho,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Haoran Mei,Wei Duan*

Main category: cs.AR

TL;DR: FaA repurposes frequency agility in wireless radios as a virtual sensing aperture, enabling near-field perception with minimal hardware complexity by shifting spatial sampling from antenna to frequency domain.


<details>
  <summary>Details</summary>
Motivation: Current mmWave sensing solutions rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes, while future 6G needs native ISAC support.

Method: Frequency-as-Aperture (FaA) uses a single RF chain with frequency-scanning leaky-wave antenna, reusing LO frequency sweeps for wideband communication to achieve 2D spatial sensing by embedding spatial fingerprints directly into the communication RF chain.

Result: FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical constraints.

Conclusion: Near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

Abstract: Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [Sustainable Open-Source AI Requires Tracking the Cumulative Footprint of Derivatives](https://arxiv.org/abs/2601.21632)
*Shaina Raza,Iuliia Eyriay,Ahmed Y. Radwan,Nate Lesperance,Deval Pandya,Sedef Akinli Kocak,Graham W. Taylor*

Main category: cs.ET

TL;DR: The paper proposes Data and Impact Accounting (DIA), a transparency framework to track and report environmental impacts (carbon, water) across AI model lineages and derivatives, addressing the sustainability gap in open-source AI.


<details>
  <summary>Details</summary>
Motivation: Current open-source AI scaling creates millions of model artifacts with unmeasured environmental impacts. Compute efficiency alone is insufficient for sustainability because lower per-run costs can accelerate deployment and increase aggregate footprint. There's no consistent measurement or disclosure of energy, water, and emissions across derivative lineages, making ecosystem-level impacts invisible.

Method: Proposes Data and Impact Accounting (DIA) - a lightweight, non-restrictive transparency layer with three components: 1) Standardizes carbon and water reporting metadata, 2) Integrates low-friction measurement into common training and inference pipelines, 3) Aggregates reports through public dashboards to summarize cumulative impacts across releases and derivatives.

Result: DIA framework makes derivative costs visible and supports ecosystem-level accountability while preserving openness. The approach enables tracking impacts across entire model lineages rather than just base models.

Conclusion: Sustainable open-source AI requires coordination infrastructure to track environmental impacts across model lineages. DIA provides a practical transparency solution that can make derivative costs visible and support ecosystem-level accountability without restricting openness.

Abstract: Open-source AI is scaling rapidly, and model hubs now host millions of artifacts. Each foundation model can spawn large numbers of fine-tunes, adapters, quantizations, merges, and forks. We take the position that compute efficiency alone is insufficient for sustainability in open-source AI: lower per-run costs can accelerate experimentation and deployment, increasing aggregate environmental footprint unless impacts are measurable and comparable across derivative lineages. However, the energy use, water consumption, and emissions of these derivative lineages are rarely measured or disclosed in a consistent, comparable manner, leaving ecosystem-level impact largely invisible. We argue that sustainable open-source AI requires coordination infrastructure that tracks impacts across model lineages, not only base models. We propose Data and Impact Accounting (DIA), a lightweight, non-restrictive transparency layer that (i) standardizes carbon and water reporting metadata, (ii) integrates low-friction measurement into common training and inference pipelines, and (iii) aggregates reports through public dashboards to summarize cumulative impacts across releases and derivatives. DIA makes derivative costs visible and supports ecosystem-level accountability while preserving openness. https://vectorinstitute.github.io/ai-impact-accounting/

</details>


### [11] [Optimal Energy-Aware Service Management in Future Networks with a Gamified Incentives Mechanism](https://arxiv.org/abs/2601.21846)
*Konstantinos Varsos,Adamantia Stamou,George D. Stamoulis,Vasillios A. Siris*

Main category: cs.ET

TL;DR: Gamified incentives and user acceptance modeling encourage energy-efficient behaviors in video streaming, with service providers optimizing incentives and game parameters to reduce energy consumption while maintaining QoE.


<details>
  <summary>Details</summary>
Motivation: Address rising energy demands in ICT infrastructures by engaging users in sustainable practices while maintaining acceptable Quality of Experience (QoE) levels.

Method: Combines gamified incentives with user acceptance modeling, using environmental sensitivity factors and private incentive thresholds. Implements serious-game mechanism with positive reinforcement, top-K/bottom-M rankings for peer comparison. Uses Stackelberg game formulation where service provider optimizes incentives and game parameters under budget constraints.

Result: Simulations show gamification significantly boosts user participation and energy savings when incentive and game parameters are optimally chosen. Provides measurable benefits including reduced high-bitrate traffic and increased participation in energy-saving behaviors while considering user satisfaction.

Conclusion: Gamification with optimized incentives effectively transforms passive acceptance into active engagement for energy-efficient behaviors in video streaming, offering providers proactive application-level control over energy consumption while balancing user satisfaction and QoE.

Abstract: As energy demands surge across ICT infrastructures, service providers must engage users in sustainable practices while maintaining the Quality of Experience (QoE) at acceptable levels. In this paper, we introduce such an approach, leveraging gamified incentives and a model for user's acceptance on incentives, thus encouraging energy-efficient behaviors such as adaptive bitrate streaming. Each user is characterized by an environmental sensitivity factor and a private incentive threshold, shaping probabilistic responses to energy-saving offers. A serious-game mechanism based on positive behavioral reinforcement and rewards of the users, due to their inclusion in top-K and bottom-M rankings, fosters peer comparison and competition, thus transforming passive acceptance into active engagement. Moreover, within a Stackelberg game formulation, the video streaming service provider--acting as the strategic leader--optimizes both incentive levels and game parameters to achieve network-wide energy and traffic reductions, while adhering to budgetary constraints. This structured approach empowers providers with proactive, application-level control over energy consumption, offering them measurable benefits such as reduced high-bitrate traffic and increased participation in energy-saving behaviors, while also considering user satisfaction. The results of our simulations show that indeed gamification boosts significantly user participation and energy savings provided that the incentive and game parameters are chosen optimally.

</details>


### [12] [User Acceptance Model for Smart Incentives in Sustainable Video Streaming towards 6G](https://arxiv.org/abs/2601.21903)
*Konstantinos Varsos,Adamantia Stamou,George D. Stamoulis,Vasillios A. Siris*

Main category: cs.ET

TL;DR: A user-acceptance model for 5G video streaming that balances energy efficiency with quality of experience through personalized incentives, environmental awareness, and behavioral adaptation.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of 5G video streaming is increasing energy consumption across networks, creating a need for energy-efficient solutions. However, simply reducing streaming bitrates for efficiency may compromise user-perceived quality of experience (QoE), requiring transparent, user-centric incentive models that balance sustainability with perceived value.

Method: Proposes a unified probabilistic framework combining environmental awareness, personalized responsiveness to incentives, and varying altruism levels. Includes dynamic individualized incentives, social well-being as motivator, provider-driven education strategies that gradually adjust user acceptance thresholds, and data-driven learning of user traits from historical offer-response interactions.

Result: Extensive synthetic-data experiments reveal trade-offs between provider cost and network flexibility, showing that personalized incentives and gradual behavioral adaptation can advance sustainability targets without compromising stakeholder requirements.

Conclusion: The proposed user-acceptance model demonstrates that personalized incentives and behavioral adaptation strategies can effectively balance energy efficiency goals with user experience requirements in 5G video streaming, advancing sustainability targets while maintaining stakeholder satisfaction.

Abstract: The rapid growth of 5G video streaming is intensifying energy consumption across access, core, and data-center networks, underscoring the critical need for energy and carbon-efficient solutions. While reducing streaming bitrates improves energy efficiency, its success hinges on user acceptance--particularly when lower bitrates may be perceived as reduced quality of experience (QoE). Therefore, there is a need to develop transparent, user-centric incentive models that balance sustainability with perceived value. We propose a user-acceptance model that combines diverse environmental awareness, personalized responsiveness to incentives, and varying levels of altruism into a unified probabilistic framework. The model incorporates dynamic, individualized incentives that adapt over time. We further enhance the framework by incorporating (i) social well-being as a motivator for altruistic choices, (ii) provider-driven education strategies that gradually adjust user acceptance thresholds, and (iii) data-driven learning of user traits from historical offer--response interactions. Extensive synthetic-data experiments reveal the trade-offs between provider cost and network flexibility, showing that personalized incentives and gradual behavioral adaptation can advance sustainability targets without compromising stakeholder requirements.

</details>
