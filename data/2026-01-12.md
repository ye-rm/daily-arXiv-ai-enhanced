<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](https://arxiv.org/abs/2601.05569)
*Zixuan Li,Chuanzhen Wang,Haotian Sun*

Main category: cs.DC

TL;DR: SEDMA introduces a three-layer framework for unified memory management across computation, communication, and deployment layers in distributed AI systems, achieving 87.3% memory utilization and 142.5 ops/sec.


<details>
  <summary>Details</summary>
Motivation: Distributed AI systems face critical memory management challenges across layers: RRAM-based in-memory computing has scalability limitations, decentralized frameworks struggle with memory efficiency in NAT-constrained networks, and multi-agent deployment systems prevent adaptive optimization due to tight coupling of application logic with execution environments.

Method: Self-Evolving Distributed Memory Architecture (SEDMA) with three layers: (1) memory-guided matrix processing with dynamic partitioning based on device characteristics, (2) memory-aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. Features dual memory systems tracking both long-term performance patterns and short-term workload statistics.

Result: Achieves 87.3% memory utilization efficiency and 142.5 operations per second (vs. Ray Distributed at 72.1% and 98.7 ops/sec), reduces communication latency by 30.2% to 171.2 milliseconds, and improves resource utilization to 82.7% on COCO 2017, ImageNet, and SQuAD benchmarks.

Conclusion: SEDMA provides coordinated memory management across three architectural layers, workload-adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization for scalable AI systems.

Abstract: Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.

</details>


### [2] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: FaST-PT is a Federated Domain Generalization framework that uses lightweight multi-modal style transfer for local feature augmentation and dual-prompt modules with domain-aware prompt generation for efficient unseen domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing FDG methods struggle with cross-client data heterogeneity and incur significant communication/computation overhead, limiting their practical deployment.

Method: 1) Lightweight Multi-Modal Style Transfer (MST) transforms image embeddings under text supervision to expand training distribution; 2) Dual-prompt module decomposes prompts into global (captures general knowledge) and domain (captures domain-specific knowledge) prompts; 3) Domain-aware Prompt Generation (DPG) adaptively generates suitable prompts for each sample.

Result: Extensive experiments on four cross-domain benchmark datasets (PACS, DomainNet) demonstrate superior performance over SOTA FDG methods like FedDG-GA and DiPrompt. Ablation studies validate effectiveness and efficiency.

Conclusion: FaST-PT addresses key challenges in FDG by enabling local feature augmentation and efficient unseen domain adaptation through innovative style transfer and prompt-based mechanisms, achieving better generalization with reduced overhead.

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [LACIN: Linearly Arranged Complete Interconnection Networks](https://arxiv.org/abs/2601.05668)
*Ramón Beivide,Cristóbal Camarero,Carmen Martínez,Enrique Vallejo,Mateo Valero*

Main category: cs.AR

TL;DR: LACIN introduces simplified complete graph network implementations using identically indexed ports to reduce cabling and routing complexity for parallel computers at various scales.


<details>
  <summary>Details</summary>
Motivation: Large-scale networks like Dragonfly and HyperX use hierarchical/multi-dimensional complete graphs, but their huge number of links grows rapidly with size, creating cabling and routing complexity challenges.

Method: LACIN implements complete graphs using identically indexed ports to link switches, reducing network complexity through simplified cabling and routing approaches.

Result: The method reduces cabling and routing complexity, making network deployment easier for parallel computers across different scales from VLSI systems to largest supercomputers.

Conclusion: LACIN provides a practical solution for implementing complete graph networks with reduced complexity, enabling easier deployment of parallel computing networks at various scales.

Abstract: Several interconnection networks are based on the complete graph topology. Networks with a moderate size can be based on a single complete graph. However, large-scale networks such as Dragonfly and HyperX use, respectively, a hierarchical or a multi-dimensional composition of complete graphs.
  The number of links in these networks is huge and grows rapidly with their size. This paper introduces LACIN, a set of complete graph implementations that use identically indexed ports to link switches. This way of implementing the network reduces the complexity of its cabling and its routing. LACIN eases the deployment of networks for parallel computers of different scales, from VLSI systems to the largest supercomputers.

</details>
