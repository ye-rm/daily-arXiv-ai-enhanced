{"id": "2601.14466", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14466", "abs": "https://arxiv.org/abs/2601.14466", "authors": ["Roeland Wiersema"], "title": "JAXMg: A multi-GPU linear solver in JAX", "comment": null, "summary": "Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.", "AI": {"tldr": "JAXMg enables multi-GPU dense linear algebra in JAX for large matrices exceeding single-GPU memory, integrating distributed GPU solvers as JIT-compatible primitives.", "motivation": "Scaling dense linear systems and eigenvalue problems beyond single GPUs is challenging in modern Python/JAX workflows, while existing multi-GPU solver libraries are difficult to integrate into composable, JIT-compiled Python programs.", "method": "JAXMg interfaces JAX with NVIDIA's cuSOLVERMg through XLA Foreign Function Interface, exposing distributed GPU solvers as JIT-compatible JAX primitives for Cholesky-based linear solves and symmetric eigendecompositions.", "result": "JAXMg provides multi-GPU dense linear algebra capabilities for JAX, enabling distributed execution for matrices exceeding single-GPU memory limits while preserving composability with JAX transformations.", "conclusion": "JAXMg bridges the gap between scalable linear algebra libraries and composable JAX workflows, allowing multi-GPU execution to be embedded directly within end-to-end scientific computing programs."}}
{"id": "2601.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14608", "abs": "https://arxiv.org/abs/2601.14608", "authors": ["Torben R. Lahnor", "Mia Reitz", "Jonas Posner", "Patrick Diehl"], "title": "Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI", "comment": null, "summary": "Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.\n  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).\n  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.", "AI": {"tldr": "This paper integrates two AMT runtimes (Itoyori and ItoyoriFBC) into Task Bench framework to evaluate performance and productivity against MPI and HPX, revealing distinct trade-offs between efficiency and code simplicity.", "motivation": "To provide fair comparisons between diverse Asynchronous Many-Task (AMT) runtimes and traditional MPI through a standardized evaluation framework, addressing challenges in comparing parallel programming systems.", "method": "Integrated Itoyori (PGAS with RDMA-based work stealing) and ItoyoriFBC (with future-based synchronization) into Task Bench framework. Evaluated performance using application efficiency and Minimum Effective Task Granularity (METG), and productivity using Lines of Code (LOC) and Number of Library Constructs (NLC) across various workload patterns.", "result": "MPI achieved highest efficiency for regular, communication-light workloads but required verbose code. HPX maintained stable efficiency under load imbalance but ranked last in productivity. Itoyori achieved highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC showed slightly lower efficiency than Itoyori but offered potential for irregular workloads.", "conclusion": "AMTs don't inherently guarantee improved productivity over MPI; different systems excel in different scenarios. Itoyori offers best balance of performance and productivity for communication-intensive workloads, while ItoyoriFBC's future-based synchronization shows promise for irregular workloads despite slightly lower efficiency."}}
{"id": "2601.14612", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.14612", "abs": "https://arxiv.org/abs/2601.14612", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $\u03a9(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\\%$ in cost savings, across diverse spot market conditions.", "AI": {"tldr": "ROSS is a randomized scheduling algorithm for hybrid cloud environments that achieves optimal competitive ratio of \u221aK for deadline-aware job scheduling, significantly improving cost savings over existing approaches.", "motivation": "The paper addresses the challenge of deadline-aware online scheduling in hybrid cloud environments where jobs can run on either cost-effective but unreliable spot instances or more expensive on-demand instances under hard deadlines. Existing deterministic policies have fundamental limitations with worst-case competitive ratio of \u03a9(K).", "method": "The paper presents ROSS, a novel randomized scheduling algorithm that achieves provably optimal competitive ratio of \u221aK under reasonable deadlines. The approach balances cost optimization and deadline guarantees through randomized decision-making between spot and on-demand instances.", "result": "Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS consistently outperforms state-of-the-art approaches by up to 30% in cost savings across diverse spot market conditions while effectively balancing cost optimization and deadline guarantees.", "conclusion": "ROSS provides a fundamentally improved approach to deadline-aware scheduling in hybrid clouds, achieving optimal competitive ratio and significant practical cost savings through randomized scheduling decisions between spot and on-demand instances."}}
{"id": "2601.14642", "categories": ["cs.DC", "cs.LO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14642", "abs": "https://arxiv.org/abs/2601.14642", "authors": ["Guillaume Ambal", "Max Stupple", "Brijesh Dongol", "Azalea Raad"], "title": "Specifying and Verifying RDMA Synchronisation (Extended Version)", "comment": "95 pages, extended version of ESOP 2026 paper", "summary": "Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\\text{RDMA}^\\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\\text{RDMA}^{\\text{TSO}}_{\\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$ library. Underpinned by $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\\text{RDMA}^{\\text{SC}}_{\\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.", "AI": {"tldr": "Extends RDMA semantics with remote read-modify-write operations and synchronization abstractions, enabling verification of remote locks and introducing strong RDMA model akin to sequential consistency.", "motivation": "Existing RDMA semantics lack formalization of remote synchronization, preventing verification of common abstractions like locks. The paper aims to close this gap by providing formal semantics for remote RMW operations.", "method": "Introduces RDMA^TSO_RMW semantics for remote read-modify-write instructions over TSO, builds RDMA^WAIT_RMW library for synchronization abstractions, specifies/verifies three classes of remote locks, and develops RDMA^SC_RMW strong model akin to sequential consistency.", "result": "First semantics for remote RMW operations over TSO, showing remote RMWs are weak and only atomic against other remote RMWs. Developed composable synchronization abstractions and verified lock implementations compatible with existing LOCO libraries.", "conclusion": "Successfully closes the synchronization gap in RDMA semantics, enabling verification of lock implementations and providing both weak (TSO) and strong (SC) RDMA models with composable libraries for practical use."}}
{"id": "2601.14286", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14286", "abs": "https://arxiv.org/abs/2601.14286", "authors": ["Wentao Jiang", "Jingxin Wang", "Zhang Hu", "Zhengyuan Shi", "Chengyu Ma", "Qiang Xu", "Weikang Qian", "Zhufei Chu"], "title": "GNN-based Path-aware multi-view Circuit Learning for Technology Mapping", "comment": "7pages, 4figures", "summary": "Traditional technology mapping suffers from systemic inaccuracies in delay estimation due to its reliance on abstract, technology-agnostic delay models that fail to capture the nuanced timing behavior behavior of real post-mapping circuits. To address this fundamental limitation, we introduce GPA(graph neural network (GNN)-based Path-Aware multi-view circuit learning), a novel GNN framework that learns precise, data-driven delay predictions by synergistically fusing three complementary views of circuit structure: And-Inverter Graphs (AIGs)-based functional encoding, post-mapping technology emphasizes critical timing paths. Trained exclusively on real cell delays extracted from critical paths of industrial-grade post-mapping netlists, GPA learns to classify cut delays with unprecedented accuracy, directly informing smarter mapping decisions. Evaluated on the 19 EPFL combinational benchmarks, GPA achieves 19.9%, 2.1% and 4.1% average delay reduction over the conventional heuristics methods (techmap, MCH) and the prior state-of-the-art ML-based approach SLAP, respectively-without compromising area efficiency.", "AI": {"tldr": "GPA is a GNN-based framework that learns precise delay predictions by fusing three circuit views (AIG functional encoding, post-mapping technology, and critical timing paths) to overcome traditional mapping's inaccurate delay estimation, achieving significant delay reduction over conventional and ML-based methods.", "motivation": "Traditional technology mapping suffers from systemic inaccuracies in delay estimation due to reliance on abstract, technology-agnostic delay models that fail to capture nuanced timing behavior of real post-mapping circuits.", "method": "GPA (GNN-based Path-Aware multi-view circuit learning) synergistically fuses three complementary views of circuit structure: And-Inverter Graphs (AIGs)-based functional encoding, post-mapping technology, and critical timing paths. It learns precise, data-driven delay predictions by training exclusively on real cell delays from critical paths of industrial-grade post-mapping netlists.", "result": "Evaluated on 19 EPFL combinational benchmarks, GPA achieves 19.9%, 2.1% and 4.1% average delay reduction over conventional heuristics methods (techmap, MCH) and prior state-of-the-art ML-based approach SLAP, respectively, without compromising area efficiency.", "conclusion": "GPA addresses fundamental limitations of traditional technology mapping by providing accurate data-driven delay predictions through multi-view GNN learning, enabling smarter mapping decisions and significant performance improvements."}}
{"id": "2601.14260", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14260", "abs": "https://arxiv.org/abs/2601.14260", "authors": ["Xiaoxuan Yang", "Peilin Chen", "Tergel Molom-Ochir", "Yiran Chen"], "title": "End-to-End Transformer Acceleration Through Processing-in-Memory Architectures", "comment": "ICM 2025", "summary": "Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.", "AI": {"tldr": "This paper introduces processing-in-memory solutions to address Transformer deployment challenges: attention's high data movement costs, KV cache memory bottlenecks, and quadratic attention complexity, achieving improved energy efficiency and latency.", "motivation": "Transformers face three major deployment challenges: 1) attention mechanism requires massive matrix multiplications and frequent data movement between memory and compute units (high latency/energy), 2) KV cache grows unpredictably in long-context inference, surpassing model weight size and creating memory/bandwidth bottlenecks, 3) quadratic attention complexity amplifies data movement and compute overhead, making large-scale inference inefficient.", "method": "Introduces processing-in-memory solutions that: 1) restructure attention and feed-forward computation to minimize off-chip data transfers, 2) dynamically compress and prune the KV cache to manage memory growth, 3) reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Evaluates against state-of-the-art accelerators and GPUs.", "result": "Demonstrates significant improvements in energy efficiency and latency compared to state-of-the-art accelerators and general-purpose GPUs.", "conclusion": "The processing-in-memory approaches address computation overhead, memory scalability, and attention complexity, enabling efficient, end-to-end acceleration of Transformer models."}}
{"id": "2601.14910", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14910", "abs": "https://arxiv.org/abs/2601.14910", "authors": ["Kaixuan Zhang", "Yunfan Cui", "Shuhao Zhang", "Chutong Ding", "Shiyou Qian", "Luping Wang", "Jian Cao", "Guangtao Xue", "Cheng Huang", "Guodong Yang", "Liping Zhang"], "title": "SynPerf: A Hybrid Analytical-ML Framework for GPU Performance Prediction", "comment": null, "summary": "The rapid expansion of Transformer-based large language models has dramatically increased the need for high-performance GPUs. As a result, there is growing demand for fast, accurate, and widely generalizable GPU performance models to support next-generation hardware selection and system-level exploration. However, current data-driven methods are limited, exhibiting poor generalization across hardware and inadequate modeling of complex production-level kernels common in modern inference stacks. To address these issues, we present SyncPerf, a unified GPU modeling framework. This approach first employs an analytical model to quantify a given kernel's demands on the GPU's heterogeneous instruction pipelines. These analytical features are then fed into a machine learning (ML) model to capture complex cross-pipeline interactions and resource dependencies, enabling high-fidelity performance prediction. Our evaluation across 11 GPU types from four generations of major architectures on two widely-used serving systems demonstrates that SyncPerf delivers high fidelity and strong generalizability. It achieves accurate predictions, with only 6.1% average error at the kernel level and 8.5% for end-to-end inference -- reducing the error of state-of-the-art methods by 6.7x and 4.4x, respectively. We also demonstrate SynPerf's value \"beyond simulation\" by utilizing its performance ceiling to diagnose implementation shortcomings and guide the optimization of a production fused MoE Triton kernel, achieving up to 1.7x speedup.", "AI": {"tldr": "SyncPerf is a unified GPU performance modeling framework that combines analytical modeling of kernel demands with ML to capture complex pipeline interactions, achieving high-fidelity predictions across diverse GPUs and enabling kernel optimization.", "motivation": "The rapid growth of Transformer-based LLMs has increased demand for high-performance GPUs, but current data-driven performance models lack generalization across hardware and struggle with complex production-level kernels in modern inference stacks.", "method": "SyncPerf uses a two-stage approach: first employs an analytical model to quantify kernel demands on GPU's heterogeneous instruction pipelines, then feeds these analytical features into an ML model to capture complex cross-pipeline interactions and resource dependencies for performance prediction.", "result": "Evaluation across 11 GPU types from four generations of major architectures on two serving systems shows SyncPerf achieves 6.1% average error at kernel level and 8.5% for end-to-end inference, reducing state-of-the-art error by 6.7x and 4.4x respectively. Also enabled 1.7x speedup in production fused MoE Triton kernel optimization.", "conclusion": "SyncPerf provides a high-fidelity, generalizable GPU performance modeling framework that addresses limitations of current methods and demonstrates practical value beyond simulation for kernel optimization and hardware selection."}}
{"id": "2601.14735", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14735", "abs": "https://arxiv.org/abs/2601.14735", "authors": ["Varad Kulkarni", "Vaibhav Jha", "Nikhil Reddy", "Yogesh Simmhan"], "title": "Optimizing FaaS Platforms for MCP-enabled Agentic Workflows", "comment": null, "summary": "Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.", "AI": {"tldr": "FAME is a serverless FaaS architecture for orchestrating MCP-enabled AI agent workflows that decomposes agent patterns into composable functions, enabling scalable deployment with significant performance and cost improvements.", "motivation": "The rapid rise of autonomous AI agents using LLMs and MCP servers creates challenges for scalable cloud deployment and state management. Traditional VM hosting is resource-intensive and lacks elasticity, while FaaS platforms offer modularity and cost efficiency but are inherently stateless.", "method": "FAME decomposes agentic patterns like ReAct into composable agents (Planner, Actor, Evaluator) as FaaS functions using LangGraph, orchestrated as AWS Step Functions workflows. It addresses context persistence using DynamoDB for memory, optimizes MCP server deployment with AWS Lambda wrappers, caches tool outputs in S3, and uses function fusion strategies.", "result": "Evaluation on research paper summarization and log analytics applications shows up to 13x latency reduction, 88% fewer input tokens, 66% cost savings, and improved workflow completion rates compared to traditional approaches.", "conclusion": "FAME demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale, offering significant performance improvements and cost efficiency while addressing state management challenges in stateless FaaS environments."}}
{"id": "2601.14640", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14640", "abs": "https://arxiv.org/abs/2601.14640", "authors": ["Naoya Onizawa", "Daisaku Katagiri", "Warren J. Gross", "Takahiro Hanyu"], "title": "Analog-to-Stochastic Converter Using Magnetic Tunnel Junction Devices for Vision Chips", "comment": "24 pages", "summary": "This paper introduces an analog-to-stochastic converter using a magnetic tunnel junction (MTJ) de- vice for vision chips based on stochastic computation. Stochastic computation has been recently exploited for area-efficient hardware implementation, such as low-density parity-check (LDPC) decoders and image processors. However, power-and-area hungry two-step (analog-to-digital and digital-to-stochastic) converters are required for the analog to stochastic signal conversion. To real- ize a one-step conversion, an MTJ device is used as it inherently exhibits a probabilistic switching behavior between two resistance states. Exploiting the device-based probabilistic behavior, analog signals can be directly and area-efficiently converted to stochastic signals to mitigate the signal- conversion overhead. The analog-to-stochastic signal conversion is theoretically described and the conversion characteristic is evaluated using device and circuit parameters. In addition, the resistance variability of the MTJ device is considered in order to compensate the variability effect on the sig- nal conversion. Based on the theoretical analysis, the analog-to-stochastic converter is designed in 90nm CMOS and 100nm MTJ technologies and is verified using a SPICE simulator (NS-SPICE) that handles both transistors and MTJ devices.", "AI": {"tldr": "This paper presents an analog-to-stochastic converter using magnetic tunnel junction (MTJ) devices for vision chips, enabling direct one-step conversion from analog to stochastic signals to reduce area and power overhead compared to traditional two-step converters.", "motivation": "Stochastic computation offers area-efficient hardware implementation for applications like LDPC decoders and image processors, but requires power-and-area hungry two-step converters (analog-to-digital then digital-to-stochastic). There's a need for a one-step conversion solution to reduce signal-conversion overhead.", "method": "The paper uses magnetic tunnel junction (MTJ) devices that inherently exhibit probabilistic switching behavior between two resistance states. This device-based probabilistic behavior enables direct analog-to-stochastic conversion. The conversion is theoretically described, evaluated using device/circuit parameters, and considers MTJ resistance variability. The converter is designed in 90nm CMOS and 100nm MTJ technologies and verified using NS-SPICE simulator.", "result": "The paper demonstrates a working analog-to-stochastic converter design that directly converts analog signals to stochastic signals using MTJ devices. The theoretical analysis provides conversion characteristics, and the SPICE simulation verifies the design's functionality while addressing resistance variability compensation.", "conclusion": "MTJ-based analog-to-stochastic converters provide an area-efficient one-step solution for vision chips and stochastic computing systems, overcoming the limitations of traditional two-step converters by leveraging the inherent probabilistic switching behavior of MTJ devices."}}
{"id": "2601.14347", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14347", "abs": "https://arxiv.org/abs/2601.14347", "authors": ["George Rafael Gourdoumanis", "Fotoini Oikonomou", "Maria Pantazi-Kypraiou", "Pavlos Stoikos", "Olympia Axelou", "Athanasios Tziouvaras", "Georgios Karakonstantis", "Tahani Aladwani", "Christos Anagnostopoulos", "Yixian Shen", "Anuj Pathania", "Alberto Garcia-Ortiz", "George Floros"], "title": "Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability", "comment": "DATE 2026", "summary": "As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.", "AI": {"tldr": "The paper introduces the COIN-3D project, a Horizon Europe Twinning initiative focused on developing open-source EDA tools for reliability assessment of 3D chiplet-based systems to address manufacturing challenges in advanced semiconductor nodes.", "motivation": "As semiconductor manufacturing advances to 3-nm and sub-nanometer processes with GAAFETs, complexity and manufacturing challenges increase. 3D chiplet-based approaches help address lower yields of large monolithic designs and enable modular heterogeneous systems with components from different technology nodes.", "method": "The paper describes the COIN-3D project, a collaborative European initiative that aims to develop novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for both physical- and system-level reliability analysis.", "result": "The paper introduces the project framework and goals but doesn't present specific results yet. It establishes the foundation for developing reliability assessment tools for 2.5D/3D VLSI systems through European institutional collaboration.", "conclusion": "The COIN-3D project represents a strategic initiative to strengthen research excellence in 3D VLSI reliability, addressing critical challenges in advanced semiconductor manufacturing through collaborative development of open-source reliability assessment tools for chiplet-based heterogeneous systems."}}
{"id": "2601.14912", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14912", "abs": "https://arxiv.org/abs/2601.14912", "authors": ["Guangba Yu", "Genting Mai", "Rui Wang", "Ruipeng Li", "Pengfei Chen", "Long Pan", "Ruijie Xu"], "title": "AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems", "comment": "Accepted by ASE 2025", "summary": "Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\\% alert reduction ratios) and accelerates fault diagnosis (90.5\\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.", "AI": {"tldr": "AlertGuardian framework uses LLMs and graph models to optimize alert management, reducing 94.8% of alerts and achieving 90.5% diagnosis accuracy in cloud systems.", "motivation": "Current cloud alert systems generate overwhelming volumes of alerts causing alert fatigue and degrading operational efficiency due to ineffective alert life-cycle management.", "method": "AlertGuardian framework with three phases: 1) Alert Denoise using graph learning with virtual noise, 2) Alert Summary using RAG with LLMs for actionable summaries, 3) Alert Rule Refinement using multi-agent iterative feedback.", "result": "94.8% alert reduction ratio, 90.5% diagnosis accuracy, improvement of 1,174 alert rules with 32% acceptance rate by SREs on four real-world datasets.", "conclusion": "AlertGuardian successfully mitigates alert fatigue and accelerates fault diagnosis in cloud systems, with practical deployment success and lessons learned for alert life-cycle management."}}
{"id": "2601.15151", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.15151", "abs": "https://arxiv.org/abs/2601.15151", "authors": ["Jean Bruant", "Pierre-Henri Horrein", "Olivier Muller", "Fr\u00e9d\u00e9ric P\u00e9trot"], "title": "Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA", "comment": "29 pages, 10 listings, 5 tables", "summary": "In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations.", "AI": {"tldr": "PAF is an open-source architectural parameterization framework for FPGA-based network packet processing that enables agile deployment across multiple FPGA targets while maintaining performance parity with hand-tuned designs.", "motivation": "FPGAs are ideal for low-latency, high-throughput packet processing in cloud infrastructures, but traditional hardware design processes are too slow for agile infrastructure evolution. The need to fine-tune designs for various FPGA targets creates deployment and maintenance challenges.", "method": "PAF (Pipeline Automation Framework) uses Chisel (Scala-embedded Hardware Construction Language) with a pipeline-oriented design methodology. It focuses on architectural intent rather than low-level details, enabling parameterization and reuse of pipelined designs across multiple FPGAs.", "result": "PAF demonstrates efficient parameterization abilities for industrial network packet classification systems, enabling reuse and optimization of the same pipelined design on several FPGAs. It reduces code complexity while achieving performance and resource usage comparable to exhaustively described implementations.", "conclusion": "PAF addresses the agility gap in FPGA deployment for network infrastructure by providing automation that doesn't sacrifice architectural control, enabling cloud providers to maintain evolving infrastructures with heterogeneous FPGA targets."}}
{"id": "2601.14923", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14923", "abs": "https://arxiv.org/abs/2601.14923", "authors": ["Kaddour Sidi", "Daniel Balouek", "Baptiste Jonglez"], "title": "Application-level observability for adaptive Edge to Cloud continuum systems", "comment": "UCC 2025 - IEEE/ACM 18th International Conference on Utility and Cloud Computing, Dec 2025, NANTES, France", "summary": "Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.", "AI": {"tldr": "An observability framework for Edge-to-Cloud systems that combines developer instrumentation with SLO-aware feedback for autonomous adaptation, demonstrated through video processing with improved scalability and fault tolerance.", "motivation": "Modern Edge-to-Cloud systems need fine-grained observability to ensure adaptive behavior and performance compliance across heterogeneous, dynamic environments where traditional monitoring approaches are insufficient.", "method": "Integrated application-level observability framework combining OpenTelemetry for instrumentation, Prometheus for monitoring, K3s for orchestration, and Chaos Mesh for fault injection. Uses SLO-aware feedback loops for autonomous adaptation control across the edge-cloud continuum.", "result": "Demonstrated through video processing use case showing automatic adjustments maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results show improved scalability, fault tolerance, and responsiveness.", "conclusion": "The framework provides practical foundation for adaptive, SLO-compliant Edge-to-Cloud applications by enabling real-time monitoring and autonomous adaptation through integrated observability and feedback control mechanisms."}}
