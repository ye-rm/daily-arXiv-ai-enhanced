<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft is a DSL-guided approach that enables LLMs to generate correct and performant NPU kernels for Ascend accelerators, achieving 98.1% compilation success and 90.4% functional correctness.


<details>
  <summary>Details</summary>
Motivation: Developing high-performance kernels for specialized accelerators like NPUs is time-consuming and expertise-intensive. While LLMs can generate GPU kernels effectively, NPU kernel generation remains underexplored due to domain-specific programming models, limited examples, and sparse documentation, resulting in extremely low correctness when directly generating AscendC kernels.

Method: AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples, then transcompiled into AscendC through structured, constraint-driven LLM lowering passes.

Result: Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. 46.2% of generated kernels match or exceed PyTorch eager execution performance. It also successfully generated correct kernels for newly proposed mHC architecture, substantially surpassing PyTorch eager execution.

Conclusion: DSL-guided transcompilation enables LLMs to generate both correct and competitive NPU kernels, bridging the gap between GPU and NPU kernel generation capabilities and demonstrating generality across different accelerator architectures.

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [2] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow is a fault-tolerant LLM serving system that dramatically reduces recovery time and improves latency during hardware failures through decoupled initialization, traffic rerouting, and KV cache replication.


<details>
  <summary>Details</summary>
Motivation: LLM serving systems are fragile with frequent hardware faults causing disproportionate service outages, and current recovery mechanisms are too slow (up to 10 minutes) due to resource reinitialization and model weight reloading.

Method: KevlarFlow uses three key techniques: 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain service during partial failures.

Result: KevlarFlow reduces mean-time-to-recovery by 20x, improves average latency by 3.1x, p99 latency by 2.8x, average TTFT by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead compared to state-of-the-art systems.

Conclusion: KevlarFlow successfully bridges the gap between hardware unreliability and service availability in LLM serving systems, providing robust fault tolerance with minimal performance impact.

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [3] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: Data centers can reduce grid carbon emissions by participating in frequency regulation services, offsetting their own operational emissions through "Exogenous Carbon" savings.


<details>
  <summary>Details</summary>
Motivation: AI/ML data center growth increases energy consumption and carbon emissions, while renewable energy integration and high energy demands can destabilize power grids that rely on fossil-fueled frequency regulation reserves.

Method: Introduces "Exogenous Carbon" metric to quantify grid-side carbon reductions from data center participation in regulation service, and develops EcoCenter framework to maximize frequency regulation provision from GPU data centers.

Result: Data center participation in frequency regulation can result in Exogenous carbon savings that often outweigh their Operational carbon emissions.

Conclusion: GPU data centers can coordinate with power grids to reduce fossil-fueled frequency regulation reserves, significantly reducing overall carbon emissions through participation in grid stabilization services.

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [4] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL is a collective communication library that enables RDMA-based communication across heterogeneous GPUs (NVIDIA and AMD) without driver modifications, matching vendor-specific library performance in homogeneous setups while scaling in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Organizations are expanding GPU clusters with GPUs from multiple vendors, but current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs.

Method: HetCCL unifies vendor-specific backends (NVIDIA NCCL and AMD RCCL) and enables RDMA-based communication across GPUs without requiring driver modifications. It introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries.

Result: Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments.

Conclusion: HetCCL enables practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications, solving the problem of collective communication across heterogeneous GPU environments.

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [5] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: CONCUR is a lightweight control layer that prevents middle-phase thrashing in GPU KV caches during batch inference for agentic workloads by implementing proactive agent-level admission control, improving throughput up to 4.09x.


<details>
  <summary>Details</summary>
Motivation: Batch inference for agentic workloads causes severe throughput degradation due to middle-phase thrashing in GPU KV caches, where cache efficiency collapses as long-lived agents accumulate state over time, even before memory capacity is exhausted.

Method: CONCUR implements proactive agent-level admission control inspired by congestion control in distributed systems. It views the KV cache as a shared resource and uses a cache-aware control algorithm to dynamically adjust the number of active agents based on runtime cache signals, bounding aggregate cache pressure while preserving execution continuity.

Result: CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

Conclusion: Mitigating middle-phase thrashing requires moving beyond reactive request-level cache management to proactive agent-level admission control, and CONCUR provides an effective solution that significantly improves throughput for agentic workloads across large models.

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [6] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs can exhibit "roll back" behavior in concurrent scenarios like the Duelling Admins problem, requiring external arbitration via epoch events to establish bounded total order and finality.


<details>
  <summary>Details</summary>
Motivation: CRDTs prioritize availability over consistency, which can lead to surprising behavior when materialized views appear to "roll back" previously applied events, particularly problematic for group permissions in applications like instant messaging.

Method: Introduces external arbitration via optional "epoch events" that establish an immutable happens-before relation between concurrent events asynchronously in batches, creating a bounded total order within epochs.

Result: The arbitration mechanism provides "finality" that improves consistency levels beyond what standard CRDTs can offer, preventing Byzantine admins from exploiting concurrency in permission conflicts.

Conclusion: External arbitration is necessary to resolve concurrent event conflicts in CRDTs, particularly for permission management scenarios, while preserving availability through asynchronous batch processing of epoch events.

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Toward Digital Twins in 3D IC Packaging: A Critical Review of Physics, Data, and Hybrid Architectures](https://arxiv.org/abs/2601.23226)
*Gourab Datta,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.AR

TL;DR: This paper provides a critical review of Digital Twin technology for 3D IC reliability management, clarifying terminology, synthesizing enabling technologies, and proposing a unified hybrid architecture with a standards-aligned roadmap.


<details>
  <summary>Details</summary>
Motivation: 3D IC packaging faces multi-physics challenges (thermal hot spots, warpage stresses, interconnect aging) that require real-time monitoring beyond traditional offline metrology. Existing Digital Twin literature is fragmented and often confuses static simulation with dynamic, closed-loop twins.

Method: Three main contributions: 1) Clarifies Digital Twin hierarchy (digital models, shadows, twins); 2) Synthesizes three enabling technologies (physics-based modeling with surrogate models, data-driven virtual metrology, in-situ sensing); 3) Proposes unified hybrid architecture using physics-informed machine learning (PINNs) to address data scarcity and latency constraints.

Result: The paper provides a comprehensive framework distinguishing between different Digital Twin concepts and integrates physics-based and data-driven approaches. It offers a roadmap using IEEE 1451 and UCIe standards to transition from passive digital shadows to autonomous, self-optimizing Digital Twins.

Conclusion: The review establishes a clear foundation for implementing effective Digital Twins in 3D IC manufacturing and operation, addressing current fragmentation through terminology clarification, technology synthesis, and a practical hybrid architecture with standards-based implementation guidance.

Abstract: Three-dimensional integrated circuit (3D IC) pack-aging and heterogeneous integration have emerged as central pillars of contemporary semiconductor scaling. Yet, the multi-physics coupling inherent to stacked architectures manifesting as thermal hot spots, warpage-induced stresses, and interconnect aging demands monitoring and control capabilities that surpass traditional offline metrology. Although Digital Twin (DT) technology provides a principled route to real-time reliability management, the existing literature remains fragmented and frequently blurs the distinction between static multiphysics simulation workflows and truly dynamic, closed-loop twins. This critical review distinguishes itself by addressing these deficiencies through three specific contributions. First, we clarify the Digital Twin hierarchy to resolve terminological ambiguity between digital models, shadows, and twins. Second, we synthesize three foundational enabling technologies: (1) physics-based modeling, emphasizing the shift from computationally intensive finite-element analysis (FEA) to real-time surrogate models; (2) data-driven paradigms, highlighting virtual metrology (VM) for inferring latent metrics; and (3) in-situ sensing, the nervous system coupling the physical stack to its virtual counterpart. Third, beyond a descriptive survey, we propose a unified hybrid DT architecture that leverages physics-informed machine learning (e.g., PINNs) to reconcile data scarcity with latency constraints. Finally, we outline a standards-aligned roadmap incorporating IEEE 1451 and UCIe protocols to accelerate the transition from passive digital shadows to autonomous, self-optimizing Digital Twins for 3D IC manufacturing and field operation.

</details>


### [8] [RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning](https://arxiv.org/abs/2601.22476)
*Ruizhe Zhong,Xingbo Du,Junchi Yan*

Main category: cs.AR

TL;DR: A deep reinforcement learning framework for automated 3D IC floorplanning that handles multiple complex design rules in a unified approach, reducing manual post-processing.


<details>
  <summary>Details</summary>
Motivation: Current floorplanning methods struggle with complex hardware design rules in 3D IC scenarios, requiring extensive manual adjustments and post-processing by expert engineers.

Method: Three key components: 1) novel matrix representations for design rules, 2) action space constraints to filter invalid actions, 3) quantitative constraint satisfaction as reward signals in a deep reinforcement learning framework.

Result: Experiments on public benchmarks show effectiveness and validity, with good transferability to unseen circuits. The framework is extensible to accommodate new design rules.

Conclusion: The proposed all-in-one deep reinforcement learning approach successfully addresses complex IC design rules in floorplanning, reducing manual effort and providing flexibility for future chip design challenges.

Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI

</details>


### [9] [Design of a GPU with Heterogeneous Cores for Graphics](https://arxiv.org/abs/2601.22862)
*Aurora Tomás,Juan Luis Aragón,Joan Manuel Parcerisa,Antonio González*

Main category: cs.AR

TL;DR: KHEPRI proposes a heterogeneous GPU architecture with compute-specialized and memory-specialized cores, plus a novel scheduler that predicts tile characteristics using frame coherence, achieving 9.2% performance improvement and 4.8% energy reduction for graphics applications.


<details>
  <summary>Details</summary>
Motivation: Graphics scenes have diverse computational intensity and memory bandwidth requirements across different regions, but current GPUs use homogeneous architectures that can't optimally handle this variability.

Method: Proposes KHEPRI with two core types: compute-specialized (high ILP) and memory-specialized (tolerate cache misses). Uses novel scheduler that dynamically assigns tiles to optimal cores while preserving data locality, leveraging frame-to-frame coherence to predict tile behavior.

Result: Compared to traditional homogeneous GPU: 9.2% average performance improvement, 7.3% throughput increase (FPS), 4.8% total GPU energy reduction, with no hardware overhead.

Conclusion: Heterogeneous GPU architecture with specialized cores and intelligent scheduling can significantly improve performance and energy efficiency for graphics applications by better matching hardware to scene diversity.

Abstract: Heterogeneous architectures can deliver higher performance and energy efficiency than symmetric counterparts by using multiple architectures tuned to different types of workloads. While previous works focused on CPUs, this work extends the concept of heterogeneity to GPUs by proposing KHEPRI, a heterogeneous GPU architecture for graphics applications. Scenes in graphics applications showcase diversity, as they consist of many objects with varying levels of complexity. As a result, computational intensity and memory bandwidth requirements differ significantly across different regions of each scene. To address this variability, our proposal includes two types of cores: cores optimized for high ILP (compute-specialized) and cores that tolerate a higher number of simultaneously outstanding cache misses (memory-specialized). A key component of the proposed architecture is a novel work scheduler that dynamically assigns each part of a frame (i.e., a tile) to the most suitable core. Designing this scheduler is particularly challenging, as it must preserve data locality; otherwise, the benefits of heterogeneity may be offset by the penalty of additional cache misses. Additionally, the scheduler requires knowledge of each tile's characteristics before rendering it. For this purpose, KHEPRI leverages frame-to-frame coherence to predict the behavior of each tile based on that of the corresponding tile in the previous frame. Evaluations across a wide range of commercial animated graphics applications show that, compared to a traditional homogeneous GPU, KHEPRI achieves an average performance improvement of 9.2%, a throughput increase (frames per second) of 7.3%, and a total GPU energy reduction of 4.8%. Importantly, these benefits are achieved without any hardware overhead.

</details>


### [10] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: Bayesian Optimization with Gaussian Processes automates scheduling optimization on heterogeneous multi-core systems, balancing energy efficiency vs latency trade-offs and providing interpretability through sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: In the post-Dennard era, embedded systems require navigating complex energy-latency trade-offs in high-dimensional, non-smooth optimization landscapes where traditional heuristic tuning is inefficient.

Method: Proposes a Bayesian Optimization framework using Gaussian Processes to automate optimal scheduling configuration search on heterogeneous multi-core architectures. Addresses multi-objective nature by approximating Pareto Frontier between energy and time. Incorporates Sensitivity Analysis (fANOVA) and compares different covariance kernels (Matérn vs. RBF) for interpretability.

Result: The framework provides physical interpretability to the black-box model, revealing dominant hardware parameters driving system performance while efficiently navigating the complex optimization landscape.

Conclusion: Bayesian Optimization with Gaussian Processes offers an effective automated approach for scheduling optimization on heterogeneous multi-core systems, balancing energy-latency trade-offs with improved interpretability through sensitivity analysis and kernel comparisons.

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [11] [UrbanMoE: A Sparse Multi-Modal Mixture-of-Experts Framework for Multi-Task Urban Region Profiling](https://arxiv.org/abs/2601.22746)
*Pingping Liu,Jiamiao Liu,Zijian Zhang,Hao Miao,Qi Jiang,Qingliang Li,Qiuzhan Zhou,Irwin King*

Main category: cs.ET

TL;DR: UrbanMoE: A sparse multi-modal, multi-expert framework for multi-task urban region profiling, with a new benchmark for fair evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing urban region profiling methods are limited to single-task prediction and lack standardized benchmarks, failing to capture interconnected urban indicators and impeding fair comparison.

Method: Proposed UrbanMoE framework using sparse Mixture-of-Experts architecture that dynamically routes multi-modal features to specialized sub-networks for simultaneous prediction of diverse urban indicators.

Result: UrbanMoE consistently outperforms all baselines on three real-world datasets within the new benchmark, achieving state-of-the-art performance with validated efficacy and efficiency.

Conclusion: UrbanMoE sets a new state-of-the-art for multi-task urban region profiling and provides the community with a valuable benchmark and tool for future urban analytics research.

Abstract: Urban region profiling, the task of characterizing geographical areas, is crucial for urban planning and resource allocation. However, existing research in this domain faces two significant limitations. First, most methods are confined to single-task prediction, failing to capture the interconnected, multi-faceted nature of urban environments where numerous indicators are deeply correlated. Second, the field lacks a standardized experimental benchmark, which severely impedes fair comparison and reproducible progress. To address these challenges, we first establish a comprehensive benchmark for multi-task urban region profiling, featuring multi-modal features and a diverse set of strong baselines to ensure a fair and rigorous evaluation environment. Concurrently, we propose UrbanMoE, the first sparse multi-modal, multi-expert framework specifically architected to solve the multi-task challenge. Leveraging a sparse Mixture-of-Experts architecture, it dynamically routes multi-modal features to specialized sub-networks, enabling the simultaneous prediction of diverse urban indicators. We conduct extensive experiments on three real-world datasets within our benchmark, where UrbanMoE consistently demonstrates superior performance over all baselines. Further in-depth analysis validates the efficacy and efficiency of our approach, setting a new state-of-the-art and providing the community with a valuable tool for future research in urban analytics

</details>


### [12] [MiTa: A Hierarchical Multi-Agent Collaboration Framework with Memory-integrated and Task Allocation](https://arxiv.org/abs/2601.22974)
*XiaoJie Zhang,JianHan Wu,Xiaoyang Qu,Jianzong Wang*

Main category: cs.ET

TL;DR: MiTa is a hierarchical memory-integrated task allocation framework that improves multi-agent collaboration by using a manager-member hierarchy with allocation and summary modules to prevent conflicts and maintain memory consistency.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems still suffer from memory inconsistency and agent behavioral conflicts despite mitigating single-agent inefficiency in complex tasks.

Method: MiTa organizes agents into manager-member hierarchy with allocation module for global task allocation and summary module for episodic memory integration by condensing recent collaboration history.

Result: Experimental results confirm MiTa achieves superior efficiency and adaptability in complex multi-agent cooperation over strong baseline methods.

Conclusion: By combining task allocation with episodic memory, MiTa attains clearer task understanding and facilitates globally consistent task distribution for enhanced collaborative efficiency.

Abstract: Recent advances in large language models (LLMs) have substantially accelerated the development of embodied agents. LLM-based multi-agent systems mitigate the inefficiency of single agents in complex tasks. However, they still suffer from issues such as memory inconsistency and agent behavioral conflicts. To address these challenges, we propose MiTa, a hierarchical memory-integrated task allocative framework to enhance collaborative efficiency. MiTa organizes agents into a manager-member hierarchy, where the manager incorporates additional allocation and summary modules that enable (1) global task allocation and (2) episodic memory integration. The allocation module enables the manager to allocate tasks from a global perspective, thereby avoiding potential inter-agent conflicts. The summary module, triggered by task progress updates, performs episodic memory integration by condensing recent collaboration history into a concise summary that preserves long-horizon context. By combining task allocation with episodic memory, MiTa attains a clearer understanding of the task and facilitates globally consistent task distribution. Experimental results confirm that MiTa achieves superior efficiency and adaptability in complex multi-agent cooperation over strong baseline methods.

</details>
