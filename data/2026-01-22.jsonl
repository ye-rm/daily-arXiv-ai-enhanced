{"id": "2601.14466", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14466", "abs": "https://arxiv.org/abs/2601.14466", "authors": ["Roeland Wiersema"], "title": "JAXMg: A multi-GPU linear solver in JAX", "comment": null, "summary": "Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows."}
{"id": "2601.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14608", "abs": "https://arxiv.org/abs/2601.14608", "authors": ["Torben R. Lahnor", "Mia Reitz", "Jonas Posner", "Patrick Diehl"], "title": "Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI", "comment": null, "summary": "Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.\n  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).\n  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads."}
{"id": "2601.14612", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.14612", "abs": "https://arxiv.org/abs/2601.14612", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\\%$ in cost savings, across diverse spot market conditions."}
{"id": "2601.14642", "categories": ["cs.DC", "cs.LO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14642", "abs": "https://arxiv.org/abs/2601.14642", "authors": ["Guillaume Ambal", "Max Stupple", "Brijesh Dongol", "Azalea Raad"], "title": "Specifying and Verifying RDMA Synchronisation (Extended Version)", "comment": "95 pages, extended version of ESOP 2026 paper", "summary": "Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\\text{RDMA}^\\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\\text{RDMA}^{\\text{TSO}}_{\\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$ library. Underpinned by $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\\text{RDMA}^{\\text{SC}}_{\\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability."}
{"id": "2601.14286", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14286", "abs": "https://arxiv.org/abs/2601.14286", "authors": ["Wentao Jiang", "Jingxin Wang", "Zhang Hu", "Zhengyuan Shi", "Chengyu Ma", "Qiang Xu", "Weikang Qian", "Zhufei Chu"], "title": "GNN-based Path-aware multi-view Circuit Learning for Technology Mapping", "comment": "7pages, 4figures", "summary": "Traditional technology mapping suffers from systemic inaccuracies in delay estimation due to its reliance on abstract, technology-agnostic delay models that fail to capture the nuanced timing behavior behavior of real post-mapping circuits. To address this fundamental limitation, we introduce GPA(graph neural network (GNN)-based Path-Aware multi-view circuit learning), a novel GNN framework that learns precise, data-driven delay predictions by synergistically fusing three complementary views of circuit structure: And-Inverter Graphs (AIGs)-based functional encoding, post-mapping technology emphasizes critical timing paths. Trained exclusively on real cell delays extracted from critical paths of industrial-grade post-mapping netlists, GPA learns to classify cut delays with unprecedented accuracy, directly informing smarter mapping decisions. Evaluated on the 19 EPFL combinational benchmarks, GPA achieves 19.9%, 2.1% and 4.1% average delay reduction over the conventional heuristics methods (techmap, MCH) and the prior state-of-the-art ML-based approach SLAP, respectively-without compromising area efficiency."}
{"id": "2601.14260", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14260", "abs": "https://arxiv.org/abs/2601.14260", "authors": ["Xiaoxuan Yang", "Peilin Chen", "Tergel Molom-Ochir", "Yiran Chen"], "title": "End-to-End Transformer Acceleration Through Processing-in-Memory Architectures", "comment": "ICM 2025", "summary": "Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models."}
{"id": "2601.14910", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14910", "abs": "https://arxiv.org/abs/2601.14910", "authors": ["Kaixuan Zhang", "Yunfan Cui", "Shuhao Zhang", "Chutong Ding", "Shiyou Qian", "Luping Wang", "Jian Cao", "Guangtao Xue", "Cheng Huang", "Guodong Yang", "Liping Zhang"], "title": "SynPerf: A Hybrid Analytical-ML Framework for GPU Performance Prediction", "comment": null, "summary": "The rapid expansion of Transformer-based large language models has dramatically increased the need for high-performance GPUs. As a result, there is growing demand for fast, accurate, and widely generalizable GPU performance models to support next-generation hardware selection and system-level exploration. However, current data-driven methods are limited, exhibiting poor generalization across hardware and inadequate modeling of complex production-level kernels common in modern inference stacks. To address these issues, we present SyncPerf, a unified GPU modeling framework. This approach first employs an analytical model to quantify a given kernel's demands on the GPU's heterogeneous instruction pipelines. These analytical features are then fed into a machine learning (ML) model to capture complex cross-pipeline interactions and resource dependencies, enabling high-fidelity performance prediction. Our evaluation across 11 GPU types from four generations of major architectures on two widely-used serving systems demonstrates that SyncPerf delivers high fidelity and strong generalizability. It achieves accurate predictions, with only 6.1% average error at the kernel level and 8.5% for end-to-end inference -- reducing the error of state-of-the-art methods by 6.7x and 4.4x, respectively. We also demonstrate SynPerf's value \"beyond simulation\" by utilizing its performance ceiling to diagnose implementation shortcomings and guide the optimization of a production fused MoE Triton kernel, achieving up to 1.7x speedup."}
{"id": "2601.14735", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14735", "abs": "https://arxiv.org/abs/2601.14735", "authors": ["Varad Kulkarni", "Vaibhav Jha", "Nikhil Reddy", "Yogesh Simmhan"], "title": "Optimizing FaaS Platforms for MCP-enabled Agentic Workflows", "comment": null, "summary": "Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale."}
{"id": "2601.14640", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14640", "abs": "https://arxiv.org/abs/2601.14640", "authors": ["Naoya Onizawa", "Daisaku Katagiri", "Warren J. Gross", "Takahiro Hanyu"], "title": "Analog-to-Stochastic Converter Using Magnetic Tunnel Junction Devices for Vision Chips", "comment": "24 pages", "summary": "This paper introduces an analog-to-stochastic converter using a magnetic tunnel junction (MTJ) de- vice for vision chips based on stochastic computation. Stochastic computation has been recently exploited for area-efficient hardware implementation, such as low-density parity-check (LDPC) decoders and image processors. However, power-and-area hungry two-step (analog-to-digital and digital-to-stochastic) converters are required for the analog to stochastic signal conversion. To real- ize a one-step conversion, an MTJ device is used as it inherently exhibits a probabilistic switching behavior between two resistance states. Exploiting the device-based probabilistic behavior, analog signals can be directly and area-efficiently converted to stochastic signals to mitigate the signal- conversion overhead. The analog-to-stochastic signal conversion is theoretically described and the conversion characteristic is evaluated using device and circuit parameters. In addition, the resistance variability of the MTJ device is considered in order to compensate the variability effect on the sig- nal conversion. Based on the theoretical analysis, the analog-to-stochastic converter is designed in 90nm CMOS and 100nm MTJ technologies and is verified using a SPICE simulator (NS-SPICE) that handles both transistors and MTJ devices."}
{"id": "2601.14347", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14347", "abs": "https://arxiv.org/abs/2601.14347", "authors": ["George Rafael Gourdoumanis", "Fotoini Oikonomou", "Maria Pantazi-Kypraiou", "Pavlos Stoikos", "Olympia Axelou", "Athanasios Tziouvaras", "Georgios Karakonstantis", "Tahani Aladwani", "Christos Anagnostopoulos", "Yixian Shen", "Anuj Pathania", "Alberto Garcia-Ortiz", "George Floros"], "title": "Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability", "comment": "DATE 2026", "summary": "As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis."}
{"id": "2601.14612", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.14612", "abs": "https://arxiv.org/abs/2601.14612", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\\%$ in cost savings, across diverse spot market conditions."}
{"id": "2601.14912", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14912", "abs": "https://arxiv.org/abs/2601.14912", "authors": ["Guangba Yu", "Genting Mai", "Rui Wang", "Ruipeng Li", "Pengfei Chen", "Long Pan", "Ruijie Xu"], "title": "AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems", "comment": "Accepted by ASE 2025", "summary": "Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\\% alert reduction ratios) and accelerates fault diagnosis (90.5\\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X."}
{"id": "2601.15151", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.15151", "abs": "https://arxiv.org/abs/2601.15151", "authors": ["Jean Bruant", "Pierre-Henri Horrein", "Olivier Muller", "Frédéric Pétrot"], "title": "Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA", "comment": "29 pages, 10 listings, 5 tables", "summary": "In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations."}
{"id": "2601.14923", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14923", "abs": "https://arxiv.org/abs/2601.14923", "authors": ["Kaddour Sidi", "Daniel Balouek", "Baptiste Jonglez"], "title": "Application-level observability for adaptive Edge to Cloud continuum systems", "comment": "UCC 2025 - IEEE/ACM 18th International Conference on Utility and Cloud Computing, Dec 2025, NANTES, France", "summary": "Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications."}
{"id": "2601.14640", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14640", "abs": "https://arxiv.org/abs/2601.14640", "authors": ["Naoya Onizawa", "Daisaku Katagiri", "Warren J. Gross", "Takahiro Hanyu"], "title": "Analog-to-Stochastic Converter Using Magnetic Tunnel Junction Devices for Vision Chips", "comment": "24 pages", "summary": "This paper introduces an analog-to-stochastic converter using a magnetic tunnel junction (MTJ) de- vice for vision chips based on stochastic computation. Stochastic computation has been recently exploited for area-efficient hardware implementation, such as low-density parity-check (LDPC) decoders and image processors. However, power-and-area hungry two-step (analog-to-digital and digital-to-stochastic) converters are required for the analog to stochastic signal conversion. To real- ize a one-step conversion, an MTJ device is used as it inherently exhibits a probabilistic switching behavior between two resistance states. Exploiting the device-based probabilistic behavior, analog signals can be directly and area-efficiently converted to stochastic signals to mitigate the signal- conversion overhead. The analog-to-stochastic signal conversion is theoretically described and the conversion characteristic is evaluated using device and circuit parameters. In addition, the resistance variability of the MTJ device is considered in order to compensate the variability effect on the sig- nal conversion. Based on the theoretical analysis, the analog-to-stochastic converter is designed in 90nm CMOS and 100nm MTJ technologies and is verified using a SPICE simulator (NS-SPICE) that handles both transistors and MTJ devices."}
{"id": "2601.14980", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14980", "abs": "https://arxiv.org/abs/2601.14980", "authors": ["Mengchun Xia", "Zhicheng Dong", "Donghong Cai", "Fang Fang", "Lisheng Fan", "Pingzhi Fan"], "title": "Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks", "comment": null, "summary": "Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use distributed data to train a global model. However, the computing power of a single-edge node is limited, and collaborative computing will cause information leakage and excessive communication overhead. In this paper, we design a parallel collaborative distributed alternating direction method of multipliers (ADMM) and propose a three-phase parallel collaborative ADMM privacy computing (3P-ADMM-PC2) algorithm for distributed computing in edge networks, where the Paillier homomorphic encryption is utilized to protect data privacy during interactions. Especially, a quantization method is introduced, which maps the real numbers to a positive integer interval without affecting the homomorphic operations. To address the architectural mismatch between large-integer and Graphics Processing Unit (GPU) computing, we transform high-bitwidth computations into low-bitwidth matrix and vector operations. Thus the GPU can be utilized to implement parallel encryption and decryption computations with long keys. Finally, a GPU-accelerated 3P-ADMM-PC2 is proposed to optimize the collaborative computing tasks. Meanwhile, large-scale computational tasks are conducted in network topologies with varying numbers of edge nodes. Experimental results demonstrate that the proposed 3P-ADMM-PC2 has excellent mean square error performance, which is close to that of distributed ADMM without privacy-preserving. Compared to centralized ADMM and distributed ADMM implemented with Central Processing Unit (CPU) computation, the proposed scheme demonstrates a significant speedup ratio."}
{"id": "2601.14910", "categories": ["cs.PF", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14910", "abs": "https://arxiv.org/abs/2601.14910", "authors": ["Kaixuan Zhang", "Yunfan Cui", "Shuhao Zhang", "Chutong Ding", "Shiyou Qian", "Luping Wang", "Jian Cao", "Guangtao Xue", "Cheng Huang", "Guodong Yang", "Liping Zhang"], "title": "SynPerf: A Hybrid Analytical-ML Framework for GPU Performance Prediction", "comment": null, "summary": "The rapid expansion of Transformer-based large language models has dramatically increased the need for high-performance GPUs. As a result, there is growing demand for fast, accurate, and widely generalizable GPU performance models to support next-generation hardware selection and system-level exploration. However, current data-driven methods are limited, exhibiting poor generalization across hardware and inadequate modeling of complex production-level kernels common in modern inference stacks. To address these issues, we present SyncPerf, a unified GPU modeling framework. This approach first employs an analytical model to quantify a given kernel's demands on the GPU's heterogeneous instruction pipelines. These analytical features are then fed into a machine learning (ML) model to capture complex cross-pipeline interactions and resource dependencies, enabling high-fidelity performance prediction. Our evaluation across 11 GPU types from four generations of major architectures on two widely-used serving systems demonstrates that SyncPerf delivers high fidelity and strong generalizability. It achieves accurate predictions, with only 6.1% average error at the kernel level and 8.5% for end-to-end inference -- reducing the error of state-of-the-art methods by 6.7x and 4.4x, respectively. We also demonstrate SynPerf's value \"beyond simulation\" by utilizing its performance ceiling to diagnose implementation shortcomings and guide the optimization of a production fused MoE Triton kernel, achieving up to 1.7x speedup."}
