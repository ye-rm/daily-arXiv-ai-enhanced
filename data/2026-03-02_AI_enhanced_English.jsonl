{"id": "2602.23540", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23540", "abs": "https://arxiv.org/abs/2602.23540", "authors": ["Kart Leong Lim"], "title": "Component Centric Placement Using Deep Reinforcement Learning", "comment": null, "summary": "Automated placement of components on printed circuit boards (PCBs) is a critical stage in placement layout design. While reinforcement learning (RL) has been successfully applied to system-on-chip IP block placement and chiplet arrangement in complex packages, PCB component placement presents unique challenges due to several factors: variation in component sizes, single- and double-sided boards, wirelength constraints, board constraints, and non-overlapping placement requirements. In this work, we adopt a component-centric layout for automating PCB component placement using RL: first, the main component is fixed at the center, while passive components are placed in proximity to the pins of the main component. Free space around the main component is discretized, drastically reducing the search space while still covering all feasible placement; second, we leverage prior knowledge that each passive's position has to be near to its corresponding voltage source. This allows us to design the reward function which avoids wasted exploration of infeasible or irrelevant search space. Using the component centric layout, we implemented different methods including Deep Q-Network, Actor-Critic algorithm and Simulated Annealing. Evaluation on over nine real-world PCBs of varying complexity shows that our best proposed method approaches near human-like placements in terms of wirelength and feasibility.", "AI": {"tldr": "This paper proposes a reinforcement learning approach for automated PCB component placement using a component-centric layout that fixes main components and places passives nearby, reducing search space while maintaining feasibility.", "motivation": "PCB component placement automation is challenging due to component size variations, single/double-sided boards, wirelength constraints, board constraints, and non-overlapping requirements. While RL has been successful in chip placement, PCB placement presents unique challenges that need specialized approaches.", "method": "The paper adopts a component-centric layout: 1) main component is fixed at center, passives placed near its pins; 2) free space is discretized to reduce search space; 3) prior knowledge that passives must be near corresponding voltage sources informs reward design. Methods implemented include Deep Q-Network, Actor-Critic algorithm, and Simulated Annealing.", "result": "Evaluation on over nine real-world PCBs of varying complexity shows that the best proposed method approaches near human-like placements in terms of wirelength and feasibility.", "conclusion": "The component-centric RL approach effectively automates PCB component placement by reducing search space through discretization and leveraging domain knowledge, achieving results comparable to human placements."}}
{"id": "2602.24163", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.24163", "abs": "https://arxiv.org/abs/2602.24163", "authors": ["Bojian Zhang", "Paolo Gibertini", "Meysam Akbari", "Erika Covi"], "title": "Current pulse generator: A circuit for programming RRAM in current mode", "comment": null, "summary": "Switching uniformity, as a major challenge, hinders the practical implementation of \\ac{RRAM} in memory application. Operating \\ac{RRAM} in current mode, is proposed as an efficient method to improve programming schemes accuracy within the finite readout window. In this article, we demonstrate a current generator circuit to perform current programming on \\ac{RRAM}. Current mirror topology is used in our circuit to convert an external pulse voltage into a pulse current fed to \\ac{RRAM} directly with an amplitude equivalent with the DC reference current. The targeting ranges of \\ac{RRAM}'s programming current are up to 400\\,\\textmu A and, in that case, our proposed circuit achieved minimum current mismatch of 1\\%.", "AI": {"tldr": "A current generator circuit using current mirror topology converts voltage pulses to current pulses for RRAM programming, achieving 1% current mismatch for up to 400\u03bcA programming current.", "motivation": "Switching uniformity is a major challenge hindering practical implementation of RRAM in memory applications. Operating RRAM in current mode is proposed as an efficient method to improve programming accuracy within finite readout windows.", "method": "The authors demonstrate a current generator circuit using current mirror topology to convert external pulse voltage into pulse current fed directly to RRAM. The circuit maintains amplitude equivalent to DC reference current for current programming of RRAM.", "result": "The proposed circuit achieved minimum current mismatch of 1% for RRAM programming currents up to 400\u03bcA, demonstrating effective current-mode programming capability.", "conclusion": "The current generator circuit successfully enables current-mode programming of RRAM with high accuracy (1% mismatch), addressing switching uniformity challenges and improving programming scheme accuracy for practical RRAM memory applications."}}
{"id": "2602.23455", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23455", "abs": "https://arxiv.org/abs/2602.23455", "authors": ["Yuhao Liu", "Salim Ullah", "Akash Kumar"], "title": "BiKA: Kolmogorov-Arnold-Network-inspired Ultra Lightweight Neural Network Hardware Accelerator", "comment": null, "summary": "Lightweight neural network accelerators are essential for edge devices with limited resources and power constraints. While quantization and binarization can efficiently reduce hardware cost, they still rely on the conventional Artificial Neural Network (ANN) computation pattern. The recently proposed Kolmogorov-Arnold Network (KAN) presents a novel network paradigm built on learnable nonlinear functions. However, it is computationally expensive for hardware deployment. Inspired by KAN, we propose BiKA, a multiply-free architecture that replaces nonlinear functions with binary, learnable thresholds, introducing an extremely lightweight computational pattern that requires only comparators and accumulators. Our FPGA prototype on Ultra96-V2 shows that BiKA reduces hardware resource usage by 27.73% and 51.54% compared with binarized and quantized neural network systolic array accelerators, while maintaining competitive accuracy. BiKA provides a promising direction for hardware-friendly neural network design on edge devices.", "AI": {"tldr": "BiKA: A multiply-free neural network architecture using binary, learnable thresholds instead of nonlinear functions, requiring only comparators and accumulators for ultra-lightweight edge deployment.", "motivation": "Edge devices need lightweight neural network accelerators with limited resources and power constraints. While quantization and binarization reduce hardware costs, they still use conventional ANN computation patterns. KAN networks offer a novel paradigm but are computationally expensive for hardware deployment.", "method": "Inspired by KAN, BiKA replaces nonlinear functions with binary, learnable thresholds, creating a multiply-free architecture that requires only comparators and accumulators. This eliminates multiplication operations entirely, making it extremely lightweight for hardware implementation.", "result": "FPGA prototype on Ultra96-V2 shows BiKA reduces hardware resource usage by 27.73% compared to binarized neural network systolic array accelerators and 51.54% compared to quantized neural network accelerators, while maintaining competitive accuracy.", "conclusion": "BiKA provides a promising direction for hardware-friendly neural network design on edge devices by introducing an extremely lightweight computational pattern that eliminates multiplication operations while maintaining accuracy."}}
{"id": "2602.23598", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23598", "abs": "https://arxiv.org/abs/2602.23598", "authors": ["Md Hasanur Rashid", "Jesun Firoz", "Nathan R. Tallent", "Luanzheng Guo", "Meng Tang", "Dong Dai"], "title": "QoSFlow: Ensuring Service Quality of Distributed Workflows Using Interpretable Sensitivity Models", "comment": "to be published in 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS), 2026", "summary": "With the increasing importance of distributed scientific workflows, there is a critical need to ensure Quality of Service (QoS) constraints, such as minimizing time or limiting execution to resource subsets. However, the unpredictable nature of workflow behavior, even with similar configurations, makes it difficult to provide QoS guarantees. For effective reasoning about QoS scheduling, we introduce QoSFlow, a performance modeling method that partitions a workflow's execution configuration space into regions with similar behavior. Each region groups configurations with comparable execution times according to a given statistical sensitivity, enabling efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing. Evaluation on three diverse workflows shows that QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms that QoSFlow's recommended configurations consistently match measured execution outcomes across different QoS constraints.", "AI": {"tldr": "QoSFlow partitions workflow execution configuration space into regions with similar performance behavior to enable QoS-driven scheduling without exhaustive testing.", "motivation": "Distributed scientific workflows need QoS guarantees, but unpredictable workflow behavior makes it difficult to ensure constraints like minimizing time or limiting execution to resource subsets.", "method": "QoSFlow is a performance modeling method that partitions workflow execution configuration space into regions with similar behavior, grouping configurations with comparable execution times according to statistical sensitivity.", "result": "Evaluation on three diverse workflows shows QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms recommended configurations consistently match measured execution outcomes across different QoS constraints.", "conclusion": "QoSFlow enables efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing, providing reliable performance predictions for distributed scientific workflows."}}
{"id": "2602.23787", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.23787", "abs": "https://arxiv.org/abs/2602.23787", "authors": ["Xiaofeng Zhou", "Linfeng Du", "Hanwei Fan", "Wei Zhang"], "title": "FPPS: An FPGA-Based Point Cloud Processing System", "comment": null, "summary": "Point cloud processing is a computational bottleneck in autonomous driving systems, especially for real-time applications, while energy efficiency remains a critical system constraint. This work presents FPPS, an FPGA-accelerated point cloud processing system designed to optimize the iterative closest point (ICP) algorithm, a classic cornerstone of 3D localization and perception pipelines. Evaluated on the widely used KITTI benchmark dataset, the proposed system achieves up to 35$\\times$ (and a runtime-weighted average of 15.95x) speedup over a state-of-the-art CPU baseline while maintaining equivalent registration accuracy. Notably, the design improves average power efficiency by 8.58x, offering a compelling balance between performance and energy consumption. These results position FPPS as a viable solution for resource-constrained embedded autonomous platforms where both latency and power are key design priorities.", "AI": {"tldr": "FPPS is an FPGA-accelerated system for point cloud processing that achieves up to 35x speedup and 8.58x power efficiency improvement over CPU for the ICP algorithm on autonomous driving platforms.", "motivation": "Point cloud processing is a computational bottleneck in autonomous driving systems, especially for real-time applications, while energy efficiency remains a critical system constraint for resource-constrained embedded platforms.", "method": "The work presents FPPS, an FPGA-accelerated point cloud processing system designed to optimize the iterative closest point (ICP) algorithm, which is a classic cornerstone of 3D localization and perception pipelines.", "result": "Evaluated on the KITTI benchmark dataset, the system achieves up to 35x speedup (runtime-weighted average of 15.95x) over state-of-the-art CPU baseline while maintaining equivalent registration accuracy, with 8.58x improvement in average power efficiency.", "conclusion": "FPPS offers a compelling balance between performance and energy consumption, positioning it as a viable solution for resource-constrained embedded autonomous platforms where both latency and power are key design priorities."}}
{"id": "2602.23828", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.23828", "abs": "https://arxiv.org/abs/2602.23828", "authors": ["Tsung-Han Lu", "Weihong Xu", "Tajana Rosing"], "title": "GenDRAM:Hardware-Software Co-Design of General Platform in DRAM", "comment": null, "summary": "Dynamic programming (DP) algorithms, such as All-Pairs Shortest Path (APSP) and genomic sequence alignment, are fundamental to many scientific domains but are severely bottlenecked by data movement on conventional architectures. While Processing-in-Memory (PIM) offers a promising solution, existing accelerators often address only a fraction of the work-flow, creating new system-level bottlenecks in host-accelerator communication and off-chip data streaming. In this work, we propose GenDRAM, a massively parallel PIM accelerator that overcomes these limitations. GenDRAM leverages the immense capacity and internal bandwidth of monolithic 3D DRAM(M3D DRAM) to integrate entire data-intensive pipelines, such as the full genomics workflow from seeding to alignment, onto a single heterogeneous chip. At its core is a novel architecture featuring specialized Search PUs for memory-intensive tasks and universal, multiplier-less Compute PUs for diverse DP calculations. This is enabled by a 3D-aware data mapping strategy that exploits the tiered latency of M3D DRAM for performance optimization. Through comprehensive simulation, we demonstrate that GenDRAM achieves a transformative performance leap, outperforming state-of-the-art GPU systems by over 68x on APSP and over 22x on the end-to-end genomics pipeline.", "AI": {"tldr": "GenDRAM is a massively parallel PIM accelerator using monolithic 3D DRAM that integrates entire data-intensive pipelines, achieving 68x speedup on APSP and 22x speedup on genomics workflows compared to GPUs.", "motivation": "Dynamic programming algorithms like APSP and genomic alignment are bottlenecked by data movement on conventional architectures. Existing PIM accelerators only address part of workflows, creating new system-level bottlenecks in host-accelerator communication.", "method": "GenDRAM leverages monolithic 3D DRAM's capacity and bandwidth to integrate entire pipelines on a single chip. It features specialized Search PUs for memory tasks and multiplier-less Compute PUs for DP calculations, with 3D-aware data mapping that exploits tiered latency.", "result": "GenDRAM achieves over 68x performance improvement on All-Pairs Shortest Path compared to state-of-the-art GPU systems, and over 22x speedup on end-to-end genomics pipeline.", "conclusion": "GenDRAM demonstrates a transformative performance leap for data-intensive DP algorithms by overcoming communication bottlenecks through full pipeline integration in monolithic 3D DRAM."}}
{"id": "2602.23927", "categories": ["cs.DC", "cs.FL", "cs.MA", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23927", "abs": "https://arxiv.org/abs/2602.23927", "authors": ["Laura Bocchi", "Raymond Hu", "Adriana Laura Voinea", "Simon Thompson"], "title": "Mixed Choice in Asynchronous Multiparty Session Types", "comment": null, "summary": "We present a multiparty session type (MST) framework with asynchronous mixed choice (MC). We propose a core construct for MC that allows transient inconsistencies in protocol state between distributed participants, but ensures all participants can always eventually reach a mutually consistent state. We prove the correctness of our system by establishing a progress property and an operational correspondence between global types and distributed local type projections. Based on our theory, we implement a practical toolchain for specifying and validating asynchronous MST protocols featuring MC, and programming compliant gen_statem processes in Erlang/OTP. We test our framework by using our toolchain to specify and reimplement part of the amqp_client of the RabbitMQ broker for Erlang.", "AI": {"tldr": "A multiparty session type framework with asynchronous mixed choice that handles transient inconsistencies while ensuring eventual consistency, with practical Erlang/OTP implementation.", "motivation": "To address the challenge of asynchronous mixed choice in distributed systems where participants may have transient inconsistencies in protocol state, while ensuring they can eventually reach mutually consistent states.", "method": "Proposed a core construct for mixed choice that allows temporary inconsistencies, established progress property and operational correspondence between global types and distributed local type projections, implemented a toolchain for specifying/validating asynchronous MST protocols with MC, and programmed compliant gen_statem processes in Erlang/OTP.", "result": "Successfully implemented a practical toolchain for asynchronous MST protocols with mixed choice, validated by reimplementing part of the amqp_client of RabbitMQ broker for Erlang, proving correctness through progress property and operational correspondence.", "conclusion": "The framework effectively handles asynchronous mixed choice in distributed systems by allowing transient inconsistencies while guaranteeing eventual consistency, with practical applicability demonstrated through Erlang/OTP implementation and RabbitMQ case study."}}
{"id": "2602.23935", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23935", "abs": "https://arxiv.org/abs/2602.23935", "authors": ["Bowen Sun", "Christos D. Antonopoulos", "Evgenia Smirni", "Bin Ren", "Nikolaos Bellas", "Spyros Lalis"], "title": "Green or Fast? Learning to Balance Cold Starts and Idle Carbon in Serverless Computing", "comment": null, "summary": "Serverless computing simplifies cloud deployment but introduces new challenges in managing service latency and carbon emissions. Reducing cold-start latency requires retaining warm function instances, while minimizing carbon emissions favors reclaiming idle resources. This balance is further complicated by time-varying grid carbon intensity and varying workload patterns, under which static keep-alive policies are inefficient. We present LACE-RL, a latency-aware and carbon-efficient management framework that formulates serverless pod retention as a sequential decision problem. LACE-RL uses deep reinforcement learning to dynamically tune keep-alive durations, jointly modeling cold-start probability, function-specific latency costs, and real-time carbon intensity. Using the Huawei Public Cloud Trace, we show that LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy, while achieving better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.", "AI": {"tldr": "LACE-RL is a reinforcement learning framework that dynamically manages serverless function keep-alive durations to balance latency and carbon emissions, achieving significant reductions in both cold starts and idle emissions compared to static policies.", "motivation": "Serverless computing faces a fundamental trade-off: keeping function instances warm reduces cold-start latency but increases carbon emissions from idle resources, while reclaiming resources reduces emissions but increases latency. This is complicated by time-varying grid carbon intensity and workload patterns, making static policies inefficient.", "method": "LACE-RL formulates serverless pod retention as a sequential decision problem and uses deep reinforcement learning to dynamically tune keep-alive durations. It jointly models cold-start probability, function-specific latency costs, and real-time carbon intensity to make optimal decisions.", "result": "Using Huawei Public Cloud Trace data, LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy. It achieves better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.", "conclusion": "LACE-RL demonstrates that dynamic, learning-based approaches can effectively balance the latency-carbon trade-off in serverless computing, significantly outperforming static policies and approaching optimal performance while adapting to real-time conditions."}}
{"id": "2602.24010", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24010", "abs": "https://arxiv.org/abs/2602.24010", "authors": ["Mingkai Miao", "Guangyu Hu", "Wei Zhang", "Hongce Zhang"], "title": "LeGend: A Data-Driven Framework for Lemma Generation in Hardware Model Checking", "comment": null, "summary": "Property checking of RTL designs is a central task in formal verification. Among available engines, IC3/PDR is a widely used backbone whose performance critically depends on inductive generalization, the step that generalizes a concrete counterexample-to-induction (CTI) cube into a lemma. Prior work has explored machine learning to guide this step and achieved encouraging results, yet most methods adopt a per-clause graph analysis paradigm: for each clause they repeatedly build and analyze graphs, incurring heavy overhead and creating a scalability bottleneck. We introduce LeGend, which replaces this paradigm with one-time global representation learning. LeGend pre-trains a domain-adapted self-supervised model to produce latch embeddings that capture global circuit properties. These precomputed embeddings allow a lightweight model to predict high-quality lemmas with negligible overhead, effectively decoupling expensive learning from fast inference. Experiments show LeGend accelerates two state-of-the-art IC3/PDR engines across a diverse set of benchmarks, presenting a promising path to scale up formal verification.", "AI": {"tldr": "LeGend introduces one-time global representation learning for IC3/PDR verification, replacing per-clause graph analysis with pre-trained latch embeddings to predict high-quality lemmas with low overhead.", "motivation": "Current machine learning approaches for IC3/PDR's inductive generalization step use per-clause graph analysis, which repeatedly builds and analyzes graphs, causing heavy overhead and scalability bottlenecks.", "method": "LeGend uses one-time global representation learning with a domain-adapted self-supervised model to produce latch embeddings capturing global circuit properties, enabling a lightweight model to predict lemmas with minimal overhead.", "result": "LeGend accelerates two state-of-the-art IC3/PDR engines across diverse benchmarks, demonstrating effective decoupling of expensive learning from fast inference.", "conclusion": "LeGend presents a promising path to scale up formal verification by replacing the scalability bottleneck of per-clause analysis with efficient global representation learning."}}
{"id": "2602.24269", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.24269", "abs": "https://arxiv.org/abs/2602.24269", "authors": ["William C. Tegge", "Alex K. Jones"], "title": "Shifting in-DRAM", "comment": "9 pages, 4 figures, 5 tables", "summary": "Processing-in-Memory (PIM) architectures enable computation directly within DRAM and help combat the memory wall problem. Bit-shifting is a fundamental operation that enables PIM applications such as shift-and-add multiplication, adders using carry propagation, and Galois field arithmetic used in cryptography algorithms like AES and Reed-Solomon error correction codes. Existing approaches to in-DRAM shifting require adding dedicated shifter circuits beneath the sense amplifiers to enable horizontal data movement across adjacent bitlines or vertical data layouts which store operand bits along a bitline to implement shifts as row-copy operations. In this paper, we propose a novel DRAM subarray design that enables in-DRAM bit-shifting for open-bitline architectures. In this new design, we built upon prior work that introduced a new type of cell used for row migration in asymmetric subarrays, called a \"migration cell\". We repurpose and extend the functionality by adding a row of migration cells at the top and bottom of each subarray which enables bidirectional bit-shifting within any given row. This new design maintains compatibility with standard DRAM operations. Unlike previous approaches to shifting, our design operates on horizontally-stored data, eliminating the need and overhead of data transposition, and our design leverages the existing cell structures, eliminating the need for additional complex logic and circuitry. We present an evaluation of our design that includes timing and energy analysis using NVMain, circuit-level validation of the in-DRAM shift operation using LTSPICE, and a VLSI layout implementation in Cadence Virtuoso.", "AI": {"tldr": "Novel DRAM subarray design enables in-DRAM bit-shifting for open-bitline architectures using repurposed migration cells, eliminating data transposition overhead while maintaining compatibility with standard DRAM operations.", "motivation": "Bit-shifting is fundamental for PIM applications like multiplication, adders, and cryptography, but existing in-DRAM shifting approaches require dedicated shifter circuits or vertical data layouts with transposition overhead.", "method": "Extends prior migration cell concept by adding rows of migration cells at top/bottom of each subarray, enabling bidirectional bit-shifting within any row while maintaining horizontal data storage and standard DRAM compatibility.", "result": "Design validated through NVMain timing/energy analysis, LTSPICE circuit-level validation, and Cadence Virtuoso VLSI layout implementation, showing elimination of data transposition overhead and complex additional circuitry.", "conclusion": "Proposed DRAM subarray design enables efficient in-DRAM bit-shifting for PIM applications using existing cell structures, maintaining compatibility with standard operations while reducing overhead compared to previous approaches."}}
{"id": "2602.24044", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24044", "abs": "https://arxiv.org/abs/2602.24044", "authors": ["Ferran Agullo", "Joan Oliveras", "Chen Wang", "Alberto Gutierrez-Torre", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "title": "Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving", "comment": "journal extension of the workshop paper titled as \"A data-driven ml approach for maximizing performance in llm-adapter serving\"", "summary": "Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.", "AI": {"tldr": "A data-driven pipeline for optimizing GPU resource efficiency in distributed LLM adapter serving by computing optimal adapter placements to minimize GPU count while avoiding starvation and memory errors.", "motivation": "LLM adapters enable low-cost model specialization but create complex caching and scheduling challenges in distributed serving systems. While prior work focused on latency minimization, resource efficiency through throughput maximization remains underexplored, especially when hundreds of adapters must be hosted concurrently.", "method": "Three-component pipeline: (1) Digital Twin tailored to LLM-adapter serving for high-fidelity emulation, (2) distilled ML model trained on DT-generated data for fast performance estimation, (3) greedy placement algorithm using ML-based performance estimates to maximize GPU efficiency.", "result": "The DT achieves below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking. The pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads, and can be adapted to alternative objectives like latency minimization.", "conclusion": "The proposed pipeline effectively addresses resource efficiency challenges in LLM adapter serving, offering a versatile solution for future large-scale LLM serving infrastructures that can be adapted to various optimization objectives beyond GPU efficiency."}}
