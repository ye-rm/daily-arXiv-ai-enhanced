<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies](https://arxiv.org/abs/2510.26944)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Choreographer is a simulation framework for holistic system-level evaluation of fine-grained accelerators for latency-sensitive tasks, capturing hardware/software overheads and providing accurate cache modeling.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks fail to capture all hardware and software overheads in core-accelerator and cache-accelerator interactions, and lack detailed cache modeling for accurate performance insights.

Method: Integrates gem5-based hardware stack with AMBA CHI mesh network and Linux-based software stack, offers C++ API and modular configuration for rapid prototyping, includes detailed cache model.

Result: Case studies show: data-aware prefetcher achieves 1.08x-1.88x speedups by reducing memory latency; quicksort accelerator delivers >2x speedup with minimal address translation overhead.

Conclusion: Choreographer effectively models complex hardware-software interactions and optimizes performance in small task offloading scenarios.

Abstract: In this paper, we introduce Choreographer, a simulation framework that
enables a holistic system-level evaluation of fine-grained accelerators
designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer
captures all hardware and software overheads in core-accelerator and
cache-accelerator interactions, integrating a detailed gem5-based hardware
stack featuring an AMBA coherent hub interface (CHI) mesh network and a
complete Linux-based software stack. To facilitate rapid prototyping, it offers
a C++ application programming interface and modular configuration options. Our
detailed cache model provides accurate insights into performance variations
caused by cache configurations, which are not captured by other frameworks. The
framework is demonstrated through two case studies: a data-aware prefetcher for
graph analytics workloads, and a quicksort accelerator. Our evaluation shows
that the prefetcher achieves speedups between 1.08x and 1.88x by reducing
memory access latency, while the quicksort accelerator delivers more than 2x
speedup with minimal address translation overhead. These findings underscore
the ability of Choreographer to model complex hardware-software interactions
and optimize performance in small task offloading scenarios.

</details>


### [2] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: Analysis of timing closure challenges in FPGAs vs ASICs, showing ASICs achieve superior timing (45ps setup/35ps hold) while modern FPGAs remain competitive (180ps setup/120ps hold) for high-performance designs.


<details>
  <summary>Details</summary>
Motivation: To examine timing closure challenges and constraints in FPGAs and ASICs, understanding their architectural distinctions and design methodologies that influence timing behavior.

Method: In-depth analysis of core timing principles, architectural distinctions, and design methodologies, with a case study comparing Xilinx Kintex UltraScale+ FPGA (XCKU040) with a 7nm ASIC.

Result: ASICs achieve superior timing with 45ps setup and 35ps hold times, while modern FPGAs remain competitive with 180ps setup and 120ps hold times.

Conclusion: Modern FPGAs validate their suitability for high-performance designs despite ASICs having superior timing performance.

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [3] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: This paper surveys descriptor-based, object-aware memory systems that bridge the semantic gap between hardware and software by propagating program semantics like object identity, bounds, and lifetime.


<details>
  <summary>Details</summary>
Motivation: Modern computing systems lack native architectural mechanisms to propagate high-level program semantics across hardware/software interfaces, undermining security and efficiency.

Method: The paper presents a comprehensive survey establishing foundational concepts of memory objects and descriptors, introduces a taxonomy of descriptor addressing modes, and provides unified analysis of implementations.

Result: The survey reveals how descriptor-based memory systems holistically address memory protection, management, and processing challenges, with CentroID model demonstrating practical object-aware design implementation.

Conclusion: Explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and 128-bit architectures.

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh is a multi-tenant service fabric that optimizes AI deployment pipelines by decomposing workflows into fine-grained operators, enabling work deduplication and batching across users while preserving workflow provenance.


<details>
  <summary>Details</summary>
Motivation: AI deployment is shifting from monolithic LLM jobs to complex pipelines involving data transformation, fine-tuning, and agent interactions, requiring more efficient execution and optimization of these workloads as shared services rather than isolated pipelines.

Method: FlowMesh decomposes workflows into fine-grained operators with recorded lineage, uses a global control plane with a single utility function for batch and worker selection, and employs an elastic fleet of stateless workers backed by a content-addressable store.

Result: FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy usage compared to baseline solutions, provides similar or better latency profile, and remains efficient under dynamic and failure-prone conditions.

Conclusion: FlowMesh effectively addresses the shift in AI deployment towards pipeline-based workflows by providing a multi-tenant service fabric that optimizes resource utilization while maintaining workflow provenance and efficiency across heterogeneous GPU environments.

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [5] [A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration](https://arxiv.org/abs/2510.27039)
*Zhuo Zheng,Lingran Meng,Ziyu Lin*

Main category: cs.DC

TL;DR: A cloud-based hybrid model combining Spatio-Temporal Graph Neural Networks (ST-GNN) with Transformer architecture for traffic flow prediction, achieving superior performance over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Traditional models fail to capture complex spatial-temporal dependencies in large-scale road networks, especially under external factors like weather, holidays, and traffic accidents.

Method: Integrates ST-GNN for spatial correlations across road networks with Transformer for long-term temporal dependencies, incorporating external contextual features via feature fusion, deployed on cloud computing platform for scalability.

Result: Outperforms baseline methods (LSTM, TCN, GCN, pure Transformer) with RMSE of 17.92 and MAE of 10.53.

Conclusion: Hybrid GNN-Transformer approach provides effective and scalable solution for cloud-based ITS applications, offering methodological advancements for traffic flow forecasting and practical implications for congestion mitigation.

Abstract: Accurate traffic flow forecasting is essential for the development of
intelligent transportation systems (ITS), supporting tasks such as traffic
signal optimization, congestion management, and route planning. Traditional
models often fail to effectively capture complex spatial-temporal dependencies
in large-scale road networks, especially under the influence of external
factors such as weather, holidays, and traffic accidents. To address this
challenge, this paper proposes a cloud-based hybrid model that integrates
Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture
for traffic flow prediction. The model leverages the strengths of GNNs in
modeling spatial correlations across road networks and the Transformers'
ability to capture long-term temporal dependencies. External contextual
features are incorporated via feature fusion to enhance predictive accuracy.
The proposed model is deployed on a cloud computing platform to achieve
scalability and real-time adaptability. Experimental evaluation of the dataset
shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure
Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings
suggest that the hybrid GNN-Transformer approach provides an effective and
scalable solution for cloud-based ITS applications, offering methodological
advancements for traffic flow forecasting and practical implications for
congestion mitigation.

</details>


### [6] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: A synergistic tensor and pipeline parallelism schedule that reduces both TP communication overhead and PP pipeline bubbles by decoupling forward/backward passes into fine-grained units and braiding them into composite sequences.


<details>
  <summary>Details</summary>
Motivation: Hybrid model parallelism combining TP and PP has become dominant for large model training, but TP introduces communication overhead while PP suffers from pipeline bubbles. Existing solutions address these issues separately, lacking a synergistic approach.

Method: Decouples forward and backward passes in PP into fine-grained computation units, then braids them to form composite computation sequences. This structure enables near-complete elimination of TP-related bubbles, with further PP scheduling optimization to minimize pipeline bubbles.

Result: Experimental results show 12% training throughput improvement for LLMs and 16% for MLLMs compared to existing scheduling methods.

Conclusion: The proposed synergistic schedule effectively reduces both TP and PP inefficiencies simultaneously, achieving significant performance gains in distributed training of large models.

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [7] [A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination](https://arxiv.org/abs/2510.27289)
*Zhengchang Hua,Panagiotis Oikonomou,Karim Djemame,Nikos Tziritas,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: DT-MADDPG algorithm integrates multi-agent reinforcement learning with collaborative digital twins to achieve privacy-preserving coordination in V2G networks without centralized raw data collection.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of coordinating large-scale decentralized systems like EV fleets in V2G networks while preserving individual agent privacy, as traditional methods require centralized sensitive data.

Method: A hybrid architecture combining multi-agent reinforcement learning (MADDPG) with collaborative digital twin network, using simulation-assisted learning where centralized critic is enhanced by predictive global model built from privacy-preserving DT data.

Result: Experimental results show DT-MADDPG achieves coordination performance comparable to standard MADDPG while offering significant advantages in data privacy and architectural decentralization.

Conclusion: The work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex real-world cyber-physical systems with privacy preservation.

Abstract: The coordination of large-scale, decentralised systems, such as a fleet of
Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a
significant challenge for modern control systems. While collaborative Digital
Twins have been proposed as a solution to manage such systems without
compromising the privacy of individual agents, deriving globally optimal
control policies from the high-level information they share remains an open
problem. This paper introduces Digital Twin Assisted Multi-Agent Deep
Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid
architecture that integrates a multi-agent reinforcement learning framework
with a collaborative DT network. Our core contribution is a simulation-assisted
learning algorithm where the centralised critic is enhanced by a predictive
global model that is collaboratively built from the privacy-preserving data
shared by individual DTs. This approach removes the need for collecting
sensitive raw data at a centralised entity, a requirement of traditional
multi-agent learning algorithms. Experimental results in a simulated V2G
environment demonstrate that DT-MADDPG can achieve coordination performance
comparable to the standard MADDPG algorithm while offering significant
advantages in terms of data privacy and architectural decentralisation. This
work presents a practical and robust framework for deploying intelligent,
learning-based coordination in complex, real-world cyber-physical systems.

</details>


### [8] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: An online strategy for renewable energy-powered MEC systems that dynamically schedules computational tasks and manages energy consumption through server frequency scaling and service migration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing intermittent harvested energy with dynamic user demand in sustainable MEC systems operating without grid electricity.

Method: Proposes an online strategy that dynamically schedules computational tasks with dependencies and governs energy consumption through real-time decisions on server frequency scaling and service module migration.

Result: Experiments using real-world datasets demonstrate the algorithm's effectiveness in efficiently utilizing harvested energy while maintaining low service latency.

Conclusion: The proposed strategy successfully addresses the trade-off between intermittent energy availability and service demands in EH-powered MEC systems.

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [9] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: This paper develops a machine learning-based heuristic using k-nearest neighbors (kNN) to find optimal sub-system sizes for CUDA implementations of parallel partition algorithms, and extends it to recursive versions.


<details>
  <summary>Details</summary>
Motivation: To efficiently determine the optimum sub-system size for CUDA implementations of parallel partition algorithms for solving systems of linear algebraic equations (SLAE), avoiding empirical trial-and-error approaches.

Method: Used k-nearest neighbors (kNN) classification method to build predictive models for optimal sub-system sizes based on computational experiments with different SLAE sizes, then extended the approach to recursive parallel partition algorithms.

Result: The kNN-based heuristic successfully predicted optimal sub-system sizes with acceptable accuracy when compared to empirical data, and was successfully extended to determine optimal parameters for recursive implementations.

Conclusion: Machine learning approaches like kNN can effectively automate the optimization of parallel algorithm parameters, providing a systematic alternative to empirical methods for both standard and recursive parallel partition implementations.

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


### [10] [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)
*Nandor Licker,Kevin Hu,Vladimir Zaytsev,Lequn Chen*

Main category: cs.DC

TL;DR: TransferEngine provides a uniform interface for point-to-point communication across different NICs, enabling high-throughput data transfer for LLM systems without hardware lock-in.


<details>
  <summary>Details</summary>
Motivation: Existing LLM system patterns require flexible point-to-point communication, but current implementations are locked to specific NICs, hindering integration and portability across hardware providers.

Method: TransferEngine bridges common NICs to expose a uniform interface with one-sided WriteImm operations and ImmCounter primitive for completion notification, transparently managing multiple NICs per GPU without ordering assumptions.

Result: Achieved peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS EFA, with successful implementations in three production systems: KvCache transfer for disaggregated inference, RL weight updates for trillion-parameter models, and MoE dispatch/combine with improved latencies.

Conclusion: TransferEngine provides portable point-to-point communication that complements collectives while avoiding hardware lock-in, enabling flexible LLM system deployment across different hardware platforms.

Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [11] [Lorentzian Switching Dynamics in HZO-based FeMEMS Synapses for Neuromorphic Weight Storage](https://arxiv.org/abs/2510.27095)
*Shubham Jadhav,Kaustav Roy,Luis Amaro,Thejas Basavarajappa,Madhav Ramesh,Debdeep Jena,Huili,Xing,Amit Lal*

Main category: cs.ET

TL;DR: A ferroelectric MEMS (FeMEMS) synapse that stores analog weights in the piezoelectric coefficient of HZO MEMS unimorph, enabling non-destructive mechanical readout with over 7-bit precision and bipolar weight programming.


<details>
  <summary>Details</summary>
Motivation: Conventional ferroelectric synapses require destructive electrical readout, limiting endurance and reliability. This work aims to develop a synapse with non-destructive readout capability.

Method: Using a released Hf0.5Zr0.5O2 (HZO) MEMS unimorph where partial switching of ferroelectric domains modulates the piezoelectric coefficient d31,eff, and low-amplitude mechanical drive reads weights without read-disturb.

Result: Achieved more than 7-bit programming levels, mechanical switching follows Lorentzian distribution consistent with nucleation-limited switching, and established quantitative link between mechanical weights and electrical switching kinetics.

Conclusion: The mechanically read synapse avoids depolarization and charge-injection effects, provides bipolar weights, directly reveals partial domain populations, and offers robust, energy-efficient route for high-bit neuromorphic hardware.

Abstract: Neuromorphic computing demands synaptic elements that can store and update
weights with high precision while being read non-destructively. Conventional
ferroelectric synapses store weights in remnant polarization states and might
require destructive electrical readout, limiting endurance and reliability. We
demonstrate a ferroelectric MEMS (FeMEMS) based synapse in which analog weights
are stored in the piezoelectric coefficient $d_{31,eff}$ of a released
Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) MEMS unimorph. Partial switching of
ferroelectric domains modulates $d_{31,eff}$, and a low-amplitude mechanical
drive reads out the weight without read-disturb in the device yielding more
than 7-bit of programming levels. The mechanical switching distribution
function follows a Lorentzian distribution as a logarithmic function of partial
poling voltage ($V_p$) consistent with nucleation-limited switching (NLS), and
the median threshold extracted from electromechanical data obeys a Merz-type
field-time law with a dimensionless exponent $\alpha = 3.62$. These
relationships establish a quantitative link between mechanical weights and
electrical switching kinetics. This mechanically read synapse avoids
depolarization and charge-injection effects, provides bipolar weights (well
suited for excitatory and inhibitory synapses), directly reveals partial domain
populations, and offers a robust, energy-efficient route toward high-bit
neuromorphic hardware.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [AMD MI300X GPU Performance Analysis](https://arxiv.org/abs/2510.27583)
*Chandrish Ambati,Trung Diep*

Main category: cs.PF

TL;DR: Comprehensive evaluation of AMD MI300X GPUs for large language model inference, comparing performance across compute throughput, memory bandwidth, and interconnect communication against traditional NVIDIA dominance.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of large language models (LLMs) with hundreds of billions of parameters requires high-performance, scalable GPU hardware. While NVIDIA GPUs have traditionally dominated LLM deployments due to their mature CUDA software stack, AMD's latest MI300X GPUs offer a compelling alternative with high HBM capacity, matrix cores, and proprietary interconnect.

Method: The paper presents a comprehensive evaluation of AMD MI300X GPUs across key performance domains critical to LLM inference, including compute throughput, memory bandwidth, and interconnect communication.

Result: The evaluation assesses how AMD MI300X GPUs perform in critical areas for LLM inference, though specific performance metrics and comparisons with NVIDIA GPUs are not detailed in the abstract.

Conclusion: AMD MI300X GPUs present a viable alternative to NVIDIA GPUs for LLM deployments, offering competitive features like high HBM capacity and matrix cores that could potentially challenge NVIDIA's traditional dominance in this space.

Abstract: The rapid growth of large language models (LLMs) has driven the need for
high-performance, scalable GPU hardware capable of efficiently serving models
with hundreds of billions of parameters. While NVIDIA GPUs have traditionally
dominated LLM deployments due to their mature CUDA software stack and state-of
the-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,
featuring high HBM capacity, matrix cores, and their proprietary interconnect.
In this paper, we present a comprehensive evaluation of the AMD MI300X GPUs
across key performance domains critical to LLM inference including compute
throughput, memory bandwidth, and interconnect communication.

</details>
