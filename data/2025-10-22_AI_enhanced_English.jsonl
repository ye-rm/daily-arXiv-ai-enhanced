{"id": "2510.17885", "categories": ["cs.PF", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17885", "abs": "https://arxiv.org/abs/2510.17885", "authors": ["Hongyuan Liu", "Xinyang Liu", "Guosheng Hu"], "title": "Metrics and evaluations for computational and sustainable AI efficiency", "comment": "11 pages, 2 tables", "summary": "The rapid advancement of Artificial Intelligence (AI) has created\nunprecedented demands for computational power, yet methods for evaluating the\nperformance, efficiency, and environmental impact of deployed models remain\nfragmented. Current approaches often fail to provide a holistic view, making it\ndifficult to compare and optimise systems across heterogeneous hardware,\nsoftware stacks, and numeric precisions. To address this gap, we propose a\nunified and reproducible methodology for AI model inference that integrates\ncomputational and environmental metrics under realistic serving conditions. Our\nframework provides a pragmatic, carbon-aware evaluation by systematically\nmeasuring latency and throughput distributions, energy consumption, and\nlocation-adjusted carbon emissions, all while maintaining matched accuracy\nconstraints for valid comparisons. We apply this methodology to multi-precision\nmodels across diverse hardware platforms, from data-centre accelerators like\nthe GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream\nsoftware stacks including PyTorch, TensorRT, and ONNX Runtime. By\nsystematically categorising these factors, our work establishes a rigorous\nbenchmarking framework that produces decision-ready Pareto frontiers,\nclarifying the trade-offs between accuracy, latency, energy, and carbon. The\naccompanying open-source code enables independent verification and facilitates\nadoption, empowering researchers and practitioners to make evidence-based\ndecisions for sustainable AI deployment.", "AI": {"tldr": "A unified methodology for evaluating AI model inference that integrates computational performance, energy consumption, and carbon emissions across diverse hardware and software platforms.", "motivation": "Current AI evaluation methods are fragmented and fail to provide holistic comparisons across different hardware, software stacks, and numeric precisions, making it difficult to optimize systems and assess environmental impact.", "method": "Proposes a reproducible framework that systematically measures latency, throughput distributions, energy consumption, and location-adjusted carbon emissions while maintaining matched accuracy constraints. Applied to multi-precision models across diverse hardware (from data-center accelerators to consumer GPUs) and software stacks (PyTorch, TensorRT, ONNX Runtime).", "result": "Establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying trade-offs between accuracy, latency, energy, and carbon emissions.", "conclusion": "The methodology enables evidence-based decisions for sustainable AI deployment, with open-source code provided for independent verification and adoption by researchers and practitioners."}}
{"id": "2510.18525", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18525", "abs": "https://arxiv.org/abs/2510.18525", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing", "comment": null, "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.", "AI": {"tldr": "SPEQ is a speculative decoding method that uses part of the full-model weight bits to create a quantized draft model, eliminating extra training/storage overhead while achieving significant speedups over existing methods.", "motivation": "Large language models have high inference latency due to their large parameter sizes, and existing quantization methods often cause performance degradation while speculative decoding incurs extra overheads.", "method": "Algorithm-hardware co-design that uses part of full-model weight bits to form quantized draft model, with reconfigurable processing element array for efficient execution of both draft and verification passes.", "result": "Achieves speedups of 2.07x, 1.53x, and 1.45x compared to FP16, Olive, and Tender respectively across 15 LLMs and tasks.", "conclusion": "SPEQ provides an effective solution for reducing LLM inference latency without performance degradation or additional overhead through its co-designed speculative decoding approach."}}
{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "Framework for migrating large-scale atmospheric and oceanic AI models from PyTorch to MindSpore and optimizing for Chinese chips, achieving comparable accuracy while reducing hardware dependencies and improving efficiency.", "motivation": "Current AI models for climate and weather research heavily depend on GPUs, limiting hardware independence especially for Chinese domestic hardware and frameworks, creating a need for alternative solutions.", "method": "Developed a framework focusing on software-hardware adaptation, memory optimization, and parallelism to migrate models from PyTorch to MindSpore and optimize for Chinese chips.", "result": "Experimental results show the migration preserves original model accuracy while significantly reducing system dependencies and improving operational efficiency using Chinese chips as viable alternatives.", "conclusion": "The work provides valuable insights and practical guidance for using Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence."}}
{"id": "2510.18152", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18152", "abs": "https://arxiv.org/abs/2510.18152", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation", "comment": "5 pages, 4 figures, conference", "summary": "Recent advances in distributed learning systems have introduced effective\nsolutions for implementing collaborative artificial intelligence techniques in\nwireless communication networks. Federated learning approaches provide a\nmodel-aggregation mechanism among edge devices to achieve collaborative\ntraining, while ensuring data security, communication efficiency, and sharing\ncomputational overheads. On the other hand, limited transmission resources and\ncomplex communication environments remain significant bottlenecks to the\nefficient collaborations among edge devices, particularly within large-scale\nnetworks. To address such issues, this paper proposes an over-the-air (OTA)\nanalog aggregation method designed for the distributed swarm learning (DSL),\ntermed DSL-OTA, aiming to enhance communication efficiency, enable effective\ncooperation, and ensure privacy preserving. Incorporating multi-worker\nselection strategy with over-the-air aggregation not only makes the standard\nDSL based on single best worker contributing to global model update to become\nmore federated, but also secures the aggregation from potential risks of data\nleakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA\nalgorithm in terms of fast convergence rate and low communication costs.\nSimulation results reveal that our DSL-OTA outperforms the other existing\nmethods by achieving better learning performance under both homogeneous and\nheterogeneous dataset settings.", "AI": {"tldr": "This paper proposes DSL-OTA, an over-the-air analog aggregation method for distributed swarm learning that enhances communication efficiency, enables effective cooperation, and ensures privacy preservation in wireless networks.", "motivation": "Limited transmission resources and complex communication environments are significant bottlenecks for efficient collaboration among edge devices in large-scale wireless networks, despite advances in distributed learning systems.", "method": "The paper proposes DSL-OTA, which incorporates multi-worker selection strategy with over-the-air analog aggregation, making standard DSL more federated and securing aggregation from data leakage risks.", "result": "Theoretical analyses show fast convergence rate and low communication costs. Simulations demonstrate superior learning performance under both homogeneous and heterogeneous dataset settings compared to existing methods.", "conclusion": "DSL-OTA effectively addresses communication bottlenecks in distributed swarm learning while maintaining privacy and achieving better learning performance across different dataset conditions."}}
{"id": "2510.18300", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18300", "abs": "https://arxiv.org/abs/2510.18300", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"], "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces", "comment": null, "summary": "Large-scale GPU traces play a critical role in identifying performance\nbottlenecks within heterogeneous High-Performance Computing (HPC)\narchitectures. However, the sheer volume and complexity of a single trace of\ndata make performance analysis both computationally expensive and\ntime-consuming. To address this challenge, we present an end-to-end parallel\nperformance analysis framework designed to handle multiple large-scale GPU\ntraces efficiently. Our proposed framework partitions and processes trace data\nconcurrently and employs causal graph methods and parallel coordinating chart\nto expose performance variability and dependencies across execution flows.\nExperimental results demonstrate a 67% improvement in terms of scalability,\nhighlighting the effectiveness of our pipeline for analyzing multiple traces\nindependently.", "AI": {"tldr": "An end-to-end parallel framework for efficient analysis of large-scale GPU traces in HPC systems, achieving 67% scalability improvement.", "motivation": "Large-scale GPU traces are crucial for identifying performance bottlenecks in HPC architectures, but their volume and complexity make analysis computationally expensive and time-consuming.", "method": "The framework partitions and processes trace data concurrently, employing causal graph methods and parallel coordinating charts to expose performance variability and dependencies across execution flows.", "result": "Experimental results demonstrate a 67% improvement in scalability, showing effectiveness for analyzing multiple traces independently.", "conclusion": "The proposed parallel performance analysis framework efficiently handles multiple large-scale GPU traces and significantly improves scalability for HPC performance analysis."}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "SLICE is a novel scheduling system for LLM inference services on edge devices that addresses differentiated Service Level Objectives (SLOs) for latency-sensitive applications, achieving significantly higher SLO attainment compared to existing solutions.", "motivation": "Current LLM scheduling systems prioritize throughput maximization but fail to address diverse SLO requirements for edge devices, leading to high violation rates for latency-sensitive applications like machine control and navigation planning.", "method": "SLICE combines a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates to optimize LLM inference services for differentiated SLO requirements.", "result": "Experimental results show SLICE achieves up to 35x higher SLO attainment and 3.4x better task completion time compared to state-of-the-art solutions Orca and FastServe.", "conclusion": "SLICE effectively addresses the limitations of existing LLM scheduling systems by providing differentiated SLO support for edge computing scenarios, significantly improving service quality for latency-sensitive applications."}}
{"id": "2510.18586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18586", "abs": "https://arxiv.org/abs/2510.18586", "authors": ["Zhuohang Bian", "Feiyang Wu", "Teng Ma", "Youwei Zhuo"], "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.", "AI": {"tldr": "Tokencake is a KV-Cache-centric serving framework that optimizes scheduling and memory management for LLMs in multi-agent applications with function calls, reducing latency by 47.06% and improving GPU memory utilization by 16.9% compared to vLLM.", "motivation": "LLMs deployed in multi-agent applications with external function calls face severe KV Cache performance challenges including space contention causing cache eviction and time underutilization leaving GPU memory idle during tool call stalls.", "method": "Tokencake uses agent-aware design with Space Scheduler for dynamic memory partitioning to shield critical agents from contention, and Time Scheduler with proactive offload and predictive upload to repurpose GPU memory during function call stalls.", "result": "Evaluation on multi-agent benchmarks shows Tokencake reduces end-to-end latency by over 47.06% and improves effective GPU memory utilization by up to 16.9% compared to vLLM.", "conclusion": "Tokencake successfully addresses KV Cache performance challenges in multi-agent LLM applications through co-optimized scheduling and memory management, significantly improving both latency and GPU utilization."}}
{"id": "2510.18592", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18592", "abs": "https://arxiv.org/abs/2510.18592", "authors": ["Yuval Gil", "Merav Parter"], "title": "Distributed Interactive Proofs for Planarity with Log-Star Communication", "comment": "To appear in SODA 26", "summary": "We provide new communication-efficient distributed interactive proofs for\nplanarity. The notion of a \\emph{distributed interactive proof (DIP)} was\nintroduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \\emph{prover}\nis a single centralized entity whose goal is to prove a certain claim regarding\nan input graph $G$. To do so, the prover communicates with a distributed\n\\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is\nmeasured by the amount of prover-verifier communication it requires. Namely,\nthe goal is to design a DIP with a small number of interaction rounds and a\nsmall \\emph{proof size}, i.e., a small amount of communication per round. Our\nmain result is an $O(\\log ^{*}n)$-round DIP protocol for embedded planarity and\nplanarity with a proof size of $O(1)$ and $O(\\lceil\\log \\Delta/\\log\n^{*}n\\rceil)$, respectively. In fact, this result can be generalized as\nfollows. For any $1\\leq r\\leq \\log^{*}n$, there exists an $O(r)$-round protocol\nfor embedded planarity and planarity with a proof size of $O(\\log ^{(r)}n)$ and\n$O(\\log ^{(r)}n+\\log \\Delta /r)$, respectively.", "AI": {"tldr": "New communication-efficient distributed interactive proofs for planarity with O(log* n)-round protocols and small proof sizes.", "motivation": "To design efficient distributed interactive proofs for planarity that minimize prover-verifier communication while maintaining verification capabilities.", "method": "Develop DIP protocols with multiple interaction rounds and adjustable proof sizes based on the number of rounds, specifically O(r)-round protocols for planarity with proof sizes O(log^(r)n) and O(log^(r)n + log\u0394/r).", "result": "Achieved O(log* n)-round DIP protocols for embedded planarity with O(1) proof size and for planarity with O(\u2308log \u0394/log* n\u2309) proof size.", "conclusion": "The paper presents communication-efficient distributed interactive proofs for planarity that offer flexible trade-offs between interaction rounds and proof sizes, significantly improving upon previous approaches."}}
{"id": "2510.18640", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18640", "abs": "https://arxiv.org/abs/2510.18640", "authors": ["Nils Japke", "Sebastian Koch", "Helmut Lukasczyk", "David Bermbach"], "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines", "comment": "Published in 2025 IEEE International Conference on Cloud Engineering\n  (IC2E)", "summary": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective.", "AI": {"tldr": "Performance regression detection in CI/CD systems faces challenges due to resource-intensive benchmarking. This vision paper identifies three key challenges: composability of optimization strategies, automated result evaluation, and practical CI/CD integration.", "motivation": "Performance regressions in large-scale software systems cause significant resource inefficiencies, making early detection critical. Current benchmark optimization techniques aren't practically integrated into real-world CI/CD pipelines despite their importance for maintaining service-level agreements.", "method": "The paper presents a conceptual cloud-based benchmarking framework designed to handle identified challenges transparently. It analyzes the current state of benchmark optimization and identifies gaps preventing broader adoption.", "result": "Three central challenges are identified: (a) composability of benchmark optimization strategies, (b) automated evaluation of benchmarking results, and (c) usability and complexity of applying these strategies in CI/CD systems.", "conclusion": "The paper aims to stimulate research toward making performance regression detection in CI/CD systems more practical and effective by presenting these open problems and proposing a conceptual framework to address them."}}
{"id": "2510.18838", "categories": ["cs.DC", "physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.18838", "abs": "https://arxiv.org/abs/2510.18838", "authors": ["Jacob S. Merson", "Cameron W. Smith", "Mark S. Shephard", "Fuad Hasan", "Abhiyan Paudel", "Angel Castillo-Crooke", "Joyal Mathew", "Mohammad Elahi"], "title": "PCMS: Parallel Coupler For Multimodel Simulations", "comment": null, "summary": "This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a\nnew GPU accelerated generalized coupling framework for coupling simulation\ncodes on leadership class supercomputers. PCMS includes distributed control and\nfield mapping methods for up to five dimensions. For field mapping PCMS can\nutilize discretization and field information to accommodate physics\nconstraints. PCMS is demonstrated with a coupling of the gyrokinetic\nmicroturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and\nwith a 5D distribution function coupling of an energetic particle transport\ncode (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also\ndemonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of\n85%.", "AI": {"tldr": "PCMS is a GPU-accelerated coupling framework for multimodel simulations on supercomputers, supporting up to 5D field mapping and demonstrated with plasma physics code couplings achieving 85% weak scaling efficiency on 2,080 GPUs.", "motivation": "To enable efficient coupling of multiple simulation codes on leadership class supercomputers, particularly for complex plasma physics applications requiring GPU acceleration and high-dimensional field mapping capabilities.", "method": "Developed a parallel coupler framework with distributed control and field mapping methods for up to five dimensions, utilizing discretization and field information to accommodate physics constraints. Demonstrated with couplings between gyrokinetic codes (XGC, GTC), Monte Carlo neutral transport code (DEGAS2), and energetic particle transport code (GNET).", "result": "Successfully demonstrated couplings between XGC-DEGAS2 and GNET-GTC codes. Achieved weak scaling on up to 2,080 GPUs of Frontier supercomputer with 85% efficiency.", "conclusion": "PCMS provides an effective GPU-accelerated coupling framework for multimodel simulations on leadership class systems, enabling complex plasma physics applications with high scalability and efficiency."}}
