{"id": "2510.11727", "categories": ["cs.ET", "cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11727", "abs": "https://arxiv.org/abs/2510.11727", "authors": ["Benius Dunn", "Javier Meza-Arroyo", "Armi Tiihonen", "Mark Lee", "Julia W. P. Hsu"], "title": "Multi-objective Bayesian Optimization with Human-in-the-Loop for Flexible Neuromorphic Electronics Fabrication", "comment": null, "summary": "Neuromorphic computing hardware enables edge computing and can be implemented\nin flexible electronics for novel applications. Metal oxide materials are\npromising candidates for fabricating flexible neuromorphic electronics, but\nsuffer from processing constraints due to the incompatibilities between oxides\nand polymer substrates. In this work, we use photonic curing to fabricate\nflexible metal-insulator-metal capacitors with solution-processible aluminum\noxide dielectric tailored for neuromorphic applications. Because photonic\ncuring outcomes depend on many input parameters, identifying an optimal\nprocessing condition through a traditional grid-search approach is unfeasible.\nHere, we apply multi-objective Bayesian optimization (MOBO) to determine\nphotonic curing conditions that optimize the trade-off between desired\nelectrical properties of large capacitance-frequency dispersion and low leakage\ncurrent. Furthermore, we develop a human-in-the-loop (HITL) framework for\nincorporating failed experiments into the MOBO machine learning workflow,\ndemonstrating that this framework accelerates optimization by reducing the\nnumber of experimental rounds required. Once optimization is concluded, we\nanalyze different Pareto-optimal conditions to tune the dielectrics properties\nand provide insight into the importance of different inputs through Shapley\nAdditive exPlanations analysis. The demonstrated framework of combining MOBO\nwith HITL feedback can be adapted to a wide range of multi-objective\nexperimental problems that have interconnected inputs and high experimental\nfailure rates to generate usable results for machine learning models."}
{"id": "2510.11730", "categories": ["cs.ET", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.11730", "abs": "https://arxiv.org/abs/2510.11730", "authors": ["Connor G. McMahan", "Gavin Chang", "Raymond Nguyen", "Souren Soukiazian", "David A. Smith", "Tobias Schaedler", "David Shahan"], "title": "Wireless Sensing of Temperature, Strain and Crack Growth in 3D-Printed Metal Structures via Magnetoelastic and Thermomagnetic Inclusions", "comment": "16 pages, 9 figures", "summary": "In this study, we demonstrate the first realization of wireless strain and\ntemperature sensing within 3D-printed metallic structures using standard\nelectromagnetic inspection hardware. This establishes a path toward need-based\nparts maintenance driven by accurate damage assessments instead of relying on\nregularly scheduled maintenance teardowns, extending the service intervals of\nstructures operating in harsh environments. To this end, we encapsulate\nmagnetoelastic and thermomagnetic materials inside microtubes and embed the\nsensing elements during additive manufacturing. Mechanical and thermal stimuli\naffect the magnetic permeability of the embedded materials, which modulates the\nimpedance of a coil placed on or near the surface of the printed part. We\ndemonstrate strain sensing accurate to +/-27x10-6 over at least a 6x10-4 strain\nrange, and temperature sensing accurate to +/-0.75oC over a 70oC range, both to\na 95% confidence interval. We highlight these sensors' capabilities by\ndetecting the onset of plasticity and fatigue-driven crack growth thousands of\ncycles before critical failure. This extends non-destructive eddy-current\ndamage detection to accurate, real-time strain and temperature monitoring\nwithin metallic structures."}
{"id": "2510.12278", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12278", "abs": "https://arxiv.org/abs/2510.12278", "authors": ["Alessia Ciacco", "Francesca Guerriero", "Eneko Osaba"], "title": "Quantum Annealing for Staff Scheduling in Educational Environments", "comment": "8 pages, 3 tables, and 1 figure. Paper submitted to the International\n  Conference on Quantum Communications, Networking, and Computing (QCNC 2026)", "summary": "We address a novel staff allocation problem that arises in the organization\nof collaborators among multiple school sites and educational levels. The\nproblem emerges from a real case study in a public school in Calabria, Italy,\nwhere staff members must be distributed across kindergartens, primary, and\nsecondary schools under constraints of availability, competencies, and\nfairness. To tackle this problem, we develop an optimization model and\ninvestigate a solution approach based on quantum annealing. Our computational\nexperiments on real-world data show that quantum annealing is capable of\nproducing balanced assignments in short runtimes. These results provide\nevidence of the practical applicability of quantum optimization methods in\neducational scheduling and, more broadly, in complex resource allocation tasks."}
{"id": "2510.11938", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11938", "abs": "https://arxiv.org/abs/2510.11938", "authors": ["Yanying Lin", "Shijie Peng", "Chengzhi Lu", "Chengzhong Xu", "Kejiang Ye"], "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters", "comment": "EuroSys 26", "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."}
{"id": "2510.12166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12166", "abs": "https://arxiv.org/abs/2510.12166", "authors": ["Kenneth Weiss", "Thomas M. Stitt", "Daryl Hawkins", "Olga Pearce", "Stephanie Brink", "Robert N. Rieben"], "title": "Comparing Cross-Platform Performance via Node-to-Node Scaling Studies", "comment": "16 pages; accepted to the International Journal of High Performance\n  Computing Applications (IJHPCA)", "summary": "Due to the increasing diversity of high-performance computing architectures,\nresearchers and practitioners are increasingly interested in comparing a code's\nperformance and scalability across different platforms. However, there is a\nlack of available guidance on how to actually set up and analyze such\ncross-platform studies. In this paper, we contend that the natural base unit of\ncomputing for such studies is a single compute node on each platform and offer\nguidance in setting up, running, and analyzing node-to-node scaling studies. We\npropose templates for presenting scaling results of these studies and provide\nseveral case studies highlighting the benefits of this approach."}
{"id": "2510.12277", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.12277", "abs": "https://arxiv.org/abs/2510.12277", "authors": ["Thomas Benz", "Axel Vanoni", "Michael Rogenmoser", "Luca Benini"], "title": "A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems", "comment": "6 pages, 5 figures", "summary": "With the ever-growing heterogeneity in computing systems, driven by modern\nmachine learning applications, pressure is increasing on memory systems to\nhandle arbitrary and more demanding transfers efficiently. Descriptor-based\ndirect memory access controllers (DMACs) allow such transfers to be executed by\ndecoupling memory transfers from processing units. Classical descriptor-based\nDMACs are inefficient when handling arbitrary transfers of small unit sizes.\nExcessive descriptor size and the serialized nature of processing descriptors\nemployed by the DMAC lead to large static overheads when setting up transfers.\nTo tackle this inefficiency, we propose a descriptor-based DMAC optimized to\nefficiently handle arbitrary transfers of small unit sizes. We implement a\nlightweight descriptor format in an AXI4-based DMAC. We further increase\nperformance by implementing a low-overhead speculative descriptor prefetching\nscheme without additional latency penalties in the case of a misprediction. Our\nDMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a\nKintex FPGA to evaluate its performance. Compared to an off-the-shelf\ndescriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,\nincrease bus utilization up to 2.5x in an ideal memory system with\n64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer\nflip-flops, and no block RAMs. We can extend our lead in bus utilization to\n3.6x with 64-byte-length transfers in deep memory systems. We synthesized our\nDMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44\nGHz while occupying only 49.5 kGE."}
{"id": "2510.12280", "categories": ["cs.PF", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.12280", "abs": "https://arxiv.org/abs/2510.12280", "authors": ["Yosuke Bando", "Akinobu Mita", "Kazuhiro Hiwada", "Shintaro Sano", "Tomoya Suzuki", "Yu Nakanishi", "Kazutaka Tomida", "Hirotsugu Kajihara", "Akiyuki Kaneko", "Daisuke Taki", "Yukimasa Miyamoto", "Tomokazu Yoshida", "Tatsuo Shiozawa"], "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores", "comment": null, "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."}
{"id": "2510.12196", "categories": ["cs.DC", "8W10"], "pdf": "https://arxiv.org/pdf/2510.12196", "abs": "https://arxiv.org/abs/2510.12196", "authors": ["Petr Samoldekin", "Christian Schulz", "Henning Woydt"], "title": "GPU-Accelerated Algorithms for Process Mapping", "comment": null, "summary": "Process mapping asks to assign vertices of a task graph to processing\nelements of a supercomputer such that the computational workload is balanced\nwhile the communication cost is minimized. Motivated by the recent success of\nGPU-based graph partitioners, we propose two GPU-accelerated algorithms for\nthis optimization problem. The first algorithm employs hierarchical\nmultisection, which partitions the task graph alongside the hierarchy of the\nsupercomputer. The method utilizes GPU-based graph partitioners to accelerate\nthe mapping process. The second algorithm integrates process mapping directly\ninto the modern multilevel graph partitioning pipeline. Vital phases like\ncoarsening and refinement are accelerated by exploiting the parallelism of\nGPUs. In our experiments, both methods achieve speedups exceeding 300 when\ncompared to state-of-the-art CPU-based algorithms. The first algorithm has, on\naverage, about 10 percent greater communication costs and thus remains\ncompetitive to CPU algorithms. The second approach is much faster, with a\ngeometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower\nsolution quality. To our knowledge, these are the first GPU-based algorithms\nfor process mapping."}
{"id": "2510.12436", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.12436", "abs": "https://arxiv.org/abs/2510.12436", "authors": ["Valentin Seitz", "Jordy Trilaksono", "Marta Garcia-Gasulla"], "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework", "comment": null, "summary": "Ensuring good performance is a key aspect in the development of codes that\ntarget HPC machines. As these codes are under active development, the necessity\nto detect performance degradation early in the development process becomes\napparent. In addition, having meaningful insight into application scaling\nbehavior tightly coupled to the development workflow is helpful. In this paper,\nwe introduce TALP-Pages, an easy-to-integrate framework that enables developers\nto get fast and in-repository feedback about their code performance using\nestablished fundamental performance and scaling factors. The framework relies\non TALP, which enables the on-the-fly collection of these metrics. Based on a\nfolder structure suited for CI which contains the files generated by TALP,\nTALP-Pages generates an HTML report with visualizations of the performance\nfactor regression as well as scaling-efficiency tables. We compare TALP-Pages\nto tracing-based tools in terms of overhead and post-processing requirements\nand find that TALP-Pages can produce the scaling-efficiency tables faster and\nunder tighter resource constraints. To showcase the ease of use and\neffectiveness of this approach, we extend the current CI setup of GENE-X with\nonly minimal changes required and showcase the ability to detect and explain a\nperformance improvement."}
{"id": "2510.12274", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12274", "abs": "https://arxiv.org/abs/2510.12274", "authors": ["Hao Jiang", "Meng Qin", "Ruijie Kuai", "Dandan Liang"], "title": "Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness", "comment": "16 pages, 16 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "With the rapid growth in computing power demand, cloud native networks have\nemerged as a promising solution to address the challenges of efficient resource\ncoordination, particularly in coping with the dynamic fluctuations of network\nbandwidth in clusters. We propose Metronome, a network-aware and priority-aware\nscheduling mechanism for cloud native networks. This mechanism is designed to\nsupport jobs that exhibit periodic traffic patterns and dynamic bandwidth\ndemands, particularly in the context of distributed training. Specifically,\nMetronome employs a time-division multiplexing approach that leverages job\ntraffic characteristics to construct an elastic network resource allocation\nmodel, enabling efficient bandwidth sharing across multiple jobs. In addition,\nit incorporates a multi-objective optimization strategy, jointly considering\nlatency and job priorities to achieve globally optimal as well as dynamic\nresource allocation. Finally, Metronome adapts to the dynamic environment by\nmonitoring the cluster and performing reconfiguration operations. Extensive\nexperiments with 13 common machine learning models demonstrate that Metronome\ncan enhance cluster resource utilization while guaranteeing service\nperformance. Compared with the existing Kubernetes scheduling mechanisms across\nmultiple scenarios, Metronome reduces job completion time by up to 19.50% while\nimproving average bandwidth utilization by up to 23.20%."}
{"id": "2510.12354", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12354", "abs": "https://arxiv.org/abs/2510.12354", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel", "Stefan Tai"], "title": "A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures gain traction in federated environments,\norganizations are increasingly building consumer-specific data-sharing\npipelines using modular, cloud-native transformation services. Prior work has\nshown that structuring these pipelines with reusable transformation stages\nenhances both scalability and energy efficiency. However, integrating\ntraditional cloud design patterns into such pipelines poses a challenge:\npredefining and embedding patterns can compromise modularity, reduce\nreusability, and conflict with the pipelines dynamic, consumer-driven nature.\nTo address this, we introduce a Kubernetes-based tool that enables the deferred\nand non-intrusive application of selected cloud design patterns without\nrequiring changes to service source code. The tool supports automated pattern\ninjection and collects energy consumption metrics, allowing developers to make\nenergy-aware decisions while preserving the flexible, composable structure of\nreusable data-sharing pipelines."}
{"id": "2510.12436", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.12436", "abs": "https://arxiv.org/abs/2510.12436", "authors": ["Valentin Seitz", "Jordy Trilaksono", "Marta Garcia-Gasulla"], "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework", "comment": null, "summary": "Ensuring good performance is a key aspect in the development of codes that\ntarget HPC machines. As these codes are under active development, the necessity\nto detect performance degradation early in the development process becomes\napparent. In addition, having meaningful insight into application scaling\nbehavior tightly coupled to the development workflow is helpful. In this paper,\nwe introduce TALP-Pages, an easy-to-integrate framework that enables developers\nto get fast and in-repository feedback about their code performance using\nestablished fundamental performance and scaling factors. The framework relies\non TALP, which enables the on-the-fly collection of these metrics. Based on a\nfolder structure suited for CI which contains the files generated by TALP,\nTALP-Pages generates an HTML report with visualizations of the performance\nfactor regression as well as scaling-efficiency tables. We compare TALP-Pages\nto tracing-based tools in terms of overhead and post-processing requirements\nand find that TALP-Pages can produce the scaling-efficiency tables faster and\nunder tighter resource constraints. To showcase the ease of use and\neffectiveness of this approach, we extend the current CI setup of GENE-X with\nonly minimal changes required and showcase the ability to detect and explain a\nperformance improvement."}
{"id": "2510.12597", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12597", "abs": "https://arxiv.org/abs/2510.12597", "authors": ["Ilya Baldin", "Michael Goodrich", "Vardan Gyurjyan", "Graham Heyes", "Derek Howard", "Yatish Kumar", "David Lawrence", "Brad Sawatzky", "Stacey Sheldon", "Carl Timmer"], "title": "Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT", "comment": null, "summary": "Thomas Jefferson National Accelerator Facility (JLab) has partnered with\nEnergy Sciences Network (ESnet) to define and implement an edge to compute\ncluster computational load balancing acceleration architecture. The ESnet-JLab\nFPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to\naddress compression, fragmentation, UDP packet destination redirection (Network\nAddress Translation (NAT)) and decompression and reassembly.\n  EJFAT seamlessly integrates edge and cluster computing to support direct\nprocessing of streamed experimental data. This will directly benefit the JLab\nscience program as well as data centers of the future that require high\nthroughput and low latency for both time-critical data acquisition systems and\ndata center workflows.\n  The EJFAT project will be presented along with how it is synergistic with\nother DOE activities such as an Integrated Research Infrastructure (IRI), and\nrecent results using data sources at JLab, an EJFAT LB at ESnet, and\ncomputational cluster resources at Lawrence Berkeley National Laboratory\n(LBNL)."}
{"id": "2510.12705", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.12705", "abs": "https://arxiv.org/abs/2510.12705", "authors": ["Evelyne Ringoot", "Rabab Alomairy", "Alan Edelman"], "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices", "comment": "13 pages, 7 figures, 3 tables", "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA.jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."}
