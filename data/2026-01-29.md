<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 19]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: Discontinuous DLS is a data-driven error-bounded lossy compressor that uses localized spatial/temporal subspaces informed by data structure to achieve better compression-to-error ratios than data-agnostic methods for scientific simulation data.


<details>
  <summary>Details</summary>
Motivation: The growing volume of scientific simulation data creates storage and transfer challenges, requiring efficient compression solutions that preserve data validity for scientific analysis.

Method: Discontinuous DLS leverages data-informed localized spatial and temporal subspaces to enhance compression efficiency while preserving key features. It's implemented in a distributed MPI environment and is flexible for various scientific data types.

Result: The method significantly reduces storage requirements without compromising critical data fidelity, showing improved compression-to-error ratios compared to state-of-the-art error-bounded compression methods.

Conclusion: Discontinuous DLS is a promising approach for large-scale scientific data compression in HPC environments, providing a robust solution for managing growing data demands of modern scientific simulations.

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [2] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion is a topology-aware efficient DiT serving engine that addresses limitations in current sequence parallelism techniques for Diffusion Transformers, achieving up to 1.77× speedup over state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current sequence parallelism techniques have suboptimal communication patterns, latency bottlenecks from all-to-all operations, and GPU synchronization overheads.

Method: StreamFusion incorporates three key innovations: (1) topology-aware sequence parallelism that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention enabling overlapping of inter-machine all-to-all operations with computation, and (3) one-sided communication implementation minimizing GPU sender-receiver synchronization and computation overheads.

Result: Experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of 1.35× (up to 1.77×).

Conclusion: StreamFusion provides an efficient DiT serving engine that addresses communication bottlenecks in current sequence parallelism techniques, significantly improving inference performance for high-resolution image and video generation.

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [3] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: A unified framework for multi-task contention classification in high-dimensional systems using representation transformation, graph modeling, and task decoupling with adaptive loss weighting.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately identifying multi-task contention types in high-dimensional system environments where traditional methods struggle with complex interference patterns and metric dependencies.

Method: 1. Construct system state representations from high-dimensional metric sequences with nonlinear transformations to extract cross-dimensional dynamic features. 2. Use graph-based modeling to capture latent dependencies among metrics and learn competitive propagation patterns. 3. Design task-specific mapping structures to model differences among contention types. 4. Employ adaptive multi-task loss weighting to balance shared and task-specific feature learning.

Result: Experiments on public system trace datasets show advantages in accuracy, recall, precision, and F1 scores. Sensitivity analyses on batch size, training sample scale, and metric dimensionality confirm model stability and applicability.

Conclusion: Structured representations and multi-task classification based on high-dimensional metrics significantly improve contention pattern recognition and provide a reliable technical approach for performance management in complex computing environments.

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [4] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT is an open-source distributed LLM optimization framework that automates model compression and tuning workflows, enabling non-expert teams to achieve 2x+ GPU throughput improvements while democratizing enterprise LLM deployment.


<details>
  <summary>Details</summary>
Motivation: Enterprise LLM deployment faces scalability challenges due to constrained compute budgets and scarce specialized expertise for manual optimization. Organizations struggle to optimize models systematically across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.

Method: OptiKIT is a distributed LLM optimization framework that automates complex optimization workflows through dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration. It democratizes model compression and tuning for non-expert teams.

Result: In production, OptiKIT delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. The system has been open-sourced for external contributions and broader reproducibility.

Conclusion: OptiKIT successfully addresses the enterprise LLM scalability challenge by automating optimization workflows, enabling organizations to scale AI initiatives within constrained compute budgets while democratizing access to advanced model optimization techniques for non-expert teams.

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [5] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: USF is a user-space scheduling framework with SCHED_COOP policy that reduces interference in oversubscribed parallel workloads by switching threads only upon blocking, achieving up to 2.4x performance gains.


<details>
  <summary>Details</summary>
Motivation: Traditional OS schedulers struggle with complex parallel applications combining multiple runtimes, causing performance degradation due to interference when oversubscribed (more threads than cores).

Method: Developed User-space Scheduling Framework (USF) implemented in user-space, extended GNU C library with nOS-V runtime, and created SCHED_COOP policy that switches threads only upon blocking to reduce interference.

Result: Achieved up to 2.4x performance gains in oversubscribed multi-process scenarios including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics simulations.

Conclusion: USF with SCHED_COOP policy effectively reduces scheduling interference in complex parallel workloads, enabling better performance without requiring invasive application changes or special permissions.

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [6] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap is a compiler and runtime system that enables fine-grained overlap of computation and communication within single fused kernels, addressing communication bottlenecks in large-scale GPU workloads.


<details>
  <summary>Details</summary>
Motivation: Communication is a major bottleneck in large-scale GPU workloads. Existing distributed compilers use coarse-grained overlap at stream level, which incurs extra kernel launches, device-wide synchronizations, and leaves slack when slowest tiles stretch communication tails.

Method: AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure. It performs source-to-source transformations on Triton kernels to align computation with chunk availability, allowing chunk-level plans to be ported from existing compilers, written by users, or instantiated from templates.

Result: AutoOverlap delivers an average end-to-end speedup of 1.3× and up to 4.7× on multi-GPU workloads.

Conclusion: AutoOverlap enables automatic fine-grained overlap inside single fused kernels, significantly improving performance of communication-bound GPU workloads by better overlapping computation and communication at a finer granularity than existing approaches.

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [7] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece is a distributed inference system using RDMA optimization for AI-generated content workflows, achieving 16x GPU efficiency improvements through microservices architecture and novel memory management.


<details>
  <summary>Details</summary>
Motivation: Existing AIGC systems face inefficiencies in throughput, resource utilization, and scalability under concurrent workloads, requiring better distributed solutions for production environments.

Method: Decomposes AIGC pipelines into fine-grained microservices, uses one-sided RDMA communication to reduce latency/CPU overhead, implements double-ring buffer for deadlock-free RDMA memory access, and employs dynamic Node Manager for elastic resource allocation.

Result: Achieves 16x reduction in GPU resource consumption for Wan2.1 image-to-video generation compared to monolithic inference pipelines, with improved scalability and fault tolerance.

Conclusion: OnePiece provides a scalable, fault-tolerant, and efficient distributed inference solution optimized for production AIGC environments through RDMA optimization and microservices architecture.

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [8] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: Agentic Fog (AF) is a novel framework for fog/edge computing that uses policy-driven autonomous agents with p2p coordination, formalized as an exact potential game, providing guaranteed convergence and stability while outperforming traditional approaches in latency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Fog and edge computing need adaptive control schemes for partial observability, latency requirements, and dynamic workloads. Existing Agentic AI approaches using LLMs are unsuitable for infrastructure systems due to high computational cost, stochastic nature, and poor formal analyzability.

Method: Proposes Agentic Fog (AF) model where fog nodes are policy-driven autonomous agents communicating via p2p interactions based on shared memory and localized coordination. System goals are decomposed into abstract policy guidance, and decentralized fog coordination is formalized as an exact potential game.

Result: AF framework guarantees convergence and stability under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations show AF achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions.

Conclusion: Agentic Fog provides a formally analyzable, stable, and efficient framework for fog/edge computing that addresses the limitations of existing approaches while maintaining adaptability and performance under dynamic conditions.

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: A novel bit-truncation memory with full adaptation flexibility that can truncate any number of data bits at runtime to optimize power/energy efficiency for various approximate applications, achieving up to 51.69% power savings with low implementation cost.


<details>
  <summary>Details</summary>
Motivation: Existing bit-truncation memories require custom designs for specific applications, limiting their flexibility and adaptability for different approximate computing scenarios in edge environments where power efficiency is critical.

Method: Developed a novel bit-truncation memory architecture with full adaptation flexibility that can truncate any number of data bits at runtime to meet different quality-power trade-off requirements for various approximate applications.

Result: The memory supports three different video applications (luminance-aware, content-aware, and region-of-interest-aware) with up to 47.02% power savings compared to state-of-the-art. It also achieves up to 51.69% power savings for both baseline and pruned lightweight deep learning models with only 2.89% silicon area overhead.

Conclusion: The proposed bit-truncation memory provides a flexible, efficient solution for approximate computing applications, enabling significant power savings across multiple application domains with minimal implementation cost, making it suitable for edge deployment.

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [10] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: Dual-ring wear leveling: A new memory wear-leveling technique inspired by the golden ratio and flower petal arrangement to evenly distribute memory usage and extend memory lifespan without hardware changes or performance overhead.


<details>
  <summary>Details</summary>
Motivation: Memory lifespan is crucial for e-waste reduction and sustainability. Uneven memory wear is a major problem, especially for emerging memory technologies like phase-change memory which have shorter lifespans. Existing solutions are either hardware-intensive or limited to specific program constructs.

Method: Dual-ring wear leveling: Models memory as two rings and combines this with existing memory management and garbage collection. Inspired by the golden ratio and how it helps flower petals evenly receive sunlight, the method provides deterministic wear leveling that automatically adapts to memory size.

Result: The solution effectively reduces memory wear and lengthens memory lifespan. It is deterministic, automatically adapts to memory size, requires no hardware changes, and adds no slowdown to program executions.

Conclusion: Dual-ring wear leveling offers a practical and effective approach to memory wear leveling that addresses limitations of existing solutions by being hardware-independent, broadly applicable, and performance-neutral while extending memory lifespan for sustainability.

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [11] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: STELLAR is a framework that uses structural similarity to guide LLMs in generating SystemVerilog Assertions, improving quality by retrieving relevant (RTL, SVA) pairs from a knowledge base.


<details>
  <summary>Details</summary>
Motivation: Manual writing of SystemVerilog Assertions (SVAs) for Formal Verification is slow and error-prone. Existing LLM approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions.

Method: STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts for LLM-based SVA generation.

Result: Experiments show STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness compared to existing approaches.

Conclusion: Structure-aware retrieval is a promising direction for industrial Formal Verification, enabling more effective LLM-based SVA generation.

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [12] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: DABench-LLM is a benchmarking framework for evaluating LLM training performance on dataflow AI accelerators, addressing the lack of standardized benchmarking for these emerging hardware platforms.


<details>
  <summary>Details</summary>
Motivation: Traditional CPU/GPU architectures struggle with LLM growth due to Moore's Law slowdown, while dataflow accelerators offer promise but lack comprehensive performance analysis and standardized benchmarking methodologies.

Method: DABench-LLM combines intra-chip performance profiling and inter-chip scalability analysis to evaluate LLM workloads across key metrics like resource allocation, load balance, and resource efficiency.

Result: The framework was validated on three commodity dataflow accelerators (Cerebras WSE-2, SambaNova RDU, Graphcore IPU), revealing performance bottlenecks and providing specific optimization strategies.

Conclusion: DABench-LLM demonstrates generality and effectiveness across diverse dataflow-based AI hardware platforms, helping researchers gain insights into hardware behaviors and guiding performance optimizations.

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [13] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: Physics-informed hardware-aware training recovers ideal neural network accuracy on silicon analog chips despite non-idealities like capacitive crosstalk and voltage drop.


<details>
  <summary>Details</summary>
Motivation: Silicon-based analog neural networks have non-idealities that degrade performance. Traditional approaches improve fidelity through extensive calibration and conservative design at high energy/area cost. A better approach is needed to maintain accuracy while enabling scalability.

Method: Developed a physics-informed hardware-aware model for time-domain vector-matrix multipliers using single-transistor floating-gate memory cells. Model accounts for capacitive crosstalk and bit-line voltage drop, discretizes operations into adaptive time slots, processes activation patterns in parallel, and accumulates contributions. Calibrated with 16x16 silicon array measurements, introduced improved weight extraction procedure.

Result: Model calibration showed crosstalk is layout-dependent and often dominant. Improved weight extraction doubled signal-to-error ratio versus ideal model. Hardware-aware training recovered ideal software network accuracy across three architectures: custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and VGG-style CNN on CIFAR-10.

Conclusion: Physics-informed hardware-aware training enables full recovery of inference accuracy despite analog non-idealities, establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips that is more scalable than traditional calibration-intensive approaches.

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [14] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC is a generative Transformer model that creates approximate circuits with controlled errors, achieving 6.4% area reduction and 4.3x speedup compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Approximate computing offers significant PPA (performance, power, area) improvements for error-tolerant applications by introducing controlled errors, but existing methods need better integration of error thresholds and AI-driven design automation.

Method: GTAC uses a generative Transformer-based model that innovatively integrates error thresholds into the circuit design process, combining approximate computing principles with AI-driven EDA techniques.

Result: GTAC reduces area by 6.4% under error rate constraints compared to state-of-the-art methods while being 4.3 times faster in generating approximate circuits.

Conclusion: GTAC demonstrates that generative Transformer models can effectively produce approximate circuits with better PPA trade-offs and faster design cycles, advancing AI-driven EDA for approximate computing applications.

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [15] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: RAPID-Graph is a co-designed PIM system that accelerates all-pairs shortest paths computation through algorithm, architecture, and device-level optimizations, achieving significant speedups and energy efficiency over GPU clusters and prior PIM accelerators.


<details>
  <summary>Details</summary>
Motivation: All-pairs shortest paths (APSP) is computationally intensive with cubic complexity, creating data movement bottlenecks in conventional memory hierarchies that overwhelm bandwidth limitations.

Method: Three-level co-design: 1) Algorithm: recursion-aware partitioner decomposes graphs into vertex tiles for in-place Floyd-Warshall and Min-Plus execution in PIM arrays; 2) Architecture: 2.5D PIM stack with phase-change memory compute dies, logic die, and high-bandwidth scratchpad; 3) Device: integrated advanced package with external non-volatile storage for persistent APSP results.

Result: On OGBN-Products dataset (2.45M nodes): 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters; 8.3x speedup and 104x efficiency improvement over prior PIM accelerators; up to 42.8x speedup and 392x energy savings over NVIDIA H100 GPU.

Conclusion: RAPID-Graph demonstrates that co-designed PIM systems with algorithm-aware partitioning and integrated memory-compute stacks can effectively overcome APSP bottlenecks, achieving dramatic performance and energy efficiency improvements for large-scale graph analytics.

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [16] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: CHIME is a chiplet-based heterogeneous near-memory accelerator for edge multimodal LLMs that combines M3D DRAM and RRAM chiplets to address KV cache and weight storage challenges, achieving significant speedup and energy efficiency improvements over edge GPUs and PIM accelerators.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of multimodal LLMs faces latency and energy constraints due to high-dimensional visual inputs creating extensive token sequences, which inflate KV cache and cause substantial data movement overheads to the LLM backbone.

Method: CHIME uses chiplet-based heterogeneous near-memory acceleration with integrated M3D DRAM (for low-latency attention bandwidth) and RRAM chiplets (for dense non-volatile weight storage), orchestrated by a co-designed mapping framework that executes fused kernels near data to minimize cross-chiplet traffic.

Result: Achieves up to 54x speedup and 246x better energy efficiency per inference compared to NVIDIA Jetson Orin NX, sustains 116.5-266.5 token/J vs Jetson's 0.7-1.1 token/J, delivers up to 69.2x higher throughput than state-of-the-art PIM accelerator FACIL, and improves energy efficiency by 7% and performance by 2.4x over M3D DRAM-only design.

Conclusion: CHIME's heterogeneous memory architecture effectively addresses edge MLLM inference challenges by leveraging complementary memory technologies with near-data processing, demonstrating substantial performance and efficiency gains for edge multimodal assistants.

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [17] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: KV cache offloading for long-context LLM inference faces PCIe bandwidth bottlenecks, with analysis showing typical workloads exceed critical memory-bound thresholds by orders of magnitude, leading to inefficient GPU utilization.


<details>
  <summary>Details</summary>
Motivation: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks that need to be addressed.

Method: Develops an analytical framework to derive κ_crit (critical cached-to-prefill token ratio where execution becomes memory-bound), performs empirical characterization of latency and GPU utilization, and proposes optimizations for hardware interconnects, model architectures, and scheduling algorithms.

Result: Empirical characterization reveals 99% of latency spent on transfers, and serving offloaded requests results in GPUs consuming only 28% of their rated TDP. Typical workloads exceed the critical threshold κ_crit by orders of magnitude.

Conclusion: The severe PCIe bandwidth bottlenecks in KV cache offloading necessitate optimizations across hardware interconnects, model architectures, and scheduling algorithms to improve efficiency and GPU utilization for long-context LLM inference.

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [18] [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911)
*Ilsun Chang*

Main category: cs.AR

TL;DR: Hybrid CPU-GPU architecture for OLAP systems that selectively offloads high-impact primitives (Top-K selection, join probe) to GPU using key-only transfer and risk-aware gating to improve tail latency.


<details>
  <summary>Details</summary>
Motivation: Modern OLAP systems have addressed I/O bottlenecks through storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are becoming new bottlenecks at scale.

Method: Proposes a hybrid architecture that augments vectorized execution by selectively offloading high-impact primitives to GPU. Uses key-only transfer (keys and pointers) with late materialization to reduce data movement. Introduces Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer costs, kernel costs, post-processing costs, and candidate-set complexity (K, M).

Result: Using PostgreSQL microbenchmarks and GPU proxy measurements, the approach shows improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading.

Conclusion: The work extends risk-aware gating principles from optimizer-stage GPU-assisted measurement to execution-layer OLAP primitives, demonstrating that selective GPU offloading with intelligent gating can effectively address CPU bottlenecks in modern OLAP systems.

Abstract: Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.

</details>


### [19] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: First instruction-level fault injection study of LLM inference reliability, analyzing effects of model architecture, parameter scale, and task complexity on soft error resilience.


<details>
  <summary>Details</summary>
Motivation: LLMs are compute- and memory-intensive, requiring high-performance GPUs that are increasingly susceptible to soft errors due to shrinking transistor sizes. While prior work examined GPU reliability for general applications and conventional neural networks, systematic analysis of modern large-scale LLMs remains limited despite their rapid adoption and unique characteristics.

Method: Conducted the first instruction-level fault injection study of LLM inference to systematically analyze reliability characteristics from multiple perspectives.

Result: The study reveals reliability characteristics highlighting the effects of model architecture, parameter scale, and task complexity on LLM resilience to soft errors.

Conclusion: Findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms for large language models.

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [20] [PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator](https://arxiv.org/abs/2601.19920)
*Yuval Harary,Almog Sharoni,Esteban Garzón,Marco Lanuzza,Adam Teman,Leonid Yavits*

Main category: cs.AR

TL;DR: PiC-BNN is a true end-to-end binary neural network accelerator using Hamming distance tolerant Content Addressable Memory that achieves software-equivalent accuracy without full precision operations.


<details>
  <summary>Details</summary>
Motivation: Typical BNNs still use full precision operations for certain layers (batch normalization, softmax, output layer, input layer), limiting area/energy benefits and requiring architectural support for mixed precision. There's a need for a truly end-to-end binary accelerator.

Method: PiC-BNN uses Hamming distance tolerant Content Addressable Memory (CAM) to apply the law of large numbers for accurate classification. It's designed and manufactured in 65nm process, enabling true binary operations throughout without full precision layers.

Result: Achieves 95.2% accuracy on MNIST and 93.5% on Hand Gesture dataset, with 560K inferences/s throughput and 703M inferences/s/W power efficiency for binary MLP models.

Conclusion: PiC-BNN demonstrates that true end-to-end binary neural networks can achieve software-equivalent accuracy through Hamming distance tolerance and CAM-based architecture, eliminating the need for full precision operations while maintaining high efficiency.

Abstract: Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.

</details>


### [21] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: Bench4HLS is a comprehensive benchmarking framework for evaluating LLM-generated high-level synthesis (HLS) designs, featuring 170 validated case studies and automated assessment of compilation, functionality, and PPA analysis.


<details>
  <summary>Details</summary>
Motivation: While LLMs show strong capabilities in RTL code generation, their use in HLS remains less mature but is growing rapidly. There's a need for a dedicated benchmarking framework to evaluate LLM-based HLS designs as interest in this area increases.

Method: Created Bench4HLS with 170 manually drafted and validated case studies from public repositories, supporting automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility. Includes pluggable API for PPA analysis across various HLS toolchains and architectures.

Result: Bench4HLS provides a structured, extensible testbed demonstrated with Xilinx Vitis HLS and validated on Catapult HLS, establishing foundational methodology for benchmarking LLMs in HLS workflows.

Conclusion: Bench4HLS addresses the growing need for comprehensive evaluation of LLM-generated HLS designs, enabling systematic benchmarking and comparison across different LLMs and HLS toolchains.

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [22] [Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification](https://arxiv.org/abs/2601.20061)
*Dhruv Parikh,Jebacyril Arockiaraj,Viktor Prasanna*

Main category: cs.AR

TL;DR: This paper presents a novel image-encoding algorithm for Hyperdimensional Computing (HDC) that processes local image patches similar to CNNs, and an FPGA accelerator that achieves significant speedups over CPU/GPU implementations.


<details>
  <summary>Details</summary>
Motivation: HDC uses high-dimensional, low-precision vectors for lightweight, noise-tolerant computing, but its high dimensionality and repeated data movement cause inefficiencies on conventional processors, limiting real-time performance.

Method: 1) Developed CNN-inspired image encoding algorithm that maps local patches to spatially enriched hypervectors, then merges them using HDC operations. 2) Designed FPGA accelerator with pipelined architecture exploiting parallelism across hypervector dimensionality and image patches.

Result: Encoder achieved 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based encoders. FPGA implementation on Alveo U280 achieved 0.09ms inference latency with 1300x speedup over CPU and 60x over GPU baselines.

Conclusion: The paper demonstrates both algorithmic improvements for HDC image processing and hardware acceleration that overcomes traditional processor limitations, enabling efficient real-time HDC inference.

Abstract: Hyperdimensional Computing (HDC) represents data using extremely high-dimensional, low-precision vectors, termed hypervectors (HVs), and performs learning and inference through lightweight, noise-tolerant operations. However, the high dimensionality, sparsity, and repeated data movement involved in HDC make these computations difficult to accelerate efficiently on conventional processors. As a result, executing core HDC operations: binding, permutation, bundling, and similarity search: on CPUs or GPUs often leads to suboptimal utilization, memory bottlenecks, and limits on real-time performance. In this paper, our contributions are two-fold. First, we develop an image-encoding algorithm that, similar in spirit to convolutional neural networks, maps local image patches to hypervectors enriched with spatial information. These patch-level hypervectors are then merged into a global representation using the fundamental HDC operations, enabling spatially sensitive and robust image encoding. This encoder achieves 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based image encoders. Second, we design an end-to-end accelerator that implements these compute operations on an FPGA through a pipelined architecture that exploits parallelism both across the hypervector dimensionality and across the set of image patches. Our Alveo U280 implementation delivers 0.09ms inference latency, achieving up to 1300x and 60x speedup over state-of-the-art CPU and GPU baselines, respectively.

</details>


### [23] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: The paper proposes multi-level priority encoder designs (3-4 levels) using cascading and composition techniques, analyzes their complexity/delay tradeoffs compared to existing designs, and provides design recommendations for hardware implementation.


<details>
  <summary>Details</summary>
Motivation: Priority encoders are expensive hardware components at high bit precisions, but reducing their complexity could accelerate key applications like high-precision integer arithmetic and content-addressable memory.

Method: Generalize the two-level priority encoder structure to three and four levels using cascading and composition techniques, analyze complexity and delay across FPGA and ASIC implementations, and compare with traditional single-level, tree-based, recursive, and two-level designs.

Result: Two-level architecture reduces complexity by ~50% with corresponding delay increase; additional levels have diminishing returns. Tree/recursive designs are faster but more complex. Analysis provides patterns across input lengths and implementation technologies.

Conclusion: Provides a priority encoder toolkit with recommendations for selecting optimal architecture based on input length, implementation technology, and design priorities (complexity vs. delay), enabling hardware designers to create optimal designs.

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [24] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: Analysis of NVIDIA datacenter GPU technical progress from mid-2000s to present, quantifying performance trends and estimating impact of US export controls on AI chip access.


<details>
  <summary>Details</summary>
Motivation: GPUs are critical for AI and scientific computing, but rapid advancements and US export controls create uncertainty about future access to advanced AI chips. Understanding historical GPU progress helps anticipate future constraints on scientific research.

Method: Compiled comprehensive dataset of datacenter NVIDIA GPUs with features including computational performance, memory bandwidth, release price, and power consumption. Analyzed trends and calculated progress indicators for various metrics.

Result: Found doubling times: 1.44-1.69 years for FP16/FP32 operations, 2.06-3.79 years for FP64, 3.32-3.53 years for memory size/bandwidth. Release prices doubled every 5.1 years, power consumption every 16 years. Export controls would shrink performance gap from 23.6x to 3.54x.

Conclusion: GPU computing performance grows faster than memory capabilities, with prices increasing moderately and power efficiency improving significantly. US export controls could substantially reduce international access to advanced AI chips, potentially creating significant performance gaps.

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [25] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: SATA is a dynamic scheduling scheme for selective token attention in Transformers that improves hardware efficiency by managing sparse access patterns and exploiting data locality.


<details>
  <summary>Details</summary>
Motivation: Transformers' quadratic attention scaling creates hardware efficiency challenges. While quantization and pruning help, selective token attention reduces computation by focusing on relevant tokens, but introduces sparse access patterns that need efficient hardware management.

Method: Proposed SATA (locality-centric dynamic scheduling scheme) that proactively manages sparsely distributed access patterns from selective Query-Key operations. It reorders operand flow, exploits data locality, enables early fetch/retirement of intermediate Query/Key vectors, and improves system utilization.

Result: Experimental evaluation shows SATA improves system throughput by up to 1.76x, boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

Conclusion: SATA effectively addresses hardware efficiency challenges in selective token attention for Transformers through intelligent scheduling that manages sparse access patterns and exploits data locality.

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [26] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: The paper proposes NPU architecture optimizations for diffusion LLM sampling, achieving 2.53x speedup over GPUs by addressing memory bottlenecks in token generation.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs have different sampling characteristics than transformer models, with sampling accounting for up to 70% of inference latency due to memory-intensive operations that current NPUs handle inefficiently.

Method: Identified critical NPU instructions for dLLM sampling, designed lightweight non-GEMM vector primitives, implemented in-place memory reuse strategies, and created a decoupled mixed-precision memory hierarchy.

Result: Achieved up to 2.53x speedup over NVIDIA RTX A6000 GPU under equivalent technology node, with open-sourced cycle-accurate simulation and RTL verification code confirming functional equivalence.

Conclusion: Specialized NPU architectures with targeted optimizations for dLLM sampling operations can significantly outperform general-purpose GPUs, addressing the memory bottlenecks that dominate diffusion model inference latency.

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


### [27] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: VersaQ-3D is an algorithm-architecture co-design framework that enables efficient 4-bit quantization of billion-parameter VGGT models for on-device 3D reconstruction, overcoming challenges of activation saturation, diverse 3D semantics, and hardware constraints.


<details>
  <summary>Details</summary>
Motivation: VGGT enables strong feed-forward 3D reconstruction but its billion-parameter scale creates high memory and compute demands that hinder on-device deployment. Existing LLM quantization methods fail due to saturated activation channels and diverse 3D semantics, plus hardware challenges with precision-sensitive nonlinear operators and memory-intensive global attention.

Method: Algorithmically: Introduces first calibration-free, scene-agnostic quantization for VGGT down to 4-bit using orthogonal transforms to decorrelate features and suppress outliers. Architecturally: Designs reconfigurable accelerator supporting BF16, INT8, and INT4 with unified systolic datapath handling both linear and nonlinear operators, plus two-stage recomputation-based tiling for memory-efficient long-sequence attention.

Result: VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power consumption.

Conclusion: VersaQ-3D enables efficient instant 3D reconstruction on edge devices by solving the quantization and hardware challenges of billion-parameter VGGT models through algorithm-architecture co-design, achieving high accuracy preservation and significant performance improvements.

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [28] [Colored Markov Modulated Fluid Queues](https://arxiv.org/abs/2601.20537)
*Benny Van Houdt*

Main category: cs.PF

TL;DR: The paper introduces colored Markov-modulated fluid queues (MMFQs) and colored MMFQs with fluid jumps, extending classical MMFQs by adding color-based memory to track fluid levels during specific events, enhancing modeling flexibility for complex queueing systems.


<details>
  <summary>Details</summary>
Motivation: Classical MMFQs are limited by their inability to track when specific events occurred in relation to fluid levels, making certain queueing systems intractable due to dimensionality or state-space explosion. The authors aim to enhance modeling flexibility by introducing memory through color coding.

Method: The authors generalize the MMFQ framework by introducing colored MMFQs and colored MMFQs with fluid jumps. This adds an additional form of memory where the color of incoming fluid can track fluid levels when certain events occurred, enabling analysis of previously intractable systems.

Result: The enriched framework provides enhanced modeling capabilities that can handle queueing systems that would otherwise suffer from the curse of dimensionality or state-space explosion, making previously intractable systems analyzable.

Conclusion: Colored MMFQs represent a significant generalization of classical fluid queue models, offering improved memory capabilities through color tracking that expands the range of analyzable queueing systems and addresses limitations of traditional MMFQ approaches.

Abstract: Markov-modulated fluid queues (MMFQs) are a powerful modeling framework for analyzing the performance of computer and communication systems. Their distinguishing feature is that the underlying Markov process evolves on a continuous state space, making them well suited to capture the dynamics of workloads, energy levels, and other performance-related quantities. Although classical MMFQs do not permit jumps in the fluid level, they can still be applied to analyze a wide range of jump processes.
  In this paper, we generalize the MMFQ framework in a new direction by introducing {\bf colored MMFQs} and {\bf colored MMFQs with fluid jumps}. This enriched framework provides an additional form of memory: the color of incoming fluid can be used to keep track of the fluid level when certain events took place. This capability greatly enhances modeling flexibility and enables the analysis of queueing systems that would otherwise be intractable due to the curse of dimensionality or state-space explosion.

</details>


### [29] [The Multiserver-Job Stochastic Recurrence Equation for Cloud Computing Performance Evaluation](https://arxiv.org/abs/2601.20653)
*Francois Baccelli,Diletta Olliaro,Marco Ajmone Marsan,Andrea Marin*

Main category: cs.PF

TL;DR: Researchers analyze the Multiserver-Job Queuing Model using stochastic recurrence equations and ergodic theory, proving monotonicity and separability properties to define stability conditions and develop efficient sampling algorithms.


<details>
  <summary>Details</summary>
Motivation: To study the Multiserver-Job Queuing Model (MJQM) with general independent arrivals and service times under FCFS scheduling, addressing the need for formal stability analysis and efficient performance evaluation methods for complex queuing systems.

Method: Using stochastic recurrence equations (SREs) and ergodic theory to prove monotonicity and separability properties of MJQM SRE, applying the monotone-separable extension of Loynes' theorem, and developing two algorithms: one for sub-perfect sampling of system workload and another for estimating stability conditions.

Result: Proved monotonicity and separability properties enabling formal definition of MJQM stability condition, developed massively parallelizable GPU algorithms for sub-perfect sampling and stability estimation, and demonstrated extension to more complex systems including MJQMs with typed resources.

Conclusion: The theoretical framework provides formal stability analysis for MJQM, while the developed algorithms offer efficient performance evaluation through GPU parallelization, with applicability extending to more complex queuing systems with typed resources.

Abstract: We study the Multiserver-Job Queuing Model (MJQM) with general independent arrivals and service times under FCFS scheduling, using stochastic recurrence equations (SREs) and ergodic theory. We prove the monotonicity and separability properties of the MJQM SRE, enabling the application of the monotone-separable extension of Loynes' theorem and the formal definition of the MJQM stability condition. Based on these results, we introduce and implement two algorithms: one for drawing sub-perfect samples (SPS) of the system's workload and the second one to estimate the system's stability condition given the statistics of the jobs' input stream. The SPS algorithm allows for a massive GPU parallelization, greatly improving the efficiency of performance metrics evaluation. We also show that this approach extends to more complex systems, including MJQMs with typed resources.

</details>
