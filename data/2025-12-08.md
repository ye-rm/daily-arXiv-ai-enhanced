<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing](https://arxiv.org/abs/2512.05342)
*Saitao Zhang,Yubiao Luo,Shiqing Wang,Pushen Zuo,Yongxiang Li,Lunshuai Pan,Zheng Miao,Zhong Sun*

Main category: cs.ET

TL;DR: Researchers demonstrate a second-order optimizer using analog matrix computing with RRAM that performs matrix inversion in one step, achieving faster convergence and better energy efficiency than first-order methods.


<details>
  <summary>Details</summary>
Motivation: Second-order optimization methods offer superior convergence but are impractical for large-scale neural networks due to the high computational cost of matrix inversion operations.

Method: Use in-memory analog matrix computing (AMC) with resistive random-access memory (RRAM) to perform matrix inversion in a single step, enabling efficient second-order optimization.

Result: Achieved 26% and 61% fewer training epochs than SGD with momentum and Adam respectively for CNN classification; 5.88x throughput improvement and 6.9x energy efficiency gain over digital processors.

Conclusion: AMC circuits enable practical second-order neural network training with significant speed and energy efficiency advantages, opening new paths for AI acceleration.

Abstract: Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR addresses bandwidth-constrained clients in federated learning by gradually increasing sub-model density during training, enabling effective participation throughout the process with convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Bandwidth-constrained clients in federated learning struggle to participate effectively due to limited communication capacity, leading to under-parameterized sub-models, slow convergence, and degraded generalization in heterogeneous environments.

Method: FedGMR (Federated Learning with Gradual Model Restoration) progressively increases each client's sub-model density during training, uses mask-aware aggregation for asynchronous MHFL, and provides theoretical convergence guarantees.

Result: Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 show FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

Conclusion: FedGMR effectively enables bandwidth-constrained clients to remain contributors throughout federated learning, with provable convergence properties and superior performance in heterogeneous environments.

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [3] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: This paper investigates the feasibility of bus-mounted edge servers for Internet of Vehicles, showing they effectively handle spatiotemporal user dynamics and provide better coverage than fixed infrastructure alone.


<details>
  <summary>Details</summary>
Motivation: Fixed edge servers at RSUs/base stations have limited flexibility for handling spatiotemporal user dynamics in IoV, while mobile servers like buses offer potential computation elasticity to address these limitations.

Method: Analyzed real Shanghai bus/taxi/Telecom datasets to assess coverage potential, built mathematical model, designed greedy heuristic algorithm for bus selection to maximize demand point coverage under budget constraints, and performed trace-driven simulations.

Result: Bus-based edge servers cover significant geographic areas and demand points; the greedy algorithm effectively handles dynamic user demand under realistic constraints like server capacity and purchase quantity.

Conclusion: Bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable as they provide computation elasticity to complement fixed infrastructure.

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>
