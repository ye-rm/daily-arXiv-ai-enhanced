<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 11]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.AR](#cs.AR) [Total: 26]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: This paper presents parallel bootstrapping algorithms using MPI that address communication overhead and memory constraints through local statistic aggregation and synchronized pseudo-random number generation.


<details>
  <summary>Details</summary>
Motivation: Bootstrapping becomes computationally prohibitive for large datasets or high resample counts due to its computational cost, requiring efficient parallelization strategies.

Method: Proposes two strategies: 1) Local Statistic Aggregation (transmitting sufficient statistics instead of full datasets) and 2) Synchronized Pseudo-Random Number Generation (enabling distributed resampling when full dataset doesn't fit in single process memory).

Result: Analytical models show significant reductions in communication volume and memory usage compared to naive approaches, enabling scalable parallel bootstrapping on large-scale systems.

Conclusion: The proposed methods effectively address key challenges in parallel bootstrapping, making it feasible for large datasets through reduced communication and memory requirements.

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [2] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: MeCeFO is a fault-tolerant optimization algorithm for distributed LLM training that minimizes computational and memory overhead during node failures by transferring tasks to neighboring nodes with efficient algorithmic optimizations.


<details>
  <summary>Details</summary>
Motivation: As distributed optimization scales for LLM training, hardware failures become increasingly problematic. Existing fault-tolerant methods introduce significant overhead, requiring additional resources.

Method: MeCeFO uses three key designs: Skip-connection (drops MHA during backpropagation), Recomputation (reduces FFN activation memory), and Low-rank gradient approximation (efficient FFN weight gradient estimation).

Result: Theoretically matches conventional distributed training convergence rate of O(1/√(nT)). Empirically maintains robust performance under high failure rates with only 4.18% throughput drop, showing 5.0× to 6.7× greater resilience than previous SOTA.

Conclusion: MeCeFO provides efficient fault tolerance for distributed LLM training with minimal overhead, significantly outperforming existing approaches in resilience while maintaining training performance.

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [3] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress is a novel activation compression framework for collaborative LLM inference that uses FFT to exploit frequency-domain sparsity, achieving 7.6x compression ratio with minimal accuracy loss and 32x faster compression than Top-k.


<details>
  <summary>Details</summary>
Motivation: Collaborative LLM inference on edge devices faces communication bottlenecks from transmitting high-dimensional intermediate activations, especially with autoregressive decoding where bandwidth scales with output length. Existing compression methods struggle to balance high compression ratios, low error, and computational efficiency.

Method: FourierCompress exploits frequency-domain sparsity by transforming activations via FFT, retaining only low-frequency coefficients, and reconstructing using conjugate symmetry. It leverages the observation that first Transformer layer activations exhibit strong smoothness and energy concentration in low frequencies.

Result: Extensive experiments on Llama 3 and Qwen2.5 models across 10 datasets show FourierCompress preserves performance close to uncompressed baseline (<0.3% accuracy loss), outperforming Top-k, QR, and SVD methods. Achieves 7.6x average activation size reduction and 32x faster compression than Top-k via hardware acceleration.

Conclusion: FourierCompress successfully bridges communication efficiency, near-lossless inference, and fast compression for edge-device LLM inference by exploiting frequency-domain properties of LLM activations, enabling practical deployment on resource-constrained devices.

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [4] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: A methodology to train language models that can interact with performance tools during reasoning to improve GPU kernel optimization.


<details>
  <summary>Details</summary>
Motivation: Current language models fail at code performance tasks because they don't comprehend environment-hardware interactions that aren't represented in source code.

Method: Train language models to interact with performance tools during their reasoning process.

Result: Demonstrated by training a state-of-the-art GPU kernel optimization model using this methodology.

Conclusion: The proposed methodology enables language models to better handle code performance optimization tasks by incorporating environmental and hardware interactions.

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [5] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: A novel edge-cloud parallel framework for speech transcription and synthesis in Kinyarwanda and Swahili, using Whisper and SpeechT5 models to address limited technological infrastructure in East Africa.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of powerful language processing tools for widely spoken East African languages (Kinyarwanda and Swahili) in regions with limited technological infrastructure.

Method: Utilizes Whisper and SpeechT5 pre-trained models with a cascading mechanism that distributes model inference workload between edge devices and cloud, reducing latency and resource usage.

Result: Achieved 9.5% memory compression for SpeechT5 and 14% for Whisper, with max memory usage of 149 MB. On a 1.7 GHz CPU edge device with 1 MB/s bandwidth, processes 270-character text in under a minute for both STT and TTS.

Conclusion: The cascaded edge-cloud architecture serves as an excellent platform for speech-to-text and text-to-speech transcription with good accuracy and response time, validated by real-world survey data from Kenya.

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [6] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris is a domain-specific RDMA transport that removes retransmissions and in-order delivery for ML workloads, leveraging ML's tolerance for data loss to reduce tail latency and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional RDMA designs with strict reliability and in-order delivery introduce complexity and latency that scale poorly for large-scale ML workloads, where tail latency in collective communication becomes a primary bottleneck.

Method: Celeris removes retransmissions and in-order delivery from RDMA NICs, implementing best-effort transport while retaining congestion control. It uses software-level mechanisms like adaptive timeouts and data prioritization, and shifts loss recovery to the ML pipeline using techniques like Hadamard Transform.

Result: Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by 67%, and nearly doubles NIC resilience to faults.

Conclusion: Celeris provides a resilient, scalable transport specifically tailored for ML workloads at cluster scale by exploiting ML's inherent tolerance for lost or partial data.

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [7] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: A novel MPI abstraction for C++ using Noarr library that enables layout-agnostic design of distributed applications while maintaining performance comparable to state-of-the-art MPI C++ bindings.


<details>
  <summary>Details</summary>
Motivation: MPI's pure-C interface lacks modern language features like type-checking and generic code design, making it cumbersome for modern C++ development in distributed high-performance computing.

Method: Implemented as an extension of the C++ Noarr library, following Noarr paradigms with first-class layout and traversal abstraction to create layout-agnostic MPI applications.

Result: The abstraction achieves performance comparable to state-of-the-art MPI C++ bindings while providing more flexible design capabilities for distributed applications.

Conclusion: The proposed MPI abstraction successfully bridges the gap between MPI's legacy C interface and modern C++ programming paradigms, enabling more flexible and maintainable distributed application design without sacrificing performance.

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [8] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: Proposes an integrated fault-tolerant architecture for multicore systems that uses stability metrics and periodic diagnostics to isolate permanent faults and enable adaptive task scheduling without extra hardware, achieving 30% workload reduction and superior fault coverage compared to TMR.


<details>
  <summary>Details</summary>
Motivation: Existing approaches like Two-Phase TMR reduce energy but fail under permanent faults, while Reactive-TMR adds hardware complexity and has reduced fault tolerance with multiple failures. Need for a solution that handles both transient and permanent faults without additional hardware overhead.

Method: Constructs stability metrics to identify reliable machines, performs periodic diagnostics for permanent fault isolation, and implements adaptive task scheduling in interconnected multicore systems without requiring extra hardware components.

Result: Reduces task workload by approximately 30% compared to baseline TMR, achieves superior fault coverage and isolation accuracy, significantly improves both reliability and energy efficiency.

Conclusion: The proposed integrated architecture effectively handles both transient and permanent faults in multicore systems without hardware overhead, providing better reliability and energy efficiency than existing TMR approaches.

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [9] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: LLMs can generate optimized CUDA code but need tutoring to reach expert-level optimization quality.


<details>
  <summary>Details</summary>
Motivation: To evaluate recent reasoning models' capabilities in generating optimized CUDA code for predefined tasks and determine if tutoring improves their performance.

Method: Used LLMs to generate CUDA code with and without tutoring (detailed hints), evaluated automatically for correctness/speedup and manually via code reviews, and tested interactive error correction.

Result: LLMs are skilled coders but require tutoring to achieve optimized solutions comparable to parallel computing experts.

Conclusion: While LLMs show strong coding capabilities, they need guidance through tutoring to produce expert-level optimized parallel code.

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [10] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: eBPF-based telemetry system for diagnosing GPU tail latency spikes in cloud/HPC environments, providing unified host-side monitoring with GPU event correlation.


<details>
  <summary>Details</summary>
Motivation: Existing monitoring tools lack granularity for root cause analysis of GPU tail latency spikes in shared computing environments, affecting performance predictability and resource utilization.

Method: eBPF-based telemetry system that correlates eBPF-derived host metrics with GPU-internal events for holistic system observability, operating with low overhead.

Result: Achieves 81-88% diagnostic accuracy, detects spikes within 5 seconds, completes root cause analysis in 6-8 seconds with 1.21% CPU overhead at 100Hz sampling.

Conclusion: The system effectively identifies root causes (NIC contention, PCIe pressure, CPU interference) and enables operational debugging for multi-tenant GPU infrastructure without cluster-wide instrumentation.

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [11] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: This paper addresses whether round elimination fixed points are a universal technique for proving lower bounds in distributed graph algorithms. It shows that previous obstacles (homomorphism problems) can be overcome using tripotent inputs, but also reveals new obstacles for problems with inputs.


<details>
  <summary>Details</summary>
Motivation: To determine if round elimination fixed points are a universal technique for proving lower bounds in distributed graph algorithms, particularly addressing the key obstacle of homomorphism problems that previously required Marks' technique.

Method: Developed a new technique using tripotent inputs to construct round elimination lower bounds systematically, and proved the first fully general lower bound theorem applicable to any problem that is a fixed point in round elimination.

Result: Showed that homomorphism problems admit round elimination fixed point proofs, eliminating the only known obstacle for universality. However, discovered new obstacles for problems with inputs that require Ω(log n) rounds but have no proofs based on relaxations to nontrivial round elimination fixed points.

Conclusion: Round elimination cannot be a universal technique for problems with inputs, but might be universal for problems without inputs. The paper provides the first fully general lower bound theorem for round elimination fixed points.

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [12] [Quantum Approximate Optimization Algorithm for MIMO with Quantized b-bit Beamforming](https://arxiv.org/abs/2510.15935)
*Nikos A Mitsiou,Ioannis Krikidis,George K Karagiannidis*

Main category: cs.ET

TL;DR: This paper proposes using Quantum Approximate Optimization Algorithm (QAOA) and alternating optimization to solve the NP-hard combinatorial problem of b-bit quantized phase shifters in MIMO systems, achieving improved beamforming performance over classical methods.


<details>
  <summary>Details</summary>
Motivation: Conventional fully digital MIMO designs face high hardware complexity and power consumption, while low-bit MIMO architectures using b-bit quantized phase shifters offer cost-effective alternatives but introduce NP-hard combinatorial optimization problems.

Method: The paper uses QAOA and alternating optimization to address b-bit quantized phase shifters at both transmitter and receiver. It maps phase shifts to quantum rotation gates, presents Hamiltonian derivation for b-bit case, and implements warm-start QAOA for improved computational efficiency.

Result: Numerical results demonstrate that the proposed methods achieve improved quantized beamforming gain compared to classical optimization benchmarks from existing literature.

Conclusion: The structure of quantized beamforming problems naturally aligns with hybrid-classical methods like QAOA, and this work establishes the first theoretical connection between phase shifts in beamforming and quantum rotation gates, with potential applications in integrated sensing/communication and quantum machine learning.

Abstract: Multiple-input multiple-output (MIMO) is critical for 6G communication,
offering improved spectral efficiency and reliability. However, conventional
fully digital designs face significant challenges due to high hardware
complexity and power consumption. Low-bit MIMO architectures, such as those
employing b-bit quantized phase shifters, provide a cost-effective alternative
but introduce NP-hard combinatorial problems in the pre- and post-coding
design. This paper explores the use of the Quantum Approximate Optimization
Algorithm (QAOA) and alternating optimization to address the problem of b-bit
quantized phase shifters both at the transmitter and the receiver. We
demonstrate that the structure of this quantized beamforming problem aligns
naturally with hybrid-classical methods like QAOA, as the phase shifts used in
beamforming can be directly mapped to rotation gates in a quantum circuit.
Notably, this paper is the first to show that theoretical connection. Then, the
Hamiltonian derivation analysis for the b-bit case is presented, which could
have applications in different fields, such as integrated sensing and
communication, and emerging quantum algorithms such as quantum machine
learning. In addition, a warm-start QAOA approach is studied which improves
computational efficiency. Numerical results highlight the effectiveness of the
proposed methods in achieving an improved quantized beamforming gain over their
classical optimization benchmarks from the literature.

</details>


### [13] [Navigate in Demanding Missions: Integrating Human Intelligence and Brain-Inspired Intelligence](https://arxiv.org/abs/2510.17530)
*Xu He,Xiaolin Meng,Youdong Zhang,Lingfei Mo,Wenxuan Yin*

Main category: cs.ET

TL;DR: This paper advocates for integrating neuromorphic-empowered Brain-Computer Interfaces (BCIs) with Brain-Inspired Navigation (BIN) to enhance unmanned systems' reliability in demanding missions like deep space exploration, while using human intelligence as a safeguard against machine failures.


<details>
  <summary>Details</summary>
Motivation: The paper identifies a current lack of cooperation between BCIs and BIN fields and aims to bridge this gap to improve unmanned systems' navigation capabilities in challenging environments.

Method: The proposed approach involves integrating neuromorphic-empowered BCIs into BIN systems, creating a symbiotic relationship where machine intelligence is reinforced by brain-inspired artificial consciousness and human intelligence serves as a backup safety mechanism.

Result: The integration enables enhanced unmanned systems capabilities for demanding missions and facilitates diagnostics of spatial cognition disorders, though ethical and security concerns need consideration.

Conclusion: The study concludes that combining neuromorphic BCIs with BIN can significantly advance unmanned systems' navigation reliability while providing human oversight as a safety net, though ethical implications must be carefully addressed.

Abstract: This perspective analyzes the intricate interplay among neuroscience,
Brain-Inspired Intelligence (BII), and Brain-Inspired Navigation (BIN),
revealing a current lack of cooperative relationship between Brain-Computer
Interfaces (BCIs) and BIN fields. We advocate for the integration of
neuromorphic-empowered BCI into BIN, thereby bolstering the unmanned systems'
reliable navigation in demanding missions, such as deep space exploration, etc.
We highlight that machine intelligence, reinforced by brain-inspired artificial
consciousness, can extend human intelligence, with human intelligence mediated
by neuromorphic-enabled BCI acting as a safeguard in case machine intelligence
failures. This study also discusses the potentials of the proposed approach to
enhance unmanned systems' capabilities and facilitate the diagnostics of
spatial cognition disorders, while considering associated ethical and security
concerns.

</details>


### [14] [Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring](https://arxiv.org/abs/2510.17688)
*Shawn M. Gibford,Mohammad Reza Boskabadi,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.ET

TL;DR: The paper proposes using Quantum Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP) to generate synthetic time series data for industrial bioprocesses, addressing data scarcity issues in bio-manufacturing.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and sparsity in bio-manufacturing pose challenges for accurate model development, process monitoring, and optimization. The authors aim to replicate complex dynamics of industrial bioprocesses to reduce dependence on scarce experimental data.

Method: The methodology uses a Quantum Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP) where the generator is composed of a Parameterized Quantum Circuit (PQC) to generate synthetic time series data for bioprocesses.

Result: The results demonstrate acceptable performance in capturing temporal dynamics of real bioprocess data, with generated data showing high fidelity to actual historical experimental data, particularly for Optical Density measurements used in Dry Biomass estimation.

Conclusion: The intersection of quantum computing and machine learning opens new frontiers in data analysis and generation for computationally intensive fields, enabling more efficient bioprocess management through improved prediction accuracy for soft sensor design and predictive control applications.

Abstract: Data scarcity and sparsity in bio-manufacturing poses challenges for accurate
model
  development, process monitoring, and optimization. We aim to replicate and
capture
  the complex dynamics of industrial bioprocesses by proposing the use of a
Quantum
  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)
to
  generate synthetic time series data for industrially relevant processes. The
  generator within our GAN is comprised of a Parameterized Quantum Circuit
(PQC). This
  methodology offers potential advantages in process monitoring, modeling,
  forecasting, and optimization, enabling more efficient bioprocess management
by
  reducing the dependence on scarce experimental data. Our results demonstrate
  acceptable performance in capturing the temporal dynamics of real bioprocess
data.
  We focus on Optical Density, a key measurement for Dry Biomass estimation.
The data
  generated showed high fidelity to the actual historical experimental data.
This
  intersection of quantum computing and machine learning has opened new
frontiers in
  data analysis and generation, particularly in computationally intensive
fields, for
  use cases such as increasing prediction accuracy for soft sensor design or
for use
  in predictive control.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: This paper proposes a method to make program context visible at memory devices by encoding user-visible state as detectable packets in memory read address streams, enabling better memory optimization without significant overhead.


<details>
  <summary>Details</summary>
Motivation: Hardware cache prefetching, memory scheduling, and interleaving cause loss of observability that limits data movement and tiering optimizations. Program context is stripped from memory bus, losing valuable hints for memory device optimization.

Method: Encoding any user-visible state as detectable packets in memory read address stream in a nondestructive manner without significant capacity overhead, drivers, or special access privileges. Prototyped end-to-end system with metadata injection that can be reliably detected and decoded.

Result: Successfully demonstrated a use case with precise code execution markers and object address range tracking. The system can be detected and decoded from memory address traces by host processors or memory modules.

Conclusion: The approach enables context visibility at memory devices. Future work could use near-memory computing for real-time metadata decoding to provide customized telemetry, statistics, and perform functions like request prioritization, data remapping, and device reconfiguration.

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [16] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+ is an enhanced emulator for consumer-grade zoned flash storage that adds block interface support to the original ConZone emulator, enabling file system mounting and improved usability for exploring storage architecture and system optimizations.


<details>
  <summary>Details</summary>
Motivation: To facilitate understanding and enhancement of software/hardware design for consumer-grade zoned flash storage by providing an accurate emulator that models resource constraints and architectural features typical of such systems.

Method: Extends the original ConZone emulator with block interface support, deployment scripts, and several enhancements. The emulator incorporates limited logical to physical mapping caches, constrained write buffers, and hybrid flash media management to model consumer-grade device constraints.

Result: Validated accuracy by comparing with representative hardware architecture and state-of-the-art systems. Case studies demonstrated the emulator's capability to investigate zoned storage design and identify file system inadequacies.

Conclusion: ConZone+ provides an effective tool for exploring consumer-grade zoned flash storage architecture, enabling users to integrate optimizations with system software and investigate current file system limitations.

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [17] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: A Multimodal Large Language Model Assistant (MLLMA) is introduced to predict routing congestion and provide human-interpretable design suggestions for chip physical design, outperforming existing models in accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Electronic Design Automation (EDA) tools struggle to provide interpretable feedback or actionable guidance for improving routing congestion in chip physical design, creating a need for more transparent and helpful assistance.

Method: Combines automated feature generation through MLLM-guided genetic prompting with an interpretable preference learning framework that models congestion-relevant tradeoffs across visual, tabular, and textual inputs, compiling insights into a "Design Suggestion Deck".

Result: Outperforms existing models on both accuracy and explainability on the CircuitNet benchmark, with design suggestion guidance and qualitative analyses confirming that learned preferences align with real-world design principles and are actionable for engineers.

Conclusion: This work highlights the potential of MLLMs as interactive assistants for interpretable and context-aware physical design optimization in chip design workflows.

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [18] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: The paper reveals inefficiencies in UPMEM's PIM software stack and demonstrates optimization techniques including assembly modifications, bit-serial processing for low-precision data, and NUMA-aware API extensions, achieving significant speedups in integer operations and matrix computations.


<details>
  <summary>Details</summary>
Motivation: PIM platforms like UPMEM present unique challenges in data management and parallel programming on limited processing units, with existing SDKs leaving significant room for performance optimization despite providing essential tools.

Method: The authors analyze UPMEM software stack inefficiencies, modify assembly code generated by UPMEM compiler, implement bit-serial processing for low-precision data (INT4), and extend APIs for NUMA-aware PIM allocation to optimize host-PIM data transfers.

Result: Achieved speedups of 1.6-2x in integer addition, 1.4-5.9x in integer multiplication, over 2.7x speedup in INT4 bit-serial dot-product, and up to 2.9x improvement in host-PIM data transfer consistency and throughput. Optimized kernels outperform dual-socket CPU server by over 3x for INT8 GEMV and 10x for INT4 GEMV.

Conclusion: Simple modifications to UPMEM's software stack and non-standard programming techniques can yield substantial performance improvements in PIM platforms, making bit-serial processing viable for low-precision data and demonstrating significant speedups over both baseline implementations and traditional CPU servers.

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [19] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D integration faces challenges in design, manufacturing, and testing due to higher power density and the need for co-design across layers, independent testing of subsystems, and ensuring correct spatial correspondence during assembly.


<details>
  <summary>Details</summary>
Motivation: As lithographic scaling becomes more challenging, 3D integration offers a way to improve density, but it introduces new design and manufacturing challenges that need to be addressed.

Method: The paper discusses the need for co-design across layers, independent testing of subsystems, and proper assembly techniques to ensure correct spatial correspondence and sensible yield.

Result: The analysis highlights the importance of articulating the leverages of 3D systems and enumerating the challenges in design, assembly, and test to make the most of 3D integration.

Conclusion: To fully leverage 3D integration, it is crucial to address the new challenges in design, manufacturing, and testing, including co-design across layers, independent testing, and ensuring proper assembly for yield and testability.

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [20] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink is a collective communication framework that aggregates heterogeneous links (NVLink, PCIe, RDMA NICs) into a single high-performance fabric, improving bandwidth by up to 26-27% over NCCL through adaptive load balancing.


<details>
  <summary>Details</summary>
Motivation: Current communication libraries like NCCL use single interconnects, creating performance bottlenecks when primary interconnects become bandwidth-limited while leaving other hardware resources like PCIe and RDMA NICs underutilized.

Method: FlexLink employs a two-stage adaptive load balancing strategy that dynamically partitions communication traffic across all available heterogeneous links (NVLink, PCIe, RDMA NICs), ensuring faster interconnects are not throttled by slower ones.

Result: On an 8-GPU H800 server, FlexLink improves bandwidth of collective operators like AllReduce and AllGather by up to 26% and 27% respectively over NCCL baseline, offloading 2-22% of total communication traffic to previously underutilized PCIe and RDMA NICs.

Conclusion: FlexLink provides significant performance improvements as a lossless, drop-in replacement compatible with NCCL API, enabling easy adoption while systematically addressing communication bottlenecks in multi-node LLM deployments.

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [21] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: Automated framework for requirements-driven functional verification of configurable IPs that reduces verification effort through automation of vPlan generation, testbench creation, regression execution, and reporting.


<details>
  <summary>Details</summary>
Motivation: Increasing competition in semiconductor industry creates pressure to reduce chip prices while maintaining quality, with functional verification being a major cost driver due to complexity and resource-intensive nature.

Method: Proposes a fully automated framework that automates key verification processes including vPlan generation, testbench creation, regression execution, and reporting in requirements management tools.

Result: Drastically reduces verification effort, accelerates development cycles, minimizes human error, and enhances coverage.

Conclusion: The framework offers a scalable and efficient solution to the challenges of verifying configurable IPs in the semiconductor industry.

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [22] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: This paper extends numerical feature analysis methodology to consumer-grade NVIDIA GPUs, developing an architecture-independent test scheme for various precision formats without device-specific constants.


<details>
  <summary>Details</summary>
Motivation: To analyze numerical features (rounding, normalization, accumulator precision) of matrix multiplier hardware in consumer-grade NVIDIA GPUs, extending previous work that focused on data center GPUs.

Method: Implemented an architecture-independent test scheme using a novel test vector generation method that avoids exhaustive search and device-specific constants, applicable to wide range of mixed-precision formats.

Result: Applied the scheme to RTX-3060 (Ampere) and Ada RTX-1000 (Ada Lovelace) GPUs, determining numerical features for binary16, TensorFloat32, and bfloat16 input formats with binary16 and binary32 output formats. Found RTX-3060 features identical to data center A100 GPU.

Conclusion: The methodology is robust and expected to work without changes for newer NVIDIA GPUs (Hopper, Blackwell) and future architectures, supporting various input/output format combinations including latest 8-bit floating-point formats.

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [23] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s is an open-source RISC-V RV32I framework that provides a practical roadmap from basic single-cycle to 5-stage pipelined core with hazard forwarding, branch prediction, and exception handling, implemented on FPGA with 1.09 DMIPS/MHz performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical knowledge and practical hardware implementation for RISC-V RV32I architecture, providing a reproducible instructional pathway for the open-source hardware ecosystem.

Method: Following Patterson and Hennessy methodology, the design evolves from single-cycle to 5-stage pipelined core with full hazard forwarding, dynamic branch prediction, and exception handling. The core is integrated into an SoC with UART communication and implemented on Xilinx Artix-7 FPGA.

Result: Achieved 1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50 MHz operating frequency on FPGA implementation.

Conclusion: BASIC_RV32s successfully provides a comprehensive, open-source framework with all RTL source code, diagrams, and development logs released under MIT license, offering a practical educational roadmap for RISC-V hardware implementation.

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [24] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: The paper proposes a simplified Hardware Transactional Memory (HTM) implementation that uses existing Load-Linked and Store-Conditional instructions without ISA modifications or cache coherence protocol changes, confined to L1 Data Cache modifications with small transaction sizes.


<details>
  <summary>Details</summary>
Motivation: Existing HTM implementations face adoption barriers due to high hardware complexity, ISA extensions, and cache coherence protocol modifications. The authors aim to create a simpler HTM that avoids these issues.

Method: Extends semantics of Load-Linked and Store-Conditional instructions without new ISA additions. Implementation confined to L1 Data Cache modifications, restricting transactions to small read/write sets (max 8 cache lines). Two forward progress mechanisms based on retrial detection.

Result: Simulated in Gem5 with popular concurrent data structures. HTM works effectively with maximum 8 cache lines for read/write sets. Provides good performance when contention is spread across multiple nodes, with low abort rates. Outperforms TTS lock in atomic fetch-and-increment benchmark under low congestion.

Conclusion: A simplified HTM implementation is feasible without ISA extensions or cache coherence changes, using existing instructions and small transaction sizes, providing good performance for concurrent data structures with distributed contention.

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [25] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 3D-stacked optics and logic enable photonic scale-up solutions for connecting hundreds of GPU packages across multiple racks, achieving 8X scale-up capability and 2.7X reduction in training time for trillion-parameter models.


<details>
  <summary>Details</summary>
Motivation: Traditional copper interconnects limit scale-up domains to single racks due to 1-meter reach constraints, while AI workloads demand larger logical GPUs across multiple data center racks for training frontier LLMs.

Method: Modeling 3D Co-Packaged Optics (CPO) enabled GPUs and switches within scale-up domains, using photonic solutions to connect hundreds of GPU packages across multiple racks for training trillion-parameter Mixture of Experts models.

Result: 3D CPO enables 8X increase in scale-up capability through substantial bandwidth and radix improvements, allowing multi-dimensional parallelism that reduces time-to-train by 2.7X for models exceeding one trillion parameters.

Conclusion: Photonic scale-up solutions using 3D-stacked optics are essential for achieving aggressive power and performance targets in frontier AI workloads, unlocking unprecedented model scaling beyond traditional interconnect limitations.

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [26] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace is a diffusion-based framework for chip placement that formulates placement as a conditional denoising process, enabling transferable policies that generalize to unseen circuit netlists without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional chip placement methods struggle with hard constraints or require expensive online training for each new circuit design, creating limitations in VLSI design automation.

Method: Uses conditional denoising diffusion process with energy-guided sampling and constrained manifold diffusion to explore placement space while conditioning on circuit connectivity and quality metrics.

Result: Achieves extremely low overlap across all experimental scenarios and provides transferable placement policies that work on unseen circuit netlists without retraining.

Conclusion: Bridges optimization-based and learning-based approaches, offering a practical path toward automated, high-quality chip placement for modern VLSI design.

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [27] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: VeriPPA framework uses LLMs for chip design automation, achieving high success rates in generating syntactically and functionally correct Verilog code while optimizing Power-Performance-Area (PPA) metrics.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs' content generation capabilities for automating complex chip design processes, specifically addressing PPA optimization and accurate Verilog code generation which are crucial in semiconductor design.

Method: A two-stage framework: first stage improves functional and syntactic correctness of generated Verilog codes, second stage optimizes Verilog codes to meet PPA constraints of circuit designs.

Result: Achieved 81.37% syntactic correctness and 62.06% functional correctness on RTLLM dataset, and 99.56% syntactic correctness and 43.79% functional correctness on VerilogEval dataset, outperforming SOTA methods. Successfully optimized PPA of designs.

Conclusion: LLMs show significant potential in handling complex technical domains like chip design, indicating promising developments in automating semiconductor design processes through improved code generation and PPA optimization.

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [28] [NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme](https://arxiv.org/abs/2510.15904)
*Subhradip Chakraborty,Ankur Singh,Xuming Chen,Gourav Datta,Akhilesh R. Jaiswal*

Main category: cs.AR

TL;DR: Proposes NVM-in-Cache architecture integrating RRAM into 6T-SRAM cells to enable Processing-in-Memory for AI workloads, achieving high throughput and energy efficiency without area overhead.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of storage density and computation efficiency in deep neural network workloads where SRAM arrays occupy substantial die area in ML applications.

Method: Integrates resistive RAM (RRAM) devices into conventional 6T-SRAM cells to form compact 6T-2R bit-cells, enabling Processing-in-Memory mode that performs parallel MAC operations directly on cache power lines while preserving stored data.

Result: Achieves 0.4 TOPS throughput and 491.78 TOPS/W energy efficiency in 22nm FDSOI technology. CIFAR-10 classification with Resnet-18 achieves 91.27% accuracy for 128 row-parallel operations.

Conclusion: The NVM-in-Cache approach provides a scalable, energy-efficient computing method by repurposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.

Abstract: The rapid growth of deep neural network (DNN) workloads has significantly
increased the demand for large-capacity on-chip SRAM in machine learning (ML)
applications, with SRAM arrays now occupying a substantial fraction of the
total die area. To address the dual challenges of storage density and
computation efficiency, this paper proposes an NVM-in-Cache architecture that
integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,
forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory
(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)
operations directly on cache power lines while preserving stored cache data. By
exploiting the intrinsic properties of the 6T-2R structure, the architecture
achieves additional storage capability, high computational throughput without
any bit-cell area overhead. Circuit- and array-level simulations in
GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design
achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel
operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18
neural network, achieving an accuracy of 91.27%. These results highlight the
potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient
computing method by re-purposing existing 6T SRAM cache architecture for
next-generation AI accelerators and general purpose processors.

</details>


### [29] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: Proposes Intent-Driven Storage Systems (IDSS) using LLMs to interpret workload intent from unstructured signals, enabling adaptive storage optimization and achieving up to 2.45X IOPS improvement.


<details>
  <summary>Details</summary>
Motivation: Existing storage systems lack visibility into workload intent, leading to brittle heuristics and fragmented optimizations that can't adapt to modern data-intensive applications.

Method: Uses large language models (LLMs) to infer workload and system intent from unstructured signals, guiding adaptive cross-layer parameter reconfiguration within policy guardrails. Presents four design principles for integrating LLMs into storage control loops.

Result: Initial FileBench workload testing shows IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components like caching and prefetching.

Conclusion: LLMs can function as high-level semantic optimizers when constrained by guardrails and embedded in structured workflows, bridging application goals with low-level system control. Points toward adaptive, autonomous storage systems aligned with dynamic workload demands.

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [30] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: FVDebug is an intelligent system that automates root-cause analysis for formal verification failures by combining waveforms, RTL code, and design specifications to transform failure traces into actionable insights and fixes.


<details>
  <summary>Details</summary>
Motivation: Debugging formal verification failures is a major bottleneck in hardware design, requiring engineers to manually trace complex counter-examples across multiple cycles, analyze waveforms, and cross-reference specifications - a process that can take hours or days per bug.

Method: FVDebug uses a novel pipeline: (1) Causal Graph Synthesis to structure failure traces into directed acyclic graphs, (2) Graph Scanner using batched LLM analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations, plus a Fix Generator for RTL fixes.

Result: Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates, with successful results on two proprietary, production-scale FV counterexamples, demonstrating applicability from academic benchmarks to industrial designs.

Conclusion: FVDebug provides an effective automated solution for formal verification debugging that transforms failure traces into actionable insights and concrete fixes, addressing a major bottleneck in hardware design workflows.

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [31] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: A symbolic timing analysis method for digital circuits using analytic delay formulas for basic gates, enabling closed-form delay expressions without simulation and supporting sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: To enable precise timing analysis of digital circuits through analytic methods rather than simulation, allowing for direct computation of delay dependencies on input signals and gate parameters.

Method: Uses analytic delay formulas for 2-input NOR, NAND, and Muller-C gates to compute closed-form delay expressions for internal signal transitions based on symbolic input transition times and gate parameters, implemented in SageMath.

Result: Successfully applied to the NOR-gate version of c17 slack benchmark circuit, demonstrating the ability to perform per-transition timing analysis and sensitivity analysis through differentiation of symbolic delay expressions.

Conclusion: The proposed framework provides an effective analytic approach for symbolic timing analysis that enables both direct delay computation and sensitivity analysis without requiring circuit simulation.

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [32] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: Belenos provides workload characterization for finite element biomechanics using FEBio, identifying significant backend bottlenecks in larger workloads (59.9%-82.2% backend-bound cycles) and optimal DSA configurations through gem5 sensitivity studies.


<details>
  <summary>Details</summary>
Motivation: Current hardware/software inefficiencies limit performance and scalability in biomechanical simulations, forcing workflows to sacrifice fidelity for tractability, while FPGAs offer promising domain-specific acceleration potential that remains underexplored.

Method: Comprehensive workload characterization using FEBio simulator, gem5 sensitivity studies, and VTune analysis to identify performance bottlenecks and optimal hardware configurations for Domain-Specific Accelerators.

Result: Smaller workloads show moderate front-end stalls (~13.1%), while larger workloads are dominated by backend bottlenecks (59.9%-82.2% backend-bound cycles). Suboptimal hardware settings can degrade performance by up to 37.1%.

Conclusion: Architecture-aware co-design is essential for efficiently supporting biomechanical simulation workloads, as identified bottlenecks and sensitivity studies reveal significant optimization opportunities for domain-specific acceleration.

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [33] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: SoCks is a flexible build framework that partitions SoC images into encapsulated blocks with minimal dependencies, enabling faster builds and easier development workflows.


<details>
  <summary>Details</summary>
Motivation: Modern heterogeneous SoCs are complex to manage, with development tools becoming increasingly complex and lacking adequate support, leading to steep learning curves and challenging troubleshooting.

Method: Partitions SoC image into high-level units called blocks, builds each firmware/software block independently with minimal dependencies, uses standardized interfaces for information exchange between blocks.

Result: SoCks can build complete SoC images up to three times faster than established tools, enables easier reuse of block implementations and seamless substitution between versions.

Conclusion: SoCks framework successfully reduces complexity in SoC development by enabling encapsulated block-based building with minimal dependencies, standardized interfaces, and support for decentralized CI/CD workflows.

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [34] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: VeriGRAG is a framework that enhances LLM-generated Verilog code by extracting structural graph embeddings from Verilog using GNNs, selecting relevant embeddings via a multimodal retriever, and generating structure-aware prompts through VeriFormer, achieving state-of-the-art performance on Verilog benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to effectively leverage the structural information inherent in Verilog code, which is crucial for generating functionally and syntactically correct hardware circuit descriptions.

Method: Extract structural graph embeddings from Verilog code using GNNs, use a multimodal retriever to select task-relevant embeddings, and align them with code modality through VeriFormer to generate structure-aware soft prompts.

Result: VeriGRAG substantially improves Verilog code generation correctness, achieving state-of-the-art or superior performance on both VerilogEval and RTLLM benchmarks.

Conclusion: The proposed framework successfully addresses the challenge of leveraging structural information in Verilog code generation, demonstrating significant improvements in correctness and benchmark performance.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


### [35] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe is a table-lookup-based ternary LLM accelerator for edge FPGAs that uses 1.58-bit weights and 8-bit activations, achieving up to 25 tokens/s decoding throughput and 0.45-0.96s TTFT under 5W power budget.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on edge platforms is challenging due to high computational/memory demands, limited on-chip resources, power constraints, and long prefill stage latency, despite recent low-bit quantization methods.

Method: Uses table-lookup-based ternary matrix multiplication with grouped activations, fine-grained URAM weight buffer management, streaming dataflow architecture, reversed-reordered prefill attention, and specialized decoding attention.

Result: Achieves up to 25 tokens/s decoding throughput and 0.45-0.96s time-to-first-token for 64-128 token prompts under 5W power budget.

Conclusion: TeLLMe represents a significant energy-efficiency advancement for LLM inference on edge FPGAs, addressing both prefill and decoding stages with low resource utilization.

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [36] [Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales](https://arxiv.org/abs/2510.15930)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: A library of configurable convolution blocks and a framework for predicting FPGA resource usage to optimize CNN implementation on FPGAs, addressing challenges in hardware design complexity and resource constraints.


<details>
  <summary>Details</summary>
Motivation: FPGAs offer advantages over GPUs for CNNs (lower latency, power efficiency, flexibility), but implementation complexity, long design cycles, and resource optimization challenges hinder rapid network exploration.

Method: Developed a library of configurable convolution blocks and a methodological framework for creating mathematical models that predict FPGA resource utilization, validated through parameter correlation analysis and error metrics.

Result: The convolution blocks successfully adapt to hardware constraints, and the resource prediction models demonstrate accurate performance in forecasting FPGA resource consumption.

Conclusion: The proposed approach provides an effective tool for FPGA selection and optimized CNN deployment, enabling better adaptation to resource constraints and accurate resource prediction for FPGA-based CNN implementations.

Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate
arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower
latency, greater power efficiency and greater flexibility. However, this
development remains complex due to the hardware knowledge required and the long
synthesis, placement and routing stages, which slow down design cycles and
prevent rapid exploration of network configurations, making resource
optimisation under severe constraints particularly challenging. This paper
proposes a library of configurable convolution Blocks designed to optimize FPGA
implementation and adapt to available resources. It also presents a
methodological framework for developing mathematical models that predict FPGA
resources utilization. The approach is validated by analyzing the correlation
between the parameters, followed by error metrics. The results show that the
designed blocks enable adaptation of convolution layers to hardware
constraints, and that the models accurately predict resource consumption,
providing a useful tool for FPGA selection and optimized CNN deployment.

</details>


### [37] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: Kelle is a software-hardware co-design solution that uses embedded DRAM (eDRAM) for KV cache storage in edge devices running LLMs, achieving 3.9× speedup and 4.5× energy savings through optimized memory management.


<details>
  <summary>Details</summary>
Motivation: Running LLMs on edge devices reduces latency and improves privacy but faces challenges with KV cache memory requirements. Edge devices have limited memory and computational power, making large KV caches difficult to store and access efficiently.

Method: Proposed Kelle - a software-hardware co-design using eDRAM as primary KV cache storage with fine-grained memory eviction, recomputation, and refresh control algorithms to optimize performance.

Result: The Kelle accelerator delivers 3.9× speedup and 4.5× energy savings compared to existing baseline solutions.

Conclusion: Kelle effectively addresses KV cache challenges in edge LLM deployment by leveraging eDRAM with optimized memory management, significantly improving performance and energy efficiency.

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [38] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS develops non-conventional hardware accelerators (optoelectronic, PIM, neuromorphic) to address AI power and scalability bottlenecks for defense applications, along with supporting system architecture and simulation tools.


<details>
  <summary>Details</summary>
Motivation: To overcome power, efficiency, and scalability limitations in AI systems, particularly for defense use cases like autonomous vehicles, drones, and space platforms.

Method: Design and evaluate various non-conventional hardware accelerators including optoelectronic, processing-in-memory (volatile/non-volatile), and neuromorphic architectures, supported by system architecture, software stack, and simulation tools.

Result: The paper presents the planned system architecture and software stack for integrating these accelerators, plus simulation software for early prototyping of the full system and components.

Conclusion: ARCHYTAS aims to provide a comprehensive hardware-software co-design approach to address AI bottlenecks through novel accelerator technologies and supporting infrastructure.

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [39] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: Development of an intelligent traffic signaling system for non-lane-based, heterogeneous traffic in Dhaka using RTSP feeds, Raspberry Pi 4B, YOLO-based detection on NHT-1071 dataset, and NSGA-II optimization to minimize waiting time and maximize throughput.


<details>
  <summary>Details</summary>
Motivation: Address traffic congestion in developing countries like Bangladesh where traditional intelligent traffic signaling systems designed for structured traffic in developed countries are ineffective due to non-lane-based, heterogeneous traffic conditions.

Method: Pipeline using RTSP feeds for real-time data, low-resource Raspberry Pi 4B processing, YOLO-based object detection trained on NHT-1071 dataset to classify heterogeneous traffic, and NSGA-II multi-objective optimization algorithm for signal timing.

Result: Tested at a five-road intersection in Palashi, Dhaka, demonstrating significant potential for improving traffic management in similar complex traffic scenarios.

Conclusion: The developed system provides a contextual and effective Intelligent Traffic Signaling solution suitable for developing areas with complicated traffic dynamics like Dhaka City.

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [40] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: smaRTLy is a new RTL optimization technique that focuses on multiplexer trees, achieving significant area reduction through logical inference and structural rebuilding.


<details>
  <summary>Details</summary>
Motivation: Traditional RTL synthesis tools like Yosys don't fully exploit logical relationships among signals or structural optimization potential in multiplexer trees, which are very common in RTL designs.

Method: Developed innovative strategies to remove redundant multiplexer trees and restructure remaining ones through logic inferencing and structural rebuilding techniques.

Result: Achieved 8.95% additional AIG area reduction on IWLS-2005 and RISC-V benchmarks compared to Yosys, and 47.2% more AIG area removal on industrial million-gate scale benchmarks.

Conclusion: smaRTLy effectively enhances RTL optimization process through logic inferencing and structural rebuilding, leading to more efficient hardware designs with significant area reductions.

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>
