<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Forward-only learning in memristor arrays with month-scale stability](https://arxiv.org/abs/2601.09903)
*Adrien Renaudineau,Mamadou Hawa Diallo,Théo Dupuis,Bastien Imbert,Mohammed Akib Iftakher,Kamel-Eddine Harabi,Clément Turck,Tifenn Hirtzlin,Djohan Bonnet,Franck Melul,Jorge-Daniel Aguirre-Morales,Elisa Vianello,Marc Bocquet,Jean-Michel Portal,Damien Querlioz*

Main category: cs.ET

TL;DR: Researchers demonstrate energy-efficient on-chip learning using standard filamentary HfOx/Ti memristor arrays with sub-1V reset-only pulses and forward-only training algorithms, achieving near-backpropagation accuracy with 460x less energy.


<details>
  <summary>Details</summary>
Motivation: Memristor arrays are efficient for inference but challenging for on-chip learning due to high energy costs, device wear, analog state drift, and the need for backward signal flow in backpropagation.

Method: Two key design choices: 1) Using sub-1V reset-only pulses on standard filamentary HfOx/Ti memristors to reduce energy, improve endurance, and stabilize analog states; 2) Employing forward-only training algorithms derived from Hinton's Forward-Forward that use only inference-style operations.

Result: Trained two-layer classifiers on ImageNet-resolution four-class tasks using arrays up to 8,064 devices. Forward-only variants achieved 89.5-89.6% test accuracy vs. 90.0% for backpropagation. Models retained accuracy for at least one month. Sub-1V reset updates used 460x less energy than conventional programming and only 46% more energy than inference-only operation.

Conclusion: The work establishes a practical route to adaptive edge intelligence by demonstrating forward-only, sub-1V learning on standard filamentary memristor arrays at scale, addressing key challenges in on-chip learning with energy-efficient, stable operation.

Abstract: Turning memristor arrays from efficient inference engines into systems capable of on-chip learning has proved difficult. Weight updates have a high energy cost and cause device wear, analog states drift, and backpropagation requires a backward pass with reversed signal flow. Here we experimentally demonstrate learning on standard filamentary HfOx/Ti arrays that addresses these challenges with two design choices. First, we realize that standard filamentary HfOx/Ti memristors support sub-1 V reset-only pulses that cut energy, improve endurance, and yield stable analog states. Second, we rely on forward-only training algorithms derived from Hinton's Forward-Forward that use only inference-style operations. We train two-layer classifiers on an ImageNet-resolution four-class task using arrays up to 8,064 devices. Two forward-only variants, the double-pass supervised Forward-Forward and a single-pass competitive rule, achieve test accuracies of 89.5% and 89.6%, respectively; a reference experiment using backpropagation reaches 90.0%. Across five independent runs per method, these accuracies match within statistical uncertainty. Trained models retain accuracy for at least one month under ambient conditions, consistent with the stability of reset-only states. Sub-1 V reset updates use 460 times less energy than conventional program-and-verify programming and require just 46% more energy than inference-only operation. Together, these results establish forward-only, sub-1 V learning on standard filamentary stacks at array scale, outlining a practical, pulse-aware route to adaptive edge intelligence.

</details>


### [2] [Resistive Memory based Efficient Machine Unlearning and Continual Learning](https://arxiv.org/abs/2601.10037)
*Ning Lin,Jichang Yang,Yangu He,Zijian Ye,Kwun Hang Wong,Xinyuan Zhang,Songqi Wang,Yi Li,Kemi Xu,Leo Yu Zhang,Xiaoming Chen,Dashan Shang,Han Wang,Xiaojuan Qi,Zhongrui Wang*

Main category: cs.ET

TL;DR: Hardware-software co-design enables efficient machine unlearning and continual learning on resistive memory accelerators through low-rank adaptation and hybrid analog-digital architecture.


<details>
  <summary>Details</summary>
Motivation: Resistive memory neuromorphic systems lack active forgetting mechanisms for data privacy, while existing unlearning schemes have prohibitive programming overheads on RM hardware due to device variability and write-verify cycles.

Method: Software: Low-rank adaptation (LoRA) framework confines updates to compact parameter branches, reducing trainable parameters. Hardware: Hybrid analog-digital compute-in-memory system stores well-trained weights in analog RM arrays while implementing dynamic LoRA updates in digital computing unit with SRAM buffer.

Result: Prototype fabricated in 180nm CMOS achieves up to 147.76× reduction in training cost, 387.95× reduction in deployment overhead, and 48.44× reduction in inference energy across privacy-sensitive tasks including face recognition, speaker authentication, and stylized image generation.

Conclusion: The hardware-software co-design enables secure and efficient neuromorphic intelligence at the edge by providing efficient machine unlearning and continual learning capabilities while maintaining high energy efficiency during inference.

Abstract: Resistive memory (RM) based neuromorphic systems can emulate synaptic plasticity and thus support continual learning, but they generally lack biologically inspired mechanisms for active forgetting, which are critical for meeting modern data privacy requirements. Algorithmic forgetting, or machine unlearning, seeks to remove the influence of specific data from trained models to prevent memorization of sensitive information and the generation of harmful content, yet existing exact and approximate unlearning schemes incur prohibitive programming overheads on RM hardware owing to device variability and iterative write-verify cycles. Analogue implementations of continual learning face similar barriers. Here we present a hardware-software co-design that enables an efficient training, deployment and inference pipeline for machine unlearning and continual learning on RM accelerators. At the software level, we introduce a low-rank adaptation (LoRA) framework that confines updates to compact parameter branches, substantially reducing the number of trainable parameters and therefore the training cost. At the hardware level, we develop a hybrid analogue-digital compute-in-memory system in which well-trained weights are stored in analogue RM arrays, whereas dynamic LoRA updates are implemented in a digital computing unit with SRAM buffer. This hybrid architecture avoids costly reprogramming of analogue weights and maintains high energy efficiency during inference. Fabricated in a 180 nm CMOS process, the prototype achieves up to a 147.76-fold reduction in training cost, a 387.95-fold reduction in deployment overhead and a 48.44-fold reduction in inference energy across privacy-sensitive tasks including face recognition, speaker authentication and stylized image generation, paving the way for secure and efficient neuromorphic intelligence at the edge.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Mitigating GIL Bottlenecks in Edge AI Systems](https://arxiv.org/abs/2601.10582)
*Mridankan Mandal,Smit Sanjay Shende*

Main category: cs.DC

TL;DR: Python edge AI agents face runtime optimization challenges due to Python's GIL causing throughput degradation at high thread counts. The paper presents a profiling tool and adaptive runtime system using a Blocking Ratio metric to distinguish I/O wait from GIL contention, achieving 96.5% of optimal performance without manual tuning.


<details>
  <summary>Details</summary>
Motivation: Deploying Python-based AI agents on resource-constrained edge devices presents runtime optimization challenges: high thread counts are needed to mask I/O latency, but Python's Global Interpreter Lock (GIL) serializes execution, causing throughput degradation at overprovisioned thread counts.

Method: The paper presents a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. This library-based solution adapts thread allocation dynamically without manual tuning.

Result: The solution achieves 96.5% of optimal performance, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles demonstrates 93.9% average efficiency, including real ML inference with ONNX Runtime MobileNetV2.

Conclusion: The Blocking Ratio metric effectively distinguishes I/O wait from GIL contention, providing practical optimization for edge AI systems. Even with Python 3.13t's free threading (GIL elimination), the saturation cliff persists on single-core devices, validating the metric's usefulness in both GIL and no-GIL environments.

Abstract: Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a "saturation cliff": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.

</details>


### [4] [Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions](https://arxiv.org/abs/2601.09978)
*Jer Shyuan Ng,Wathsara Daluwatta,Shehan Edirimannage,Charitha Elvitigala,Asitha Kottahachchi Kankanamge Don,Ibrahim Khalil,Heng Zhang,Dusit Niyato*

Main category: cs.DC

TL;DR: Survey paper on Federated Unlearning (FUL) - extending machine unlearning to federated learning to enable data deletion compliance with regulations like Right to be Forgotten.


<details>
  <summary>Details</summary>
Motivation: Federated Learning addresses data privacy but lacks mechanisms for data deletion required by regulations like RTBF. Federated Unlearning aims to remove specific client contributions from trained models in distributed environments.

Method: Survey methodology: introduces FUL fundamentals, reviews frameworks addressing three main challenges (communication cost, resource allocation, security/privacy), discusses applications in distributed networks.

Result: Comprehensive review of existing FUL approaches, identification of implementation challenges, discussion of applications, and mapping of open research problems.

Conclusion: FUL is an emerging research area crucial for building trustworthy, regulation-compliant federated systems. The survey serves as foundational reference for advancing FL with data deletion capabilities.

Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.

</details>


### [5] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: This paper studies heterogeneous distributed linearly separable computation with arbitrary data assignments, characterizing the fundamental tradeoff between computable dimension and communication cost.


<details>
  <summary>Details</summary>
Motivation: Existing homogeneous settings assume uniform data distribution across workers, but real-world systems often have heterogeneous data assignments where workers hold different numbers of datasets. The paper aims to address this more general and practical scenario.

Method: The authors propose a universal computing scheme and a universal converse bound by characterizing the structure of arbitrary heterogeneous data assignment, first for integer communication costs and then extended to fractional communication costs.

Result: The proposed computing scheme and converse bound coincide under some parameter regimes, establishing fundamental tradeoffs between computable dimension and communication cost for heterogeneous distributed linearly separable computation.

Conclusion: The paper provides a comprehensive framework for analyzing heterogeneous distributed linearly separable computation, extending beyond homogeneous settings to more practical scenarios with arbitrary data assignments across workers.

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


### [6] [SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks](https://arxiv.org/abs/2601.10277)
*Evangelos Kolyvas,Alexandros Antonov,Spyros Voulgaris*

Main category: cs.DC

TL;DR: SCRamble is a decentralized protocol that reduces blockchain block dissemination time using innovative link selection strategies combining scoring based on block arrival times and network latency considerations.


<details>
  <summary>Details</summary>
Motivation: Blockchain transaction throughput is limited by network latency in block propagation through random peer-to-peer connections. Faster block dissemination would improve both transaction rates and system security by reducing fork probability.

Method: SCRamble uses a decentralized protocol with innovative link selection strategy integrating two heuristics: 1) scoring mechanism assessing block arrival times from neighboring peers, and 2) network latency considerations.

Result: The protocol significantly reduces block dissemination time in blockchain networks.

Conclusion: SCRamble's approach addresses a fundamental limitation in blockchain scalability by accelerating block propagation, which should improve both transaction throughput and system security.

Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization](https://arxiv.org/abs/2601.09773)
*Binglei Lou,Ruilin Wu,Philip Leong*

Main category: cs.AR

TL;DR: SparseLUT is a framework for efficient DNN deployment on FPGAs that addresses LUT size explosion and inefficient connectivity in existing LUT-based DNNs through architectural enhancements and training optimizations.


<details>
  <summary>Details</summary>
Motivation: Deploying DNNs on resource-constrained edge devices like FPGAs requires balancing latency, power, and hardware resources while maintaining accuracy. Existing LUT-based DNNs suffer from exponential LUT size growth and inefficient random sparse connectivity.

Method: Two orthogonal optimizations: 1) Architectural enhancement aggregating multiple PolyLUT sub-neurons via an adder to reduce LUT consumption and latency. 2) Non-greedy training algorithm that selectively prunes less significant inputs and strategically regrows more effective ones.

Result: Reduces LUT consumption by 2.0x-13.9x, lowers inference latency by 1.2x-1.6x while maintaining comparable accuracy. Training optimization achieves up to 2.13% accuracy gain on MNIST and 0.94% on Jet Substructure Classification with no additional area/latency overhead.

Conclusion: SparseLUT provides a comprehensive framework that effectively addresses key challenges in LUT-based DNN deployment on FPGAs through synergistic architectural and training optimizations, enabling efficient edge deployment with improved performance.

Abstract: Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.

</details>


### [8] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: XR systems need ultra-low-latency performance under power constraints, but face challenges from diverse workloads. This paper analyzes XR workload characteristics through cross-layer profiling to provide architectural guidelines for next-gen XR SoCs.


<details>
  <summary>Details</summary>
Motivation: Extended reality (XR) platforms require deterministic ultra-low-latency performance under strict power/area constraints, but face challenges from increasingly diverse workloads with heterogeneous operators and complex dataflows. Conventional CNN-centric accelerator architectures show diminishing returns, and there's a lack of systematic architectural understanding of the full XR pipeline.

Method: The authors use a cross-layer methodology combining model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. They analyze a representative set of workloads spanning 12 distinct XR kernels to distill architectural characteristics into cross-layer workload archetypes.

Result: The study identifies key workload archetypes (e.g., capacity-limited and overhead-sensitive) and extracts architectural insights. It provides actionable design guidelines for next-generation XR SoCs, showing that XR architecture must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation.

Conclusion: XR architecture design needs to move beyond traditional compute-centric optimization and generic resource scaling. Future XR systems require phase-aware scheduling and elastic resource allocation to achieve greater energy efficiency and high performance, addressing the diverse workload characteristics identified through cross-layer analysis.

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Emergency Department Patient Flow Optimization with an Alternative Care Threshold Policy](https://arxiv.org/abs/2601.10041)
*Sahba Baniasadi,Paul M. Griffin,Prakash Chakraborty*

Main category: cs.PF

TL;DR: A threshold-based admission policy for emergency departments redirects non-urgent patients to alternative care during peak congestion, modeled as a two-class M/M/c queuing system with preemptive priority, achieving up to 5.9% performance gains.


<details>
  <summary>Details</summary>
Motivation: Emergency department overcrowding and patient boarding compromise care quality, creating a need for systematic approaches to manage patient flow while maintaining clinical priorities.

Method: Model ED as two-class M/M/c preemptive-priority queuing system with high-acuity patients prioritized and low-acuity patients subject to state-dependent redirection. Analyze via level-dependent Quasi-Birth-Death process to determine optimal threshold maximizing long-run time-averaged objective function.

Result: Optimal policies are context-dependent: rural EDs optimize at lower redirection thresholds, urban EDs at moderate thresholds. Optimal policy yields performance gains up to 4.84% in rural settings and 5.90% in urban environments.

Conclusion: Provides mathematically rigorous framework for balancing clinical priority with operational efficiency across diverse ED settings through threshold-based admission policies.

Abstract: Emergency department (ED) overcrowding and patient boarding represent critical systemic challenges that compromise care quality. We propose a threshold-based admission policy that redirects non-urgent patients to alternative care pathways, such as telemedicine, during peak congestion. The ED is modeled as a two-class $M/M/c$ preemptive-priority queuing system, where high-acuity patients are prioritized and low-acuity patients are subject to state-dependent redirection. Analyzed via a level-dependent Quasi-Birth-Death (QBD) process, the model determines the optimal threshold by maximizing a long-run time-averaged objective function comprising redirection-affected revenue and costs associated with patient balking and system occupancy. Numerical analysis using national healthcare data reveals that optimal policies are highly context-dependent. While rural EDs generally optimize at lower redirection thresholds, urban EDs exhibit performance peaks at moderate thresholds. Results indicate that our optimal policy yields significant performance gains of up to $4.84\%$ in rural settings and $5.90\%$ in urban environments. This research provides a mathematically rigorous framework for balancing clinical priority with operational efficiency across diverse ED settings.

</details>


### [10] [Long-term Monitoring of Kernel and Hardware Events to Understand Latency Variance](https://arxiv.org/abs/2601.10572)
*Fang Zhou,Yuyang Huang,Miao Yu,Sixiang Ma,Tongping Liu,Yang Wang*

Main category: cs.PF

TL;DR: VarMRI is a tool chain for long-term monitoring and analysis of kernel/hardware events causing latency variance, using selective recording and efficient analysis methods to handle big data from extended monitoring periods.


<details>
  <summary>Details</summary>
Motivation: To understand latency variance caused by kernel and hardware events that are invisible at the application level, requiring long-term monitoring to capture rare but impactful events.

Method: Built VarMRI tool chain with selective event recording (only events affecting application requests, coarse-grained first with additional info when needed) and efficient analysis methods robust to large datasets and missing data.

Result: 3,000-hour study of six applications on CloudLab revealed diverse latency variance causes (interrupt preemption, Java GC, pipeline stall, NUMA balancing, etc.), with simple optimizations reducing tail latencies by up to 31%.

Conclusion: Long-term monitoring is necessary as latency variance events vary significantly across experiments, and VarMRI provides an effective framework for understanding and mitigating these invisible performance issues.

Abstract: This paper presents our experience to understand latency variance caused by kernel and hardware events, which are often invisible at the application level. For this purpose, we have built VarMRI, a tool chain to monitor and analyze those events in the long term. To mitigate the "big data" problem caused by long-term monitoring, VarMRI selectively records a subset of events following two principles: it only records events that are affecting the requests recorded by the application; it records coarse-grained information first and records additional information only when necessary. Furthermore, VarMRI introduces an analysis method that is efficient on large amount of data, robust on different data set and against missing data, and informative to the user.
  VarMRI has helped us to carry out a 3,000-hour study of six applications and benchmarks on CloudLab. It reveals a wide variety of events causing latency variance, including interrupt preemption, Java GC, pipeline stall, NUMA balancing etc.; simple optimization or tuning can reduce tail latencies by up to 31%. Furthermore, the impacts of some of these events vary significantly across different experiments, which confirms the necessity of long-term monitoring.

</details>
