{"id": "2602.19433", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.19433", "abs": "https://arxiv.org/abs/2602.19433", "authors": ["Paul Borrill"], "title": "Why iCloud Fails: The Category Mistake of Cloud Synchronization", "comment": "18 pages, 3 figures, 37 references", "summary": "iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality.", "AI": {"tldr": "iCloud Drive's cloud sync semantics fundamentally diverge from POSIX filesystem expectations, causing failures with tools like Time Machine and git due to projecting distributed causal graphs onto linear temporal chains - a structural error that also manifests as link flapping in networks.", "motivation": "To analyze why iCloud Drive fails when used with standard developer tools and workflows, and to identify the fundamental category mistake underlying these failures that also appears in distributed systems design.", "method": "Unified analysis of iCloud's incompatibilities with POSIX semantics, examination of documented corruption events, case study of 366 GB of divergent state accumulation, and theoretical analysis of the structural error of projecting distributed causal graphs onto linear temporal chains.", "result": "Identifies five interlocking incompatibilities rooted in a single structural error: projecting distributed causal graphs onto linear temporal chains. Shows this same category mistake causes epistemic collapse in network fabrics through link flapping.", "conclusion": "Open Atomic Ethernet (OAE) transactional semantics - bilateral, reversible, and conservation-preserving - provide the structural foundation for resolving these failures by aligning protocol behavior with physical reality rather than trying to defeat physics."}}
{"id": "2602.18568", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18568", "abs": "https://arxiv.org/abs/2602.18568", "authors": ["Matthew Adiletta", "Gu-Yeon Wei", "David Brooks"], "title": "RPU -- A Reasoning Processing Unit", "comment": null, "summary": "Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.\n  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B.", "AI": {"tldr": "RPU is a chiplet-based architecture designed to overcome memory bandwidth bottlenecks in LLM inference, achieving up to 45.3x lower latency and 18.6x higher throughput compared to H100 systems.", "motivation": "LLM inference is increasingly bottlenecked by memory bandwidth limitations, especially for emerging reasoning applications with long output sequences, low arithmetic intensity, and tight latency constraints. Current GPUs struggle with memory-bound workloads, leading to low system utilization and high energy consumption per inference.", "method": "The Reasoning Processing Unit (RPU) introduces three key innovations: 1) Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost, 2) a scalable chiplet architecture with bandwidth-first power and area provisioning, and 3) a decoupled microarchitecture separating memory, compute, and communication pipelines to sustain high bandwidth utilization.", "result": "Simulation results show RPU achieves up to 45.3x lower latency and 18.6x higher throughput compared to an H100 system at ISO-TDP (same thermal design power) when running Llama3-405B.", "conclusion": "RPU provides an optimized system architecture that addresses the memory bandwidth bottleneck in LLM inference, enabling scalable performance for memory-bound workloads and improving energy efficiency for emerging reasoning applications."}}
{"id": "2602.19031", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19031", "abs": "https://arxiv.org/abs/2602.19031", "authors": ["Meng Zhang", "Ziang Yin", "Nicholas Gangi", "Alexander Chen", "Brett Bamfo", "Tianle Xu", "Jiaqi Gu", "Zhaoran Rena Huang"], "title": "SKYLIGHT: A Scalable Hundred-Channel 3D Photonic In-Memory Tensor Core Architecture for Real-time AI Inference", "comment": null, "summary": "The growing computational demands of artificial intelligence (AI) are challenging conventional electronics, making photonic computing a promising alternative. However, existing photonic architectures face fundamental scalability and reliability barriers. This paper introduces SKYLIGHT, a scalable 3D photonic in-memory tensor core architecture designed for real-time AI inference. By co-designing its topology, wavelength routing, accumulation, and programming in a 3D stack, SKYLIGHT overcomes key limitations. Its innovations include a low-loss 3D Si/SiN crossbar topology, a thermally robust non-micro-ring resonator (MRR)-based wavelength-division multiplexing (WDM) component, a hierarchical signal accumulation using a multi-port photodetector (PD), and optically programmed non-volatile phase-change material (PCM) weights. Importantly, SKYLIGHT enables in-situ weight updates that support label-free, layer-local learning (e.g., forward-forward local updates) in addition to inference. Using SimPhony for system-level modeling, we show that a single 144 x 256 SKYLIGHT core is feasible within a single reticle and delivers 342.1 TOPS at 23.7 TOPS/W, enabling ResNet-50 inference at 1212 FPS with 27 mJ per image, and achieves 84.17 FPS/W end-to-end (1.61 x higher than an NVIDIA RTX PRO 6000 Blackwell GPU) under the same workload in real-time measurements. System-level evaluations on four representative machine learning tasks, including unsupervised local self-learning, demonstrate SKYLIGHT's robustness to realistic hardware non-idealities (low-bit quantization and signal-proportional analog noise capturing modulation, PCM programming, and readout variations). With noise-aware training, SKYLIGHT maintains high task accuracy, validating its potential as a comprehensive solution for energy-efficient, large-scale photonic AI accelerators.", "AI": {"tldr": "SKYLIGHT is a scalable 3D photonic in-memory tensor core architecture for AI inference that overcomes photonic computing limitations through co-designed topology, wavelength routing, accumulation, and programming, achieving high performance and energy efficiency.", "motivation": "Growing computational demands of AI challenge conventional electronics, making photonic computing promising, but existing photonic architectures face fundamental scalability and reliability barriers that need to be overcome.", "method": "Co-designs topology, wavelength routing, accumulation, and programming in a 3D stack with innovations: low-loss 3D Si/SiN crossbar topology, thermally robust non-MRR-based WDM component, hierarchical signal accumulation using multi-port photodetector, and optically programmed non-volatile PCM weights.", "result": "Single 144 x 256 SKYLIGHT core delivers 342.1 TOPS at 23.7 TOPS/W, enabling ResNet-50 inference at 1212 FPS with 27 mJ per image, achieving 84.17 FPS/W end-to-end (1.61x higher than NVIDIA RTX PRO 6000 Blackwell GPU). Maintains high accuracy with noise-aware training across four ML tasks.", "conclusion": "SKYLIGHT validates potential as a comprehensive solution for energy-efficient, large-scale photonic AI accelerators, overcoming key photonic computing limitations and supporting both inference and in-situ weight updates for local learning."}}
{"id": "2602.18641", "categories": ["cs.DC", "physics.hist-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18641", "abs": "https://arxiv.org/abs/2602.18641", "authors": ["Paul Borrill"], "title": "The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist", "comment": "13 pages, no figures", "summary": "In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats \"synchronized time\" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum \"mysteries\" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution.", "AI": {"tldr": "The paper argues that NASA's Coordinated Lunar Time program is based on a philosophical category mistake - treating synchronized time as an objective entity that can be transmitted, when it's actually an epistemic construct dependent on observer relationships.", "motivation": "To critique NASA's plan to establish Coordinated Lunar Time (LTC) by 2026, arguing it rests on a fundamental philosophical confusion about the nature of time synchronization.", "method": "Analyzes the cislunar time program using philosophical frameworks: Ryle's category mistakes, Spekkens' quantum foundations, FITO assumptions, Leibnizian operationalism, Wood-Spekkens fine-tuning argument, and the ontic/epistemic distinction from quantum mechanics.", "result": "Shows that the same conceptual move that resolves quantum puzzles (distinguishing epistemic from ontic) reveals the cislunar time program as incoherent - it treats synchronized time as something that can be transmitted when it's actually a model-dependent representation.", "conclusion": "The paper concludes that the lunar time program is an engineering project built on philosophical confusion, and proposes a transactional alternative based on bilateral atomic interactions rather than unidirectional time distribution."}}
{"id": "2602.18750", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.18750", "abs": "https://arxiv.org/abs/2602.18750", "authors": ["He Sun", "Li Li", "Mingjun Xiao"], "title": "HillInfer: Efficient Long-Context LLM Inference on the Edge with Hierarchical KV Eviction using SmartSSD", "comment": "12 pages, 12 figures, under review", "summary": "Deploying Large Language Models (LLMs) on edge devices such as PCs enables low-latency inference with strong privacy guarantees, but long-context inference is fundamentally constrained by limited memory and compute resources. Beyond model parameters, the KV cache becomes the dominant bottleneck due to its linear growth with context length. Although prior work exploits contextual sparsity to evict unimportant KV data, these approaches are largely designed for memory-rich platforms and incur prohibitive data transfer overhead when applied to resource-constrained edge devices with external storage. In this paper, we propose HillInfer, an importance-aware long-context LLM inference framework on the edge that leverages SmartSSD-assisted hierarchical KV cache management. HillInfer jointly manages KV cache pools across the CPU and SmartSSD, and performs in-storage importance evaluation to reduce unnecessary data movement. Furthermore, we design an adaptive, prefetch-based pipeline that overlaps computation and KV data transfer across GPU, CPU, and SmartSSD, minimizing end-to-end inference latency without sacrificing accuracy. We implement HillInfer on a PC with a commodity GPU, and experiments across multiple models and benchmarks demonstrate up to 8.56 $\\times$ speedup over baselines while preserving model accuracy.", "AI": {"tldr": "HillInfer is a framework for efficient long-context LLM inference on edge devices using SmartSSD-assisted hierarchical KV cache management to overcome memory constraints.", "motivation": "LLMs on edge devices face memory/compute constraints for long-context inference, with KV cache being the dominant bottleneck. Existing approaches designed for memory-rich platforms incur prohibitive data transfer overhead on resource-constrained edge devices.", "method": "HillInfer uses SmartSSD-assisted hierarchical KV cache management across CPU and SmartSSD, performs in-storage importance evaluation to reduce data movement, and implements adaptive prefetch-based pipeline overlapping computation and KV data transfer across GPU, CPU, and SmartSSD.", "result": "Implementation on PC with commodity GPU shows up to 8.56\u00d7 speedup over baselines while preserving model accuracy across multiple models and benchmarks.", "conclusion": "HillInfer enables efficient long-context LLM inference on resource-constrained edge devices through hierarchical KV cache management and optimized data transfer, achieving significant speedups without accuracy loss."}}
{"id": "2602.19312", "categories": ["cs.ET", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.19312", "abs": "https://arxiv.org/abs/2602.19312", "authors": ["Kyriakos Stylianopoulos", "Mario Edoardo Pandolfo", "Paolo Di Lorenzo", "George C. Alexandropoulos"], "title": "Metasurfaces-Integrated Wireless Neural Networks for Lightweight Over-The-Air Edge Inference", "comment": "9 pages, 6 figures, submitted for magazine publication", "summary": "The upcoming sixth Generation (6G) of wireless networks envisions ultra-low latency and energy efficient Edge Inference (EI) for diverse Internet of Things (IoT) applications. However, traditional digital hardware for machine learning is power intensive, motivating the need for alternative computation paradigms. Over-The-Air (OTA) computation is regarded as an emerging transformative approach assigning the wireless channel to actively perform computational tasks. This article introduces the concept of Metasurfaces-Integrated Neural Networks (MINNs), a physical-layer-enabled deep learning framework that leverages programmable multi-layer metasurface structures and Multiple-Input Multiple-Output (MIMO) channels to realize computational layers in the wave propagation domain. The MINN system is conceptualized as three modules: Encoder, Channel (uncontrollable propagation features and metasurfaces), and Decoder. The first and last modules, realized respectively at the multi-antenna transmitter and receiver, consist of conventional digital or purposely designed analog Deep Neural Network (DNN) layers, and the metasurfaces responses of the Channel module are optimized alongside all modules as trainable weights. This architecture enables computation offloading into the end-to-end physical layer, flexibly among its constituent modules, achieving performance comparable to fully digital DNNs while significantly reducing power consumption. The training of the MINN framework, two representative variations, and performance results for indicative applications are presented, highlighting the potential of MINNs as a lightweight and sustainable solution for future EI-enabled wireless systems. The article is concluded with a list of open challenges and promising research directions.", "AI": {"tldr": "MINNs integrate programmable metasurfaces with MIMO channels to perform neural network computations in the physical layer, reducing power consumption for edge inference in 6G IoT systems.", "motivation": "6G networks require ultra-low latency and energy-efficient edge inference for IoT applications, but traditional digital ML hardware is power-intensive. OTA computation offers an alternative by using wireless channels for computational tasks.", "method": "Proposes Metasurfaces-Integrated Neural Networks (MINNs) with three modules: Encoder (transmitter DNN layers), Channel (uncontrollable propagation + programmable metasurfaces), and Decoder (receiver DNN layers). Metasurfaces responses are optimized as trainable weights alongside all modules, enabling computation offloading into the physical layer.", "result": "MINNs achieve performance comparable to fully digital DNNs while significantly reducing power consumption. The framework, training methods, and two representative variations are presented with performance results for indicative applications.", "conclusion": "MINNs offer a lightweight and sustainable solution for future edge inference-enabled wireless systems, with potential for 6G IoT applications. The article concludes with open challenges and promising research directions."}}
{"id": "2602.18723", "categories": ["cs.DC", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.18723", "abs": "https://arxiv.org/abs/2602.18723", "authors": ["Paul Borrill"], "title": "What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature", "comment": "15 pages, no figures", "summary": "The foundational impossibility results of distributed computing -- the Fischer-Lynch-Paterson theorem, the Two Generals Problem, the CAP theorem -- are widely understood as discoveries about the physical limits of coordination. This paper argues that they are nothing of the sort. They are consequences of a category mistake: treating Forward-In-Time-Only (FITO) information flow as a law of nature rather than recognizing it as a design choice inherited from Shannon's channel model and Lamport's happened-before relation. We develop this argument in six steps. First, we introduce the category mistake framework from Ryle through Spekkens' ontic/epistemic distinction in quantum foundations. Second, we identify FITO as the hidden axiom that unifies the classical impossibility results. Third, we apply Spekkens' Leibnizian principle to show that FITO-based models contain surplus ontological structure. Fourth, we develop the counterfactual: what changes when FITO is dropped. Fifth, we demonstrate that the impossibility theorems are theorems about FITO systems, not about physics. Sixth, we sketch the transactional alternative -- bilateral interactions that dissolve the apparent impossibilities by replacing unidirectional message passing with atomic bilateral transactions. The implication is that distributed computing has spent fifty years optimizing within the wrong design space.", "AI": {"tldr": "The paper argues that classical distributed computing impossibility results (FLP, Two Generals, CAP) are not physical limits but consequences of a design choice - Forward-In-Time-Only (FITO) information flow - which is a category mistake treated as a law of nature rather than a modeling assumption.", "motivation": "To challenge the fundamental interpretation of distributed computing impossibility theorems as physical limits, arguing they stem from a modeling assumption (FITO information flow) rather than inherent physical constraints.", "method": "Six-step argument: 1) Introduce category mistake framework using Ryle and Spekkens' ontic/epistemic distinction; 2) Identify FITO as hidden axiom unifying classical impossibility results; 3) Apply Spekkens' Leibnizian principle to show FITO models contain surplus ontological structure; 4) Develop counterfactual without FITO; 5) Demonstrate theorems are about FITO systems, not physics; 6) Sketch transactional alternative with bilateral interactions.", "result": "The paper shows that distributed computing impossibility results are consequences of the FITO design choice rather than fundamental physical limits, suggesting the field has been optimizing within the wrong design space for 50 years.", "conclusion": "Distributed computing's foundational impossibility theorems are not about physical coordination limits but about the FITO information flow model, and replacing unidirectional message passing with atomic bilateral transactions could dissolve these apparent impossibilities."}}
{"id": "2602.19341", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments.", "AI": {"tldr": "AMoD-NDP framework for joint strategic design of service network and fleet capacity in autonomous mobility systems with optimality guarantees.", "motivation": "Existing AMoD research focuses on fleet control on fixed networks, ignoring the joint strategic design of service infrastructure and fleet capacity needed for real-world deployment.", "method": "Path-based mixed-integer formulation with column-generation algorithm: master problem optimizes restricted paths, pricing problem uses label-correcting algorithm for elementary shortest paths with resource constraints.", "result": "Framework produces stable, interpretable operation subnetworks, quantifies infrastructure-fleet trade-offs, accommodates path-level constraints, and scales to city-sized networks with optimality certificates.", "conclusion": "The AMoD-NDP approach supports strategic planning and policy analysis for future AMoD deployments by addressing the critical joint design problem of service networks and fleet capacity."}}
{"id": "2602.18755", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18755", "abs": "https://arxiv.org/abs/2602.18755", "authors": ["Omar Basit", "Yunzhao Liu", "Z. Jonny Kong", "Y. Charlie Hu"], "title": "BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS", "comment": null, "summary": "Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.\n  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.\n  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.", "AI": {"tldr": "BiScale is a two-tier energy optimization framework for disaggregated LLM serving that jointly optimizes placement and DVFS across prefill/decode phases to reduce energy while meeting latency SLOs.", "motivation": "LLM inference is energy-intensive, and existing approaches like autoscaling are too coarse-grained to track fast workload fluctuations. Fine-grained DVFS is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control in disaggregated serving architectures.", "method": "Two-tier hierarchical framework: 1) Coarse timescale: computes phase-aware placement and baseline frequencies using predictive latency/power models; 2) Fine timescale: dynamically adapts GPU frequency per iteration with stage-specific control - MPC for prefill to account for queue evolution and TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics.", "result": "Evaluation on 16x H100 cluster serving Llama 3.3 70B with production traces shows BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.", "conclusion": "BiScale successfully addresses energy optimization in disaggregated LLM serving through coordinated two-tier control that maintains strict serving SLOs while achieving significant energy savings across both prefill and decode phases."}}
{"id": "2602.19242", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19242", "abs": "https://arxiv.org/abs/2602.19242", "authors": ["Zheng Li", "Guangyi Zeng", "Paul Delestrac", "Enyi Yao", "Simei Yang"], "title": "pHNSW: PCA-Based Filtering to Accelerate HNSW Approximate Nearest Neighbor Search", "comment": "6 pages for ASPDAC2026 conference", "summary": "Hierarchical Navigable Small World (HNSW) has demonstrated impressive accuracy and low latency for high-dimensional nearest neighbor searches. However, its high computational demands and irregular, large-volume data access patterns present significant challenges to search efficiency. To address these challenges, we introduce pHNSW, an algorithm-hardware co-optimized solution that accelerates HNSW through Principal Component Analysis (PCA) filtering. On the algorithm side, we apply PCA filtering to reduce the dimensionality of the dataset, thereby lowering the volume of neighbor access and decreasing the computational load for distance calculations. On the hardware side, we design the pHNSW processor with custom instructions to optimize search throughput and energy efficiency. In the experiments, we synthesized the pHNSW processor RTL design with a 65nm technology node and evaluated it using DDR4 and HBM1.0 DRAM standards. The results show that pHNSW boosts Queries per Second (QPS) by 14.47x-21.37x on a CPU and 5.37x-8.46x on a GPU, while reducing energy consumption by up to 57.4% compared to standard HNSW implementation.", "AI": {"tldr": "pHNSW is an algorithm-hardware co-optimized solution that accelerates HNSW nearest neighbor search using PCA filtering for dimensionality reduction and custom hardware design, achieving 5-21x speedup and up to 57% energy reduction.", "motivation": "HNSW provides good accuracy and low latency for high-dimensional nearest neighbor searches, but suffers from high computational demands and irregular, large-volume data access patterns that limit search efficiency.", "method": "Algorithm-hardware co-optimization: 1) Algorithm side: Apply PCA filtering to reduce dataset dimensionality, lowering neighbor access volume and computational load for distance calculations. 2) Hardware side: Design pHNSW processor with custom instructions to optimize search throughput and energy efficiency.", "result": "pHNSW processor synthesized with 65nm technology using DDR4 and HBM1.0 DRAM standards. Achieves 14.47x-21.37x QPS improvement on CPU and 5.37x-8.46x on GPU compared to standard HNSW, with up to 57.4% energy consumption reduction.", "conclusion": "pHNSW successfully addresses HNSW's computational and memory access challenges through algorithm-hardware co-optimization, delivering significant performance improvements and energy efficiency gains for high-dimensional nearest neighbor search."}}
{"id": "2602.19694", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.19694", "abs": "https://arxiv.org/abs/2602.19694", "authors": ["Bo Liu", "Tong Li", "Zhu Xiao", "Ruihui Li", "Geyong Min", "Zhuo Tang", "Kenli Li"], "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs", "comment": "under review", "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks.", "AI": {"tldr": "UniMob is a unified human mobility generation model that works across cities regardless of data availability, using LLM-powered travel planning, unified spatial embeddings, and diffusion-based generation to create realistic movement patterns.", "motivation": "Existing human mobility generation methods work well only in data-rich cities but fail in cities with limited data. All cities deserve equal consideration for reliable mobility data generation regardless of their size or available resources.", "method": "Three main components: 1) LLM-powered travel planner for high-level, temporally-aware, semantically meaningful travel plans; 2) Unified spatial embedding module projecting spatial regions of various cities into shared representation space; 3) Diffusion-based mobility generator capturing joint spatiotemporal characteristics guided by travel plans.", "result": "UniMob significantly outperforms state-of-the-art baselines with over 30% improvement across multiple evaluation metrics. Shows robustness in zero- and few-shot scenarios, demonstrates importance of LLM guidance, verifies privacy-preserving nature, and showcases applicability for downstream tasks.", "conclusion": "UniMob addresses the critical gap in human mobility generation for data-scarce cities, providing a unified solution that works across diverse urban environments while maintaining privacy and enabling practical applications."}}
{"id": "2602.18797", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18797", "abs": "https://arxiv.org/abs/2602.18797", "authors": ["Mubshra Zulfiqar", "Muhammad Ayzed Mirza", "Basit Qureshi"], "title": "Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning", "comment": null, "summary": "Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-user interference in multi-antenna (MIMO) uplinks complicate real-time resource management. Traditional centralized optimization and off-policy reinforcement learning struggle with scalability and signaling overhead in dense networks. This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework based on multi-agent proximal policy optimization. The multi-user MIMO-MEC system is modeled as a Decentralized Partially Observable Markov Decision Process (DEC-POMDP) to jointly minimize carbon emissions and buffer latency and energy wastage. A scalable architecture utilizes decentralized execution with parameter sharing (DEPS), which enables autonomous IoT agents to make fine-grained power control and offloading decisions based solely on local observations. Additionally, a carbon-first reward structure adaptively prioritizes green time slots for data transmission to decouple system throughput from grid-dependent carbon footprints. Finally, experimental results demonstrate CADDTO-PPO outperforms deep deterministic policy gradient (DDPG) and lyapunov-based baselines. The framework achieves the lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling validates the framework to demonstrate a constant $O(1)$ inference complexity and theoretical lightweight feasibility for future generation sustainable IoT deployments.", "AI": {"tldr": "CADDTO-PPO is a carbon-aware decentralized task offloading framework using multi-agent PPO for sustainable IoT edge computing, minimizing carbon emissions while maintaining low latency under stochastic renewable energy and traffic conditions.", "motivation": "Massive IoT microservices need sustainable eScience infrastructures integrating renewable energy with mobile edge computing. Challenges include spatiotemporal mismatches between stochastic task arrivals and intermittent green energy, plus complex interference in MIMO uplinks. Traditional centralized optimization and off-policy RL struggle with scalability and signaling overhead in dense networks.", "method": "Proposes CADDTO-PPO: carbon-aware decentralized dynamic task offloading using multi-agent proximal policy optimization. Models multi-user MIMO-MEC system as Decentralized Partially Observable Markov Decision Process (DEC-POMDP). Uses decentralized execution with parameter sharing (DEPS) for autonomous IoT agents making power control and offloading decisions based on local observations. Implements carbon-first reward structure prioritizing green time slots for data transmission.", "result": "Outperforms deep deterministic policy gradient (DDPG) and Lyapunov-based baselines. Achieves lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling shows constant O(1) inference complexity, demonstrating theoretical lightweight feasibility for sustainable IoT deployments.", "conclusion": "CADDTO-PPO provides an effective framework for sustainable IoT edge computing by enabling decentralized, carbon-aware resource management that decouples system throughput from grid-dependent carbon footprints while maintaining scalability and low latency performance."}}
{"id": "2602.19268", "categories": ["cs.AR", "cs.AI", "cs.CV", "cs.NE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.19268", "abs": "https://arxiv.org/abs/2602.19268", "authors": ["Sonu Kumar", "Mohd Faisal Khan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications", "comment": null, "summary": "This brief presents a runtime-adaptive, performance-enhanced vector engine featuring a low-resource, iterative CORDIC-based MAC unit for edge AI acceleration. The proposed design enables dynamic reconfiguration between approximate and accurate modes, exploiting the latency-accuracy trade-off for a wide range of workloads. Its resource-efficient approach further enables up to 4x throughput improvement within the same hardware resources by leveraging vectorised, time-multiplexed execution and flexible precision scaling. With a time-multiplexed multi-AF block and a lightweight pooling and normalisation unit, the proposed vector engine supports flexible precision (4/8/16-bit) and high MAC density. The ASIC implementation results show that each MAC stage can save up to 33% of time and 21% of power, with a 256-PE configuration that achieves higher compute density (4.83 TOPS/mm2 ) and energy efficiency (11.67 TOPS/W) than previous state-of-the-art work. A detailed hardware-software co-design methodology for object detection and classification tasks on Pynq-Z2 is discussed to assess the proposed architecture, demonstrating a scalable, energy-efficient solution for edge AI applications.", "AI": {"tldr": "A runtime-adaptive vector engine with iterative CORDIC-based MAC unit for edge AI, featuring dynamic approximate/accurate modes, 4x throughput improvement, and achieving 4.83 TOPS/mm\u00b2 compute density.", "motivation": "To address the need for energy-efficient, scalable AI acceleration at the edge by exploiting latency-accuracy trade-offs and maximizing hardware utilization within constrained resources.", "method": "Proposes a vector engine with low-resource iterative CORDIC-based MAC unit, dynamic reconfiguration between approximate/accurate modes, vectorized time-multiplexed execution, flexible precision scaling (4/8/16-bit), multi-AF block, and lightweight pooling/normalization unit.", "result": "ASIC implementation shows 33% time savings and 21% power savings per MAC stage, with 256-PE configuration achieving 4.83 TOPS/mm\u00b2 compute density and 11.67 TOPS/W energy efficiency, outperforming previous state-of-the-art.", "conclusion": "The proposed architecture provides a scalable, energy-efficient solution for edge AI applications, validated through hardware-software co-design methodology for object detection/classification on Pynq-Z2 platform."}}
{"id": "2602.20083", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20083", "abs": "https://arxiv.org/abs/2602.20083", "authors": ["Xinzhao Li", "Alptekin Vardar", "Franz M\u00fcller", "Navya Goli", "Umamaheswara Tida", "Kai Ni", "X. Sharon Hu", "Thomas K\u00e4mpfe", "Ruiyang Qin"], "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval", "comment": "Accepted by DAC'26", "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG.", "AI": {"tldr": "CQ-CiM is a hardware-aware framework that jointly learns compression and quantization to create low-bit embeddings compatible with diverse Compute-in-Memory architectures for RAG deployment on edge devices.", "motivation": "Deploying RAG on edge devices is hindered by latency from massive data movement and computation on traditional architectures. CiM addresses this bottleneck but faces a \"representation gap\" where high-precision embeddings are incompatible with CiM's low-precision constraints, compounded by diverse CiM implementations requiring different data shaping.", "method": "CQ-CiM is a unified hardware-aware data shaping framework that jointly learns compression and quantization to produce CiM-compatible low-bit embeddings. It addresses the limitations of current disjoint dimension and precision handling methods by providing a comprehensive solution for diverse CiM designs.", "result": "This is the first work to shape data for comprehensive CiM usage on RAG, addressing the fundamental representation gap that has limited CiM adoption for RAG applications.", "conclusion": "CQ-CiM enables efficient RAG deployment on edge devices by bridging the gap between high-precision embeddings and diverse CiM hardware constraints through joint compression-quantization learning, overcoming current limitations in CiM adoption for RAG systems."}}
{"id": "2602.18931", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18931", "abs": "https://arxiv.org/abs/2602.18931", "authors": ["Noah Martin", "Fahad Dogar"], "title": "WANSpec: Leveraging Global Compute Capacity for LLM Inference", "comment": null, "summary": "Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%.", "AI": {"tldr": "WANSpec is a system that offloads speculative decoding draft models to under-utilized data centers to reduce load on high-demand GPUs while maintaining low latency for LLM inference.", "motivation": "Data centers with high-end GPUs for large LLMs are under high demand, causing latency issues, while other data centers with less capable hardware are under-utilized. There's an opportunity to balance this uneven load distribution.", "method": "WANSpec uses speculative decoding but moves the draft model execution to under-utilized data centers. It employs judicious redundancy to avoid latency increases while offloading compute from high-demand regions.", "result": "Experiments in simulation and cloud deployments show WANSpec reduces draft model forward passes in high-demand data centers by over 50% without increasing latency.", "conclusion": "WANSpec effectively leverages under-utilized compute resources to mitigate capacity issues in high-demand data centers, enabling more efficient use of global compute infrastructure for LLM inference."}}
{"id": "2602.19305", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19305", "abs": "https://arxiv.org/abs/2602.19305", "authors": ["Irisha M. Goswami", "D. G. Perera"], "title": "Closed-Loop Environmental Control System on Embedded Systems", "comment": "19 pages, 6 figures", "summary": "In this paper, our objective is to design, build, and verify a closed-loop environmental control system tailored for small-scale agriculture applications. This project aims to develop a low-cost, safety-critical embedded solution using the Nuvoton NUC140 microcontroller to automate temperature regulation. The goal was to mitigate crop yield losses caused by environmental fluctuations in a greenhouse. Our final implemented system successfully meets all design specifications, demonstrating robust temperature regulation through a PID control loop and ensuring hardware safety through galvanic isolation", "AI": {"tldr": "Design and implementation of a low-cost embedded environmental control system using NUC140 microcontroller for greenhouse temperature regulation with PID control and safety features.", "motivation": "To address crop yield losses caused by environmental fluctuations in small-scale agriculture by developing an automated, low-cost temperature control system for greenhouse applications.", "method": "Designed and built a closed-loop control system using Nuvoton NUC140 microcontroller with PID control algorithm for temperature regulation, incorporating galvanic isolation for hardware safety.", "result": "Successfully implemented system meeting all design specifications, demonstrating robust temperature regulation through PID control loop while ensuring hardware safety.", "conclusion": "The developed low-cost embedded solution effectively automates greenhouse temperature control, potentially mitigating crop yield losses from environmental fluctuations in small-scale agriculture."}}
{"id": "2602.19084", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19084", "abs": "https://arxiv.org/abs/2602.19084", "authors": ["Emir Gencer", "Mohammad Kefah Taha Issa", "Ilyas Turimbetov", "James D. Trotter", "Didem Unat"], "title": "ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication", "comment": "11 pages, 8 figures. To appear in the 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2026)", "summary": "UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.\n  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace.", "AI": {"tldr": "ucTrace is a novel profiler that exposes and visualizes UCX-driven communication in HPC environments, providing fine-grained communication traces at the UCX level to help optimize performance and debug communication patterns.", "motivation": "Existing profiling tools lack fine-grained communication traces at the UCX level, don't capture transport-layer behavior, or are limited to specific MPI implementations, creating a gap in understanding UCX-driven communication in HPC systems.", "method": "ucTrace profiles message passing at the UCX level, linking operations between hosts and devices (GPUs, NICs) directly to their originating MPI functions, and provides interactive visualizations of process- and device-specific interactions.", "result": "The paper demonstrates ucTrace's capabilities through various experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale.", "conclusion": "ucTrace effectively addresses the gap in UCX-level profiling, providing valuable insights for system administrators, library and application developers to optimize performance and debug communication patterns in large-scale HPC workloads, and is publicly available as an open-source tool."}}
{"id": "2602.19720", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19720", "abs": "https://arxiv.org/abs/2602.19720", "authors": ["Xiaoke Wang", "Raveena Raikar", "Markus Rein", "Ruiqi Chen", "Chang Meng", "Dirk Stroobandt"], "title": "Interconnect-Aware Logic Resynthesis for Multi-Die FPGAs", "comment": null, "summary": "Multi-die FPGAs enable device scaling beyond reticle limits but introduce severe interconnect overhead across die boundaries. Inter-die connections, commonly referred to as super-long lines (SLLs), incur high delay and consume scarce interposer interconnect resources, often dominating critical paths and complicating physical design. To address this, this work proposes an interconnect-aware logic resynthesis method that restructures the LUT-level netlist to reduce the number of SLLs. The resynthesis engine uses die partitioning information to apply logic resubstitutions, which simplifies local circuit structures and eliminates SLLs. By reducing the number of SLLs early in the design flow, prior to physical implementation, the proposed method shortens critical paths, alleviates pressure on scarce interposer interconnect resources, and improves overall physical design flexibility. We further build a tool flow for multi-die FPGAs by integrating the proposed resynthesis method with packing and placement. Experimental results on the EPFL benchmarks show that, compared with a state-of-the-art framework, the proposed method reduces the number of SLLs by up to 24.8% for a 2-die FPGA and up to 27.38% for a 3-die FPGA. On MCNC benchmarks, our tool flow achieves an average SLL reduction of 1.65% while preserving placement quality. On Koios benchmarks, where fewer removable SLLs exist, several designs still exhibit considerable inter-die edge reductions. Overall, the results confirm that reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs.", "AI": {"tldr": "Proposes interconnect-aware logic resynthesis for multi-die FPGAs to reduce super-long lines (SLLs) between dies, improving performance and resource utilization.", "motivation": "Multi-die FPGAs enable scaling beyond reticle limits but suffer from severe interconnect overhead across die boundaries. Inter-die connections (SLLs) incur high delay, consume scarce interposer resources, dominate critical paths, and complicate physical design.", "method": "An interconnect-aware logic resynthesis method that restructures LUT-level netlist to reduce SLLs. Uses die partitioning information to apply logic resubstitutions that simplify local circuit structures and eliminate SLLs. Integrated with packing and placement in a complete tool flow for multi-die FPGAs.", "result": "Reduces SLLs by up to 24.8% for 2-die FPGAs and up to 27.38% for 3-die FPGAs on EPFL benchmarks. Achieves average 1.65% SLL reduction on MCNC benchmarks while preserving placement quality. Several Koios benchmarks show considerable inter-die edge reductions despite fewer removable SLLs.", "conclusion": "Reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs. Early SLL reduction in the design flow shortens critical paths, alleviates pressure on scarce interposer resources, and improves physical design flexibility."}}
{"id": "2602.19088", "categories": ["cs.DC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.19088", "abs": "https://arxiv.org/abs/2602.19088", "authors": ["Ziwei Zhou", "Si Liu", "Zhou Zhou", "Peixin Wang", "MIn Zhang"], "title": "A Formal Framework for Predicting Distributed System Performance under Faults", "comment": "32 pages, 3 figures. Accepted by FM 2026", "summary": "Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments.", "AI": {"tldr": "PERF is the first formal framework for predicting distributed system performance across diverse fault scenarios using automated fault injection and statistical analysis.", "motivation": "Distributed systems operate in complex environments with inevitable faults and adversarial behaviors, making performance prediction from formal designs a longstanding challenge.", "method": "A formal framework with fault injector library, model compositions integrating system and fault injector into unified model, implemented in Maude as automated tool PERF for statistical analysis of performance properties.", "result": "PERF accurately predicts system performance under varying fault settings, with formal design estimations consistent with real deployment evaluations for representative distributed systems.", "conclusion": "The framework enables systematic performance prediction of distributed systems across diverse faulty scenarios, bridging formal designs with real-world performance evaluation."}}
{"id": "2602.19884", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.19884", "abs": "https://arxiv.org/abs/2602.19884", "authors": ["Harry Fitchett", "Jasmine Ritchie", "Charles Fox"], "title": "Extending CPU-less parallel execution of lambda calculus in digital logic with lists and arithmetic", "comment": null, "summary": "Computer architecture is searching for new ways to make use of increasingly available digital logic without the serial bottlenecks of CPU-based design. Recent work has demonstrated a fully CPU-less approach to executing functional programs, by exploiting their inherent parallelisability to compile them directly into parallel digital logic. This work uses lambda-calculus as a hyper simple functional language to prove the concept, but is impractical for real-world programming due to the well-known inefficiencies of pure lambda$-calculus. It is common in language design to extend basic lambda-calculus with additional primitives to short-cut common tasks such as arithmetic and lists. In this work, we build upon our previous research to examine how such extensions may be applied to CPU-less functional execution in digital logic, with the objective of advancing the approach toward practical implementation. We present a set of structures and algorithms for representing new primitives, describe a systematic process for selecting, implementing, and evaluating them, and demonstrate substantial reductions in execution time and node usage. These improvements are implemented in an open-source system, which is shown to correctly evaluate a range of representative lambda expressions.", "AI": {"tldr": "This paper extends CPU-less functional program execution by adding practical primitives to lambda-calculus, reducing execution time and resource usage for real-world implementation.", "motivation": "To advance CPU-less functional execution toward practical implementation by addressing the inefficiencies of pure lambda-calculus through extension with practical primitives.", "method": "Extend basic lambda-calculus with additional primitives (arithmetic, lists) using structures and algorithms for representation, with systematic selection, implementation, and evaluation processes.", "result": "Demonstrated substantial reductions in execution time and node usage, implemented in an open-source system that correctly evaluates representative lambda expressions.", "conclusion": "The approach successfully advances CPU-less functional execution toward practical implementation by overcoming lambda-calculus inefficiencies through strategic primitive extensions."}}
{"id": "2602.19121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19121", "abs": "https://arxiv.org/abs/2602.19121", "authors": ["Matthias F\u00fcgger", "Thomas Nowak"], "title": "Asymptotic Subspace Consensus in Dynamic Networks", "comment": null, "summary": "We introduce the problem of asymptotic subspace consensus, which requires the outputs of processes to converge onto a common subspace while remaining inside the convex hull of initial vectors.This is a relaxation of asymptotic consensus in which outputs have to converge to a single point, i.e., a zero-dimensional affine subspace.\n  We give a complete characterization of the solvability of asymptotic subspace consensus in oblivious message adversaries. In particular, we show that a large class of algorithms used for asymptotic consensus gracefully degrades to asymptotic subspace consensus in distributed systems with weaker assumptions on the communication network. We also present bounds on the rate by which a lower-than-initial dimension is reached.", "AI": {"tldr": "This paper introduces asymptotic subspace consensus, a relaxed version of asymptotic consensus where outputs converge to a common subspace rather than a single point, and provides complete solvability characterization for oblivious message adversaries.", "motivation": "The motivation is to develop a more flexible consensus model that can handle weaker communication assumptions by allowing processes to converge to a subspace rather than a single point, enabling graceful degradation of existing consensus algorithms in less reliable networks.", "method": "The paper introduces the asymptotic subspace consensus problem, provides complete characterization of solvability under oblivious message adversaries, analyzes how existing asymptotic consensus algorithms degrade to subspace consensus, and presents bounds on convergence rates to lower dimensions.", "result": "Complete characterization of solvability for asymptotic subspace consensus in oblivious message adversaries, showing that many existing asymptotic consensus algorithms naturally degrade to subspace consensus in weaker networks, with bounds on dimension reduction rates.", "conclusion": "Asymptotic subspace consensus provides a practical relaxation of traditional consensus that maintains convergence guarantees under weaker communication assumptions, with existing algorithms showing graceful degradation and predictable dimension reduction behavior."}}
{"id": "2602.19231", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19231", "abs": "https://arxiv.org/abs/2602.19231", "authors": ["Georgii Semenov", "Vitaly Aksenov"], "title": "Semantic Conflict Model for Collaborative Data Structures", "comment": "6 pages, 7 figures, submitted to PaPoC 2026", "summary": "Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation.", "AI": {"tldr": "A conflict model for collaborative data structures enabling explicit, local-first conflict resolution without central coordination, using semantic dependencies and three-way merge over replicated journals.", "motivation": "Current CRDTs have opaque conflict resolution, and existing reconciliation techniques often require centralized coordination, creating a need for explicit, local-first conflict resolution in collaborative systems.", "method": "Introduces a conflict model that identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via three-way merge over a replicated journal.", "result": "Demonstrated on collaborative registers, including explicit formulation of Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation.", "conclusion": "The proposed model enables explicit, local-first conflict resolution without central coordination, addressing limitations of current CRDTs and reconciliation techniques."}}
{"id": "2602.19338", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.19338", "abs": "https://arxiv.org/abs/2602.19338", "authors": ["Halit Uyan\u0131k", "Tolga Ovatman"], "title": "Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement", "comment": null, "summary": "The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations.", "AI": {"tldr": "A Python library for optimizing complex event processing (CEP) on IoT edge devices using constrained programming to balance execution costs and improve critical path performance.", "motivation": "IoT environments face challenges with limited hardware and computation power on edge devices while handling increasingly complex tasks like complex event processing (CEP), which requires reading and aggregating data from multiple sources to detect important events.", "method": "Developed a constrained programming optimization approach to balance execution costs between different paths of CEP task graphs, improving critical path performance. Implemented as a Python library that abstracts communication details and enables shared memory virtualization between IoT devices.", "result": "The optimization approach increases throughput and reduces delay across multiple devices during CEP operations, allowing small-scale IoT devices to adaptively optimize code and I/O assignments.", "conclusion": "The proposed Python library effectively addresses IoT edge device limitations by optimizing CEP task execution through constrained programming, improving overall latency and throughput while abstracting communication complexities."}}
{"id": "2602.19683", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19683", "abs": "https://arxiv.org/abs/2602.19683", "authors": ["Henrik M\u00f6llmann", "Dirk Pfl\u00fcger", "Alexander Strack"], "title": "GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX", "comment": "13 pages, 7 figures, Workshop on Asynchronous Many-Task Systems and Applications 2026", "summary": "Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.\n  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent.", "AI": {"tldr": "GPU-accelerated Gaussian process regression using tiled algorithms and CUDA streams achieves up to 4.6x speedup over CPU implementation for datasets >128 samples.", "motivation": "Gaussian processes are widely used for regression but suffer from cubic computational complexity that limits scalability. Existing CPU implementations are too slow for large datasets.", "method": "Extended GPRat library with fully GPU-resident GP prediction pipeline using HPX-based task parallelism. Implemented tiled algorithms with optimized CUDA libraries, evaluated optimal CUDA streams configuration.", "result": "GPU implementation provides speedups for datasets >128 samples: up to 4.3x for Cholesky decomposition, 4.6x for GP prediction. Combining HPX with multiple CUDA streams matches or surpasses cuSOLVER by up to 11% for large datasets.", "conclusion": "GPU acceleration with task-based parallelism and optimized CUDA streams significantly improves Gaussian process regression performance, making it scalable to larger datasets while maintaining library usability."}}
{"id": "2602.19742", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.19742", "abs": "https://arxiv.org/abs/2602.19742", "authors": ["Yulun Huang", "Zhiyu Wang", "Rajkumar Buyya"], "title": "A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response", "comment": null, "summary": "Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations.", "AI": {"tldr": "An integrated UAV-edge computing framework for wildfire monitoring that co-optimizes route planning, fleet sizing, and service provisioning to minimize response time while meeting energy, revisit time, and capacity constraints.", "motivation": "Wildfire monitoring requires timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is promising but faces challenges in jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints.", "method": "Integrated framework combining: 1) fire-history-weighted clustering to prioritize high-risk areas, 2) QoS-aware edge assignment balancing proximity and computational load, 3) 2-opt route optimization with adaptive fleet sizing, and 4) dynamic emergency rerouting mechanism.", "result": "The framework reduces average response time by 70.6-84.2%, energy consumption by 73.8-88.4%, and fleet size by 26.7-42.1% compared to GA, PSO, and greedy baselines. Emergency mechanism responds within 233 seconds (under 300-second deadline) with negligible impact on normal operations.", "conclusion": "The proposed integrated framework effectively addresses the interdependent challenges of UAV-assisted wildfire monitoring by co-optimizing multiple aspects simultaneously, achieving significant improvements in response time, energy efficiency, and resource utilization while maintaining emergency responsiveness."}}
{"id": "2602.19802", "categories": ["cs.DC", "cs.NE", "math.CV", "math.DS"], "pdf": "https://arxiv.org/pdf/2602.19802", "abs": "https://arxiv.org/abs/2602.19802", "authors": ["Romain de Coudenhove", "Yannis Bendi-Ouis", "Anthony Strock", "Xavier Hinaut"], "title": "Linear Reservoir: A Diagonalization-Based Optimization", "comment": null, "summary": "We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues.", "AI": {"tldr": "Diagonalization optimization reduces Linear ESN computational complexity from O(N\u00b2) to O(N) by reformulating reservoir dynamics in eigenbasis, enabling independent element-wise operations instead of matrix multiplication.", "motivation": "To address the high computational cost of Linear Echo State Networks (ESNs) where reservoir state updates have O(N\u00b2) complexity due to matrix multiplication, limiting scalability and efficiency.", "method": "Three optimization methods: 1) Eigenbasis Weight Transformation (EWT) preserves standard/trained ESN dynamics, 2) End-to-End Eigenbasis Training (EET) directly optimizes readout weights in transformed space, 3) Direct Parameter Generation (DPG) bypasses diagonalization by directly sampling eigenvalues/eigenvectors.", "result": "Methods preserve predictive accuracy while offering significant computational speedups, achieving comparable performance to standard Linear ESNs with reduced complexity.", "conclusion": "The optimization serves as a replacement for standard Linear ESN computations and training, suggesting a paradigm shift toward direct eigenvalue selection in linear ESNs."}}
{"id": "2602.20097", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20097", "abs": "https://arxiv.org/abs/2602.20097", "authors": ["Pu Jiao", "Sheng Di", "Jiannan Tian", "Mingze Xia", "Xuan Wu", "Yang Zhang", "Xin Liang", "Franck Cappello"], "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation", "comment": null, "summary": "Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput.", "AI": {"tldr": "Proposes a novel quantization-aware interpolation algorithm to mitigate artifacts in pre-quantization based lossy compressors, improving decompressed data quality while maintaining high throughput.", "motivation": "Pre-quantization based compressors offer high throughput but suffer from low data quality with medium/large error bounds, creating artifacts that need mitigation.", "method": "Characterizes artifacts in pre-quantization compressors, proposes quantization-aware interpolation algorithm, parallelizes it for shared/distributed memory environments.", "result": "Algorithm effectively improves decompressed data quality while maintaining high compression throughput, validated with two leading compressors using five real-world datasets.", "conclusion": "Proposed artifact mitigation algorithm successfully addresses quality limitations of pre-quantization compressors, enabling both high throughput and improved data quality."}}
