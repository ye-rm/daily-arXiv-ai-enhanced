<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 11]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT enables parallel LLM agent coordination through shared state monitoring using CRDTs instead of explicit messaging, achieving up to 21.1% speedup with 100% convergence but showing variable performance depending on task structure.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems currently fail to achieve parallel speedups due to expensive coordination overhead from explicit message passing between agents.

Method: CodeCRDT uses observation-driven coordination with Conflict-Free Replicated Data Types (CRDTs), allowing agents to monitor shared state with observable updates and deterministic convergence rather than explicit message passing.

Result: Evaluation across 600 trials showed up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, with 100% convergence and zero merge failures. Semantic conflict rates were 5-10%.

Conclusion: Observation-driven coordination with CRDTs enables lock-free, conflict-free concurrent code generation with strong eventual consistency, but performance varies based on task structure, revealing quality-performance tradeoffs.

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [2] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: AI-driven distributed systems policy design using LLM-generated code and deterministic verification in a domain-specific simulator, applied to FaaS scheduler optimization.


<details>
  <summary>Details</summary>
Motivation: To automate and scale the design of distributed systems policies by combining the creative generation capabilities of LLMs with rigorous verification methods.

Method: Iterative generate-and-verify loop: LLM proposes Python policies, simulator evaluates on standardized traces, structured feedback guides subsequent generations using Bauplan FaaS runtime and Eudoxia simulator.

Result: Preliminary results show throughput improvements across multiple models, demonstrating the approach's effectiveness for scheduler optimization.

Conclusion: The methodology shows promise for automated policy design and AI could help bootstrap new simulators to scale this approach further.

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [3] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: Comparative analysis of Apache Spark performance across Java, Python, and Scala for ETL workflows with Apache Iceberg, showing language-dependent performance variations based on data size and operation complexity.


<details>
  <summary>Details</summary>
Motivation: To address the limited comprehensive comparisons of full ETL workflows across programming languages using Apache Iceberg, and to understand how programming language choice affects Apache Spark performance for different data processing scenarios.

Method: Executed comparative analysis by running ETL operations including data download from CSV files, transformation, and loading into Apache Iceberg tables using Apache Spark in Java, Python, and Scala programming languages, testing with different data sizes (5MB and 1.6GB CSV files) and complex operations combining multiple files.

Result: Python performed best with small data (5MB: 6.71s vs Scala 9.13s, Java 9.62s), all languages showed similar performance with large data (1.6GB: Python 46.34s, Scala 47.72s, Java 50.56s), and Scala was fastest for complex operations (374.42s vs Java 379.8s, Python 398.32s).

Conclusion: Programming language significantly impacts Apache Spark efficiency: Python excels with small datasets, while Scala and Java are more productive for large data volumes and complex operations, providing optimization guidance based on specific performance requirements and data processing needs.

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [4] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: The paper studies the locality of matching problems in regular graphs, showing that (1+ε)-approximate matching has locality depending only on ε (logarithmic in 1/ε for sufficiently large Δ), while maximal matching requires dependence on n or Δ. It establishes a strong separation between node-averaged and worst-case complexity for maximal matching, with the former being O(1).


<details>
  <summary>Details</summary>
Motivation: To understand the locality of symmetry-breaking problems in distributed computing, particularly matching problems in regular graphs, which serve as benchmarks for establishing lower bounds and classification results.

Method: Developed randomized algorithms for approximate matching and used novel martingale-based analysis of Luby's algorithm (applied to line graphs of regular graphs) to show that one round produces an almost Δ/2-regular graph.

Result: Showed that (1+ε)-approximate matching in regular graphs has locality depending only on ε (logarithmic in 1/ε when Δ ≥ poly(1/ε)), while maximal matching requires dependence on n or Δ. Established that node-averaged complexity of maximal matching is O(1) vs worst-case complexity.

Conclusion: There is a fundamental difference between approximate and maximal matching in regular graphs regarding locality requirements, with approximate matching being truly local while maximal matching requires global parameters. The node-averaged complexity of maximal matching is significantly better than worst-case complexity.

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [5] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost is a hybrid architecture that efficiently uses preemptible GPU resources for RL training by dynamically offloading rollout workloads, enabling cost-efficient training with 28%-49% cost savings and 1.51x-1.97x throughput improvement.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks struggle with resource tension between rollout (scales efficiently) and training (requires tightly-coupled GPUs), while preemptible GPU resources offer cost-saving opportunities if efficiently utilized for rollout.

Method: RLBoost uses a hybrid architecture with three key techniques: adaptive rollout offload to dynamically adjust workloads, pull-based weight transfer for quick instance provisioning, and token-level response collection/migration for preemption handling and load balancing.

Result: Extensive experiments show RLBoost increases training throughput by 1.51x-1.97x while improving cost efficiency by 28%-49% compared to using only on-demand GPU resources.

Conclusion: RLBoost successfully addresses the resource tension in RL workflows by efficiently harvesting preemptible GPU resources for rollout, achieving significant cost savings and throughput improvements.

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [6] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS is a distributed load-balancing framework that optimizes all-to-all communication in MoE training by leveraging Rail topology symmetry and using local LPT scheduling to achieve near-optimal load balance and significantly reduce completion time.


<details>
  <summary>Details</summary>
Motivation: MoE training suffers from sparse and imbalanced all-to-all communication that dominates iteration time, and conventional load-balancing methods fail to exploit Rail architecture topology, leaving multi-NIC bandwidth underutilized.

Method: RailS leverages Rail topology symmetry to prove uniform sending ensures uniform receiving, transforming global coordination into local scheduling. Each node independently executes LPT spraying scheduler and activates N parallel rails for fine-grained, topology-aware multipath transmission.

Result: RailS improves bus bandwidth by 20%-78%, reduces completion time by 17%-78%, shortens iteration time by 18%-40% for Mixtral workloads, and achieves near-optimal load balance.

Conclusion: RailS fully exploits architectural parallelism in distributed MoE training by providing efficient topology-aware load balancing that significantly improves communication performance and reduces training iteration time.

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [7] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: FLASH Viterbi is a fast, lightweight, adaptive Viterbi decoding operator that improves time and memory efficiency for resource-constrained edge platforms through non-recursive divide-and-conquer, pruning, and parallelization techniques.


<details>
  <summary>Details</summary>
Motivation: Standard Viterbi decoding is memory-intensive and computationally inflexible for edge platforms, while existing methods trade decoding time for space efficiency with significant runtime overhead and lack adaptability to system constraints.

Method: Combines non-recursive divide-and-conquer strategy with pruning and parallelization techniques. Also presents FLASH-BS Viterbi variant with dynamic beam search using memory-efficient data structure to decouple space complexity from hidden state space size.

Result: Extensive experiments show consistent outperformance of existing baselines in both decoding time and memory efficiency, with FPGA-based hardware accelerators demonstrating high throughput and low resource usage.

Conclusion: FLASH Viterbi algorithms preserve adaptability and hardware-friendly characteristics essential for modern data systems, making them well-suited for resource-constrained edge deployments.

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [8] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: HybridEP is a framework that optimizes Expert Parallelism (EP) for Mixture-of-Experts models in cross-data center training by dynamically transforming expert placement to reduce communication overhead under constrained bandwidth.


<details>
  <summary>Details</summary>
Motivation: The rapid scaling of MoE models outpaces single DC training capabilities, requiring cross-DC training. However, EP faces severe scalability issues due to limited cross-DC bandwidth, as existing communication-computation overlap optimizations are ineffective in low-bandwidth scenarios.

Method: HybridEP uses a stream-based model to determine optimal transmission ratios, incorporates domain-based partition to map hybrid patterns to GPU-level communication topology, and employs parameter-efficient migration to reduce expert transmission overhead and enlarge domain size.

Result: HybridEP outperforms state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth, and achieves up to 1.45x speedup with 1k DCs in large-scale simulations across different bandwidths.

Conclusion: HybridEP provides a more general EP approach with better scalability by dynamically optimizing expert placement and communication patterns, effectively addressing the bandwidth constraints in cross-DC MoE model training.

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [9] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: Propius is a novel system for collaborative machine learning that addresses infrastructure challenges through efficient resource management and scalable data handling, outperforming existing frameworks in resource utilization, throughput, and job completion time.


<details>
  <summary>Details</summary>
Motivation: Current collaborative ML systems are often built as specific server-client implementations that lack scalability and reusability. As collaborative ML scales, there's a pressing need for scalable, efficient multi-tenant resource management systems that can handle heterogeneous client machines.

Method: Propius uses a two-plane architecture: a control plane for efficient resource sharing among multiple collaborative ML jobs with various sharing policies, and a data plane for scalable model sharing and result collection. The system adapts to client machine heterogeneity and manages computation flow between ML jobs and edge resources.

Result: Evaluations show Propius outperforms existing resource management techniques and frameworks with improvements in resource utilization (up to 1.88×), throughput (up to 2.76×), and job completion time (up to 1.26×).

Conclusion: Propius provides an effective solution for scalable collaborative ML infrastructure that efficiently manages heterogeneous resources and improves system performance metrics significantly compared to existing approaches.

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [10] [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Srinivas Vippagunta,Suchitra Raman,Shreeshankar Chatterjee,Ju Lin,Shang Liu,Mary Schladenhauffen,Jeffrey Luo,Hailong Jiang*

Main category: cs.DC

TL;DR: A serverless GPU-based BDaaS blueprint using TabNet achieves 4.5x higher throughput, 98x lower latency, and 90% lower cost than Spark baselines while maintaining compliance through interpretability features.


<details>
  <summary>Details</summary>
Motivation: Traditional distributed frameworks like Spark introduce coordination complexity and auditing overheads that are unsuitable for moderate-scale, latency-sensitive inference in regulated environments. Serverless GPUs and interpretable models like TabNet enable new deployment approaches.

Method: Developed a production-oriented BDaaS blueprint integrating single-node serverless GPU runtime with TabNet, leveraging GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for compliance.

Result: GPU pipelines achieved 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines. Compliance mechanisms added only ~5.7 ms latency with p99 < 22 ms, and interpretability remained stable under peak load.

Conclusion: The approach provides a practical, secure, interpretable, and cost-efficient serverless GPU analytics solution for regulated enterprise and government settings, with a reproducible Helm-packaged blueprint and decision framework.

Abstract: Industrial and government organizations increasingly depend on data-driven
analytics for workforce, finance, and regulated decision processes, where
timeliness, cost efficiency, and compliance are critical. Distributed
frameworks such as Spark and Flink remain effective for massive-scale batch or
streaming analytics but introduce coordination complexity and auditing
overheads that misalign with moderate-scale, latency-sensitive inference.
Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet
enable interpretable tabular ML, motivating new deployment blueprints for
regulated environments. In this paper, we present a production-oriented Big
Data as a Service (BDaaS) blueprint that integrates a single-node serverless
GPU runtime with TabNet. The design leverages GPU acceleration for throughput,
serverless elasticity for cost reduction, and feature-mask interpretability for
IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,
comparing our approach against Spark and CPU baselines. Our results show that
GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%
lower cost per 1K inferences compared to Spark baselines, while compliance
mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains
stable under peak load, ensuring reliable auditability. Taken together, these
findings provide a compliance-aware benchmark, a reproducible Helm-packaged
blueprint, and a decision framework that demonstrate the practicality of
secure, interpretable, and cost-efficient serverless GPU analytics for
regulated enterprise and government settings.

</details>


### [11] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: This paper benchmarks Redis alternatives (Valkey, KeyDB, Garnet) in Kubernetes deployments, evaluating performance metrics and long-term viability trade-offs.


<details>
  <summary>Details</summary>
Motivation: In-memory key-value datastores are crucial for cloud-native infrastructures but face scalability, compatibility, and sustainability constraints. The literature lacks experimental evaluation of state-of-the-art tools in this domain.

Method: Systematic benchmarking of Valkey, KeyDB, and Garnet under realistic workloads within Kubernetes deployments, measuring throughput, tail latency, CPU/memory efficiency, and migration complexity.

Result: The results demonstrate clear trade-offs among the benchmarked data systems, with comprehensive performance and viability assessments showing differences in metrics across the evaluated tools.

Conclusion: The study highlights trade-offs between performance, compatibility, and long-term viability including project maturity, community support, and sustained development for emerging in-memory key-value stores.

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [12] [Tidying Up the Address Space](https://arxiv.org/abs/2510.19765)
*Vinay Banakar,Suli Yang,Kan Wu,Andrea C. Arpaci-Dusseau,Remzi H. Arpaci-Dusseau,Kimberly Keeton*

Main category: cs.OS

TL;DR: HADES introduces address-space engineering to solve hotness fragmentation in memory tiering by reorganizing virtual address spaces to create uniformly hot/cold regions, enabling existing page-level tiering systems to reclaim memory more effectively.


<details>
  <summary>Details</summary>
Motivation: Current memory tiering in datacenters suffers from hotness fragmentation - the mixing of hot and cold objects within memory pages, which prevents page-based reclamation systems from accurately identifying and managing truly hot pages, limiting memory efficiency despite skewed access patterns.

Method: HADES uses a compiler-runtime system that implements address-space engineering by dynamically reorganizing application virtual address spaces. It tracks and migrates objects based on access patterns to create uniformly hot and cold regions, requiring minimal developer intervention.

Result: Evaluations across ten data structures show up to 70% memory reduction with only 3% performance overhead, demonstrating that address space engineering enables existing reclamation systems to reclaim memory aggressively without performance degradation.

Conclusion: Address-space engineering effectively solves hotness fragmentation in memory tiering, allowing page-level tiering backends to manage memory more efficiently by creating uniformly hot and cold regions, achieving significant memory savings with minimal performance impact.

Abstract: Memory tiering in datacenters does not achieve its full potential due to
hotness fragmentation -- the intermingling of hot and cold objects within
memory pages. This fragmentation prevents page-based reclamation systems from
distinguishing truly hot pages from pages containing mostly cold objects,
fundamentally limiting memory efficiency despite highly skewed accesses. We
introduce address-space engineering: dynamically reorganizing application
virtual address spaces to create uniformly hot and cold regions that any
page-level tiering backend can manage effectively. HADES demonstrates this
frontend/backend approach through a compiler-runtime system that tracks and
migrates objects based on access patterns, requiring minimal developer
intervention. Evaluations across ten data structures achieve up to 70% memory
reduction with 3% performance overhead, showing that address space engineering
enables existing reclamation systems to reclaim memory aggressively without
performance degradation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration](https://arxiv.org/abs/2510.19577)
*Zuoming Fu,Alex Manley,Mohammad Alian*

Main category: cs.AR

TL;DR: This paper presents gem5 Co-Pilot, an AI agent that uses Large Language Models to automate design space exploration for computer architecture using the gem5 simulator, featuring a GUI interface and database system for efficient parameter optimization.


<details>
  <summary>Details</summary>
Motivation: Computer architecture design space exploration is complex and time-consuming, requiring analysis of numerous parameters and simulation statistics. The emergence of LLMs offers potential to accelerate text analysis and decision-making in this domain.

Method: Built gem5 Co-Pilot with webpage GUI, implemented a design space exploration language and Design Space Database (DSDB), creating a Retrieval Augmented Generation system for gem5 optimization.

Result: The system successfully identified optimal parameters for specific design constraints across four cost ranges, outperforming two baseline models with limited user interaction required.

Conclusion: gem5 Co-Pilot effectively automates design space exploration using LLMs, enabling quick identification of optimal parameters based on performance and cost constraints while reducing user effort.

Abstract: Generative AI is increasing the productivity of software and hardware
development across many application domains. In this work, we utilize the power
of Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5
users with automating design space exploration. Computer architecture design
space exploration is complex and time-consuming, given that numerous parameter
settings and simulation statistics must be analyzed before improving the
current design. The emergence of LLMs has significantly accelerated the
analysis of long-text data as well as smart decision making, two key functions
in a successful design space exploration task. In this project, we first build
gem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI
for smooth user interaction, agent automation, and result summarization. We
also implemented a language for design space exploration, as well as a Design
Space Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a
Retrieval Augmented Generation system for gem5 design space exploration. We
experiment on cost-constraint optimization with four cost ranges and compare
our results with two baseline models. Results show that gem5 Co-Pilot can
quickly identify optimal parameters for specific design constraints based on
performance and cost, with limited user interaction.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [14] [Machine Olfaction and Embedded AI Are Shaping the New Global Sensing Industry](https://arxiv.org/abs/2510.19660)
*Andreas Mershin,Nikolas Stefanou,Adan Rotteveel,Matthew Kung,George Kung,Alexandru Dan,Howard Kivell,Zoia Okulova,Zoi Kountouri,Paul Pu Liang*

Main category: cs.ET

TL;DR: Machine olfaction is emerging as a transformative technology with applications in medical diagnostics, industrial monitoring, agriculture, and security, potentially creating a global chemosensory infrastructure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to survey the scientific foundations, technological frontiers, and strategic applications of machine olfaction, highlighting its potential to create a new industry and global sensing ecosystem.

Method: This is a review and industry roadmap that analyzes recent advances in stabilizing mammalian olfactory receptors and integrating them into biophotonic and bioelectronic systems, along with convergence with multimodal AI and distributed sensor networks.

Result: The technology has enabled detection at near single-molecule resolution, placing machines on par with trained detection dogs, and is poised to create a planet-wide molecular awareness tech layer.

Conclusion: Machine olfaction is positioned to spawn vast emerging markets in health, security, and environmental sensing via scent, though ethical and legal concerns need to be addressed.

Abstract: Machine olfaction is rapidly emerging as a transformative capability, with
applications spanning non-invasive medical diagnostics, industrial monitoring,
agriculture, and security and defense. Recent advances in stabilizing mammalian
olfactory receptors and integrating them into biophotonic and bioelectronic
systems have enabled detection at near single-molecule resolution thus placing
machines on par with trained detection dogs. As this technology converges with
multimodal AI and distributed sensor networks imbued with embedded AI, it
introduces a new, biochemical layer to a sensing ecosystem currently dominated
by machine vision and audition. This review and industry roadmap surveys the
scientific foundations, technological frontiers, and strategic applications of
machine olfaction making the case that we are currently witnessing the rise of
a new industry that brings with it a global chemosensory infrastructure. We
cover exemplary industrial, military and consumer applications and address some
of the ethical and legal concerns arising. We find that machine olfaction is
poised to bring forth a planet-wide molecular awareness tech layer with the
potential of spawning vast emerging markets in health, security, and
environmental sensing via scent.

</details>
