{"id": "2512.04527", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04527", "abs": "https://arxiv.org/abs/2512.04527", "authors": ["Xingyu Liu", "Jiawei Liang", "Linfeng Du", "Yipu Zhang", "Chaofang Ma", "Hanwei Fan", "Jiang Xu", "Wei Zhang"], "title": "FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration", "comment": null, "summary": "In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.", "AI": {"tldr": "FLEX is an FPGA-CPU accelerator for mixed-cell-height legalization that achieves up to 18.3x speedup over CPU-GPU solutions through optimized task partitioning and multi-granularity pipelining.", "motivation": "To accelerate mixed-cell-height legalization tasks in physical design by leveraging FPGA-CPU heterogeneous computing to overcome performance bottlenecks in traditional CPU-GPU approaches.", "method": "1) Optimized task assignment strategy with efficient FPGA-CPU partitioning; 2) Multi-granularity pipelining for the most time-consuming step (FOP); 3) Special optimization of cell shifting process in FOP to align with pipelining framework.", "result": "Achieves up to 18.3x speedup over state-of-the-art CPU-GPU legalizers and 5.4x over multi-threaded CPU solutions, with better scalability and improved legalization quality by 4% and 1% respectively.", "conclusion": "FLEX demonstrates that FPGA-CPU heterogeneous acceleration with optimized task partitioning and multi-granularity pipelining can significantly outperform traditional CPU-GPU approaches for mixed-cell-height legalization tasks."}}
{"id": "2512.04867", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.04867", "abs": "https://arxiv.org/abs/2512.04867", "authors": ["Bychkov Oleksii", "Senysh Taras"], "title": "Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project", "comment": "14 pages", "summary": "This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.", "AI": {"tldr": "Hardware redundancy approach using individual ESP32 microcomputers per neuron to ensure neural network functional stability against hardware failures during operation.", "motivation": "To address the problem of neural network vulnerability to hardware failures during operation, moving beyond training-time regularization methods like Dropout to ensure operational resilience.", "method": "Implement each neuron on a separate ESP32 microcomputer, creating hardware redundancy at the individual neuron level to maintain system functionality even when computational nodes fail.", "result": "The system achieves functional stability and resilience to hardware failures during network operation through distributed hardware redundancy.", "conclusion": "Hardware redundancy at the neuron level provides effective operational resilience for neural networks, offering a practical solution for reliable deployment in hardware-failure-prone environments."}}
{"id": "2512.04910", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04910", "abs": "https://arxiv.org/abs/2512.04910", "authors": ["Fang Li"], "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming", "comment": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)", "summary": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.", "AI": {"tldr": "Automated stripboard circuit layout design using Answer Set Programming (ASP) for synthesis and multi-objective optimization of board area and component strip crossing.", "motivation": "To advance automated stripboard layout design by addressing the complex geometric and electrical constraints in a declarative manner, providing a practical tool for electronics prototyping and education.", "method": "Formulates layout as synthesis and multi-objective optimization using Answer Set Programming (ASP) with two-phase solving: first ensures feasibility, then optimizes layout quality (minimizing board area and component strip crossing).", "result": "Generates compact, manufacturable layouts for a range of circuit complexities, demonstrating the effectiveness of the ASP-based approach.", "conclusion": "This work represents a significant advancement in automated stripboard layout, showcasing the power of declarative programming for solving complex design automation problems in electronics prototyping and education."}}
{"id": "2512.04088", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04088", "abs": "https://arxiv.org/abs/2512.04088", "authors": ["Kolichala Rajashekar", "Nafiseh Sharghivand", "Radu Prodan", "Reza Farahani"], "title": "Toward Sustainability-Aware LLM Inference on Edge Clusters", "comment": "4 pages, 5 figures, 3 tables, conference paper", "summary": "Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.", "AI": {"tldr": "This paper presents sustainability-aware LLM inference for edge clusters using carbon- and latency-aware routing strategies to balance inference latency and carbon footprint.", "motivation": "LLMs require substantial computational resources leading to significant carbon emissions and operational costs, with inference being particularly problematic due to massive global query volume. Cloud-based inference has latency and bandwidth issues, while edge clusters face trade-offs between performance, energy efficiency, and device constraints.", "method": "The authors implement sustainability-aware LLM inference for edge clusters using NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. They develop carbon- and latency-aware routing strategies based on empirical benchmarking of energy consumption and execution time across diverse prompts and batch configurations. They compare baseline greedy strategies to carbon-aware and latency-aware strategies for prompt routing.", "result": "Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput and energy efficiency, while larger batches risk GPU memory saturation. The routing strategies effectively balance inference latency and carbon footprint.", "conclusion": "Sustainability-aware LLM inference on edge clusters with carbon- and latency-aware routing strategies can effectively balance performance and environmental impact, with optimal batch sizes identified to prevent GPU memory saturation while maintaining efficiency."}}
{"id": "2512.04089", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04089", "abs": "https://arxiv.org/abs/2512.04089", "authors": ["Mario Colosi", "Reza Farahani", "Lauri Loven", "Radu Prodan", "Massimo Villari"], "title": "Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud", "comment": "7 pages, 8 Figures, 2 Tables, conference paper", "summary": "WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.", "AI": {"tldr": "This paper evaluates WebAssembly (Wasm) for serverless workflow execution across browser, edge, and cloud environments, analyzing performance factors like compilation methods and resource variability.", "motivation": "WebAssembly enables portable, sandboxed execution across platforms but its performance depends on startup overhead, compilation models (AOT/JIT), and resource variability across deployment contexts. The paper aims to evaluate Wasm-based serverless workflows consistently from browser to edge to cloud.", "method": "The study uses wasm32-wasi modules: browser execution occurs within web workers, while edge and cloud use an HTTP shim to stream frames to Wasm runtime. They measure cold/warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization across environments.", "result": "AOT compilation and instance warming substantially reduce startup latency. For small payloads, browsers achieve competitive performance due to in-memory data exchanges. As payloads grow, workflows become compute/memory-intensive, where AOT execution on edge/cloud nodes distinctly surpasses browser performance.", "conclusion": "Wasm-based serverless workflows show environment-dependent performance characteristics: browsers excel with small payloads using in-memory exchanges, while edge/cloud nodes with AOT compilation outperform for compute-intensive workloads with larger payloads."}}
{"id": "2512.04355", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.04355", "abs": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "comment": "13 pages, 6 figures, MLSys 2026 Submission", "summary": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "AI": {"tldr": "gpuFLOPBench is a benchmark for evaluating LLMs' ability to predict FLOP counts for CUDA kernels without execution, revealing current models struggle with implicit FLOPs from division, math functions, and compiler optimizations.", "motivation": "Current LLMs lack testing on forward-looking performance reasoning needed for GPU development, where predicting FLOP counts before kernel execution is crucial for tuning, scheduling, and hardware decisions.", "method": "Created gpuFLOPBench with 577 CUDA kernels from HeCBench, annotated with ground-truth profiles and eight execution attributes to distinguish analyzable code from kernels with hidden compiler/runtime FLOP dependencies.", "result": "Latest LLMs achieve perfect classification on straightforward kernels but make order-of-magnitude errors when implicit FLOPs arise from division, intrinsic math functions, or common subexpressions.", "conclusion": "Reveals core limitation in existing code assistants: inability to internalize hardware-specific microcode effects; positions gpuFLOPBench as testbed for developing LLM tooling that can reason about performance like experienced GPU developers."}}
{"id": "2512.04320", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.04320", "abs": "https://arxiv.org/abs/2512.04320", "authors": ["Yineng Yan", "William Ruys", "Hochan Lee", "Ian Henriksen", "Arthur Peters", "Sean Stephens", "Bozhi You", "Henrique Fingler", "Martin Burtscher", "Milos Gligoric", "Keshav Pingali", "Mattan Erez", "George Biros", "Christopher J. Rossbach"], "title": "VLCs: Managing Parallelism with Virtualized Libraries", "comment": "Research Paper accepted to the ACM Symposium on Cloud Computing (SoCC'25)", "summary": "As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.\n  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.\n  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.", "AI": {"tldr": "Virtual Library Contexts (VLCs) enable safe composition of parallel libraries by creating process subunits that encapsulate libraries and their resource allocations, preventing contention without modifying library code.", "motivation": "Modern parallel machines require composition of software libraries for parallelism, but many libraries assume exclusive resource access, leading to contention and performance degradation when used concurrently. Existing solutions require library or OS modifications, which are often impractical.", "method": "Propose Virtual Library Contexts (VLCs) - process subunits that encapsulate sets of libraries and their resource allocations. VLCs control library resource utilization without code modifications, enabling resource partitioning between libraries and loading multiple copies of thread-unsafe libraries within the same process.", "result": "C++ and Python prototypes of VLCs demonstrate speedups up to 2.85x on benchmarks using OpenMP, OpenBLAS, and LibTorch, showing effective prevention of library contention.", "conclusion": "VLCs provide a practical solution for safe composition of parallel libraries without requiring library modifications, enabling better resource management and performance in complex parallel computing environments."}}
{"id": "2512.04093", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04093", "abs": "https://arxiv.org/abs/2512.04093", "authors": ["Ali Akbar Vali", "Sadoon Azizi", "Mohammad Shojafar", "Rajkumar Buyya"], "title": "Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions", "comment": "46 pages", "summary": "The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.", "AI": {"tldr": "A comprehensive survey of energy-efficient resource management strategies in microservices-based fog and edge computing, covering 136+ studies from 2020-2024 across five key subdomains.", "motivation": "The exponential growth of IoT devices demands efficient, responsive services, but fog/edge computing faces resource management challenges due to constraints, heterogeneity, dynamic workloads, and diverse QoS requirements.", "method": "Systematic review and classification of 136+ studies (2020-2024) into five subdomains: service placement, resource provisioning, task scheduling/offloading, resource allocation, and instance selection, based on optimization techniques, objectives, and strengths/limitations.", "result": "Identified gaps in literature, lack of synergy among resource management components, and outlined promising research directions including AI-driven optimization, quantum computing, and serverless computing.", "conclusion": "Provides a comprehensive reference with unified, energy-aware perspective on resource management in microservices-based fog/edge computing, paving way for more integrated, efficient, and sustainable future solutions."}}
{"id": "2512.04096", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04096", "abs": "https://arxiv.org/abs/2512.04096", "authors": ["Sushant Kumar Gupta", "Anil Raghunath Iyer", "Chang Yu", "Neel Bagora", "Olivier Pomerleau", "Vivek Kumar", "Prunthaban Kanthakumar"], "title": "Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale", "comment": null, "summary": "Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.\n  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.", "AI": {"tldr": "Fast ACS is a file-based ordered message delivery system using RPC and RMA for low-latency global message delivery to thousands of consumers with ordering and at-least-once guarantees.", "motivation": "Real-time systems need low-latency message delivery across global clusters with thousands of consumers, requiring robust messaging with ordering guarantees and consumer-friendly pacing.", "method": "File-based ordered message delivery system combining two-sided (inter-cluster RPC) and one-sided (intra-cluster RMA) communication primitives for efficient message transmission.", "result": "Successfully deployed to dozens of production clusters, scales to thousands of consumers per cluster with Tbps-scale intra-cluster traffic, delivers messages globally within seconds/sub-seconds (p99) at low resource cost.", "conclusion": "Fast ACS provides an effective solution for scalable, low-latency ordered message delivery across global clusters with robust guarantees and efficient resource utilization."}}
{"id": "2512.04226", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04226", "abs": "https://arxiv.org/abs/2512.04226", "authors": ["Ryan Swann", "Muhammad Osama", "Xiaohu Guo", "Bryant Nelson", "Lixun Zhang", "Alex Brown", "Yen Ong", "Ali Yazdani", "Sean Siddens", "Ganesh Dasika", "Alex Underwood"], "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection", "comment": null, "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.", "AI": {"tldr": "tritonBLAS is a deterministic analytical model that generates high-performance GPU GEMM kernels without runtime autotuning by modeling architectural parameters, achieving over 95% of autotuned performance while eliminating tuning time.", "motivation": "To eliminate the significant time overhead of runtime autotuning for GPU GEMM kernels while maintaining near-optimal performance, making it practical for production HPC and ML workloads.", "method": "Developed a deterministic analytical model that uses architectural parameters (cache hierarchy, code/data placement) to model relationships between GPU topology, matrix shapes, and algorithmic blocking behavior, then implemented a lightweight GEMM framework within Triton.", "result": "tritonBLAS achieves over 95% of the performance of autotuning solutions across diverse GEMM problem sizes on modern GPUs, while reducing autotuning time to zero.", "conclusion": "tritonBLAS serves as a practical drop-in replacement for empirical tuning in production environments, offering near-optimal performance without the time cost of autotuning."}}
{"id": "2512.04291", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04291", "abs": "https://arxiv.org/abs/2512.04291", "authors": ["Huda Ibeid", "Anthony-Trung Nguyen", "Aditya Nishtala", "Premanand Sakarda", "Larry Kaplan", "Nilakantan Mahadevan", "Michael Woodacre", "Victor Anisimov", "Kalyan Kumaran", "JaeHyuk Kwack", "Vitali Morozov", "Servesh Muralidharan", "Scott Parker"], "title": "Scaling MPI Applications on Aurora", "comment": null, "summary": "The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.", "AI": {"tldr": "The Aurora supercomputer at Argonne National Laboratory is an Exascale system with Intel Max Series GPUs/CPUs and HPE Slingshot fabric, achieving top performance on HPC benchmarks and enabling breakthrough scientific applications.", "motivation": "To document the design and validation of the Aurora supercomputer, one of the world's three Exascale systems, focusing on its network fabric and demonstrating its capability for AI and HPC simulations in open science.", "method": "The paper details Aurora's system architecture with Intel Max Series components and HPE Slingshot fabric in dragonfly topology, then validates performance through MPI benchmarks (HPL, HPL-MxP, Graph500, HPCG) and diverse scientific applications (HACC, AMR-Wind, LAMMPS, FMM).", "result": "Aurora achieved second fastest on Top500 (June 2024) and fastest on HPL MxP benchmark, demonstrating exceptional throughput, latency, and bandwidth across large node counts for scientific applications.", "conclusion": "Aurora's combination of Intel hardware and Slingshot fabric enables breakthrough scientific capabilities, making it one of the most powerful AI/HPC systems for open science with validated performance across diverse benchmarks and applications."}}
{"id": "2512.04389", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04389", "abs": "https://arxiv.org/abs/2512.04389", "authors": ["Zhen Hu", "Dongliang Xiong", "Kai Huang", "Changjun Wu", "Xiaowen Jiang"], "title": "A Structure-Aware Irregular Blocking Method for Sparse LU Factorization", "comment": null, "summary": "In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.", "AI": {"tldr": "Proposes structure-aware irregular blocking for sparse LU factorization to address workload imbalance from non-uniform nonzero distribution, achieving significant speedups over existing methods.", "motivation": "Sparse LU factorization has non-uniform nonzero distribution (diagonal and right-bottom regions), causing workload imbalance with regular 2D blocking. Existing matrix features fail to effectively guide blocking strategies.", "method": "Introduces diagonal block-based feature to characterize local nonzero distribution, then proposes irregular blocking method that adjusts block sizes according to local distribution - fine-grained blocks in dense regions and coarse-grained blocks in sparse regions.", "result": "On single NVIDIA A100 GPU: 1.50x speedup over PanguLU and 3.32x over SuperLU_DIST. On 4 NVIDIA A100 GPUs: 1.40x speedup over PanguLU and 3.84x over SuperLU_DIST.", "conclusion": "The structure-aware irregular blocking method effectively balances workload across blocks by adapting to local nonzero distribution, achieving significant performance improvements over state-of-the-art sparse LU factorization methods."}}
{"id": "2512.04449", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04449", "abs": "https://arxiv.org/abs/2512.04449", "authors": ["Suyeon Lee", "Kangkyu Park", "Kwangsik Shin", "Ada Gavrilovska"], "title": "Offloading to CXL-based Computational Memory", "comment": null, "summary": "CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.", "AI": {"tldr": "KAI system introduces Asynchronous Back-Streaming protocol for CXL-based Computational Memory, reducing end-to-end runtime by up to 50.4% and improving idle times significantly.", "motivation": "CXL-based Computational Memory enables near-memory processing but existing operation offloading mechanisms cannot leverage trade-offs of different CXL protocol models, limiting performance optimization for diverse workloads.", "method": "Proposes Asynchronous Back-Streaming protocol by layering data and control transfer operations on top of underlying CXL protocols, and designs KAI system that implements this model with asynchronous data movement and lightweight pipelining in host-CCM interactions.", "result": "KAI reduces end-to-end runtime by up to 50.4%, and reduces CCM and host idle times by average 22.11x and 3.85x, respectively.", "conclusion": "The Asynchronous Back-Streaming protocol and KAI system effectively address data movement costs in disaggregated memory systems by leveraging CXL protocol trade-offs, significantly improving performance and system efficiency for diverse workloads."}}
{"id": "2512.04984", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04984", "abs": "https://arxiv.org/abs/2512.04984", "authors": ["O. Tansel Baydas", "Ozgur B. Akan"], "title": "Federated Learning for Terahertz Wireless Communication", "comment": "10 pages, 4 figures", "summary": "The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.", "AI": {"tldr": "THz-FL systems face convergence degradation due to wideband impairments like beam squint and spectral holes, requiring SNR-weighted aggregation to overcome harmonic mean SNR limitations.", "motivation": "The convergence of Terahertz communications and Federated Learning promises ultra-fast distributed learning, but the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized.", "method": "Developed a multicarrier stochastic framework that couples local gradient updates with frequency-selective THz effects (beam squint, molecular absorption, jitter), analyzed convergence error floor, and proposed SNR-weighted aggregation strategy.", "result": "Uncovered a critical diversity trap where convergence error floor is driven by harmonic mean of subcarrier SNRs, identified fundamental bandwidth limits, and demonstrated that SNR-weighted aggregation suppresses variance singularity at spectral holes.", "conclusion": "Standard unbiased aggregation fails in high-squint THz-FL regimes due to spectral holes, but SNR-weighted aggregation recovers convergence by addressing the harmonic mean SNR limitation and variance singularity issues."}}
