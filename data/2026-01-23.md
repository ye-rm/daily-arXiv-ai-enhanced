<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10](https://arxiv.org/abs/2601.16032)
*Yifan Zhu,Yekai Pan,Chen Ding*

Main category: cs.PF

TL;DR: This paper analyzes CuTile-based Flash Attention memory behavior, identifies L2 cache miss causes on NVIDIA GB10, and introduces Sawtooth Wavefront Reordering to improve cache performance, achieving 50%+ L2 miss reduction and up to 60% throughput increase.


<details>
  <summary>Details</summary>
Motivation: High-performance attention kernels are critical for Large Language Models, and understanding memory behavior in CuTile-based Flash Attention implementations is essential for optimizing cache performance on modern hardware like NVIDIA's Grace Blackwell architecture.

Method: The paper conducts analysis of CuTile-based Flash Attention memory behavior on NVIDIA GB10 to identify main causes of L2 cache misses. Based on this analysis, the authors introduce a new programming technique called Sawtooth Wavefront Reordering designed to reduce L2 misses.

Result: The Sawtooth Wavefront Reordering technique achieves 50% or greater reduction in L2 cache misses and up to 60% increase in throughput on NVIDIA GB10 (Grace Blackwell) hardware, validated in both CUDA and CuTile implementations.

Conclusion: The proposed Sawtooth Wavefront Reordering technique effectively addresses L2 cache miss issues in Flash Attention implementations, significantly improving cache performance and throughput on modern GPU architectures like NVIDIA's Grace Blackwell.

Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to 60\% increase in throughput on GB10.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [2] [Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems](https://arxiv.org/abs/2601.15561)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.ET

TL;DR: The paper analyzes limitations of simulated annealing with probabilistic bits (pSA) for large-scale combinatorial optimization, identifies oscillation issues causing energy stagnation, and proposes two improved algorithms (TApSA and SpSA) that show significant performance gains on maximum cut benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of pSA in solving large-scale combinatorial optimization problems, particularly the issue of unexpected oscillations among p-bits that hinder energy reduction in the Ising model and prevent successful execution in complex tasks.

Method: The researchers conducted detailed simulations to analyze pSA processes, identified the feedback mechanism as the root cause of disruptive oscillations, and proposed two novel algorithms: time average pSA (TApSA) and stalled pSA (SpSA) based on partial deactivation of p-bits. They tested these using Python simulations on maximum cut benchmarks.

Result: On 16 benchmarks ranging from 800 to 5,000 nodes, the proposed methods improved normalized cut values from 0.8% to 98.4% on average compared to conventional pSA, demonstrating significant performance enhancements.

Conclusion: The study successfully identifies and addresses the oscillation problem in pSA through novel algorithmic modifications, showing that partial deactivation strategies in TApSA and SpSA can substantially improve performance on large-scale combinatorial optimization problems.

Abstract: This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.

</details>


### [3] [Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload](https://arxiv.org/abs/2601.16169)
*Robert Walkup,Juha Jäykkä,Igor Pasichnyk,Zachary Streeter,Kasia Świrydowicz,Mikko Tukiainen,Yasuko Eckert,Luke Bertels,Daniel Claudino,Peter Groszkowski,Travis S. Humble,Constantinos Evangelinos,Javier Robledo-Moreno,William Kirby,Antonio Mezzacapo,Antonio Córcoles,Seetharami Seelam*

Main category: cs.ET

TL;DR: Hybrid quantum-HPC algorithm uses GPUs to dramatically accelerate diagonalization step in sample-based quantum diagonalization, achieving 100x speedup per node.


<details>
  <summary>Details</summary>
Motivation: Diagonalization is the most computationally demanding task in hybrid quantum-HPC algorithms like SQD, and previous CPU-based approaches need acceleration for practical applications.

Method: Port diagonalization to heterogeneous GPU systems using Davidson algorithm with selected electron configurations, implementing offload strategies, code transformations, and optimized data movement.

Result: GPU acceleration provides ~100x performance boost per node, reducing classical processing time from hours to minutes on systems like Frontier supercomputer.

Conclusion: GPU acceleration enables efficient, scalable, and portable diagonalization for hybrid quantum-HPC algorithms, dramatically expediting ground and excited state energy calculations.

Abstract: Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.
  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: An open-source, multi-tenant platform enables small businesses to deploy customized LLM chatbots via no-code workflows using distributed k3s clusters, with built-in security against prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: LLM-based question-answering systems have potential for small business automation but face deployment challenges including infrastructure costs, engineering complexity, and security risks, especially in RAG-based settings.

Method: Developed an open-source platform using distributed, lightweight k3s clusters on heterogeneous low-cost machines with encrypted overlay networks, featuring container-based isolation, per-tenant data controls, and platform-level defenses against prompt injection attacks without requiring model retraining.

Result: Successfully deployed in a real-world e-commerce setting, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic small business constraints for cost, operations, and security.

Conclusion: The platform provides a practical solution for small businesses to deploy customized LLM chatbots with enterprise-grade security and efficiency using cost-effective infrastructure and no-code workflows.

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [5] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: Three techniques to improve particle FRNN physics simulations on RT Cores: BVH update/rebuild optimizer, neighbor-list-free RT core variants, and periodic boundary condition support.


<details>
  <summary>Details</summary>
Motivation: To further enhance the performance and efficiency of particle-based Fixed-Radius Nearest Neighbor (FRNN) physics simulations running on RT Cores by addressing limitations in BVH management, neighbor list requirements, and periodic boundary condition support.

Method: Three main techniques: 1) Real-time update/rebuild ratio optimizer for BVH structure that adapts to simulation dynamics; 2) Two RT core variants that eliminate neighbor lists; 3) Technique enabling RT cores for FRNN with periodic boundary conditions.

Result: BVH optimizer achieves up to ~3.4× faster RT core pipeline; neighbor-list-free variants improve speedup from ~1.3× to ~2.0× for different radius distributions; periodic BC technique works without performance penalty; methods scale across GPU generations.

Conclusion: The proposed techniques significantly improve RT core performance for FRNN simulations, enable previously impossible cases (like log-normal radius clusters), and identify scenarios where traditional GPU computation remains preferable, advancing understanding of RT core capabilities.

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM is a composable HLS library for rapid development of domain-specific LLM accelerators, enabling hybrid designs with stage-customized inference and comprehensive quantization support.


<details>
  <summary>Details</summary>
Motivation: To bridge algorithmic innovation in LLM inference with high-performance accelerators while minimizing manual effort, enabling rapid development of specialized LLM hardware.

Method: FlexLLM provides a composable HLS library exposing architectural degrees of freedom for stage-customized inference, supporting hybrid designs with different temporal reuse and spatial dataflow for prefill/decode stages, plus a comprehensive quantization suite.

Result: Built Llama-3.2 1B inference system in under 2 months with 1K lines of code; achieved 1.29× speedup, 1.64× higher decode throughput, and 3.14× better energy efficiency than A100 GPU on U280 FPGA; with HMT plugin reduced prefill latency by 23.23× and extended context window by 64×.

Conclusion: FlexLLM successfully bridges algorithmic LLM innovations with high-performance accelerators, demonstrating significant performance improvements over GPU baselines while enabling rapid development of specialized hardware.

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [7] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: Hypergraph-based mapping of SNNs to neuromorphic hardware improves partitioning and placement by capturing spike replication within cores, reducing communication traffic and resource usage beyond traditional graph methods.


<details>
  <summary>Details</summary>
Motivation: Mapping SNNs to neuromorphic hardware is challenging due to NP-hard partitioning and placement problems that become increasingly difficult as SNNs and hardware scale to billions of neurons. Current graph-based approaches fail to adequately capture spike replication within cores.

Method: Proposes raising SNN abstraction from graphs to hypergraphs to better model spike replication via hyperedge co-membership. Develops new mapping algorithms that exploit hyperedge overlap and locality properties, grouping neurons through shared hyperedges to reduce communication and resource usage.

Result: Hypergraph-based techniques achieve better mappings than state-of-the-art across various execution time regimes, with improved reduction of communication traffic and hardware resource usage beyond what individual connection contraction attains.

Conclusion: Hypergraph abstraction provides a more faithful model for SNN mapping to neuromorphic hardware, enabling more effective partitioning and placement algorithms that scale better with increasing SNN and hardware sizes.

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>
