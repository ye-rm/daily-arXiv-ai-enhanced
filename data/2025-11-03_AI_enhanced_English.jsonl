{"id": "2510.27583", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.27583", "abs": "https://arxiv.org/abs/2510.27583", "authors": ["Chandrish Ambati", "Trung Diep"], "title": "AMD MI300X GPU Performance Analysis", "comment": null, "summary": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication.", "AI": {"tldr": "Comprehensive evaluation of AMD MI300X GPUs for large language model inference, comparing performance across compute throughput, memory bandwidth, and interconnect communication against traditional NVIDIA dominance.", "motivation": "The rapid growth of large language models (LLMs) with hundreds of billions of parameters requires high-performance, scalable GPU hardware. While NVIDIA GPUs have traditionally dominated LLM deployments due to their mature CUDA software stack, AMD's latest MI300X GPUs offer a compelling alternative with high HBM capacity, matrix cores, and proprietary interconnect.", "method": "The paper presents a comprehensive evaluation of AMD MI300X GPUs across key performance domains critical to LLM inference, including compute throughput, memory bandwidth, and interconnect communication.", "result": "The evaluation assesses how AMD MI300X GPUs perform in critical areas for LLM inference, though specific performance metrics and comparisons with NVIDIA GPUs are not detailed in the abstract.", "conclusion": "AMD MI300X GPUs present a viable alternative to NVIDIA GPUs for LLM deployments, offering competitive features like high HBM capacity and matrix cores that could potentially challenge NVIDIA's traditional dominance in this space."}}
{"id": "2510.26944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.26944", "abs": "https://arxiv.org/abs/2510.26944", "authors": ["Hoa Nguyen", "Pongstorn Maidee", "Jason Lowe-Power", "Alireza Kaviani"], "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies", "comment": null, "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.", "AI": {"tldr": "Choreographer is a simulation framework for holistic system-level evaluation of fine-grained accelerators for latency-sensitive tasks, capturing hardware/software overheads and providing accurate cache modeling.", "motivation": "Existing frameworks fail to capture all hardware and software overheads in core-accelerator and cache-accelerator interactions, and lack detailed cache modeling for accurate performance insights.", "method": "Integrates gem5-based hardware stack with AMBA CHI mesh network and Linux-based software stack, offers C++ API and modular configuration for rapid prototyping, includes detailed cache model.", "result": "Case studies show: data-aware prefetcher achieves 1.08x-1.88x speedups by reducing memory latency; quicksort accelerator delivers >2x speedup with minimal address translation overhead.", "conclusion": "Choreographer effectively models complex hardware-software interactions and optimizes performance in small task offloading scenarios."}}
{"id": "2510.26985", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26985", "abs": "https://arxiv.org/abs/2510.26985", "authors": ["Mostafa Darvishi"], "title": "Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies", "comment": "5 figures, 3 tables", "summary": "This paper presents an in-depth analysis of timing closure challenges and\nconstraints in Field Programmable Gate Arrays (FPGAs) and Application Specific\nIntegrated Circuits (ASICs). We examine core timing principles, architectural\ndistinctions, and design methodologies influencing timing behavior in both\ntechnologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA\n(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance\ntrade-offs. Experimental results show ASICs achieve superior timing of 45ps\nsetup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and\n120ps hold times, validating their suitability for high-performance designs.", "AI": {"tldr": "Analysis of timing closure challenges in FPGAs vs ASICs, showing ASICs achieve superior timing (45ps setup/35ps hold) while modern FPGAs remain competitive (180ps setup/120ps hold) for high-performance designs.", "motivation": "To examine timing closure challenges and constraints in FPGAs and ASICs, understanding their architectural distinctions and design methodologies that influence timing behavior.", "method": "In-depth analysis of core timing principles, architectural distinctions, and design methodologies, with a case study comparing Xilinx Kintex UltraScale+ FPGA (XCKU040) with a 7nm ASIC.", "result": "ASICs achieve superior timing with 45ps setup and 35ps hold times, while modern FPGAs remain competitive with 180ps setup and 120ps hold times.", "conclusion": "Modern FPGAs validate their suitability for high-performance designs despite ASICs having superior timing performance."}}
{"id": "2510.26913", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26913", "abs": "https://arxiv.org/abs/2510.26913", "authors": ["Junyi Shen", "Noppanat Wadlom", "Lingfeng Zhou", "Dequan Wang", "Xu Miao", "Lei Fang", "Yao Lu"], "title": "FlowMesh: A Service Fabric for Composable LLM Workflows", "comment": null, "summary": "AI deployment increasingly resembles a pipeline of data transformation,\nfine-tuning, and agent interactions rather than a monolithic LLM job; recent\nexamples include RLHF/RLAIF training and agentic workflows. To cope with this\nshift, we propose FlowMesh, a multi-tenant service fabric that executes and\noptimizes these workloads as one shared service instead of isolated pipelines.\nIt decomposes workflows into fine-grained operators with recorded lineage,\nenabling de-duplication of work across users and batching requests on the same\nhardware while preserving per-workflow provenance. A global control plane\nmaintains a cluster-wide pool of ready operators and uses a single utility\nfunction to pick both the batch and the worker, balancing throughput, cost, and\ndata locality on heterogeneous GPUs. The data plane is an elastic fleet of\nstateless workers backed by a content-addressable store, enabling rapid,\nautomatic scale-out, safe retry after preemption, and portability across\nmanaged clusters such as Kubernetes and geo-distributed GPU marketplaces such\nas Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost\nreduction and 2.0x lower energy usage, provides a similar or better latency\nprofile, and remains efficient under dynamic and failure-prone conditions.", "AI": {"tldr": "FlowMesh is a multi-tenant service fabric that optimizes AI deployment pipelines by decomposing workflows into fine-grained operators, enabling work deduplication and batching across users while preserving workflow provenance.", "motivation": "AI deployment is shifting from monolithic LLM jobs to complex pipelines involving data transformation, fine-tuning, and agent interactions, requiring more efficient execution and optimization of these workloads as shared services rather than isolated pipelines.", "method": "FlowMesh decomposes workflows into fine-grained operators with recorded lineage, uses a global control plane with a single utility function for batch and worker selection, and employs an elastic fleet of stateless workers backed by a content-addressable store.", "result": "FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy usage compared to baseline solutions, provides similar or better latency profile, and remains efficient under dynamic and failure-prone conditions.", "conclusion": "FlowMesh effectively addresses the shift in AI deployment towards pipeline-based workflows by providing a multi-tenant service fabric that optimizes resource utilization while maintaining workflow provenance and efficiency across heterogeneous GPU environments."}}
{"id": "2510.27095", "categories": ["cs.ET", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.27095", "abs": "https://arxiv.org/abs/2510.27095", "authors": ["Shubham Jadhav", "Kaustav Roy", "Luis Amaro", "Thejas Basavarajappa", "Madhav Ramesh", "Debdeep Jena", "Huili", "Xing", "Amit Lal"], "title": "Lorentzian Switching Dynamics in HZO-based FeMEMS Synapses for Neuromorphic Weight Storage", "comment": "19 pages, 5 figures", "summary": "Neuromorphic computing demands synaptic elements that can store and update\nweights with high precision while being read non-destructively. Conventional\nferroelectric synapses store weights in remnant polarization states and might\nrequire destructive electrical readout, limiting endurance and reliability. We\ndemonstrate a ferroelectric MEMS (FeMEMS) based synapse in which analog weights\nare stored in the piezoelectric coefficient $d_{31,eff}$ of a released\nHf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) MEMS unimorph. Partial switching of\nferroelectric domains modulates $d_{31,eff}$, and a low-amplitude mechanical\ndrive reads out the weight without read-disturb in the device yielding more\nthan 7-bit of programming levels. The mechanical switching distribution\nfunction follows a Lorentzian distribution as a logarithmic function of partial\npoling voltage ($V_p$) consistent with nucleation-limited switching (NLS), and\nthe median threshold extracted from electromechanical data obeys a Merz-type\nfield-time law with a dimensionless exponent $\\alpha = 3.62$. These\nrelationships establish a quantitative link between mechanical weights and\nelectrical switching kinetics. This mechanically read synapse avoids\ndepolarization and charge-injection effects, provides bipolar weights (well\nsuited for excitatory and inhibitory synapses), directly reveals partial domain\npopulations, and offers a robust, energy-efficient route toward high-bit\nneuromorphic hardware.", "AI": {"tldr": "A ferroelectric MEMS (FeMEMS) synapse that stores analog weights in the piezoelectric coefficient of HZO MEMS unimorph, enabling non-destructive mechanical readout with over 7-bit precision and bipolar weight programming.", "motivation": "Conventional ferroelectric synapses require destructive electrical readout, limiting endurance and reliability. This work aims to develop a synapse with non-destructive readout capability.", "method": "Using a released Hf0.5Zr0.5O2 (HZO) MEMS unimorph where partial switching of ferroelectric domains modulates the piezoelectric coefficient d31,eff, and low-amplitude mechanical drive reads weights without read-disturb.", "result": "Achieved more than 7-bit programming levels, mechanical switching follows Lorentzian distribution consistent with nucleation-limited switching, and established quantitative link between mechanical weights and electrical switching kinetics.", "conclusion": "The mechanically read synapse avoids depolarization and charge-injection effects, provides bipolar weights, directly reveals partial domain populations, and offers robust, energy-efficient route for high-bit neuromorphic hardware."}}
{"id": "2510.27070", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27070", "abs": "https://arxiv.org/abs/2510.27070", "authors": ["Dong Tong"], "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review", "comment": null, "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.", "AI": {"tldr": "This paper surveys descriptor-based, object-aware memory systems that bridge the semantic gap between hardware and software by propagating program semantics like object identity, bounds, and lifetime.", "motivation": "Modern computing systems lack native architectural mechanisms to propagate high-level program semantics across hardware/software interfaces, undermining security and efficiency.", "method": "The paper presents a comprehensive survey establishing foundational concepts of memory objects and descriptors, introduces a taxonomy of descriptor addressing modes, and provides unified analysis of implementations.", "result": "The survey reveals how descriptor-based memory systems holistically address memory protection, management, and processing challenges, with CentroID model demonstrating practical object-aware design implementation.", "conclusion": "Explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and 128-bit architectures."}}
{"id": "2510.27039", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27039", "abs": "https://arxiv.org/abs/2510.27039", "authors": ["Zhuo Zheng", "Lingran Meng", "Ziyu Lin"], "title": "A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration", "comment": null, "summary": "Accurate traffic flow forecasting is essential for the development of\nintelligent transportation systems (ITS), supporting tasks such as traffic\nsignal optimization, congestion management, and route planning. Traditional\nmodels often fail to effectively capture complex spatial-temporal dependencies\nin large-scale road networks, especially under the influence of external\nfactors such as weather, holidays, and traffic accidents. To address this\nchallenge, this paper proposes a cloud-based hybrid model that integrates\nSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture\nfor traffic flow prediction. The model leverages the strengths of GNNs in\nmodeling spatial correlations across road networks and the Transformers'\nability to capture long-term temporal dependencies. External contextual\nfeatures are incorporated via feature fusion to enhance predictive accuracy.\nThe proposed model is deployed on a cloud computing platform to achieve\nscalability and real-time adaptability. Experimental evaluation of the dataset\nshows that our model outperforms baseline methods (LSTM, TCN, GCN, pure\nTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings\nsuggest that the hybrid GNN-Transformer approach provides an effective and\nscalable solution for cloud-based ITS applications, offering methodological\nadvancements for traffic flow forecasting and practical implications for\ncongestion mitigation.", "AI": {"tldr": "A cloud-based hybrid model combining Spatio-Temporal Graph Neural Networks (ST-GNN) with Transformer architecture for traffic flow prediction, achieving superior performance over baseline methods.", "motivation": "Traditional models fail to capture complex spatial-temporal dependencies in large-scale road networks, especially under external factors like weather, holidays, and traffic accidents.", "method": "Integrates ST-GNN for spatial correlations across road networks with Transformer for long-term temporal dependencies, incorporating external contextual features via feature fusion, deployed on cloud computing platform for scalability.", "result": "Outperforms baseline methods (LSTM, TCN, GCN, pure Transformer) with RMSE of 17.92 and MAE of 10.53.", "conclusion": "Hybrid GNN-Transformer approach provides effective and scalable solution for cloud-based ITS applications, offering methodological advancements for traffic flow forecasting and practical implications for congestion mitigation."}}
{"id": "2510.27257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27257", "abs": "https://arxiv.org/abs/2510.27257", "authors": ["Mengshi Qi", "Jiaxuan Peng", "Jie Zhang", "Juan Zhu", "Yong Li", "Huadong Ma"], "title": "Synergistic Tensor and Pipeline Parallelism", "comment": null, "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP.", "AI": {"tldr": "A synergistic tensor and pipeline parallelism schedule that reduces both TP communication overhead and PP pipeline bubbles by decoupling forward/backward passes into fine-grained units and braiding them into composite sequences.", "motivation": "Hybrid model parallelism combining TP and PP has become dominant for large model training, but TP introduces communication overhead while PP suffers from pipeline bubbles. Existing solutions address these issues separately, lacking a synergistic approach.", "method": "Decouples forward and backward passes in PP into fine-grained computation units, then braids them to form composite computation sequences. This structure enables near-complete elimination of TP-related bubbles, with further PP scheduling optimization to minimize pipeline bubbles.", "result": "Experimental results show 12% training throughput improvement for LLMs and 16% for MLLMs compared to existing scheduling methods.", "conclusion": "The proposed synergistic schedule effectively reduces both TP and PP inefficiencies simultaneously, achieving significant performance gains in distributed training of large models."}}
{"id": "2510.27289", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27289", "abs": "https://arxiv.org/abs/2510.27289", "authors": ["Zhengchang Hua", "Panagiotis Oikonomou", "Karim Djemame", "Nikos Tziritas", "Georgios Theodoropoulos"], "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination", "comment": "16 pages, 8 figures. Accepted by the 25th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP'25)", "summary": "The coordination of large-scale, decentralised systems, such as a fleet of\nElectric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a\nsignificant challenge for modern control systems. While collaborative Digital\nTwins have been proposed as a solution to manage such systems without\ncompromising the privacy of individual agents, deriving globally optimal\ncontrol policies from the high-level information they share remains an open\nproblem. This paper introduces Digital Twin Assisted Multi-Agent Deep\nDeterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid\narchitecture that integrates a multi-agent reinforcement learning framework\nwith a collaborative DT network. Our core contribution is a simulation-assisted\nlearning algorithm where the centralised critic is enhanced by a predictive\nglobal model that is collaboratively built from the privacy-preserving data\nshared by individual DTs. This approach removes the need for collecting\nsensitive raw data at a centralised entity, a requirement of traditional\nmulti-agent learning algorithms. Experimental results in a simulated V2G\nenvironment demonstrate that DT-MADDPG can achieve coordination performance\ncomparable to the standard MADDPG algorithm while offering significant\nadvantages in terms of data privacy and architectural decentralisation. This\nwork presents a practical and robust framework for deploying intelligent,\nlearning-based coordination in complex, real-world cyber-physical systems.", "AI": {"tldr": "DT-MADDPG algorithm integrates multi-agent reinforcement learning with collaborative digital twins to achieve privacy-preserving coordination in V2G networks without centralized raw data collection.", "motivation": "To address the challenge of coordinating large-scale decentralized systems like EV fleets in V2G networks while preserving individual agent privacy, as traditional methods require centralized sensitive data.", "method": "A hybrid architecture combining multi-agent reinforcement learning (MADDPG) with collaborative digital twin network, using simulation-assisted learning where centralized critic is enhanced by predictive global model built from privacy-preserving DT data.", "result": "Experimental results show DT-MADDPG achieves coordination performance comparable to standard MADDPG while offering significant advantages in data privacy and architectural decentralization.", "conclusion": "The work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex real-world cyber-physical systems with privacy preservation."}}
{"id": "2510.27317", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27317", "abs": "https://arxiv.org/abs/2510.27317", "authors": ["Shuyi Chen", "Panagiotis Oikonomou", "Zhengchang Hua", "Nikos Tziritas", "Karim Djemame", "Nan Zhang", "Georgios Theodoropoulos"], "title": "Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing", "comment": "Accepted by the 21st IEEE International Conference on Green Computing\n  and Communications (GreenCom 2025)", "summary": "Multi-access Edge Computing (MEC) delivers low-latency services by hosting\napplications near end-users. To promote sustainability, these systems are\nincreasingly integrated with renewable Energy Harvesting (EH) technologies,\nenabling operation where grid electricity is unavailable. However, balancing\nthe intermittent nature of harvested energy with dynamic user demand presents a\nsignificant resource allocation challenge. This work proposes an online\nstrategy for an MEC system powered exclusively by EH to address this trade-off.\nOur strategy dynamically schedules computational tasks with dependencies and\ngoverns energy consumption through real-time decisions on server frequency\nscaling and service module migration. Experiments using real-world datasets\ndemonstrate our algorithm's effectiveness in efficiently utilizing harvested\nenergy while maintaining low service latency.", "AI": {"tldr": "An online strategy for renewable energy-powered MEC systems that dynamically schedules computational tasks and manages energy consumption through server frequency scaling and service migration.", "motivation": "To address the challenge of balancing intermittent harvested energy with dynamic user demand in sustainable MEC systems operating without grid electricity.", "method": "Proposes an online strategy that dynamically schedules computational tasks with dependencies and governs energy consumption through real-time decisions on server frequency scaling and service module migration.", "result": "Experiments using real-world datasets demonstrate the algorithm's effectiveness in efficiently utilizing harvested energy while maintaining low service latency.", "conclusion": "The proposed strategy successfully addresses the trade-off between intermittent energy availability and service demands in EH-powered MEC systems."}}
{"id": "2510.27351", "categories": ["cs.DC", "65Y05, 65Y10, 90C59, 68T20"], "pdf": "https://arxiv.org/pdf/2510.27351", "abs": "https://arxiv.org/abs/2510.27351", "authors": ["Milena Veneva"], "title": "ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method", "comment": "10 pages, 6 figures, 4 tables, DLCP conference 2025, Moscow, Russia", "summary": "This paper presents a machine learning (ML)-based heuristic for finding the\noptimum sub-system size for the CUDA implementation of the parallel partition\nalgorithm. Computational experiments for different system of linear algebraic\nequation (SLAE) sizes are conducted, and the optimum sub-system size for each\nof them is found empirically. To estimate a model for the sub-system size, we\nperform the k-nearest neighbors (kNN) classification method. Statistical\nanalysis of the results is done. By comparing the predicted values with the\nactual data, the algorithm is deemed to be acceptably good. Next, the heuristic\nis expanded to work for the recursive parallel partition algorithm as well. An\nalgorithm for determining the optimum sub-system size for each recursive step\nis formulated. A kNN model for predicting the optimum number of recursive steps\nfor a particular SLAE size is built.", "AI": {"tldr": "This paper develops a machine learning-based heuristic using k-nearest neighbors (kNN) to find optimal sub-system sizes for CUDA implementations of parallel partition algorithms, and extends it to recursive versions.", "motivation": "To efficiently determine the optimum sub-system size for CUDA implementations of parallel partition algorithms for solving systems of linear algebraic equations (SLAE), avoiding empirical trial-and-error approaches.", "method": "Used k-nearest neighbors (kNN) classification method to build predictive models for optimal sub-system sizes based on computational experiments with different SLAE sizes, then extended the approach to recursive parallel partition algorithms.", "result": "The kNN-based heuristic successfully predicted optimal sub-system sizes with acceptable accuracy when compared to empirical data, and was successfully extended to determine optimal parameters for recursive implementations.", "conclusion": "Machine learning approaches like kNN can effectively automate the optimization of parallel algorithm parameters, providing a systematic alternative to empirical methods for both standard and recursive parallel partition implementations."}}
{"id": "2510.27656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27656", "abs": "https://arxiv.org/abs/2510.27656", "authors": ["Nandor Licker", "Kevin Hu", "Vladimir Zaytsev", "Lequn Chen"], "title": "RDMA Point-to-Point Communication for LLM Systems", "comment": null, "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.", "AI": {"tldr": "TransferEngine provides a uniform interface for point-to-point communication across different NICs, enabling high-throughput data transfer for LLM systems without hardware lock-in.", "motivation": "Existing LLM system patterns require flexible point-to-point communication, but current implementations are locked to specific NICs, hindering integration and portability across hardware providers.", "method": "TransferEngine bridges common NICs to expose a uniform interface with one-sided WriteImm operations and ImmCounter primitive for completion notification, transparently managing multiple NICs per GPU without ordering assumptions.", "result": "Achieved peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS EFA, with successful implementations in three production systems: KvCache transfer for disaggregated inference, RL weight updates for trillion-parameter models, and MoE dispatch/combine with improved latencies.", "conclusion": "TransferEngine provides portable point-to-point communication that complements collectives while avoiding hardware lock-in, enabling flexible LLM system deployment across different hardware platforms."}}
