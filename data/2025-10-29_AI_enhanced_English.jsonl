{"id": "2510.23638", "categories": ["cs.ET", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23638", "abs": "https://arxiv.org/abs/2510.23638", "authors": ["Songyuan Li", "Teng Wang", "Jinrong Tang", "Ruiqi Liu", "Yuyao Lu", "Feng Xu", "Bin Gao", "Xiangwei Zhu"], "title": "Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks", "comment": null, "summary": "Achieving fully analog neural computation requires hardware that can natively\nimplement both linear and nonlinear operations with high efficiency. While\nanalogue matrix-vector multiplication has advanced via compute-in-memory\narchitectures, nonlinear activation functions remain a bottleneck, often\nrequiring digital or hybrid solutions. Inspired by the Kolmogorov-Arnold\nframework, we propose KANalogue, a fully analogue implementation of\nKolmogorov-Arnold Networks (KANs) using negative differential resistance\ndevices as physical realizations of learnable univariate basis functions. By\nleveraging the intrinsic negative differential resistance characteristics of\ntunnel diodes fabricated from NbSi2N4/HfSi2N4 heterostructures, we construct\ncoordinate-wise nonlinearities with distinct curvature and support profiles. We\nextract I-V data from fabricated armchair and zigzag devices, fit high-order\npolynomials to emulate diode behavior in software, and train KANs on vision\nbenchmarks using these learned basis functions. Our results demonstrate that\nKANalogue can approximate complex functions with minimal parameters while\nmaintaining classification accuracy competitive with digital baselines. This\nwork bridges device-level physics and function approximation theory, charting a\npath toward scalable, energy-efficient analogue machine learning systems.", "AI": {"tldr": "KANalogue implements fully analog Kolmogorov-Arnold Networks using negative differential resistance devices as physical basis functions, achieving competitive accuracy with minimal parameters while enabling energy-efficient analog machine learning.", "motivation": "To overcome the bottleneck of nonlinear activation functions in analog neural computation, which often require digital or hybrid solutions, by creating a fully analog implementation using physical device characteristics.", "method": "Leverage negative differential resistance characteristics of tunnel diodes from NbSi2N4/HfSi2N4 heterostructures to construct coordinate-wise nonlinearities, extract I-V data from fabricated devices, fit high-order polynomials to emulate diode behavior, and train KANs on vision benchmarks using these learned basis functions.", "result": "KANalogue can approximate complex functions with minimal parameters while maintaining classification accuracy competitive with digital baselines, demonstrating the feasibility of fully analog neural computation.", "conclusion": "This work successfully bridges device-level physics and function approximation theory, providing a pathway toward scalable, energy-efficient analog machine learning systems through physical realization of learnable basis functions."}}
{"id": "2510.24510", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.24510", "abs": "https://arxiv.org/abs/2510.24510", "authors": ["Hugo Alcaraz-Herrera", "Michail-Antisthenis Tsompanas", "Igor Balaz", "Andrew Adamatzky"], "title": "Evaluating Fitness Averaging Strategies in Cooperative NeuroCoEvolution for Automated Soft Actuator Design", "comment": null, "summary": "Soft robotics are increasingly favoured in specific applications such as\nhealthcare, due to their adaptability, which stems from the non-linear\nproperties of their building materials. However, these properties also pose\nsignificant challenges in designing the morphologies and controllers of soft\nrobots. The relatively short history of this field has not yet produced\nsufficient knowledge to consistently derive optimal solutions. Consequently, an\nautomated process for the design of soft robot morphologies can be extremely\nhelpful. This study focusses on the cooperative NeuroCoEvolution of networks\nthat are indirect representations of soft robot actuators. Both the\nmorphologies and controllers represented by Compositional Pattern Producing\nNetworks are evolved using the well-established method NeuroEvolution of\nAugmented Topologies (CPPN-NEAT). The CoEvolution of controllers and\nmorphologies is implemented using the top n individuals from the cooperating\npopulation, with various averaging methods tested to determine the fitness of\nthe evaluated individuals. The test-case application for this research is the\noptimisation of a soft actuator for a drug delivery system. The primary metric\nused is the maximum displacement of one end of the actuator in a specified\ndirection. Additionally, the robustness of the evolved morphologies is assessed\nagainst a range of randomly generated controllers to simulate potential noise\nin real-world applications. The results of this investigation indicate that\nCPPN-NEAT produces superior morphologies compared to previously published\nresults from multi-objective optimisation, with reduced computational effort\nand time. Moreover, the best configuration is found to be CoEvolution with the\ntwo best individuals from the cooperative population and the averaging of their\nfitness using the weighted mean method.", "AI": {"tldr": "This paper presents a cooperative NeuroCoEvolution approach using CPPN-NEAT to automatically design soft robot morphologies and controllers, showing superior performance over previous methods with reduced computational effort.", "motivation": "Soft robotics face challenges in designing morphologies and controllers due to non-linear material properties. The field's short history lacks sufficient knowledge for optimal solutions, necessitating automated design processes.", "method": "Uses cooperative NeuroCoEvolution of CPPN networks representing soft robot actuators. Evolves both morphologies and controllers using CPPN-NEAT, with various averaging methods tested for fitness evaluation.", "result": "CPPN-NEAT produces superior morphologies compared to multi-objective optimization with reduced computational effort. Best configuration uses CoEvolution with two best individuals and weighted mean fitness averaging.", "conclusion": "The proposed automated design approach effectively addresses soft robot morphology and controller optimization challenges, demonstrating improved performance and efficiency in drug delivery system applications."}}
{"id": "2510.23911", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.23911", "abs": "https://arxiv.org/abs/2510.23911", "authors": ["Arno Uhlig", "Iris Braun", "Matthias W\u00e4hlisch"], "title": "The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing", "comment": "15 pages", "summary": "Allocating resources in a distributed environment is a fundamental challenge.\nIn this paper, we analyze the scheduling and placement of virtual machines\n(VMs) in the cloud platform of SAP, the world's largest enterprise resource\nplanning software vendor. Based on data from roughly 1,800 hypervisors and\n48,000 VMs within a 30-day observation period, we highlight potential\nimprovements for workload management. The data was measured through\nobservability tooling that tracks resource usage and performance metrics across\nthe entire infrastructure. In contrast to existing datasets, ours uniquely\noffers fine-grained time-series telemetry data of fully virtualized\nenterprise-level workloads from both long-running and memory-intensive SAP\nS/4HANA and diverse, general-purpose applications. Our key findings include\nseveral suboptimal scheduling situations, such as CPU resource contention\nexceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced\ncompute hosts with a maximum CPU~utilization on intra-building block hosts of\nup to 99%, and overprovisioned CPU and memory resources resulting into over 80%\nof VMs using less than 70% of the provided resources. Bolstered by these\nfindings, we derive requirements for the design and implementation of novel\nplacement and scheduling algorithms and provide guidance to optimize resource\nallocations. We make the full dataset used in this study publicly available to\nenable data-driven evaluations of scheduling approaches for large-scale cloud\ninfrastructures in future research.", "AI": {"tldr": "Analysis of VM scheduling in SAP's cloud platform reveals significant resource inefficiencies including CPU contention, imbalanced hosts, and overprovisioning, leading to requirements for improved scheduling algorithms.", "motivation": "To identify and analyze resource allocation inefficiencies in large-scale enterprise cloud infrastructure using real-world data from SAP's production environment.", "method": "Analyzed observational data from 1,800 hypervisors and 48,000 VMs over 30 days using fine-grained time-series telemetry data from SAP S/4HANA and general-purpose applications.", "result": "Found multiple suboptimal scheduling situations: CPU resource contention >40%, CPU ready times up to 220 seconds, imbalanced hosts with 99% CPU utilization, and over 80% of VMs using <70% of provided resources.", "conclusion": "Identified requirements for novel placement and scheduling algorithms to optimize resource allocations, and made the dataset publicly available for future research on large-scale cloud infrastructure scheduling."}}
{"id": "2510.24112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.24112", "abs": "https://arxiv.org/abs/2510.24112", "authors": ["Junchi Wu", "Xinfei Wan", "Zhuoran Li", "Yuyang Jin", "Guangyu Sun", "Yun Liang", "Diyu Zhou", "Youwei Zhuo"], "title": "SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems", "comment": "15 pages, 15 figures", "summary": "Many-core architectures are essential for high-performance computing, but\ntheir performance is undermined by widespread fail-slow failures. Detecting\nsuch failures on-chip is challenging, as prior methods from distributed systems\nare unsuitable due to strict memory limits and their inability to track\nfailures across the hardware topology. This paper introduces SlowPoke, a\nlightweight, hardware-aware framework for practical on-chip fail-slow\ndetection. SlowPoke combines compiler-based instrumentation for low-overhead\nmonitoring, on-the-fly trace compression to operate within kilobytes of memory,\nand a novel topology-aware ranking algorithm to pinpoint a failure's root\ncause. We evaluate SlowPoke on a wide range of representative many-core\nworkloads, and the results demonstrate that SlowPoke reduces the storage\noverhead of detection traces by an average of 115.9$\\times$, while achieving an\naverage fail-slow detection accuracy of 86.77% and a false positive rate (FPR)\nof 12.11%. More importantly, SlowPoke scales effectively across different\nmany-core architectures, making it practical for large-scale deployments.", "AI": {"tldr": "SlowPoke is a lightweight hardware-aware framework for detecting fail-slow failures in many-core architectures, achieving 86.77% accuracy with 115.9x storage reduction.", "motivation": "Many-core architectures suffer from fail-slow failures that undermine performance, but existing distributed system methods are unsuitable due to memory constraints and inability to track failures across hardware topology.", "method": "Combines compiler-based instrumentation for low-overhead monitoring, on-the-fly trace compression to operate within kilobytes of memory, and a novel topology-aware ranking algorithm to pinpoint failure root causes.", "result": "Reduces storage overhead by 115.9x average, achieves 86.77% detection accuracy with 12.11% false positive rate, and scales effectively across different many-core architectures.", "conclusion": "SlowPoke provides a practical, scalable solution for on-chip fail-slow detection in large-scale many-core deployments."}}
{"id": "2510.23993", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23993", "abs": "https://arxiv.org/abs/2510.23993", "authors": ["Anthony Carreon", "Jagmohan Singh", "Shivank Sharma", "Shuzhi Zhang", "Venkat Raman"], "title": "A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales", "comment": "32 pages, 12 figures", "summary": "High-speed chemically active flows present significant computational\nchallenges due to their disparate space and time scales, where stiff chemistry\noften dominates simulation time. While modern supercomputing scientific codes\nachieve exascale performance by leveraging graphics processing units (GPUs),\nexisting GPU-based compressible combustion solvers face critical limitations in\nmemory management, load balancing, and handling the highly localized nature of\nchemical reactions. To this end, we present a high-performance compressible\nreacting flow solver built on the AMReX framework and optimized for multi-GPU\nsettings. Our approach addresses three GPU performance bottlenecks: memory\naccess patterns through column-major storage optimization, computational\nworkload variability via a bulk-sparse integration strategy for chemical\nkinetics, and multi-GPU load distribution for adaptive mesh refinement\napplications. The solver adapts existing matrix-based chemical kinetics\nformulations to multigrid contexts. Using representative combustion\napplications including hydrogen-air detonations and jet in supersonic crossflow\nconfigurations, we demonstrate $2-5\\times$ performance improvements over\ninitial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA\nH100 GPUs. Roofline analysis reveals substantial improvements in arithmetic\nintensity for both convection ($\\sim 10 \\times$) and chemistry ($\\sim 4\n\\times$) routines, confirming efficient utilization of GPU memory bandwidth and\ncomputational resources.", "AI": {"tldr": "A high-performance GPU-optimized compressible reacting flow solver that addresses memory access, workload balancing, and multi-GPU distribution challenges for combustion simulations, achieving 2-5x performance improvements with near-ideal scaling.", "motivation": "High-speed chemically active flows face computational challenges due to disparate scales and stiff chemistry dominating simulation time, while existing GPU-based solvers have limitations in memory management, load balancing, and handling localized chemical reactions.", "method": "Built on AMReX framework with column-major storage optimization, bulk-sparse integration for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. Adapts matrix-based chemical kinetics formulations to multigrid contexts.", "result": "Demonstrated 2-5x performance improvements over initial GPU implementations with near-ideal weak scaling across 1-96 NVIDIA H100 GPUs. Roofline analysis shows substantial improvements in arithmetic intensity for convection (~10x) and chemistry (~4x) routines.", "conclusion": "The solver efficiently utilizes GPU memory bandwidth and computational resources, providing a scalable solution for high-speed chemically active flow simulations with significant performance gains."}}
{"id": "2510.24113", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24113", "abs": "https://arxiv.org/abs/2510.24113", "authors": ["Arnav Shukla", "Harsh Sharma", "Srikant Bharadwaj", "Vinayak Abrol", "Sujay Deb"], "title": "Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators", "comment": null, "summary": "Heterogeneous chiplet-based systems improve scaling by disag-gregating\nCPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package\ndisaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe\nthat in modern large-modelinference, parameters and activations routinely move\nbackand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.\nThese memory-driven transfers inflate tail latency andviolate Service Level\nAgreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this\ngap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown\nunder contention.We then formulate NoI synthesis as a multi-objective\noptimization(MOO) problem. We develop PARL (Partition-Aware\nReinforcementLearner), a topology generator that balances throughput,\nlatency,and power. PARL-generated topologies reduce contention at the memory\ncut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining\ncompetitive mean throughput relative to link-rich meshes. Overall, this\nreframes NoI design for heterogeneouschiplet accelerators with workload-aware\nobjectives.", "AI": {"tldr": "PARL is a reinforcement learning-based topology generator that optimizes Network-on-Interposer designs for chiplet systems to reduce memory-driven contention and meet SLAs while maintaining throughput.", "motivation": "Heterogeneous chiplet systems face latency issues from memory-driven transfers between HBM/DRAM and CPUs/GPUs, causing tail latency violations and SLA failures in traditional NoI topologies.", "method": "Developed PARL (Partition-Aware Reinforcement Learner) that formulates NoI synthesis as multi-objective optimization, using an Interference Score to quantify worst-case slowdown and balance throughput, latency, and power.", "result": "PARL-generated topologies reduce contention at memory interfaces, meet SLAs, cut worst-case slowdown to 1.2x, and maintain competitive mean throughput compared to link-rich meshes.", "conclusion": "The approach reframes NoI design for heterogeneous chiplet accelerators with workload-aware optimization objectives to address memory-driven performance bottlenecks."}}
{"id": "2510.24175", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24175", "abs": "https://arxiv.org/abs/2510.24175", "authors": ["Nitin Shukla", "Alessandro Romeo", "Caterina Caravita", "Michael Redenti", "Radim Vavrik", "Lubomir Riha", "Andrea Mignone", "Marco Rossazza", "Stefano Truzzi", "Luca Tornatore", "Antonio Ragagnin", "Tiago Castro", "Geray S. Karademir", "Klaus Dolag", "Pranab J. Deka", "Fabio Bacchini", "Rostislav-Paul Wilhelm", "Daniele Gregori", "Elisabetta Boella"], "title": "Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System", "comment": null, "summary": "Developing and redesigning astrophysical, cosmological, and space plasma\nnumerical codes for existing and next-generation accelerators is critical for\nenabling large-scale simulations. To address these challenges, the SPACE Center\nof Excellence (SPACE-CoE) fosters collaboration between scientists, code\ndevelopers, and high-performance computing experts to optimize applications for\nthe exascale era. This paper presents our strategy and initial results on the\nLeonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3\nand iPIC3D, using profiling tools to analyze performance on single and multiple\nnodes. Preliminary tests show all three codes scale efficiently, reaching 80%\nscalability up to 1,024 GPUs.", "AI": {"tldr": "The SPACE Center of Excellence optimizes three astrophysical simulation codes (gPLUTO, OpenGadget3, iPIC3D) for exascale computing, achieving 80% scalability up to 1,024 GPUs on the Leonardo system.", "motivation": "To enable large-scale astrophysical, cosmological, and space plasma simulations by developing and redesigning numerical codes for existing and next-generation accelerators in the exascale era.", "method": "Collaboration between scientists, code developers, and HPC experts; using profiling tools to analyze performance on single and multiple nodes of the Leonardo system at CINECA.", "result": "All three flagship codes scale efficiently, reaching 80% scalability up to 1,024 GPUs in preliminary tests.", "conclusion": "The SPACE-CoE strategy successfully optimizes astrophysical simulation codes for exascale computing, demonstrating efficient scaling on modern GPU architectures."}}
{"id": "2510.24205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24205", "abs": "https://arxiv.org/abs/2510.24205", "authors": ["Telmo Ribeiro", "Jos\u00e9 Proen\u00e7a", "M\u00e1rio Florido"], "title": "CoMPSeT: A Framework for Comparing Multiparty Session Types", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "Concurrent systems are often complex and difficult to design. Choreographic\nlanguages, such as Multiparty Session Types (MPST), allow the description of\nglobal protocols of interactions by capturing valid patterns of interactions\nbetween participants. Many variations of MPST exist, each one with its rather\nspecific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that\nprovides clearer insights over different features in existing MPST. We select a\nrepresentative set of MPST examples and provide mechanisms to combine different\nfeatures and to animate and compare the semantics of concrete examples. CoMPSeT\nis open-source, compiled into JavaScript, and can be directly executed from any\nbrowser, becoming useful both for researchers who want to better understand the\nlandscape of MPST and for teachers who want to explain global choreographies.", "AI": {"tldr": "CoMPSeT is a tool that helps understand and compare different features of Multiparty Session Types (MPST) by providing mechanisms to combine features, animate semantics, and analyze concrete examples.", "motivation": "Concurrent systems are complex to design, and while choreographic languages like MPST help describe global interaction protocols, the many variations with specific features make it difficult to understand and compare them.", "method": "The tool selects representative MPST examples and provides mechanisms to combine different features, animate semantics, and compare concrete examples. It's open-source, compiled to JavaScript, and browser-executable.", "result": "CoMPSeT provides clearer insights into different MPST features and becomes useful for both researchers wanting to understand the MPST landscape and teachers explaining global choreographies.", "conclusion": "CoMPSeT successfully addresses the complexity of MPST variations by offering an accessible tool that enables better understanding and comparison of different MPST features through interactive examples and semantic animations."}}
{"id": "2510.24452", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24452", "abs": "https://arxiv.org/abs/2510.24452", "authors": ["Xi Cheng", "Weijie Shen", "Haoming Chen", "Chaoyi Shen", "Jean Ortega", "Jiashang Liu", "Steve Thomas", "Honglin Zheng", "Haoyun Wu", "Yuxiang Li", "Casey Lichtendahl", "Jenny Ortiz", "Gang Liu", "Haiyang Qi", "Omid Fatemieh", "Chris Fry", "Jing Jing Long"], "title": "ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery", "comment": null, "summary": "Time series forecasting and anomaly detection are common tasks for\npractitioners in industries such as retail, manufacturing, advertising and\nenergy. Two unique challenges stand out: (1) efficiently and accurately\nforecasting time series or detecting anomalies in large volumes automatically;\nand (2) ensuring interpretability of results to effectively incorporate\nbusiness insights. We present ARIMA_PLUS, a novel framework to overcome these\ntwo challenges by a unique combination of (a) accurate and interpretable time\nseries models and (b) scalable and fully managed system infrastructure. The\nmodel has a sequential and modular structure to handle different components of\nthe time series, including holiday effects, seasonality, trend, and anomalies,\nwhich enables high interpretability of the results. Novel enhancements are made\nto each module, and a unified framework is established to address both\nforecasting and anomaly detection tasks simultaneously. In terms of accuracy,\nits comprehensive benchmark on the 42 public datasets in the Monash forecasting\nrepository shows superior performance over not only well-established\nstatistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer\nneural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms\nof infrastructure, it is directly built into the query engine of BigQuery in\nGoogle Cloud. It uses a simple SQL interface and automates tedious\ntechnicalities such as data cleaning and model selection. It automatically\nscales with managed cloud computational and storage resources, making it\npossible to forecast 100 million time series using only 1.5 hours with a\nthroughput of more than 18000 time series per second. In terms of\ninterpretability, we present several case studies to demonstrate time series\ninsights it generates and customizability it offers.", "AI": {"tldr": "ARIMA_PLUS is a novel framework for time series forecasting and anomaly detection that combines accurate interpretable models with scalable cloud infrastructure, achieving superior performance on benchmarks and handling 100M time series in 1.5 hours.", "motivation": "Address two key challenges: (1) efficiently forecasting/detecting anomalies in large volumes automatically, and (2) ensuring interpretability of results to incorporate business insights effectively.", "method": "Uses sequential modular structure to handle holiday effects, seasonality, trend, and anomalies. Built into Google BigQuery with SQL interface, automating data cleaning and model selection. Novel enhancements to each module with unified framework for both forecasting and anomaly detection.", "result": "Superior performance on 42 Monash forecasting datasets over statistical alternatives (ETS, ARIMA, TBATS, Prophet) and neural networks (DeepAR, N-BEATS, PatchTST, TimeMixer). Scales to 100M time series in 1.5 hours with throughput >18,000 time series per second.", "conclusion": "ARIMA_PLUS successfully addresses both efficiency and interpretability challenges through its unique combination of accurate models and scalable cloud infrastructure, demonstrating practical value for industrial applications."}}
