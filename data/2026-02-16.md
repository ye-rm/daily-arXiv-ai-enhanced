<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Design Environment of Quantization-Aware Edge AI Hardware for Few-Shot Learning](https://arxiv.org/abs/2602.12295)
*R. Kanda,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: This paper implements fixed-point data processing for edge AI hardware in few-shot learning, using Brevitas quantization to reduce computational resources while maintaining accuracy comparable to floating-point operations.


<details>
  <summary>Details</summary>
Motivation: To ensure consistency in accuracy throughout the entire design flow for edge AI hardware implementing few-shot learning, addressing the need for efficient fixed-point data processing that reduces computational resources while maintaining performance.

Method: Implemented fixed-point data processing using Brevitas quantization module, which allows arbitrary specification of integer and fractional bit widths. Applied both quantization-aware training (QAT) and post-training quantization (PTQ) methods. Compared with Tensil's hardware requirements of 8/16 bits per part.

Result: Performance validation showed that accuracy comparable to floating-point operations can be maintained with only 6 bits or 5 bits each for integer and fractional parts, indicating potential for significant reduction in computational resources beyond current hardware requirements.

Conclusion: The approach contributes to creating a versatile design and evaluation environment for edge AI hardware in few-shot learning, demonstrating that fixed-point quantization can maintain accuracy while substantially reducing computational requirements.

Abstract: This study aims to ensure consistency in accuracy throughout the entire design flow in the implementation of edge AI hardware for few-shot learning, by implementing fixed-point data processing in the pre-training and evaluation phases. Specifically, the quantization module, called Brevitas, is applied to implement fixed-point data processing, which allows for arbitrary specification of the bit widths for the integer and fractional parts. Two methods of fixed-point data quantization, quantization-aware training (QAT) and post-training quantization (PTQ), are utilized in Brevitas. With Tensil, which is used in the current design flow, the bit widths of the integer and fractional parts need to be 8 bits each or 16 bits each when implemented in hardware, but performance validation has shown that accuracy comparable to floating-point operations can be maintained even with 6 bits or 5 bits each, indicating potential for further reduction in computational resources. These results clearly contribute to the creation of a versatile design and evaluation environment for edge AI hardware for few-shot learning.

</details>


### [2] [CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement](https://arxiv.org/abs/2602.12422)
*Kaushal Mhapsekar,Azam Ghanbari,Bita Aslrousta,Samira Mirbagher-Ajorpaz*

Main category: cs.AR

TL;DR: CacheMind is a conversational tool using RAG and LLMs for semantic reasoning over cache traces, enabling architects to ask natural language questions about cache behavior and receive trace-grounded answers.


<details>
  <summary>Details</summary>
Motivation: Cache replacement is challenging with hand-crafted heuristics, and cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive.

Method: Introduces CacheMind using Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) for semantic reasoning over cache traces, with two retrievers (SIEVE and RANGER) evaluated on CacheMindBench benchmark.

Result: CacheMind achieves 66.67-89.33% accuracy on trace-grounded questions and 64.80-84.80% on policy-specific reasoning. With RANGER, achieves 100% accuracy on 4/6 categories. Demonstrates significant improvements over existing RAGs (10% vs 60-90% retrieval success).

Conclusion: CacheMind enables natural language interface for cache analysis, providing actionable insights (7.66% hit rate improvement, 2.04-76% speedups) and shows existing RAGs are insufficient for precise microarchitectural reasoning.

Abstract: Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, "Why is the memory access associated with PC X causing more evictions?", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.

</details>


### [3] [MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator](https://arxiv.org/abs/2602.12480)
*George Karfakis,Samyak Chakrabarty,Vinod Kurian Jacob,Siyun Qiao,Subramanian S. Iyer,Sudhakar Pamarti,Puneet Gupta*

Main category: cs.AR

TL;DR: MXFormer is a hybrid weight-stationary Compute-in-Memory accelerator using Charge-Trap Transistors for high-throughput Transformer inference with no weight movement, achieving 3.3x-60.5x better compute density than state-of-the-art alternatives.


<details>
  <summary>Details</summary>
Motivation: Transformer models face deployment constraints due to high computational and memory bandwidth demands. There's a need for efficient accelerators that can handle large models with minimal weight movement while maintaining accuracy.

Method: Hybrid CIM accelerator using ultra-dense Charge-Trap Transistors in Microscaling MXFP4 arrays for weight storage. Features statically partitioned design with 12 Transformer blocks, analog CTT arrays for static-weight layers, and digital blocks for dynamic computations with MXFP-enabled systolic arrays for attention and vector units for normalization.

Result: Achieves 58,275 FPS on ViT-L/32 (dual-chip) and 41,269 FPS on ViT-B/16 (single chip). Outperforms non-FWS accelerators by 3.3x-60.5x in compute density and 1.7x-2.5x in energy efficiency. Improves FWS accelerators by 20.9x in compute density and 2x in weight storage density with <1% accuracy drop.

Conclusion: MXFormer demonstrates that weight-stationary CIM architectures with CTT technology can achieve high throughput and efficiency for Transformer inference while eliminating weight movement and maintaining near-digital accuracy without model retraining.

Abstract: The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.
  We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.
  By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.

</details>


### [4] [Arcalis: Accelerating Remote Procedure Calls Using a Lightweight Near-Cache Solution](https://arxiv.org/abs/2602.12596)
*Johnson Umeike,Pongstorn Maidee,Bahar Asgari*

Main category: cs.AR

TL;DR: Arcalis is a near-cache RPC accelerator that places hardware engines next to the last-level cache to offload RPC processing, achieving 1.79-4.16× speedup over CPU baselines while reducing microarchitectural overhead by up to 88%.


<details>
  <summary>Details</summary>
Motivation: Modern microservices rely on high-performance RPCs, but CPU overhead from serialization, deserialization, and protocol handling has become a critical bottleneck, especially with fast networking stacks like DPDK. Existing solutions (software optimizations, FPGA offloads) remain physically distant from CPU memory hierarchy, causing unnecessary data movement and cache pollution.

Method: Arcalis positions a lightweight hardware engine adjacent to the last-level cache (LLC). It uses dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. The system decouples RPC processing logic, enables microservice-specific execution, and positions itself near the LLC to immediately consume data injected by network cards.

Result: Arcalis achieves 1.79-4.16× end-to-end speedup compared to CPU baseline, reduces microarchitectural overhead by up to 88%, and achieves up to 1.62× higher throughput than prior solutions.

Conclusion: Near-cache RPC acceleration represents a practical solution for high-performance microservice deployment, addressing CPU bottlenecks while minimizing data movement and cache pollution through strategic hardware positioning.

Abstract: Modern microservices increasingly depend on high-performance remote procedure calls (RPCs) to coordinate fine-grained, distributed computation. As network bandwidths continue to scale, the CPU overhead associated with RPC processing, particularly serialization, deserialization, and protocol handling, has become a critical bottleneck. This challenge is exacerbated by fast user-space networking stacks such as DPDK, which expose RPC processing as the dominant performance limiter. While prior work has explored software optimizations and FPGA-based offload engines, these approaches remain physically distant from the CPU's memory hierarchy, incurring unnecessary data movement and cache pollution. We present Arcalis, a near-cache RPC accelerator that positions a lightweight hardware engine adjacent to the last-level cache (LLC). Arcalis offloads RPC processing to dedicated microengines on receive and transmit paths that operate with cache-line latency while preserving programmability. By decoupling RPC processing logic, enabling microservice-specific execution, and positioning itself near the LLC to immediately consume data injected by network cards, Arcalis achieves 1.79-4.16$\times$ end-to-end speedup compared to the CPU baseline, while significantly reducing microarchitectural overhead by up to 88%, and achieves up to a 1.62$\times$ higher throughput than prior solutions. These results highlight the potential of near-cache RPC acceleration as a practical solution for high-performance microservice deployment.

</details>


### [5] [DPUConfig: Optimizing ML Inference in FPGAs Using Reinforcement Learning](https://arxiv.org/abs/2602.12847)
*Alexandros Patras,Spyros Lalis,Christos D. Antonopoulos,Nikolaos Bellas*

Main category: cs.AR

TL;DR: DPUConfig is a reinforcement learning-based runtime framework that dynamically selects optimal FPGA DPU configurations for ML inference, achieving 95% of optimal energy efficiency on average.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous embedded systems with FPGAs offer fast ML inference for latency-sensitive applications like autonomous driving, but efficiently allocating computational resources for deep learning on FPGAs is challenging.

Method: DPUConfig uses a custom Reinforcement Learning agent that dynamically selects optimal DPU configurations by monitoring real-time telemetry data, system utilization, power consumption, and application performance.

Result: The RL agent achieves energy efficiency 95% (on average) of the optimal attainable energy efficiency for several CNN models on the Xilinx Zynq UltraScale+ MPSoC ZCU102 platform.

Conclusion: DPUConfig demonstrates that RL-based dynamic configuration selection can effectively optimize energy efficiency for ML inference on FPGA-based heterogeneous embedded systems.

Abstract: Heterogeneous embedded systems, with diverse computing elements and accelerators such as FPGAs, offer a promising platform for fast and flexible ML inference, which is crucial for services such as autonomous driving and augmented reality, where delays can be costly. However, efficiently allocating computational resources for deep learning applications in FPGA-based systems is a challenging task. A Deep Learning Processor Unit (DPU) is a parameterizable FPGA-based accelerator module optimized for ML inference. It supports a wide range of ML models and can be instantiated multiple times within a single FPGA to enable concurrent execution. This paper introduces DPUConfig, a novel runtime management framework, based on a custom Reinforcement Learning (RL) agent, that dynamically selects optimal DPU configurations by leveraging real-time telemetry data monitoring, system utilization, power consumption, and application performance to inform its configuration selection decisions. The experimental evaluation demonstrates that the RL agent achieves energy efficiency 95% (on average) of the optimal attainable energy efficiency for several CNN models on the Xilinx Zynq UltraScale+ MPSoC ZCU102.

</details>


### [6] [TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design](https://arxiv.org/abs/2602.12962)
*Jonghun Lee,Junghoon Lee,Hyeonjin Kim,Seoho Jeon,Jisup Yoon,Hyunbin Park,Meejeong Park,Heonjae Ha*

Main category: cs.AR

TL;DR: TriGen is a novel NPU architecture for resource-constrained devices that uses software-hardware co-design with low-precision microscaling, LUT-based nonlinear operations, and optimized scheduling to accelerate LLM inference with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs have large model sizes with low parameter reuse, making end-to-end execution on resource-constrained devices extremely challenging. Existing NPU architectures need better optimization for LLM workloads in on-device environments.

Method: 1) Low-precision computation using microscaling (MX) for optimization while preserving accuracy; 2) Using fast and accurate LUTs to eliminate specialized hardware for nonlinear operations; 3) Scheduling techniques to maximize computational utilization under limited on-chip memory constraints.

Result: TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over baseline NPU design with negligible accuracy loss across various LLMs.

Conclusion: TriGen demonstrates that software-hardware co-design with low-precision microscaling, LUT-based nonlinear operations, and optimized scheduling can effectively accelerate LLM inference on resource-constrained devices while maintaining accuracy.

Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Distance-based certification for leader election in meshed graphs and local recognition of their subclasses](https://arxiv.org/abs/2602.12894)
*Jérémie Chalopin,Victor Chepoi,Maria Kokkou*

Main category: cs.DC

TL;DR: The paper presents 2-local proof labeling schemes for leader election in anonymous meshed graphs using 3-state labels, and 3-local schemes for recognizing subclasses of meshed graphs with O(log D) labels.


<details>
  <summary>Details</summary>
Motivation: To develop efficient distributed verification schemes for important graph classes that have been extensively studied in metric graph theory, geometric group theory, and discrete mathematics, enabling local verification of global properties like leader election.

Method: The authors use distance verification techniques in meshed graphs, showing that vertices can be locally verified to be labeled by their distance to an arbitrary root. For subclass recognition, they verify that the triangle-square complex is simply connected and apply existing local-to-global characterizations. For leader election, they check if vertices are labeled by distance modulo 3 to a designated root leader.

Result: A 2-local proof labeling scheme for leader election in anonymous meshed graphs using labels in {0,1,2}, and 3-local proof labeling schemes to recognize various subclasses of meshed graphs (median, bridged, chordal, Helly, dual polar, modular, weakly modular graphs, and basis graphs of matroids) using labels of size O(log D).

Conclusion: The paper successfully develops efficient local verification schemes for important graph classes, demonstrating that global properties like leader election can be verified locally with constant-size labels, and that complex graph class recognition can be achieved with logarithmic-size labels.

Abstract: In this paper, we present a 2-local proof labeling scheme with labels in $\{ 0,1,2\}$ for leader election in anonymous meshed graphs. Meshed graphs form a general class of graphs defined by a distance condition. They comprise several important classes of graphs, which have long been the subject of intensive studies in metric graph theory, geometric group theory, and discrete mathematics: median graphs, bridged graphs, chordal graphs, Helly graphs, dual polar graphs, modular, weakly modular graphs, and basis graphs of matroids. We also provide 3-local proof labeling schemes to recognize these subclasses of meshed graphs using labels of size $O(\log D)$ (where $D$ is the diameter of the graph).
  To establish these results, we show that in meshed graphs, we can verify locally that every vertex $v$ is labeled by its distance $d(s,v)$ to an arbitrary root $s$. To design proof labeling schemes to recognize the subclasses of meshed graphs mentioned above, we use this distance verification to ensure that the triangle-square complex of the graph is simply connected and we then rely on existing local-to-global characterizations for the different classes we consider.
  To get a proof-labeling scheme for leader election with labels of constant size, we then show that we can check locally if every $v$ is labeled by $d(s,v) \pmod{3}$ for some root $s$ that we designate as the leader.

</details>


### [8] [Classification of Local Optimization Problems in Directed Cycles](https://arxiv.org/abs/2602.13046)
*Thomas Boudier,Fabian Kuhn,Augusto Modanese,Ronja Stimpert,Jukka Suomela*

Main category: cs.DC

TL;DR: Complete classification of distributed complexity for local optimization problems in directed cycles shows only four possible complexity classes exist for constant approximation ratios.


<details>
  <summary>Details</summary>
Motivation: Previous work only covered local search problems (LCLs), but local optimization problems are a strict generalization that includes important distributed tasks like maximum independent set, vertex cover, dominating set, and vertex coloring approximations.

Method: The authors develop a complete classification framework for local optimization problems in directed cycles, analyzing deterministic and randomized LOCAL models. They create an efficient centralized meta-algorithm to automatically determine the complexity class and synthesize optimal distributed algorithms.

Result: Four distinct complexity classes are identified: (1) O(1) deterministic & randomized, (2) Θ(log* n) deterministic & O(1) randomized, (3) Θ(log* n) for both, (4) Θ(n) for both. The classification covers all local optimization problems with constant approximation ratios.

Conclusion: This work provides a comprehensive understanding of distributed computational complexity for local optimization in cycles, extending beyond previous LCL results to include important approximation problems with practical applications.

Abstract: We present a complete classification of the distributed computational complexity of local optimization problems in directed cycles for both the deterministic and the randomized LOCAL model. We show that for any local optimization problem $Π$ (that can be of the form min-sum, max-sum, min-max, or max-min, for any local cost or utility function over some finite alphabet), and for any \emph{constant} approximation ratio $α$, the task of finding an $α$-approximation of $Π$ in directed cycles has one of the following complexities:
  1. $O(1)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,
  2. $Θ(\log^* n)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,
  3. $Θ(\log^* n)$ rounds in deterministic LOCAL, $Θ(\log^* n)$ rounds in randomized LOCAL,
  4. $Θ(n)$ rounds in deterministic LOCAL, $Θ(n)$ rounds in randomized LOCAL.
  Moreover, for any given $Π$ and $α$, we can determine the complexity class automatically, with an efficient (centralized, sequential) meta-algorithm, and we can also efficiently synthesize an asymptotically optimal distributed algorithm.
  Before this work, similar results were only known for local search problems (e.g., locally checkable labeling problems). The family of local optimization problems is a strict generalization of local search problems, and it contains numerous commonly studied distributed tasks, such as the problems of finding approximations of the maximum independent set, minimum vertex cover, minimum dominating set, and minimum vertex coloring.

</details>


### [9] [Bloom Filter Look-Up Tables for Private and Secure Distributed Databases in Web3 (Revised Version)](https://arxiv.org/abs/2602.13167)
*Shlomi Dolev,Ehud Gudes,Daniel Shlomo*

Main category: cs.DC

TL;DR: A decentralized database scheme for secure key management in Web3 using BFLUT algorithm to encode/distribute keys without explicit storage, preventing discovery even if nodes are compromised.


<details>
  <summary>Details</summary>
Motivation: Decentralized Web3 systems face challenges in data security, privacy, and scalability, particularly with key management. Traditional approaches risk key exposure when nodes are compromised, requiring a secure distributed solution.

Method: Proposes a decentralized database scheme using BFLUT algorithm to encode and distribute cryptographic keys without explicit storage. Leverages OrbitDB, IPFS, and IPNS for decentralized data management, ensuring secure key retrieval without direct exposure.

Result: The system demonstrates capability to securely manage keys, prevent unauthorized access, and ensure privacy while maintaining high performance and reliability for Web3 applications.

Conclusion: The proposed scheme provides a foundational solution for Web3 applications requiring decentralized security by combining secure key encoding with distributed technologies to enhance security, privacy, and scalability.

Abstract: The rapid growth of decentralized systems in theWeb3 ecosystem has introduced numerous challenges, particularly in ensuring data security, privacy, and scalability [3, 8]. These systems rely heavily on distributed architectures, requiring robust mechanisms to manage data and interactions among participants securely. One critical aspect of decentralized systems is key management, which is essential for encrypting files, securing database segments, and enabling private transactions. However, securely managing cryptographic keys in a distributed environment poses significant risks, especially when nodes in the network can be compromised [9]. This research proposes a decentralized database scheme specifically designed for secure and private key management. Our approach ensures that cryptographic keys are not stored explicitly at any location, preventing their discovery even if an attacker gains control of multiple nodes. Instead of traditional storage, keys are encoded and distributed using the BFLUT (Bloom Filter for Private Look-Up Tables) algorithm [7], which enables secure retrieval without direct exposure. The system leverages OrbitDB [4], IPFS [1], and IPNS [10] for decentralized data management, providing robust support for consistency, scalability, and simultaneous updates. By combining these technologies, our scheme enhances both security and privacy while maintaining high performance and reliability. Our findings demonstrate the system's capability to securely manage keys, prevent unauthorized access, and ensure privacy, making it a foundational solution for Web3 applications requiring decentralized security.

</details>
