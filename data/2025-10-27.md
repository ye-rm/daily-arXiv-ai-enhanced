<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads](https://arxiv.org/abs/2510.21048)
*Jiabo Shi,Dimitrios Pezaros,Yehia Elkhatib*

Main category: cs.PF

TL;DR: xMem is a CPU-only dynamic analysis framework that accurately estimates peak GPU memory requirements for deep learning jobs, achieving 91% reduction in median relative error and 75% reduction in OOM failure probability compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: GPU scarcity requires better memory estimation for scheduling and sharing to prevent OOM errors and improve utilization. Current methods fail to capture runtime dynamics, consume GPU resources, or require intrusive code changes.

Method: Proposes xMem framework that uses CPU-only dynamic analysis to estimate GPU memory requirements without consuming GPU resources or requiring code modifications.

Result: Evaluation on 25 models with 5209 runs shows xMem reduces median relative error by 91%, decreases OOM failure probability by 75%, and increases memory conservation potential by 368% over state-of-the-art solutions.

Conclusion: xMem provides accurate GPU memory estimation through non-intrusive CPU analysis, enabling better resource scheduling and utilization while preventing OOM errors in shared GPU environments.

Abstract: The global scarcity of GPUs necessitates more sophisticated strategies for
Deep Learning jobs in shared cluster environments. Accurate estimation of how
much GPU memory a job will require is fundamental to enabling advanced
scheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and
resource underutilization. However, existing estimation methods have
limitations. Approaches relying on static analysis or historical data with
machine learning often fail to accurately capture runtime dynamics.
Furthermore, direct GPU analysis consumes scarce resources, and some techniques
require intrusive code modifications. Thus, the key challenge lies in precisely
estimating dynamic memory requirements, including memory allocator nuances,
without consuming GPU resources and non-intrusive code changes. To address this
challenge, we propose xMem, a novel framework that leverages CPU-only dynamic
analysis to accurately estimate peak GPU memory requirements a priori. We
conducted a thorough evaluation of xMem against state-of-the-art solutions
using workloads from 25 different models, including architectures like
Convolutional Neural Networks and Transformers. The analysis of 5209 runs,
which includes ANOVA and Monte Carlo results, highlights xMem's benefits: it
decreases the median relative error by 91% and significantly reduces the
probability of estimation failure as safe OOM thresholds by 75%, meaning that
the estimated value can often be used directly without causing OOM. Ultimately,
these improvements lead to a 368% increase in memory conservation potential
over current solutions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Lincoln AI Computing Survey (LAICS) and Trends](https://arxiv.org/abs/2510.20931)
*Albert Reuther,Peter Michaleas,Michael Jones,Vijay Gadepally,Jeremy Kepner*

Main category: cs.DC

TL;DR: This paper updates the Lincoln AI Computing Survey (LAICS) with new data on AI accelerators and processors, focusing on generative AI systems. It includes performance/power analysis, market segmentation, and new architectural categorizations.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative AI models has increased demand for specialized computing systems, necessitating an updated survey of current AI accelerators and processors to track industry trends.

Method: The survey collects and analyzes publicly announced commercial accelerators, plotting their peak performance and power consumption on scatter graphs, identifying market segments, and categorizing computing architectures.

Result: The updated survey provides comprehensive performance/power analysis of current AI accelerators, identifies market segments through scatter plots, and introduces new architectural categorizations for modern computing systems.

Conclusion: This annual survey update successfully captures the evolving landscape of AI accelerators, particularly for generative AI workloads, providing valuable insights into performance trends and architectural developments in the industry.

Abstract: In the past year, generative AI (GenAI) models have received a tremendous
amount of attention, which in turn has increased attention to computing systems
for training and inference for GenAI. Hence, an update to this survey is due.
This paper is an update of the survey of AI accelerators and processors from
past seven years, which is called the Lincoln AI Computing Survey -- LAICS
(pronounced "lace"). This multi-year survey collects and summarizes the current
commercial accelerators that have been publicly announced with peak performance
and peak power consumption numbers. In the same tradition of past papers of
this survey, the performance and power values are plotted on a scatter graph,
and a number of dimensions and observations from the trends on this plot are
again discussed and analyzed. Market segments are highlighted on the scatter
plot, and zoomed plots of each segment are also included. A brief description
of each of the new accelerators that have been added in the survey this year is
included, and this update features a new categorization of computing
architectures that implement each of the accelerators.

</details>


### [3] [Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach](https://arxiv.org/abs/2510.21155)
*Dandan Liang,Jianing Zhang,Evan Chen,Zhe Li,Rui Li,Haibo Yang*

Main category: cs.DC

TL;DR: MU-SplitFed is a straggler-resilient Split Federated Learning algorithm that uses unbalanced updates to decouple training from straggler delays, achieving linear speedup in communication rounds.


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning suffers from the straggler problem due to synchronization requirements between servers and clients, which creates bottlenecks and reduces system scalability and efficiency.

Method: Proposes MU-SplitFed with an unbalanced update mechanism where the server performs τ local updates per client round, using zeroth-order optimization to decouple training progress from straggler delays.

Result: Achieves convergence rate of O(√(d/(τT))) for non-convex objectives with linear speedup of τ in communication rounds, consistently outperforming baseline methods with stragglers present.

Conclusion: MU-SplitFed effectively mitigates straggler impact through adaptive tuning of τ and demonstrates superior performance compared to existing methods in Split Federated Learning systems.

Abstract: Split Federated Learning (SFL) enables scalable training on edge devices by
combining the parallelism of Federated Learning (FL) with the computational
offloading of Split Learning (SL). Despite its great success, SFL suffers
significantly from the well-known straggler issue in distributed learning
systems. This problem is exacerbated by the dependency between Split Server and
clients: the Split Server side model update relies on receiving activations
from clients. Such synchronization requirement introduces significant time
latency, making straggler a critical bottleneck to the scalability and
efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a
straggler-resilient SFL algorithm in zeroth-order optimization that decouples
training progress from straggler delays via a simple yet effective unbalanced
update mechanism.
  By enabling the server to perform $\tau$ local updates per client round,
MU-SplitFed achieves a convergence rate of $O(\sqrt{d/(\tau T)})$ for
non-convex objectives, demonstrating a linear speedup of $\tau$ in
communication rounds. Experiments demonstrate that MU-SplitFed consistently
outperforms baseline methods with the presence of stragglers and effectively
mitigates their impact through adaptive tuning of $\tau$. The code for this
project is available at https://github.com/Johnny-Zip/MU-SplitFed.

</details>


### [4] [From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker](https://arxiv.org/abs/2510.21173)
*Víctor Rampérez,Javier Soriano,David Lizcano,Shadi Aljawarneh,Juan A. Lara*

Main category: cs.DC

TL;DR: This paper presents an intelligent system that automatically translates high-level SLAs into vendor-neutral metrics, enabling multi-cloud deployment without provider lock-in.


<details>
  <summary>Details</summary>
Motivation: Current cloud providers require consumers to implement SLA compliance mechanisms, but consumers lack expertise and face provider lock-in due to different low-level metrics across providers, preventing multi-cloud benefits.

Method: Proposed an intelligent knowledge-based system that translates high-level SLAs into vendor-neutral metrics, plus defined a set of measurable vendor-neutral metrics across different cloud providers.

Result: Validated through two use cases (IaaS and PaaS) in a multi-cloud environment with leading providers, demonstrating automatic and transparent multi-cloud exploitation across application domains.

Conclusion: The solution enables cloud consumers to automatically translate SLAs into vendor-neutral metrics, overcoming provider lock-in and facilitating multi-cloud deployment as endorsed by cloud experts.

Abstract: Cloud computing has been consolidated as a support for the vast majority of
current and emerging technologies. However, there are some barriers that
prevent the exploitation of the full potential of this technology. First, the
major cloud providers currently put the onus of implementing the mechanisms
that ensure compliance with the desired service levels on cloud consumers.
However, consumers do not have the required expertise. Since each cloud
provider exports a different set of low-level metrics, the strategies defined
to ensure compliance with the established service-level agreement (SLA) are
bound to a particular cloud provider. This fosters provider lock-in and
prevents consumers from benefiting from the advantages of multi-cloud
environments. This paper presents a solution to the problem of automatically
translating SLAs into objectives expressed as metrics that can be measured
across multiple cloud providers. First, we propose an intelligent
knowledge-based system capable of automatically translating high-level SLAs
defined by cloud consumers into a set of conditions expressed as vendor-neutral
metrics, providing feedback to cloud consumers (intelligent tutoring system).
Secondly, we present the set of vendor-neutral metrics and explain how they can
be measured for the different cloud providers. Finally, we report a validation
based on two use cases (IaaS and PaaS) in a multi-cloud environment formed by
leading cloud providers. This evaluation has demonstrated that, thanks to the
complementarity of the two solutions, cloud consumers can automatically and
transparently exploit the multi-cloud in many application domains, as endorsed
by the cloud experts consulted in the course of this study.

</details>


### [5] [Generative Federated Learning for Smart Prediction and Recommendation Applications](https://arxiv.org/abs/2510.21183)
*Anwesha Mukherjee,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Proposes GFL combining GANs and federated learning for smart prediction applications, addressing data privacy, scarcity, and response time issues. Applied to heart health monitoring with improved accuracy and reduced latency.


<details>
  <summary>Details</summary>
Motivation: Address challenges in smart prediction applications including high response time, compromised data privacy, data scarcity, and class imbalance issues.

Method: Integrates generative adversarial networks with federated learning (GFL) to generate synthetic datasets for data augmentation. Implements both centralized and decentralized federated learning approaches in edge computing paradigm.

Result: Prediction accuracy improved by 12% compared to conventional framework; response time reduced by 73% compared to cloud-only system. Outperforms existing heart health monitoring applications.

Conclusion: The proposed GFL framework effectively addresses data privacy, scarcity, and response time challenges while improving prediction performance in smart health monitoring applications.

Abstract: This paper proposes a generative adversarial network and federated
learning-based model to address various challenges of the smart prediction and
recommendation applications, such as high response time, compromised data
privacy, and data scarcity. The integration of the generative adversarial
network and federated learning is referred to as Generative Federated Learning
(GFL). As a case study of the proposed model, a heart health monitoring
application is considered. The realistic synthetic datasets are generated using
the generated adversarial network-based proposed algorithm for improving data
diversity, data quality, and data augmentation, and remove the data scarcity
and class imbalance issues. In this paper, we implement the centralized and
decentralized federated learning approaches in an edge computing paradigm. In
centralized federated learning, the edge nodes communicate with the central
server to build the global and personalized local models in a collaborative
manner. In the decentralized federated learning approach, the edge nodes
communicate among themselves to exchange model updates for collaborative
training. The comparative study shows that the proposed framework outperforms
the existing heart health monitoring applications. The results show that using
the proposed framework (i) the prediction accuracy is improved by 12% than the
conventional framework, and (ii) the response time is reduced by 73% than the
conventional cloud-only system.

</details>


### [6] [Arbitration-Free Consistency is Available (and Vice Versa)](https://arxiv.org/abs/2510.21304)
*Hagit Attiya,Constantin Enea,Enrique Román-Calvo*

Main category: cs.DC

TL;DR: The paper introduces a general semantic framework for distributed storage systems that combines operation semantics and consistency models, and proves the Arbitration-Free Consistency (AFC) theorem which identifies arbitration-freedom as the fundamental property determining whether a consistency model admits available implementations without coordination.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental tension between availability and consistency in distributed storage systems, and provide a precise explanation of which combinations of object semantics and consistency models allow for available implementations, going beyond the simple read-write interfaces covered by classical results like the CAP theorem.

Method: Develops a general semantic framework that encompasses various objects (key-value stores, counters, sets, CRDTs, transactional databases) and consistency models (causal consistency, sequential consistency, snapshot isolation, SQL). Within this framework, proves the Arbitration-Free Consistency theorem.

Result: The AFC theorem shows that an object specification within a consistency model admits an available implementation if and only if it is arbitration-free - meaning it does not require a total arbitration order to resolve visibility or read dependencies.

Conclusion: Arbitration-freedom is identified as the fundamental property that delineates coordination-free consistency from inherently synchronized behavior, unifying and generalizing previous results in distributed systems theory.

Abstract: The fundamental tension between \emph{availability} and \emph{consistency}
shapes the design of distributed storage systems. Classical results capture
extreme points of this trade-off: the CAP theorem shows that strong models like
linearizability preclude availability under partitions, while weak models like
causal consistency remain implementable without coordination. These theorems
apply to simple read-write interfaces, leaving open a precise explanation of
the combinations of object semantics and consistency models that admit
available implementations.
  This paper develops a general semantic framework in which storage
specifications combine operation semantics and consistency models. The
framework encompasses a broad range of objects (key-value stores, counters,
sets, CRDTs, and transactional databases) and consistency models (from causal
consistency and sequential consistency to snapshot isolation and transactional
and non-transactional SQL).
  Within this framework, we prove the \emph{Arbitration-Free Consistency} (AFC)
theorem, showing that an object specification within a consistency model admits
an available implementation if and only if it is \emph{arbitration-free}, that
is, it does not require a total arbitration order to resolve visibility or read
dependencies.
  The AFC theorem unifies and generalizes previous results, revealing
arbitration-freedom as the fundamental property that delineates
coordination-free consistency from inherently synchronized behavior.

</details>


### [7] [Parsley's Group Size Study](https://arxiv.org/abs/2510.21348)
*João A. Silva,Hervé Paulino,João M. Lourenço*

Main category: cs.DC

TL;DR: Parsley is a resilient group-based DHT that uses preemptive peer relocation and dynamic data sharding to improve robustness and load balancing. It introduces soft group size limits alongside hard limits to proactively maintain stability under churn.


<details>
  <summary>Details</summary>
Motivation: Existing DHT systems specify group size limits without proper justification. The motivation is to provide a systematic analysis of parameter effects on performance and scalability, offering grounded explanations for configuration choices.

Method: Parsley incorporates preemptive peer relocation, dynamic data sharding, and introduces both hard limits (min/max thresholds) and soft limits (target intervals) for group sizes. A systematic overlay characterization study was conducted to analyze topology operations, large group behavior, and performance trade-offs.

Result: The study provides an in-depth analysis of how different parameter values affect Parsley's performance and scalability. It examines the effects of group size limits on system stability under churn conditions.

Conclusion: Parsley's approach with soft boundaries allows proactive prevention of hard limit violations, improving system stability. The systematic characterization study offers justified parameter configurations for group-based DHTs, addressing the gap in related systems that specify limits without proper rationale.

Abstract: Parsley is a resilient group-based Distributed Hash Table that incorporates a
preemptive peer relocation technique and a dynamic data sharding mechanism to
enhance robustness and balance. In addition to the hard limits on group size,
defined by minimum and maximum thresholds, Parsley introduces two soft limits
that define a target interval for maintaining stable group sizes. These soft
boundaries allow the overlay to take proactive measures to prevent violations
of the hard limits, improving system stability under churn. This work provides
an in-depth analysis of the rationale behind the parameter values adopted for
Parsley's evaluation. Unlike related systems, which specify group size limits
without justification, we conduct a systematic overlay characterization study
to understand the effects of these parameters on performance and scalability.
The study examines topology operations, the behavior of large groups, and the
overall trade-offs observed, offering a grounded explanation for the chosen
configuration values.

</details>


### [8] [LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science](https://arxiv.org/abs/2510.21373)
*Sankalpa Timilsina,Susmit Shannigrahi*

Main category: cs.DC

TL;DR: The paper introduces a decentralized control plane using semantic names to place computations across geographically distributed clusters, overcoming limitations of centralized approaches like Kubernetes in multi-organizational settings.


<details>
  <summary>Details</summary>
Motivation: Centralized controllers like Kubernetes are unsuitable for multi-organizational collaborations and cannot adapt to dynamic infrastructure changes, requiring manual configurations for single platforms.

Method: Uses semantic names assigned to computations to match requests with named Kubernetes service endpoints, enabling decentralized control without prior knowledge of cluster locations.

Result: Enables location-independent job placement where any cluster with sufficient resources can execute computations, and facilitates dynamic compute placement without predefined configurations.

Conclusion: The semantic naming approach provides a decentralized solution for geographically distributed computing that overcomes limitations of centralized controllers in multi-organizational environments.

Abstract: Scientific communities are increasingly using geographically distributed
computing platforms. The current methods of compute placement predominantly use
logically centralized controllers such as Kubernetes (K8s) to match tasks to
available resources. However, this centralized approach is unsuitable in
multi-organizational collaborations. Furthermore, workflows often need to use
manual configurations tailored for a single platform and cannot adapt to
dynamic changes across infrastructure. Our work introduces a decentralized
control plane for placing computations on geographically dispersed compute
clusters using semantic names. We assign semantic names to computations to
match requests with named Kubernetes (K8s) service endpoints. We show that this
approach provides multiple benefits. First, it allows placement of
computational jobs to be independent of location, enabling any cluster with
sufficient resources to execute the computation. Second, it facilitates dynamic
compute placement without requiring prior knowledge of cluster locations or
predefined configurations.

</details>


### [9] [Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads](https://arxiv.org/abs/2510.21419)
*Sankalpa Timilsina,Susmit Shannigrahi*

Main category: cs.DC

TL;DR: A network-aware job scheduler using supervised learning to predict job completion times and optimize placement in distributed cloud environments, achieving 34-54% higher accuracy than default Kubernetes scheduler.


<details>
  <summary>Details</summary>
Motivation: Traditional host-level metrics like CPU/memory don't capture network congestion, asymmetric bandwidth, and inter-node data shuffling issues that cause slowdowns in distributed cloud environments, leading to poor placement decisions and suboptimal job performance.

Method: Uses supervised learning to predict job completion times through a prediction-and-ranking mechanism that collects real-time telemetry from all nodes, estimates job duration per node using trained models, and ranks nodes for optimal placement.

Result: Evaluated on geo-distributed Kubernetes cluster on FABRIC testbed with network-intensive Spark workloads. Achieved 34-54% higher accuracy in selecting optimal nodes compared to default Kubernetes scheduler that only considers current resource availability.

Conclusion: Demonstrates successful application of supervised learning for real-time, network-aware job scheduling in multi-site clusters, addressing network-related performance bottlenecks that traditional schedulers overlook.

Abstract: Distributed cloud environments hosting data-intensive applications often
experience slowdowns due to network congestion, asymmetric bandwidth, and
inter-node data shuffling. These factors are typically not captured by
traditional host-level metrics like CPU or memory. Scheduling without
accounting for these conditions can lead to poor placement decisions, longer
data transfers, and suboptimal job performance. We present a network-aware job
scheduler that uses supervised learning to predict the completion time of
candidate jobs. Our system introduces a prediction-and-ranking mechanism that
collects real-time telemetry from all nodes, uses a trained supervised model to
estimate job duration per node, and ranks them to select the best placement. We
evaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the
FABRIC testbed by running network-intensive Spark workloads. Compared to the
default Kubernetes scheduler, which makes placement decisions based on current
resource availability alone, our proposed supervised scheduler achieved 34-54%
higher accuracy in selecting optimal nodes for job placement. The novelty of
our work lies in the demonstration of supervised learning for real-time,
network-aware job scheduling on a multi-site cluster.

</details>


### [10] [On Reduction and Synthesis of Petri's Cycloids](https://arxiv.org/abs/2510.21493)
*Rüdiger Valk,Daniel Moldt*

Main category: cs.DC

TL;DR: This paper analyzes cycloids, a type of Petri net for modeling action and event processes, by developing reduction systems to study their structure and proving properties of irreducible cycloids, ultimately enabling efficient cycloid isomorphism testing.


<details>
  <summary>Details</summary>
Motivation: To deepen the understanding of cycloid structures within Petri's general systems theory and develop methods for analyzing their properties and relationships.

Method: Defined reduction systems for cycloids in the style of rewriting systems, proved properties of irreducible cycloids, and derived synthesis of cycloid parameters from their Petri net structure.

Result: Developed an efficient method for decision procedure for cycloid isomorphism through parameter synthesis from the Petri net structure.

Conclusion: The reduction system approach provides effective tools for analyzing cycloid structures and enables practical isomorphism testing for these synchronized sequential process models.

Abstract: Cycloids are particular Petri nets for modelling processes of actions and
events, belonging to the fundaments of Petri's general systems theory. Defined
by four parameters they provide an algebraic formalism to describe strongly
synchronized sequential processes. To further investigate their structure,
reduction systems of cycloids are defined in the style of rewriting systems and
properties of irreducible cycloids are proved. In particular the synthesis of
cycloid parameters from their Petri net structure is derived, leading to an
efficient method for a decision procedure for cycloid isomorphism.

</details>


### [11] [Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence](https://arxiv.org/abs/2510.21549)
*Marc Fuchs,Fabian Kuhn*

Main category: cs.DC

TL;DR: This paper improves deterministic complexity bounds for (Δ+1)-coloring in graphs with bounded neighborhood independence θ, achieving quasipolylogarithmic time in Δ when θ is polylogarithmic in Δ.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research, determining the deterministic complexity of (Δ+1)-coloring remains a major open problem in distributed graph algorithms. The paper aims to improve understanding by focusing on graphs with bounded neighborhood independence where faster algorithms are known.

Method: The authors analyze graphs with bounded neighborhood independence θ and develop new algorithms that achieve improved time complexity bounds for (Δ+1)-coloring, leveraging the structural properties of these graphs.

Result: The paper shows that in graphs with neighborhood independence θ, (Δ+1)-coloring can be computed in (θ·logΔ)^O(log logΔ / log log logΔ) + O(log* n) rounds, which is quasipolylogarithmic in Δ when θ is polylogarithmic in Δ.

Conclusion: The work significantly improves the state-of-the-art for (Δ+1)-coloring in graphs with bounded neighborhood independence and demonstrates limitations of existing approaches for edge coloring in hypergraphs of rank ≥3.

Abstract: The distributed coloring problem is arguably one of the key problems studied
in the area of distributed graph algorithms. The most standard variant of the
problem asks for a proper vertex coloring of a graph with $\Delta+1$ colors,
where $\Delta$ is the maximum degree of the graph. Despite an immense amount of
work on distributed coloring problems in the distributed setting, determining
the deterministic complexity of $(\Delta+1)$-coloring in the standard message
passing model remains one of the most important open questions of the area. In
this paper, we aim to improve our understanding of the deterministic complexity
of $(\Delta+1)$-coloring as a function of $\Delta$ in a special family of
graphs for which significantly faster algorithms are already known. The
neighborhood independence $\theta$ of a graph is the maximum number of pairwise
non-adjacent neighbors of some node of the graph. In general, in graphs of
neighborhood independence $\theta=O(1)$ (e.g., line graphs), it is known that
$(\Delta+1)$-coloring can be solved in $2^{O(\sqrt{\log\Delta})}+O(\log^* n)$
rounds. In the present paper, we significantly improve this result, and we show
that in graphs of neighborhood independence $\theta$, a $(\Delta+1)$-coloring
can be computed in $(\theta\cdot\log\Delta)^{O(\log\log\Delta /
\log\log\log\Delta)}+O(\log^* n)$ rounds and thus in quasipolylogarithmic time
in $\Delta$ as long as $\theta$ is at most polylogarithmic in $\Delta$. We also
show that the known approach that leads to a polylogarithmic in $\Delta$
algorithm for $(2\Delta-1)$-edge coloring already fails for edge colorings of
hypergraphs of rank at least $3$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs](https://arxiv.org/abs/2510.20981)
*Stefan Abi-Karam,Rishov Sarkar,Suhail Basalama,Jason Cong,Callie Hao*

Main category: cs.AR

TL;DR: FIFOAdvisor is an automated framework that determines optimal FIFO buffer sizes in HLS dataflow designs using fast cycle-accurate simulation, solving the dual-objective optimization problem of balancing latency and memory usage.


<details>
  <summary>Details</summary>
Motivation: Current methods for sizing FIFO channels in HLS dataflow designs are problematic - they rely on restrictive assumptions, conservative over-allocation, or slow RTL simulations. Undersized FIFOs cause stalls and deadlocks, while oversized ones waste memory resources.

Method: FIFOAdvisor uses LightningSim (99.9% cycle-accurate simulator) for millisecond-scale incremental runs with new FIFO configurations. It formulates FIFO sizing as a dual-objective black-box optimization problem and explores heuristic and search-based methods to characterize latency-resource trade-offs.

Result: Evaluation on Stream-HLS benchmarks shows Pareto-optimal latency-memory frontiers across optimization strategies. FIFOAdvisor achieves much lower memory usage with minimal delay overhead compared to baseline designs, and provides significant runtime speedups over traditional HLS/RTL co-simulation.

Conclusion: FIFOAdvisor enables practical rapid design space exploration for dataflow hardware designs, effectively balancing FIFO sizing trade-offs while ensuring deadlock-free operation, especially for data-dependent designs where runtime analysis is essential.

Abstract: Dataflow hardware designs enable efficient FPGA implementations via
high-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO)
channel buffers remains challenging. FIFO sizes are user-defined and balance
latency and area-undersized FIFOs cause stalls and potential deadlocks, while
oversized ones waste memory. Determining optimal sizes is non-trivial: existing
methods rely on restrictive assumptions, conservative over-allocation, or slow
RTL simulations. We emphasize that runtime-based analyses (i.e., simulation)
are the only reliable way to ensure deadlock-free FIFO optimization for
data-dependent designs.
  We present FIFOAdvisor, a framework that automatically determines FIFO sizes
in HLS designs. It leverages LightningSim, a 99.9\% cycle-accurate simulator
supporting millisecond-scale incremental runs with new FIFO configurations.
FIFO sizing is formulated as a dual-objective black-box optimization problem,
and we explore heuristic and search-based methods to characterize the
latency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a
framework for optimizing affine dataflow designs lowered from C++, MLIR, or
PyTorch, enabling deeper optimization of FIFOs in these workloads.
  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear
algebra and deep learning workloads. Our results reveal Pareto-optimal
latency-memory frontiers across optimization strategies. Compared to baseline
designs, FIFOAdvisor achieves much lower memory usage with minimal delay
overhead. Additionally, it delivers significant runtime speedups over
traditional HLS/RTL co-simulation, making it practical for rapid design space
exploration. We further demonstrate its capability on a complex accelerator
with data-dependent control flow.
  Code and results: https://github.com/sharc-lab/fifo-advisor

</details>


### [13] [Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs](https://arxiv.org/abs/2510.21533)
*Misaki Kida,Shimpei Sato*

Main category: cs.AR

TL;DR: A hardware-efficient 4-bit multiplier design for AMD Xilinx 7-series FPGAs using only 11 LUTs and two CARRY4 blocks, achieving reduced resource usage and shorter critical path compared to prior 12-LUT designs.


<details>
  <summary>Details</summary>
Motivation: The growing need to optimize area and delay in LUT-based multipliers for IoT and edge inference applications that implement large numbers of low-bitwidth operations in parallel.

Method: Proposes a 4-bit multiplier design by reorganizing the logic functions mapped to the LUTs, using only 11 LUTs and two CARRY4 blocks.

Result: The design reduces LUT count by one compared to prior 12-LUT designs while shortening the critical path, achieving minimal resource usage and a critical-path delay of 2.750 ns.

Conclusion: The proposed method successfully achieves hardware efficiency with reduced resource consumption and improved performance for 4-bit multipliers in FPGA implementations.

Abstract: As IoT and edge inference proliferate,there is a growing need to
simultaneously optimize area and delay in lookup-table (LUT)-based multipliers
that implement large numbers of low-bitwidth operations in parallel. This paper
proposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx
7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the
logic functions mapped to the LUTs, the proposed method reduces the LUT count
by one compared with the prior 12-LUT design while also shortening the critical
path. Evaluation confirms that the circuit attains minimal resource usage and a
critical-path delay of 2.750 ns.

</details>


### [14] [Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation](https://arxiv.org/abs/2510.21547)
*Hangyu Zhang,Sachin S. Sapatnekar*

Main category: cs.AR

TL;DR: Using AccFFT for electric field computation in global placement significantly reduces runtime while maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: Improve scalability and efficiency of electrostatics-based analytic placement for complex VLSI designs by accelerating FFT computations.

Method: Incorporated accelerated FFT technique (AccFFT) into existing ePlace-MS and Pplace-MS algorithms for electric field computation in global placement.

Result: Achieved 5.78x speedup in FFT computation, 32% total runtime improvement against ePlace-MS, with only 1.0% reduction in scaled half-perimeter wirelength after detailed placement.

Conclusion: AccFFT technique effectively accelerates global placement while maintaining comparable solution quality, making it suitable for modern VLSI design flows.

Abstract: Global placement is essential for high-quality and efficient circuit
placement for complex modern VLSI designs. Recent advancements, such as
electrostatics-based analytic placement, have improved scalability and solution
quality. This work demonstrates that using an accelerated FFT technique,
AccFFT, for electric field computation significantly reduces runtime.
Experimental results on standard benchmarks show significant improvements when
incorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup
in FFT computation and a 32% total runtime improvement against ePlace-MS, with
1.0% reduction of scaled half-perimeter wirelength after detailed placement.

</details>
