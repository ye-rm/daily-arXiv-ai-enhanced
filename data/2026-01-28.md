<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [M$^{\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization](https://arxiv.org/abs/2601.19213)
*Weiming Hu,Zihan Zhang,Haoyan Zhang,Chen Zhang,Cong Guo,Yu Feng,Tianchi Hu,Guanglin Li,Guipeng Hu,Junsong Wang,Jingwen Leng*

Main category: cs.AR

TL;DR: Proposes M2XFP, a low-bit quantization method with flexible metadata to reduce accuracy loss in LLMs while maintaining bit efficiency, achieving significant improvements over MXFP4 and NVFP4 with hardware acceleration benefits.


<details>
  <summary>Details</summary>
Motivation: Existing low-bit MX formats like MXFP4 suffer from substantial accuracy degradation due to shared scaling factors with Power-of-Two format, creating a need for minimal metadata strategies to recover accuracy while maintaining bit efficiency.

Method: Algorithm-hardware co-design with flexible metadata featuring online quantization with simple encoding, implemented via lightweight hardware unit integrated into accelerator.

Result: 70.63% reduction in accuracy loss compared to MXFP4 and 37.30% reduction relative to NVFP4 on LLM benchmarks, with up to 1.91× speedup and 1.75× energy savings over state-of-the-art accelerators.

Conclusion: The proposed M2XFP method effectively narrows the accuracy gap in low-bit quantization for LLMs through flexible metadata and hardware co-design, achieving significant improvements in accuracy, speed, and energy efficiency.

Abstract: Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\times$ speedup and 1.75$\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26.

</details>


### [2] [A Reconfigurable Framework for AI-FPGA Agent Integration and Acceleration](https://arxiv.org/abs/2601.19263)
*Aybars Yunusoglu,Talha Coskun,Hiruna Vishwamith,Murat Isik,I. Can Dikmen*

Main category: cs.AR

TL;DR: AI FPGA Agent is an agent-driven framework that simplifies and accelerates deep neural network inference on FPGAs, achieving 10x latency reduction vs CPUs and 2-3x better energy efficiency than GPUs.


<details>
  <summary>Details</summary>
Motivation: AI deployment in real-time, energy-constrained environments requires hardware platforms with high performance and power efficiency. CPUs and GPUs have inefficiencies under strict latency/power budgets, while FPGAs offer custom parallelism but face challenges in hardware-software co-design and data orchestration.

Method: The framework uses a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic.

Result: Achieves over 10x latency reduction compared to CPU baselines, 2-3x higher energy efficiency than GPU implementations, while preserving classification accuracy within 0.2% of full-precision references.

Conclusion: The findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment, demonstrating that agent-driven frameworks can effectively simplify FPGA-based AI acceleration.

Abstract: Artificial intelligence (AI) is increasingly deployed in real-time and energy-constrained environments, driving demand for hardware platforms that can deliver high performance and power efficiency. While central processing units (CPUs) and graphics processing units (GPUs) have traditionally served as the primary inference engines, their general-purpose nature often leads to inefficiencies under strict latency or power budgets. Field-Programmable Gate Arrays (FPGAs) offer a promising alternative by enabling custom-tailored parallelism and hardware-level optimizations. However, mapping AI workloads to FPGAs remains challenging due to the complexity of hardware-software co-design and data orchestration. This paper presents AI FPGA Agent, an agent-driven framework that simplifies the integration and acceleration of deep neural network inference on FPGAs. The proposed system employs a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers with minimal developer intervention. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic. Experimental results demonstrate that the AI FPGA Agent achieves over 10x latency reduction compared to CPU baselines and 2-3x higher energy efficiency than GPU implementations, all while preserving classification accuracy within 0.2% of full-precision references. These findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment.

</details>


### [3] [GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping](https://arxiv.org/abs/2601.19384)
*Julien Eudine,Chu Li,Zhuo Cheng,Renzo Andri,Can Firtina,Mohammad Sadrosadati,Nika Mansouri Ghiasi,Konstantina Koliogeorgi,Anirban Nag,Arash Tavakkol,Haiyu Mao,Onur Mutlu,Shai Bergman,Ji Zhang*

Main category: cs.AR

TL;DR: GenPairX: A hardware-algorithm co-designed accelerator for paired-end read mapping that improves filtering effectiveness and replaces expensive dynamic programming operations, achieving 1575x higher throughput per watt vs CPU-based solutions.


<details>
  <summary>Details</summary>
Motivation: Read mapping is a major performance bottleneck in genome analysis due to expensive dynamic programming. Existing filters are ineffective for paired-end reads as they evaluate each read independently with low filtering ratios, and current hardware acceleration approaches have limitations.

Method: GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most dynamic programming operations; (2) two specialized hardware mechanisms to support these algorithms.

Result: GenPairX delivers substantial performance improvements, achieving 1575x higher throughput per watt compared to leading CPU-based read mappers and 1.43x higher throughput per watt compared to accelerator-based solutions, without compromising accuracy.

Conclusion: GenPairX successfully addresses the performance bottleneck in paired-end read mapping through hardware-algorithm co-design, offering significant efficiency gains while maintaining accuracy, making it a promising solution for genome analysis workflows.

Abstract: Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy.

</details>


### [4] [Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation](https://arxiv.org/abs/2601.19747)
*Jiale Liu,Taiyu Zhou,Tianqi Jiang*

Main category: cs.AR

TL;DR: Veri-Sure is a multi-agent framework for silicon-grade RTL design using LLMs, featuring design contracts, dependency-guided patching, and multi-branch verification to ensure functional correctness beyond simulations.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based RTL design faces three main bottlenecks: (1) limited test coverage and reliability of simulation-centric evaluation, (2) regressions and repair hallucinations from iterative debugging, and (3) semantic drift during agent handoffs.

Method: Veri-Sure uses a multi-agent framework with design contracts to align agent intent, static dependency slicing for precise localized repairs, and a multi-branch verification pipeline combining trace-driven temporal analysis with formal verification (assertion-based checking and boolean equivalence proofs).

Result: The paper introduces VerilogEval-v2-EXT benchmark with 53 additional industrial-grade design tasks and shows Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.

Conclusion: Veri-Sure addresses key bottlenecks in LLM-based RTL design by establishing design contracts, enabling precise repairs, and integrating comprehensive verification, achieving verified functional correctness beyond pure simulations.

Abstract: In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: X-Sched is a middleware that uses explainable AI techniques to provide actionable guidance for resource configuration in containerized environments, helping users meet Service Level Objectives (SLOs) with transparent decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing container schedulers optimize performance metrics but lack transparency in decision-making and don't provide clear guidance on what actions developers should take to meet Service Level Objectives (SLOs). There's a need for systems that not only optimize performance but also explain their decisions and provide actionable insights.

Method: X-Sched integrates counterfactual explanations with advanced machine learning models (specifically Random Forests) to efficiently identify optimal resource configurations for task execution in containerized environments under resource and time constraints.

Result: Experimental results using data from real-world execution environments demonstrate the efficiency, benefits, and practicality of the X-Sched approach in ensuring tasks are executed according to performance goals while providing clear, actionable insights.

Conclusion: X-Sched successfully addresses the transparency gap in container scheduling by providing explainable, actionable guidance for resource configuration, enabling users to understand scheduling decisions and meet SLOs effectively in containerized environments.

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [6] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: Axe Layout is a hardware-aware abstraction that maps logical tensor coordinates to physical space via named axes, unifying data placement across device meshes, memory hierarchies, and heterogeneous accelerators.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning workloads require coordinated placement of data and compute across complex hardware environments including device meshes, memory hierarchies, and heterogeneous accelerators. Current approaches lack a unified abstraction for managing these complex data layouts.

Method: Axe Layout uses named axes to map logical tensor coordinates to multi-axis physical space, unifying tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts. A multi-granularity, distribution-aware DSL and compiler composes thread-local control with collective operators in single kernels.

Result: Experiments show the unified approach can achieve performance close to hand-tuned kernels across latest GPU devices, multi-device environments, and accelerator backends.

Conclusion: Axe Layout provides a hardware-aware abstraction that enables consistent expression of collective primitives from device meshes to threads, offering near-hand-tuned performance across diverse hardware environments.

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [7] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT is a Kubernetes-based cluster manager for FaaS that bypasses API Server bottlenecks through direct message passing while maintaining compatibility, achieving 26.7x lower latency than Knative.


<details>
  <summary>Details</summary>
Motivation: Kubernetes-based FaaS platforms face scalability bottlenecks due to extensive state exchange through the API Server during bursts. Existing clean-slate solutions sacrifice compatibility with the Kubernetes ecosystem.

Method: Identifies a common narrow waist in FaaS platforms that enables bypassing the API Server via direct message passing. Uses a novel state management scheme treating the narrow waist as a hierarchical write-back cache to ensure consistency without centralized coordination.

Result: KUBEDIRECT reduces serving latency by 26.7x compared to Knative and achieves similar performance to state-of-the-art clean-slate platform Dirigent, while adding only ~150 lines of code per controller.

Conclusion: KUBEDIRECT demonstrates that efficiency and compatibility can be achieved simultaneously in Kubernetes-based FaaS platforms by leveraging the narrow waist structure for direct communication while maintaining consistency through innovative state management.

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [8] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: ODC adapts parameter servers into FSDP for LLM post-training with imbalanced sequence lengths, replacing collective communication with point-to-point to reduce synchronization barriers and improve throughput.


<details>
  <summary>Details</summary>
Motivation: Large language model post-training suffers from high variance in sequence lengths, creating imbalanced workloads where collective communication in FSDP causes synchronization barriers and device under-utilization.

Method: ODC adapts parameter servers into FSDP by replacing collective all-gather and reduce-scatter with direct point-to-point communication, reducing synchronization frequency and decoupling device workloads.

Result: ODC consistently improves device utilization and training throughput across diverse LLM post-training tasks, achieving up to 36% speedup over standard FSDP.

Conclusion: ODC is a superior fit for imbalanced workloads in LLM post-training, demonstrating that parameter server paradigms remain relevant for modern training scenarios with workload imbalance.

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


### [9] [Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization](https://arxiv.org/abs/2601.19563)
*Juan Zhu,Zixin Wang,Shenghui Song,Jun Zhang,Khaled Ben Letaief*

Main category: cs.DC

TL;DR: A microservice-based framework for deploying foundation models at the edge with two-tier service placement: static core services and dynamic light services to ensure QoS under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Cloud-centric deployment of foundation models lacks real-time responsiveness and compromises privacy, while edge deployment faces resource limitations and network uncertainty.

Method: Two-tier microservice framework exploiting functional asymmetry: core services placed statically via network-aware integer programming with sparsity constraints; light services orchestrated dynamically using effective capacity theory and Lyapunov optimization.

Result: Achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as system load scales.

Conclusion: The proposed framework bridges the gap between cloud-centric FM deployment and edge limitations by providing probabilistic latency guarantees and fault-tolerant backbone for real-time applications.

Abstract: Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [Configurable p-Neurons Using Modular p-Bits](https://arxiv.org/abs/2601.18943)
*Saleh Bunaiyan,Mohammad Alsharif,Abdelrahman S. Abdelrahman,Hesham ElSawy,Suraj S. Cheema,Suhaib A. Fahmy,Kerem Y. Camsari,Feras Al-Dirini*

Main category: cs.ET

TL;DR: The paper introduces a modular p-bit design that decouples stochastic and data paths, enabling configurable probabilistic activation functions like Logistic Sigmoid, Tanh, and ReLU, with spintronic designs and FPGA implementation showing 10x hardware savings.


<details>
  <summary>Details</summary>
Motivation: Current p-bit implementations in neural networks are limited to sigmoidal probabilistic activation functions, while many other probabilistic activation functions remain unexplored. The authors aim to create a more flexible p-bit architecture that can support various configurable probabilistic activation functions.

Method: The authors re-engineer the p-bit by decoupling its stochastic signal path from its input data path, creating a modular p-bit architecture. This enables realization of probabilistic neurons (p-neurons) with configurable probabilistic activation functions. They present spintronic designs (CMOS + sMTJ) and implement digital-CMOS versions on FPGA with stochastic unit sharing.

Result: The modular p-bit design shows wide and tunable probabilistic ranges of operation. FPGA implementation demonstrates an order of magnitude (10x) saving in required hardware resources compared to conventional digital p-bit implementations.

Conclusion: The proposed modular p-bit architecture successfully enables flexible probabilistic activation functions beyond traditional sigmoidal ones, with significant hardware efficiency improvements through stochastic unit sharing, making probabilistic neural networks more practical and versatile for implementation.

Abstract: Probabilistic bits (p-bits) have recently been employed in neural networks (NNs) as stochastic neurons with sigmoidal probabilistic activation functions. Nonetheless, there remain a wealth of other probabilistic activation functions that are yet to be explored. Here we re-engineer the p-bit by decoupling its stochastic signal path from its input data path, giving rise to a modular p-bit that enables the realization of probabilistic neurons (p-neurons) with a range of configurable probabilistic activation functions, including a probabilistic version of the widely used Logistic Sigmoid, Tanh and Rectified Linear Unit (ReLU) activation functions. We present spintronic (CMOS + sMTJ) designs that show wide and tunable probabilistic ranges of operation. Finally, we experimentally implement digital-CMOS versions on an FPGA, with stochastic unit sharing, and demonstrate an order of magnitude (10x) saving in required hardware resources compared to conventional digital p-bit implementations.

</details>


### [11] [Enabling SSI-Compliant Use of EUDI Wallet Credentials through Trusted Execution Environment and Zero-Knowledge Proof](https://arxiv.org/abs/2601.19893)
*Nacereddine Sitouah,Francesco Bruschi,Stefano De Cillis*

Main category: cs.ET

TL;DR: This paper proposes an architecture to bridge the gap between Italy's eIDAS-compliant digital wallet (IO app) and Self-Sovereign Identity (SSI) principles using Trusted Execution Environments and Zero-Knowledge Proofs.


<details>
  <summary>Details</summary>
Motivation: The eIDAS 2.0 implementation and European Digital Identity Wallet (EUDIW) are centralized and user-centric rather than truly self-sovereign, diverging from SSI principles despite SSI's promise of decentralized, user-controlled digital identity.

Method: The paper proposes an architecture that enables the use of Italian Wallet (IO app) credentials and services in an SSI-compliant environment through the use of Trusted Execution Environments (TEEs) and Zero-Knowledge Proofs (ZKPs).

Result: The proposed architecture aims to bridge the gap between eIDAS-compliant systems and SSI principles, allowing for the use of government-issued digital credentials while maintaining true self-sovereignty through decentralized control.

Conclusion: The paper presents a technical solution to reconcile eIDAS compliance with SSI principles, enabling Italy's digital identity infrastructure to operate within a truly self-sovereign framework while maintaining security and legal protections.

Abstract: The passing of the eIDAS amendment marks an important milestone for EU countries and changes how they must manage digital credentials for both public services and businesses. Italy has led in adopting eIDAS, first with CIE and SPID identity schemes, and now with the Italian Wallet (IO app) aligned to eIDAS 2.0. Self-Sovereign Identity (SSI) is a decentralized model born from the success of Distributed Ledgers, giving individuals full control over their digital identity. The current eIDAS 2.0 and its implementation acts diverge from SSI principles, rendering the European Digital Identity Wallet (EUDIW) centralized and merely user-centric, prioritizing security and legal protection over true self-sovereignty.
  This paper proposes an architecture that enables the use of IT Wallet credentials and services in an SSI-compliant environment through Trusted Execution Environments and Zero-Knowledge Proofs.

</details>
