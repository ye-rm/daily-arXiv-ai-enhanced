<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks](https://arxiv.org/abs/2510.23638)
*Songyuan Li,Teng Wang,Jinrong Tang,Ruiqi Liu,Yuyao Lu,Feng Xu,Bin Gao,Xiangwei Zhu*

Main category: cs.ET

TL;DR: KANalogue implements fully analog Kolmogorov-Arnold Networks using negative differential resistance devices as physical basis functions, achieving competitive accuracy with minimal parameters while enabling energy-efficient analog machine learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the bottleneck of nonlinear activation functions in analog neural computation, which often require digital or hybrid solutions, by creating a fully analog implementation using physical device characteristics.

Method: Leverage negative differential resistance characteristics of tunnel diodes from NbSi2N4/HfSi2N4 heterostructures to construct coordinate-wise nonlinearities, extract I-V data from fabricated devices, fit high-order polynomials to emulate diode behavior, and train KANs on vision benchmarks using these learned basis functions.

Result: KANalogue can approximate complex functions with minimal parameters while maintaining classification accuracy competitive with digital baselines, demonstrating the feasibility of fully analog neural computation.

Conclusion: This work successfully bridges device-level physics and function approximation theory, providing a pathway toward scalable, energy-efficient analog machine learning systems through physical realization of learnable basis functions.

Abstract: Achieving fully analog neural computation requires hardware that can natively
implement both linear and nonlinear operations with high efficiency. While
analogue matrix-vector multiplication has advanced via compute-in-memory
architectures, nonlinear activation functions remain a bottleneck, often
requiring digital or hybrid solutions. Inspired by the Kolmogorov-Arnold
framework, we propose KANalogue, a fully analogue implementation of
Kolmogorov-Arnold Networks (KANs) using negative differential resistance
devices as physical realizations of learnable univariate basis functions. By
leveraging the intrinsic negative differential resistance characteristics of
tunnel diodes fabricated from NbSi2N4/HfSi2N4 heterostructures, we construct
coordinate-wise nonlinearities with distinct curvature and support profiles. We
extract I-V data from fabricated armchair and zigzag devices, fit high-order
polynomials to emulate diode behavior in software, and train KANs on vision
benchmarks using these learned basis functions. Our results demonstrate that
KANalogue can approximate complex functions with minimal parameters while
maintaining classification accuracy competitive with digital baselines. This
work bridges device-level physics and function approximation theory, charting a
path toward scalable, energy-efficient analogue machine learning systems.

</details>


### [2] [Evaluating Fitness Averaging Strategies in Cooperative NeuroCoEvolution for Automated Soft Actuator Design](https://arxiv.org/abs/2510.24510)
*Hugo Alcaraz-Herrera,Michail-Antisthenis Tsompanas,Igor Balaz,Andrew Adamatzky*

Main category: cs.ET

TL;DR: This paper presents a cooperative NeuroCoEvolution approach using CPPN-NEAT to automatically design soft robot morphologies and controllers, showing superior performance over previous methods with reduced computational effort.


<details>
  <summary>Details</summary>
Motivation: Soft robotics face challenges in designing morphologies and controllers due to non-linear material properties. The field's short history lacks sufficient knowledge for optimal solutions, necessitating automated design processes.

Method: Uses cooperative NeuroCoEvolution of CPPN networks representing soft robot actuators. Evolves both morphologies and controllers using CPPN-NEAT, with various averaging methods tested for fitness evaluation.

Result: CPPN-NEAT produces superior morphologies compared to multi-objective optimization with reduced computational effort. Best configuration uses CoEvolution with two best individuals and weighted mean fitness averaging.

Conclusion: The proposed automated design approach effectively addresses soft robot morphology and controller optimization challenges, demonstrating improved performance and efficiency in drug delivery system applications.

Abstract: Soft robotics are increasingly favoured in specific applications such as
healthcare, due to their adaptability, which stems from the non-linear
properties of their building materials. However, these properties also pose
significant challenges in designing the morphologies and controllers of soft
robots. The relatively short history of this field has not yet produced
sufficient knowledge to consistently derive optimal solutions. Consequently, an
automated process for the design of soft robot morphologies can be extremely
helpful. This study focusses on the cooperative NeuroCoEvolution of networks
that are indirect representations of soft robot actuators. Both the
morphologies and controllers represented by Compositional Pattern Producing
Networks are evolved using the well-established method NeuroEvolution of
Augmented Topologies (CPPN-NEAT). The CoEvolution of controllers and
morphologies is implemented using the top n individuals from the cooperating
population, with various averaging methods tested to determine the fitness of
the evaluated individuals. The test-case application for this research is the
optimisation of a soft actuator for a drug delivery system. The primary metric
used is the maximum displacement of one end of the actuator in a specified
direction. Additionally, the robustness of the evolved morphologies is assessed
against a range of randomly generated controllers to simulate potential noise
in real-world applications. The results of this investigation indicate that
CPPN-NEAT produces superior morphologies compared to previously published
results from multi-objective optimisation, with reduced computational effort
and time. Moreover, the best configuration is found to be CoEvolution with the
two best individuals from the cooperative population and the averaging of their
fitness using the weighted mean method.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: SlowPoke is a lightweight hardware-aware framework for detecting fail-slow failures in many-core architectures, achieving 86.77% accuracy with 115.9x storage reduction.


<details>
  <summary>Details</summary>
Motivation: Many-core architectures suffer from fail-slow failures that undermine performance, but existing distributed system methods are unsuitable due to memory constraints and inability to track failures across hardware topology.

Method: Combines compiler-based instrumentation for low-overhead monitoring, on-the-fly trace compression to operate within kilobytes of memory, and a novel topology-aware ranking algorithm to pinpoint failure root causes.

Result: Reduces storage overhead by 115.9x average, achieves 86.77% detection accuracy with 12.11% false positive rate, and scales effectively across different many-core architectures.

Conclusion: SlowPoke provides a practical, scalable solution for on-chip fail-slow detection in large-scale many-core deployments.

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>


### [4] [Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators](https://arxiv.org/abs/2510.24113)
*Arnav Shukla,Harsh Sharma,Srikant Bharadwaj,Vinayak Abrol,Sujay Deb*

Main category: cs.AR

TL;DR: PARL is a reinforcement learning-based topology generator that optimizes Network-on-Interposer designs for chiplet systems to reduce memory-driven contention and meet SLAs while maintaining throughput.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous chiplet systems face latency issues from memory-driven transfers between HBM/DRAM and CPUs/GPUs, causing tail latency violations and SLA failures in traditional NoI topologies.

Method: Developed PARL (Partition-Aware Reinforcement Learner) that formulates NoI synthesis as multi-objective optimization, using an Interference Score to quantify worst-case slowdown and balance throughput, latency, and power.

Result: PARL-generated topologies reduce contention at memory interfaces, meet SLAs, cut worst-case slowdown to 1.2x, and maintain competitive mean throughput compared to link-rich meshes.

Conclusion: The approach reframes NoI design for heterogeneous chiplet accelerators with workload-aware optimization objectives to address memory-driven performance bottlenecks.

Abstract: Heterogeneous chiplet-based systems improve scaling by disag-gregating
CPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package
disaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe
that in modern large-modelinference, parameters and activations routinely move
backand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.
These memory-driven transfers inflate tail latency andviolate Service Level
Agreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this
gap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown
under contention.We then formulate NoI synthesis as a multi-objective
optimization(MOO) problem. We develop PARL (Partition-Aware
ReinforcementLearner), a topology generator that balances throughput,
latency,and power. PARL-generated topologies reduce contention at the memory
cut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining
competitive mean throughput relative to link-rich meshes. Overall, this
reframes NoI design for heterogeneouschiplet accelerators with workload-aware
objectives.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: Analysis of VM scheduling in SAP's cloud platform reveals significant resource inefficiencies including CPU contention, imbalanced hosts, and overprovisioning, leading to requirements for improved scheduling algorithms.


<details>
  <summary>Details</summary>
Motivation: To identify and analyze resource allocation inefficiencies in large-scale enterprise cloud infrastructure using real-world data from SAP's production environment.

Method: Analyzed observational data from 1,800 hypervisors and 48,000 VMs over 30 days using fine-grained time-series telemetry data from SAP S/4HANA and general-purpose applications.

Result: Found multiple suboptimal scheduling situations: CPU resource contention >40%, CPU ready times up to 220 seconds, imbalanced hosts with 99% CPU utilization, and over 80% of VMs using <70% of provided resources.

Conclusion: Identified requirements for novel placement and scheduling algorithms to optimize resource allocations, and made the dataset publicly available for future research on large-scale cloud infrastructure scheduling.

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [6] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: A high-performance GPU-optimized compressible reacting flow solver that addresses memory access, workload balancing, and multi-GPU distribution challenges for combustion simulations, achieving 2-5x performance improvements with near-ideal scaling.


<details>
  <summary>Details</summary>
Motivation: High-speed chemically active flows face computational challenges due to disparate scales and stiff chemistry dominating simulation time, while existing GPU-based solvers have limitations in memory management, load balancing, and handling localized chemical reactions.

Method: Built on AMReX framework with column-major storage optimization, bulk-sparse integration for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. Adapts matrix-based chemical kinetics formulations to multigrid contexts.

Result: Demonstrated 2-5x performance improvements over initial GPU implementations with near-ideal weak scaling across 1-96 NVIDIA H100 GPUs. Roofline analysis shows substantial improvements in arithmetic intensity for convection (~10x) and chemistry (~4x) routines.

Conclusion: The solver efficiently utilizes GPU memory bandwidth and computational resources, providing a scalable solution for high-speed chemically active flow simulations with significant performance gains.

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [7] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: The SPACE Center of Excellence optimizes three astrophysical simulation codes (gPLUTO, OpenGadget3, iPIC3D) for exascale computing, achieving 80% scalability up to 1,024 GPUs on the Leonardo system.


<details>
  <summary>Details</summary>
Motivation: To enable large-scale astrophysical, cosmological, and space plasma simulations by developing and redesigning numerical codes for existing and next-generation accelerators in the exascale era.

Method: Collaboration between scientists, code developers, and HPC experts; using profiling tools to analyze performance on single and multiple nodes of the Leonardo system at CINECA.

Result: All three flagship codes scale efficiently, reaching 80% scalability up to 1,024 GPUs in preliminary tests.

Conclusion: The SPACE-CoE strategy successfully optimizes astrophysical simulation codes for exascale computing, demonstrating efficient scaling on modern GPU architectures.

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [8] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: CoMPSeT is a tool that helps understand and compare different features of Multiparty Session Types (MPST) by providing mechanisms to combine features, animate semantics, and analyze concrete examples.


<details>
  <summary>Details</summary>
Motivation: Concurrent systems are complex to design, and while choreographic languages like MPST help describe global interaction protocols, the many variations with specific features make it difficult to understand and compare them.

Method: The tool selects representative MPST examples and provides mechanisms to combine different features, animate semantics, and compare concrete examples. It's open-source, compiled to JavaScript, and browser-executable.

Result: CoMPSeT provides clearer insights into different MPST features and becomes useful for both researchers wanting to understand the MPST landscape and teachers explaining global choreographies.

Conclusion: CoMPSeT successfully addresses the complexity of MPST variations by offering an accessible tool that enables better understanding and comparison of different MPST features through interactive examples and semantic animations.

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


### [9] [ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery](https://arxiv.org/abs/2510.24452)
*Xi Cheng,Weijie Shen,Haoming Chen,Chaoyi Shen,Jean Ortega,Jiashang Liu,Steve Thomas,Honglin Zheng,Haoyun Wu,Yuxiang Li,Casey Lichtendahl,Jenny Ortiz,Gang Liu,Haiyang Qi,Omid Fatemieh,Chris Fry,Jing Jing Long*

Main category: cs.DC

TL;DR: ARIMA_PLUS is a novel framework for time series forecasting and anomaly detection that combines accurate interpretable models with scalable cloud infrastructure, achieving superior performance on benchmarks and handling 100M time series in 1.5 hours.


<details>
  <summary>Details</summary>
Motivation: Address two key challenges: (1) efficiently forecasting/detecting anomalies in large volumes automatically, and (2) ensuring interpretability of results to incorporate business insights effectively.

Method: Uses sequential modular structure to handle holiday effects, seasonality, trend, and anomalies. Built into Google BigQuery with SQL interface, automating data cleaning and model selection. Novel enhancements to each module with unified framework for both forecasting and anomaly detection.

Result: Superior performance on 42 Monash forecasting datasets over statistical alternatives (ETS, ARIMA, TBATS, Prophet) and neural networks (DeepAR, N-BEATS, PatchTST, TimeMixer). Scales to 100M time series in 1.5 hours with throughput >18,000 time series per second.

Conclusion: ARIMA_PLUS successfully addresses both efficiency and interpretability challenges through its unique combination of accurate models and scalable cloud infrastructure, demonstrating practical value for industrial applications.

Abstract: Time series forecasting and anomaly detection are common tasks for
practitioners in industries such as retail, manufacturing, advertising and
energy. Two unique challenges stand out: (1) efficiently and accurately
forecasting time series or detecting anomalies in large volumes automatically;
and (2) ensuring interpretability of results to effectively incorporate
business insights. We present ARIMA_PLUS, a novel framework to overcome these
two challenges by a unique combination of (a) accurate and interpretable time
series models and (b) scalable and fully managed system infrastructure. The
model has a sequential and modular structure to handle different components of
the time series, including holiday effects, seasonality, trend, and anomalies,
which enables high interpretability of the results. Novel enhancements are made
to each module, and a unified framework is established to address both
forecasting and anomaly detection tasks simultaneously. In terms of accuracy,
its comprehensive benchmark on the 42 public datasets in the Monash forecasting
repository shows superior performance over not only well-established
statistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer
neural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms
of infrastructure, it is directly built into the query engine of BigQuery in
Google Cloud. It uses a simple SQL interface and automates tedious
technicalities such as data cleaning and model selection. It automatically
scales with managed cloud computational and storage resources, making it
possible to forecast 100 million time series using only 1.5 hours with a
throughput of more than 18000 time series per second. In terms of
interpretability, we present several case studies to demonstrate time series
insights it generates and customizability it offers.

</details>
