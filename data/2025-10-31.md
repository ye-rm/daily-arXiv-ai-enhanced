<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry](https://arxiv.org/abs/2510.26008)
*Ziji Chen,Steven Chien,Peng Qian,Noa Zilberman*

Main category: cs.PF

TL;DR: System-X is a hardware-centric approach for ML system optimization that uses only hardware signals to detect anomalies, eliminating the need for workload knowledge and achieving performance improvements like 5.97% acceleration for DeepSeek model.


<details>
  <summary>Details</summary>
Motivation: Cloud ML platforms use virtualization which prevents operators from understanding user workloads, hindering resource optimization and cost efficiency. Traditional approaches require workload knowledge which operators lack.

Method: Proposes System-X that relies solely on hardware signals accessible to operators. Uses unsupervised learning pipeline with low-level system signals collected from analyzing over 30 popular ML models on various hardware platforms.

Result: Successfully identified both network and system configuration issues. Achieved 5.97% acceleration for the DeepSeek model through anomaly detection.

Conclusion: Workload knowledge is unnecessary for system-level optimization. Hardware-centric approach using only accessible hardware signals can effectively detect anomalies and optimize ML system performance without violating user privacy or requiring workload insights.

Abstract: Modern machine learning (ML) has grown into a tightly coupled, full-stack
ecosystem that combines hardware, software, network, and applications. Many
users rely on cloud providers for elastic, isolated, and cost-efficient
resources. Unfortunately, these platforms as a service use virtualization,
which means operators have little insight into the users' workloads. This
hinders resource optimizations by the operator, which is essential to ensure
cost efficiency and minimize execution time. In this paper, we argue that
workload knowledge is unnecessary for system-level optimization. We propose
System-X, which takes a \emph{hardware-centric} approach, relying only on
hardware signals -- fully accessible by operators. Using low-level signals
collected from the system, System-X detects anomalies through an unsupervised
learning pipeline. The pipeline is developed by analyzing over 30 popular ML
models on various hardware platforms, ensuring adaptability to emerging
workloads and unknown deployment patterns. Using System-X, we successfully
identified both network and system configuration issues, accelerating the
DeepSeek model by 5.97%.

</details>


### [2] [Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models](https://arxiv.org/abs/2510.26524)
*Abdelhakim Ziani,András Horváth,Paolo Ballarini*

Main category: cs.PF

TL;DR: A hybrid model combining Bernstein phase-type (BPH) and hyperexponential (HE) distributions is proposed to better approximate heavy-tailed distributions by leveraging BPH's strength in body regions and HE's adaptability to tails.


<details>
  <summary>Details</summary>
Motivation: Heavy-tailed distributions are common in real-world applications but challenging to model accurately. BPH distributions handle body regions well but struggle with heavy tails, while HE models adapt to tails but perform poorly in body regions and are sensitive to initial parameters.

Method: The authors propose a novel hybrid model combining BPH and HE distributions, using optimization to set initial parameters for the HE component to enhance robustness and prevent invalid models.

Result: Experimental validation shows the hybrid approach outperforms individual BPH or HE models, capturing both body and tail regions of heavy-tailed distributions with improved matching of mean and coefficient of variation. Queuing theory experiments confirm practical usefulness and accuracy.

Conclusion: The hybrid BPH-HE model provides a more effective solution for approximating heavy-tailed distributions by combining the strengths of both approaches while mitigating their individual weaknesses.

Abstract: Heavy-tailed distributions, prevalent in a lot of real-world applications
such as finance, telecommunications, queuing theory, and natural language
processing, are challenging to model accurately owing to their slow tail decay.
Bernstein phase-type (BPH) distributions, through their analytical tractability
and good approximations in the non-tail region, can present a good solution,
but they suffer from an inability to reproduce these heavy-tailed behaviors
exactly, thus leading to inadequate performance in important tail areas. On the
contrary, while highly adaptable to heavy-tailed distributions,
hyperexponential (HE) models struggle in the body part of the distribution.
Additionally, they are highly sensitive to initial parameter selection,
significantly affecting their precision.
  To solve these issues, we propose a novel hybrid model of BPH and HE
distributions, borrowing the most desirable features from each for enhanced
approximation quality. Specifically, we leverage an optimization to set initial
parameters for the HE component, significantly enhancing its robustness and
reducing the possibility that the associated procedure results in an invalid HE
model. Experimental validation demonstrates that the novel hybrid approach is
more performant than individual application of BPH or HE models. More
precisely, it can capture both the body and the tail of heavy-tailed
distributions, with a considerable enhancement in matching parameters such as
mean and coefficient of variation. Additional validation through experiments
utilizing queuing theory proves the practical usefulness, accuracy, and
precision of our hybrid approach.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [CHIPSIM: A Co-Simulation Framework for Deep Learning on Chiplet-Based Systems](https://arxiv.org/abs/2510.25958)
*Lukas Pfromm,Alish Kanani,Harsh Sharma,Janardhan Rao Doppa,Partha Pratim Pande,Umit Y. Ogras*

Main category: cs.AR

TL;DR: CHIPSIM is a co-simulation framework for chiplet-based systems that accurately models computation and communication for DNN workloads, providing significant accuracy improvements and power/thermal analysis capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional monolithic chips cannot meet the demands of data-intensive applications like DNNs due to manufacturing yield issues. Chiplet-based architectures offer scalable solutions but require accurate simulation tools that current methods lack.

Method: CHIPSIM concurrently models computation and communication, capturing network contention and pipelining effects. It also profiles chiplet and network-on-interposer power consumption at microsecond granularity for precise thermal analysis.

Result: Extensive evaluations show CHIPSIM achieves up to 340% accuracy improvement over conventional simulators. It demonstrates versatility with homogeneous/heterogeneous chiplets and different NoI architectures, while providing power/thermal analysis capabilities.

Conclusion: CHIPSIM provides a comprehensive simulation framework that addresses the accuracy, speed, and flexibility limitations of existing methods for chiplet-based systems, enabling better design and optimization of these architectures for DNN workloads.

Abstract: Due to reduced manufacturing yields, traditional monolithic chips cannot keep
up with the compute, memory, and communication demands of data-intensive
applications, such as rapidly growing deep neural network (DNN) models.
Chiplet-based architectures offer a cost-effective and scalable solution by
integrating smaller chiplets via a network-on-interposer (NoI). Fast and
accurate simulation approaches are critical to unlocking this potential, but
existing methods lack the required accuracy, speed, and flexibility. To address
this need, this work presents CHIPSIM, a comprehensive co-simulation framework
designed for parallel DNN execution on chiplet-based systems. CHIPSIM
concurrently models computation and communication, accurately capturing network
contention and pipelining effects that conventional simulators overlook.
Furthermore, it profiles the chiplet and NoI power consumptions at microsecond
granularity for precise transient thermal analysis. Extensive evaluations with
homogeneous/heterogeneous chiplets and different NoI architectures demonstrate
the framework's versatility, up to 340% accuracy improvement, and power/thermal
analysis capability.

</details>


### [4] [MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator](https://arxiv.org/abs/2510.26463)
*Xiaolin He,Cenlin Duan,Yingjie Qi,Xiao Ma,Jianlei Yang*

Main category: cs.AR

TL;DR: MIREDO is a framework that uses Mixed-Integer Programming to optimize dataflow for Computing-in-Memory architectures, achieving up to 3.2× performance improvement for DNN acceleration.


<details>
  <summary>Details</summary>
Motivation: CIM architectures show promise for DNN acceleration but face challenges in dataflow optimization due to large design spaces and architectural constraints, leading to gaps between theoretical and actual efficiency.

Method: Formulates dataflow optimization as MIP problem with hierarchical hardware abstraction and analytical latency model that captures CIM-specific data transfer behaviors and constraints.

Result: Achieves up to 3.2× performance improvement across various DNN models and hardware setups compared to existing approaches.

Conclusion: MIREDO effectively bridges the efficiency gap in CIM systems by systematically navigating the design space and optimizing dataflow configurations through mathematical programming.

Abstract: Computing-in-Memory (CIM) architectures have emerged as a promising solution
for accelerating Deep Neural Networks (DNNs) by mitigating data movement
bottlenecks. However, realizing the potential of CIM requires specialized
dataflow optimizations, which are challenged by an expansive design space and
strict architectural constraints. Existing optimization approaches often fail
to fully exploit CIM accelerators, leading to noticeable gaps between
theoretical and actual system-level efficiency. To address these limitations,
we propose the MIREDO framework, which formulates dataflow optimization as a
Mixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical
hardware abstraction coupled with an analytical latency model designed to
accurately reflect the complex data transfer behaviors within CIM systems. By
jointly modeling workload characteristics, dataflow strategies, and
CIM-specific constraints, MIREDO systematically navigates the vast design space
to determine the optimal dataflow configurations. Evaluation results
demonstrate that MIREDO significantly enhances performance, achieving up to
$3.2\times$ improvement across various DNN models and hardware setups.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow is a runtime system for Mixture-of-Experts (MoE) inference that uses adaptive expert prefetching and cache-aware routing to reduce latency caused by parameter transfers between host and GPU memory.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional MoE inference approaches that introduce significant latency due to frequent parameter transfers and lack adaptability across different hardware platforms and workloads.

Method: Combines adaptive expert prefetching that continuously adjusts prediction horizon using runtime statistics (transfer bandwidth, parameter dimensionality, model feedback) and hybrid cross-layer prediction that fuses pregating information with intermediate computational states.

Result: Reduces model stall time to less than 0.1% of the baseline, effectively decreasing cache misses and eliminating latency caused by expert swap-ins.

Conclusion: ExpertFlow successfully optimizes MoE inference under stringent memory constraints by adaptively refining prefetching decisions and aligning them with actual usage behavior.

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>
