<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Building Network Digital Twins Part II: Real-Time Adaptive PID for Enhanced State Synchronization](https://arxiv.org/abs/2510.20753)
*John Sengendo,Fabrizio Granelli*

Main category: cs.ET

TL;DR: This paper presents a framework using adaptive PID controllers to improve real-time synchronization between Network Digital Twins and physical mobile networks, addressing challenges in dynamic heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Mobile networks are becoming increasingly dynamic and heterogeneous with massive device connectivity, making real-time synchronization between Network Digital Twins and physical networks challenging. Current NDTs struggle to replicate traffic and maintain synchronization in real-time without interfering with live network operations.

Method: The authors implement a novel framework that integrates an adaptive Proportional-Integral-Derivative (PID) controller to dynamically improve synchronization between Network Digital Twins and physical networks. The framework includes an interactive user interface for monitoring and analysis.

Result: The enhanced approach demonstrates improvement in real-time traffic synchronization between Network Digital Twins and physical networks, as shown through the interactive user interface results.

Conclusion: The adaptive PID controller framework successfully addresses the synchronization challenges in Network Digital Twins for dynamic heterogeneous mobile networks, providing better real-time traffic replication and synchronization capabilities.

Abstract: As we evolve towards more heterogeneous and cutting-edge mobile networks,
Network Digital Twins (NDTs) are proving to be a promising paradigm in solving
challenges faced by network operators, as they give a possibility of
replicating the physical network operations and testing scenarios separately
without interfering with the live network. However, with mobile networks
becoming increasingly dynamic and heterogeneous due to massive device
connectivity, replicating traffic and having NDTs synchronized in real-time
with the physical network remains a challenge, thus necessitating the need to
develop real-time adaptive mechanisms to bridge this gap. In this part II of
our work, we implement a novel framework that integrates an adaptive
Proportional-Integral-Derivative (PID) controller to dynamically improve
synchronization. Additionally, through an interactive user interface, results
of our enhanced approach demonstrate an improvement in real-time traffic
synchronization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: A novel approximate adder design that improves energy and area efficiency while maintaining comparable accuracy, demonstrated through simulations and image processing applications.


<details>
  <summary>Details</summary>
Motivation: To develop energy-efficient hardware for computation-intensive multimedia applications like image, audio, and video processing by balancing performance, computational accuracy, and energy efficiency.

Method: Introduces a new approximate adder design that is more energy- and area-efficient than existing approaches, with digital reconstruction capabilities for high-quality images.

Result: Simulation results show the proposed adder achieves improved or comparable accuracy while being more energy- and area-efficient than existing adders. Successful deployment in image processing tasks demonstrates its practical utility.

Conclusion: The novel approximate adder successfully addresses the trade-off between energy efficiency and computational accuracy, making it suitable for energy-constrained multimedia applications while maintaining high-quality output.

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [3] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: Researchers demonstrate high-throughput true random number generation using simultaneous multiple-row activation (SiMRA) in commercial DRAM chips, achieving up to 1.99x higher throughput than state-of-the-art DRAM-based TRNGs while passing all NIST randomness tests.


<details>
  <summary>Details</summary>
Motivation: To develop a true random number generator (TRNG) with high throughput and low latency using commercial off-the-shelf DRAM chips by exploiting simultaneous multiple-row activation, which could provide better performance than existing DRAM-based TRNG solutions.

Method: Extensive characterization of 96 DDR4 DRAM chips using SiMRA with varying numbers of simultaneously activated rows (2, 4, 8, 16, 32), different data patterns, temperature levels, and spatial variations. Quality evaluation using NIST statistical test suite.

Result: All SiMRA-based TRNG designs passed NIST randomness tests. Throughput improvements: 2-row (1.15x), 8-row (1.99x), 16-row (1.82x), 32-row (1.39x) over state-of-the-art. Entropy increases with more activated rows (32-row has 2.51x higher entropy than 2-row). Temperature affects entropy (50°C to 90°C decreases entropy by 1.53x for 32-row).

Conclusion: SiMRA enables high-throughput, low-latency true random number generation in commercial DRAM chips, with performance scaling with the number of simultaneously activated rows. Environmental factors like temperature significantly impact entropy, and the infrastructure is open-sourced for future research.

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [4] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire is a general-purpose accelerator for dependency-bound HPC kernels that uses low-power in-order cores with fast communication and direct L2 cache access, achieving up to 7.64× speedup and 56% energy reduction with minimal area overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional accelerators struggle with fine-grain parallelism in dependency-bound kernels due to limitations in handling complex data-dependency patterns and synchronization overheads, while custom FPGA/ASIC designs are expensive and inflexible.

Method: Each Squire accelerator has multiple low-power in-order cores that can rapidly communicate and directly access L2 cache data. One accelerator is integrated per core in a multicore system, enabling acceleration of dependency-bound kernels with minimal software changes.

Result: Squire achieves speedups up to 7.64× in dynamic programming kernels and 3.66× acceleration for end-to-end applications. It reduces energy consumption by up to 56% with only 10.5% area overhead compared to Neoverse-N1 baseline.

Conclusion: Squire effectively exploits fine-grain parallelism in dependency-bound HPC kernels, providing significant performance and energy efficiency improvements with minimal hardware overhead and software changes.

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: This paper provides simplified proofs for distributed graph algorithm lower bounds using a new round elimination technique, extending previous maximal matching results to b-matching and edge coloring problems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to simplify the complex 25+ page proof by Khoury and Schild for maximal matching lower bounds, making the round elimination via self-reduction technique more accessible and generalizable to other problems.

Method: The paper presents a simplified version of the round elimination via self-reduction technique, which is then applied to prove lower bounds for maximal b-matching and edge coloring problems in the LOCAL model.

Result: The authors prove: (1) Ω(min{log_{1+b}Δ, log_Δ n}) and Ω(√log_{1+b} n) round lower bounds for maximal b-matching; (2) Ω(min{log Δ, log_Δ n}) and Ω(√log n) round lower bounds for edge coloring with Δ + k colors (for k ≤ Δ^{1-ε}).

Conclusion: The simplified round elimination technique successfully generalizes previous maximal matching lower bounds to more complex problems like b-matching and edge coloring, making these fundamental results more accessible to the research community.

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [6] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: AsyncHZP is an asynchronous variant of ZeRO that improves training efficiency through adaptive parameter sharding and multi-stream scheduling, outperforming traditional parallel methods while maintaining simplicity.


<details>
  <summary>Details</summary>
Motivation: Current approaches like ND parallelism are complex, while ZeRO suffers from communication overhead, creating bottlenecks in large-scale language model training.

Method: AsyncHZP uses adaptive parameter/gradient/optimizer state sharding across replica groups and multi-stream asynchronous scheduling with background threads for communication-computation overlap.

Result: Empirical evaluations show AsyncHZP maintains robust stability at scale, consistently outperforms ND parallelism, and achieves state-of-the-art performance without complex tuning.

Conclusion: AsyncHZP simplifies efficient large-scale training by reducing communication overhead and maintaining memory efficiency while delivering superior performance.

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [7] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: A full-stack HPC-QC framework integrating quantum computing with high-performance computing through modular hardware/software approaches, enabling hybrid workload development and portable quantum kernel invocation.


<details>
  <summary>Details</summary>
Motivation: To address the growing needs for scalable integration of High Performance Computing (HPC) and Quantum Computing (QC) by creating a unified programming environment that bridges classical and quantum computing capabilities.

Method: Developed a hybrid framework with extensible interfaces for quantum programming, dispatching, and compilation within existing HPC environments. Uses Cray LLVM-based compilation to transform LLVM IR and Quantum IR (QIR) from commercial quantum software frontends. Includes an adaptive circuit knitting hypervisor to partition large quantum circuits.

Result: Successfully demonstrated several hybrid HPC-QC multi-node multi-CPU and GPU workloads on HPE EX supercomputers, including solving linear systems, quantum optimization, and simulating quantum phase transitions. All three developed components showed functional execution viability.

Conclusion: This work provides a framework for a unified quantum-classical programming environment built upon classical HPC software stack, enabling scalable integration of quantum computing capabilities with existing high-performance computing infrastructure.

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [8] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX is a collective communication framework from Meta that optimizes performance for large language models across training and inference, supporting clusters with over 100,000 GPUs and showing significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional communication methods face throughput and latency limitations at the scale of hundreds of thousands of GPUs, hindering the development and deployment of state-of-the-art large language models.

Method: The paper presents the NCCLX collective communication framework engineered to optimize performance across the full LLM lifecycle, from synchronous large-scale training to low-latency inference requirements.

Result: Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency.

Conclusion: This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [9] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS is an auto-scaler for distributed services that combines proactive forecasting with reactive contingency systems to optimize resource scaling, validated to maintain performance requirements over 99% of the time.


<details>
  <summary>Details</summary>
Motivation: Cloud computing's elasticity is crucial for emerging technologies, but existing auto-scalers need better approaches to handle dynamic resource demands while maintaining service level agreements.

Method: FLAS combines proactive forecasting of high-level metrics trends with a reactive system that estimates high-level metrics from resource usage, requiring less instrumentation and being application-agnostic.

Result: The system was validated through multiple test cases including worst-case scenarios using Boundary-Value Analysis methodology, demonstrating performance requirement compliance over 99% of the time.

Conclusion: FLAS represents the first auto-scaling system specifically designed for content-based publish-subscribe distributed systems while being generic enough for any distributed service, effectively maintaining SLA compliance through hybrid proactive-reactive scaling.

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [10] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: A methodology for building performance predictors in edge environments that achieves 90% accuracy with inference time under 1% of RTT, using historical monitoring metrics and evaluated in dynamic co-location scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurate performance prediction is critical for scheduling and resource management in resource-constrained dynamic edge environments, where predictable performance is challenging due to application co-location and node heterogeneity.

Method: Proposes a methodology that automatically builds and assesses various performance predictors, prioritizing both accuracy and inference time to identify the most efficient model. Predictors are trained on historical state of correlated monitoring metrics and evaluated across multiple servers in dynamic co-location scenarios.

Result: Predictors achieve up to 90% accuracy while maintaining inference time of less than 1% of Round Trip Time. The approach is demonstrated using electron microscopy workflows with stringent real-time demands.

Conclusion: Emphasizes the need for systematic methodology that selects server-specific predictors by jointly optimizing accuracy and inference latency in dynamic co-location scenarios, which can improve resource utilization and result in predictable performance when integrated into edge environments.

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [11] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: This paper develops lightweight RTT predictors for performance-aware load balancing in edge/cloud environments, achieving 95% accuracy with minimal overhead to reduce application latency and resource waste.


<details>
  <summary>Details</summary>
Motivation: Traditional load-balancing strategies are reactive and use outdated metrics, leading to suboptimal routing decisions and increased tail latencies in resource-constrained edge and cloud environments.

Method: Developed lightweight RTT predictors trained on time-series monitoring data from Kubernetes-managed GPU clusters, using a reduced set of highly correlated metrics to maintain low overhead while adapting to diverse co-location scenarios.

Result: Predictors achieved up to 95% accuracy with prediction delay within 10% of application RTT. Identified minimum accuracy thresholds and key system factors for effective deployment. Simulation showed significant reduction in application RTT and resource waste.

Conclusion: Performance-aware load balancing using RTT predictors is feasible for future production systems, offering substantial improvements in latency reduction and resource efficiency.

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>
