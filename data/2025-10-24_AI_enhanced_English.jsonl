{"id": "2510.20137", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20137", "abs": "https://arxiv.org/abs/2510.20137", "authors": ["Hasnain A. Ziad", "Ashiq A. Sakib"], "title": "HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application", "comment": "5 Pages, 6 Figures, and 1 Table", "summary": "The design of approximate adders has been widely researched to advance\nenergy-efficient hardware for computation-intensive multimedia applications,\nsuch as image, audio, or video processing. The design of approximate adders has\nbeen widely researched to advance energy-efficient hardware for computation\nintensive multimedia applications, such as image/audio/video processing.\nSeveral static and dynamic approximate adders exist in the literature, each of\nwhich endeavors to balance the conflicting demands of high performance,\ncomputational accuracy, and energy efficiency. This work introduces a novel\napproximate adder that is more energy- and area-efficient than existing adders,\nwhile achieving improved or comparable accuracy, as demonstrated by simulation\nresults. The proposed adder's ability to digitally reconstruct high quality\nimages is further demonstrated by the deployment of the design for an image\nprocessing task.", "AI": {"tldr": "A novel approximate adder design that improves energy and area efficiency while maintaining comparable accuracy, demonstrated through simulations and image processing applications.", "motivation": "To develop energy-efficient hardware for computation-intensive multimedia applications like image, audio, and video processing by balancing performance, computational accuracy, and energy efficiency.", "method": "Introduces a new approximate adder design that is more energy- and area-efficient than existing approaches, with digital reconstruction capabilities for high-quality images.", "result": "Simulation results show the proposed adder achieves improved or comparable accuracy while being more energy- and area-efficient than existing adders. Successful deployment in image processing tasks demonstrates its practical utility.", "conclusion": "The novel approximate adder successfully addresses the trade-off between energy efficiency and computational accuracy, making it suitable for energy-constrained multimedia applications while maintaining high-quality output."}}
{"id": "2510.20269", "categories": ["cs.AR", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20269", "abs": "https://arxiv.org/abs/2510.20269", "authors": ["Ismail Emir Yuksel", "Ataberk Olgun", "F. Nisa Bostanci", "Oguzhan Canpolat", "Geraldo F. Oliveira", "Mohammad Sadrosadati", "Abdullah Giray Yaglikci", "Onur Mutlu"], "title": "In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips", "comment": "Extended version of our publication at the 43rd IEEE International\n  Conference on Computer Design (ICCD-43), 2025", "summary": "In this work, we experimentally demonstrate that it is possible to generate\ntrue random numbers at high throughput and low latency in commercial\noff-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row\nactivation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We\nrigorously analyze SiMRA's true random generation potential in terms of\nentropy, latency, and throughput for varying numbers of simultaneously\nactivated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature\nlevels, and spatial variations. Among our 11 key experimental observations, we\nhighlight four key results. First, we evaluate the quality of our TRNG designs\nusing the commonly-used NIST statistical test suite for randomness and find\nthat all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,\n16-, and 32-row activation-based TRNG designs outperform the state-of-theart\nDRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,\nrespectively. Third, SiMRA's entropy tends to increase with the number of\nsimultaneously activated DRAM rows. Fourth, operational parameters and\nconditions (e.g., data pattern and temperature) significantly affect entropy.\nFor example, for most of the tested modules, the average entropy of 32-row\nactivation is 2.51x higher than that of 2-row activation. For example,\nincreasing the temperature from 50{\\deg}C to 90{\\deg}C decreases SiMRA's\nentropy by 1.53x for 32-row activation. To aid future research and development,\nwe open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.", "AI": {"tldr": "Researchers demonstrate high-throughput true random number generation using simultaneous multiple-row activation (SiMRA) in commercial DRAM chips, achieving up to 1.99x higher throughput than state-of-the-art DRAM-based TRNGs while passing all NIST randomness tests.", "motivation": "To develop a true random number generator (TRNG) with high throughput and low latency using commercial off-the-shelf DRAM chips by exploiting simultaneous multiple-row activation, which could provide better performance than existing DRAM-based TRNG solutions.", "method": "Extensive characterization of 96 DDR4 DRAM chips using SiMRA with varying numbers of simultaneously activated rows (2, 4, 8, 16, 32), different data patterns, temperature levels, and spatial variations. Quality evaluation using NIST statistical test suite.", "result": "All SiMRA-based TRNG designs passed NIST randomness tests. Throughput improvements: 2-row (1.15x), 8-row (1.99x), 16-row (1.82x), 32-row (1.39x) over state-of-the-art. Entropy increases with more activated rows (32-row has 2.51x higher entropy than 2-row). Temperature affects entropy (50\u00b0C to 90\u00b0C decreases entropy by 1.53x for 32-row).", "conclusion": "SiMRA enables high-throughput, low-latency true random number generation in commercial DRAM chips, with performance scaling with the number of simultaneously activated rows. Environmental factors like temperature significantly impact entropy, and the infrastructure is open-sourced for future research."}}
{"id": "2510.20400", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20400", "abs": "https://arxiv.org/abs/2510.20400", "authors": ["Rub\u00e9n Langarita", "Jes\u00fas Alastruey-Bened\u00e9", "Pablo Ib\u00e1\u00f1ez-Mar\u00edn", "Santiago Marco-Sola", "Miquel Moret\u00f3", "Adri\u00e0 Armejach"], "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels", "comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25", "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.", "AI": {"tldr": "Squire is a general-purpose accelerator for dependency-bound HPC kernels that uses low-power in-order cores with fast communication and direct L2 cache access, achieving up to 7.64\u00d7 speedup and 56% energy reduction with minimal area overhead.", "motivation": "Traditional accelerators struggle with fine-grain parallelism in dependency-bound kernels due to limitations in handling complex data-dependency patterns and synchronization overheads, while custom FPGA/ASIC designs are expensive and inflexible.", "method": "Each Squire accelerator has multiple low-power in-order cores that can rapidly communicate and directly access L2 cache data. One accelerator is integrated per core in a multicore system, enabling acceleration of dependency-bound kernels with minimal software changes.", "result": "Squire achieves speedups up to 7.64\u00d7 in dynamic programming kernels and 3.66\u00d7 acceleration for end-to-end applications. It reduces energy consumption by up to 56% with only 10.5% area overhead compared to Neoverse-N1 baseline.", "conclusion": "Squire effectively exploits fine-grain parallelism in dependency-bound HPC kernels, providing significant performance and energy efficiency improvements with minimal hardware overhead and software changes."}}
{"id": "2510.19972", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.19972", "abs": "https://arxiv.org/abs/2510.19972", "authors": ["Alkida Balliu", "Filippo Casagrande", "Francesco d'Amore", "Dennis Olivetti"], "title": "New Hardness Results for the LOCAL Model via a Simple Self-Reduction", "comment": "21 pages, no figures", "summary": "Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL\nalgorithm that solves maximal matching requires $\\Omega(\\min\\{\\log \\Delta,\n\\log_\\Delta n\\})$ rounds, where $n$ is the number of nodes in the graph and\n$\\Delta$ is the maximum degree. This result is shown through a new technique,\ncalled round elimination via self-reduction. The lower bound proof is beautiful\nand presents very nice ideas. However, it spans more than 25 pages of technical\ndetails, and hence it is hard to digest and generalize to other problems.\nHistorically, the simplification of proofs and techniques has marked an\nimportant turning point in our understanding of the complexity of graph\nproblems. Our paper makes a step forward towards this direction, and provides\nthe following contributions.\n  1. We present a short and simplified version of the round elimination via\nself-reduction technique. The simplification of this technique enables us to\nobtain the following two hardness results.\n  2. We show that any randomized LOCAL algorithm that solves the maximal\n$b$-matching problem requires $\\Omega(\\min\\{\\log_{1+b}\\Delta, \\log_\\Delta n\\})$\nand $\\Omega(\\sqrt{\\log_{1+b} n})$ rounds. We recall that the $b$-matching\nproblem is a generalization of the matching problem where each vertex can have\nup to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain\na short proof for the maximal matching lower bound shown by Khoury and Schild.\n  3. Finally, we show that any randomized LOCAL algorithm that properly colors\nthe edges of a graph with $\\Delta + k$ colors requires $\\Omega(\\min\\{\\log\n\\Delta, \\log_\\Delta n\\})$ and $\\Omega(\\sqrt{\\log n})$ rounds, for any $k\\le\n\\Delta^{1-\\varepsilon}$ and any constant $\\varepsilon > 0$.", "AI": {"tldr": "This paper provides simplified proofs for distributed graph algorithm lower bounds using a new round elimination technique, extending previous maximal matching results to b-matching and edge coloring problems.", "motivation": "The authors aim to simplify the complex 25+ page proof by Khoury and Schild for maximal matching lower bounds, making the round elimination via self-reduction technique more accessible and generalizable to other problems.", "method": "The paper presents a simplified version of the round elimination via self-reduction technique, which is then applied to prove lower bounds for maximal b-matching and edge coloring problems in the LOCAL model.", "result": "The authors prove: (1) \u03a9(min{log_{1+b}\u0394, log_\u0394 n}) and \u03a9(\u221alog_{1+b} n) round lower bounds for maximal b-matching; (2) \u03a9(min{log \u0394, log_\u0394 n}) and \u03a9(\u221alog n) round lower bounds for edge coloring with \u0394 + k colors (for k \u2264 \u0394^{1-\u03b5}).", "conclusion": "The simplified round elimination technique successfully generalizes previous maximal matching lower bounds to more complex problems like b-matching and edge coloring, making these fundamental results more accessible to the research community."}}
{"id": "2510.20753", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.20753", "abs": "https://arxiv.org/abs/2510.20753", "authors": ["John Sengendo", "Fabrizio Granelli"], "title": "Building Network Digital Twins Part II: Real-Time Adaptive PID for Enhanced State Synchronization", "comment": "6 pages, 7 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025", "summary": "As we evolve towards more heterogeneous and cutting-edge mobile networks,\nNetwork Digital Twins (NDTs) are proving to be a promising paradigm in solving\nchallenges faced by network operators, as they give a possibility of\nreplicating the physical network operations and testing scenarios separately\nwithout interfering with the live network. However, with mobile networks\nbecoming increasingly dynamic and heterogeneous due to massive device\nconnectivity, replicating traffic and having NDTs synchronized in real-time\nwith the physical network remains a challenge, thus necessitating the need to\ndevelop real-time adaptive mechanisms to bridge this gap. In this part II of\nour work, we implement a novel framework that integrates an adaptive\nProportional-Integral-Derivative (PID) controller to dynamically improve\nsynchronization. Additionally, through an interactive user interface, results\nof our enhanced approach demonstrate an improvement in real-time traffic\nsynchronization.", "AI": {"tldr": "This paper presents a framework using adaptive PID controllers to improve real-time synchronization between Network Digital Twins and physical mobile networks, addressing challenges in dynamic heterogeneous environments.", "motivation": "Mobile networks are becoming increasingly dynamic and heterogeneous with massive device connectivity, making real-time synchronization between Network Digital Twins and physical networks challenging. Current NDTs struggle to replicate traffic and maintain synchronization in real-time without interfering with live network operations.", "method": "The authors implement a novel framework that integrates an adaptive Proportional-Integral-Derivative (PID) controller to dynamically improve synchronization between Network Digital Twins and physical networks. The framework includes an interactive user interface for monitoring and analysis.", "result": "The enhanced approach demonstrates improvement in real-time traffic synchronization between Network Digital Twins and physical networks, as shown through the interactive user interface results.", "conclusion": "The adaptive PID controller framework successfully addresses the synchronization challenges in Network Digital Twins for dynamic heterogeneous mobile networks, providing better real-time traffic replication and synchronization capabilities."}}
{"id": "2510.20111", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20111", "abs": "https://arxiv.org/abs/2510.20111", "authors": ["Huawei Bai", "Yifan Huang", "Wenqi Shi", "Ansheng You", "Feifan Shao", "Tengfei Han", "Minghui Yu"], "title": "AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training", "comment": "14 pages, 5 figures, tech report", "summary": "The training efficiency and scalability of language models on massive\nclusters currently remain a critical bottleneck. Mainstream approaches like ND\nparallelism are often cumbersome and complex, while flexible alternatives such\nas the Zero Redundancy Optimizer (ZeRO) are frequently hampered by\ncommunication overhead. In this paper, we propose Asynchronous Hierarchical\nZero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to\nachieve superior performance while maintaining simplicity and memory\nefficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding\nthat can lead to inefficient communication, AsyncHZP adaptively reshards\nparameters, gradients, and optimizer states across different replica groups.\nThis strategy optimizes device memory utilization and significantly reduces\ncommunication overhead. In addition, we also design a multi-stream asynchronous\nscheduling method that executes parameter all-gather and gradient\nreduce-scatter operations in dedicated background threads, effectively\noverlapping communication with computation while incurring negligible memory\nfragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)\nmodels confirm that AsyncHZP maintains robust stability at scale. It\nconsistently outperforms classic ND parallelism, achieving state-of-the-art\nperformance without complex strategic tuning, thereby simplifying the path to\nefficient large-scale training.", "AI": {"tldr": "AsyncHZP is an asynchronous variant of ZeRO that improves training efficiency through adaptive parameter sharding and multi-stream scheduling, outperforming traditional parallel methods while maintaining simplicity.", "motivation": "Current approaches like ND parallelism are complex, while ZeRO suffers from communication overhead, creating bottlenecks in large-scale language model training.", "method": "AsyncHZP uses adaptive parameter/gradient/optimizer state sharding across replica groups and multi-stream asynchronous scheduling with background threads for communication-computation overlap.", "result": "Empirical evaluations show AsyncHZP maintains robust stability at scale, consistently outperforms ND parallelism, and achieves state-of-the-art performance without complex tuning.", "conclusion": "AsyncHZP simplifies efficient large-scale training by reducing communication overhead and maintaining memory efficiency while delivering superior performance."}}
{"id": "2510.20128", "categories": ["cs.DC", "quant-ph", "D.2.6"], "pdf": "https://arxiv.org/pdf/2510.20128", "abs": "https://arxiv.org/abs/2510.20128", "authors": ["Xin Zhan", "K. Grace Johnson", "Aniello Esposito", "Barbara Chapman", "Marco Fiorentino", "Kirk M. Bresniker", "Raymond G. Beausoleil", "Masoud Mohseni"], "title": "A Full Stack Framework for High Performance Quantum-Classical Computing", "comment": "9 pages, 8 figures, presented at Cray User Group Meeting 2025, May\n  04-09, 2025, New York, NY", "summary": "To address the growing needs for scalable High Performance Computing (HPC)\nand Quantum Computing (QC) integration, we present our HPC-QC full stack\nframework and its hybrid workload development capability with modular\nhardware/device-agnostic software integration approach. The latest development\nin extensible interfaces for quantum programming, dispatching, and compilation\nwithin existing mature HPC programming environment are demonstrated. Our HPC-QC\nfull stack enables high-level, portable invocation of quantum kernels from\ncommercial quantum SDKs within HPC meta-program in compiled languages (C/C++\nand Fortran) as well as Python through a quantum programming interface library\nextension. An adaptive circuit knitting hypervisor is being developed to\npartition large quantum circuits into sub-circuits that fit on smaller noisy\nquantum devices and classical simulators. At the lower-level, we leverage Cray\nLLVM-based compilation framework to transform and consume LLVM IR and Quantum\nIR (QIR) from commercial quantum software frontends in a retargetable fashion\nto different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU\nand GPU workloads (including solving linear system of equations, quantum\noptimization, and simulating quantum phase transitions) have been demonstrated\non HPE EX supercomputers to illustrate functionality and execution viability\nfor all three components developed so far. This work provides the framework for\na unified quantum-classical programming environment built upon classical HPC\nsoftware stack (compilers, libraries, parallel runtime and process scheduling).", "AI": {"tldr": "A full-stack HPC-QC framework integrating quantum computing with high-performance computing through modular hardware/software approaches, enabling hybrid workload development and portable quantum kernel invocation.", "motivation": "To address the growing needs for scalable integration of High Performance Computing (HPC) and Quantum Computing (QC) by creating a unified programming environment that bridges classical and quantum computing capabilities.", "method": "Developed a hybrid framework with extensible interfaces for quantum programming, dispatching, and compilation within existing HPC environments. Uses Cray LLVM-based compilation to transform LLVM IR and Quantum IR (QIR) from commercial quantum software frontends. Includes an adaptive circuit knitting hypervisor to partition large quantum circuits.", "result": "Successfully demonstrated several hybrid HPC-QC multi-node multi-CPU and GPU workloads on HPE EX supercomputers, including solving linear systems, quantum optimization, and simulating quantum phase transitions. All three developed components showed functional execution viability.", "conclusion": "This work provides a framework for a unified quantum-classical programming environment built upon classical HPC software stack, enabling scalable integration of quantum computing capabilities with existing high-performance computing infrastructure."}}
{"id": "2510.20171", "categories": ["cs.DC", "cs.AI", "cs.NI", "C.2.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.20171", "abs": "https://arxiv.org/abs/2510.20171", "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"], "title": "Collective Communication for 100k+ GPUs", "comment": null, "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales.", "AI": {"tldr": "NCCLX is a collective communication framework from Meta that optimizes performance for large language models across training and inference, supporting clusters with over 100,000 GPUs and showing significant efficiency improvements.", "motivation": "Traditional communication methods face throughput and latency limitations at the scale of hundreds of thousands of GPUs, hindering the development and deployment of state-of-the-art large language models.", "method": "The paper presents the NCCLX collective communication framework engineered to optimize performance across the full LLM lifecycle, from synchronous large-scale training to low-latency inference requirements.", "result": "Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency.", "conclusion": "This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales."}}
{"id": "2510.20388", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20388", "abs": "https://arxiv.org/abs/2510.20388", "authors": ["V\u00edctor Ramp\u00e9rez", "Javier Soriano", "David Lizcano", "Juan A. Lara"], "title": "FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services", "comment": null, "summary": "Cloud computing has established itself as the support for the vast majority\nof emerging technologies, mainly due to the characteristic of elasticity it\noffers. Auto-scalers are the systems that enable this elasticity by acquiring\nand releasing resources on demand to ensure an agreed service level. In this\narticle we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for\ndistributed services that combines the advantages of proactive and reactive\napproaches according to the situation to decide the optimal scaling actions in\nevery moment. The main novelties introduced by FLAS are (i) a predictive model\nof the high-level metrics trend which allows to anticipate changes in the\nrelevant SLA parameters (e.g. performance metrics such as response time or\nthroughput) and (ii) a reactive contingency system based on the estimation of\nhigh-level metrics from resource use metrics, reducing the necessary\ninstrumentation (less invasive) and allowing it to be adapted agnostically to\ndifferent applications. We provide a FLAS implementation for the use case of a\ncontent-based publish-subscribe middleware (E-SilboPS) that is the cornerstone\nof an event-driven architecture. To the best of our knowledge, this is the\nfirst auto-scaling system for content-based publish-subscribe distributed\nsystems (although it is generic enough to fit any distributed service). Through\nan evaluation based on several test cases recreating not only the expected\ncontexts of use, but also the worst possible scenarios (following the\nBoundary-Value Analysis or BVA test methodology), we have validated our\napproach and demonstrated the effectiveness of our solution by ensuring\ncompliance with performance requirements over 99% of the time.", "AI": {"tldr": "FLAS is an auto-scaler for distributed services that combines proactive forecasting with reactive contingency systems to optimize resource scaling, validated to maintain performance requirements over 99% of the time.", "motivation": "Cloud computing's elasticity is crucial for emerging technologies, but existing auto-scalers need better approaches to handle dynamic resource demands while maintaining service level agreements.", "method": "FLAS combines proactive forecasting of high-level metrics trends with a reactive system that estimates high-level metrics from resource usage, requiring less instrumentation and being application-agnostic.", "result": "The system was validated through multiple test cases including worst-case scenarios using Boundary-Value Analysis methodology, demonstrating performance requirement compliance over 99% of the time.", "conclusion": "FLAS represents the first auto-scaling system specifically designed for content-based publish-subscribe distributed systems while being generic enough for any distributed service, effectively maintaining SLA compliance through hybrid proactive-reactive scaling."}}
{"id": "2510.20495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20495", "abs": "https://arxiv.org/abs/2510.20495", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Accurate Performance Predictors for Edge Computing Applications", "comment": null, "summary": "Accurate prediction of application performance is critical for enabling\neffective scheduling and resource management in resource-constrained dynamic\nedge environments. However, achieving predictable performance in such\nenvironments remains challenging due to the co-location of multiple\napplications and the node heterogeneity. To address this, we propose a\nmethodology that automatically builds and assesses various performance\npredictors. This approach prioritizes both accuracy and inference time to\nidentify the most efficient model. Our predictors achieve up to 90% accuracy\nwhile maintaining an inference time of less than 1% of the Round Trip Time.\nThese predictors are trained on the historical state of the most correlated\nmonitoring metrics to application performance and evaluated across multiple\nservers in dynamic co-location scenarios. As usecase we consider electron\nmicroscopy (EM) workflows, which have stringent real-time demands and diverse\nresource requirements. Our findings emphasize the need for a systematic\nmethodology that selects server-specific predictors by jointly optimizing\naccuracy and inference latency in dynamic co-location scenarios. Integrating\nsuch predictors into edge environments can improve resource utilization and\nresult in predictable performance.", "AI": {"tldr": "A methodology for building performance predictors in edge environments that achieves 90% accuracy with inference time under 1% of RTT, using historical monitoring metrics and evaluated in dynamic co-location scenarios.", "motivation": "Accurate performance prediction is critical for scheduling and resource management in resource-constrained dynamic edge environments, where predictable performance is challenging due to application co-location and node heterogeneity.", "method": "Proposes a methodology that automatically builds and assesses various performance predictors, prioritizing both accuracy and inference time to identify the most efficient model. Predictors are trained on historical state of correlated monitoring metrics and evaluated across multiple servers in dynamic co-location scenarios.", "result": "Predictors achieve up to 90% accuracy while maintaining inference time of less than 1% of Round Trip Time. The approach is demonstrated using electron microscopy workflows with stringent real-time demands.", "conclusion": "Emphasizes the need for systematic methodology that selects server-specific predictors by jointly optimizing accuracy and inference latency in dynamic co-location scenarios, which can improve resource utilization and result in predictable performance when integrated into edge environments."}}
{"id": "2510.20506", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20506", "abs": "https://arxiv.org/abs/2510.20506", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing", "comment": null, "summary": "Distributed applications increasingly demand low end-to-end latency,\nespecially in edge and cloud environments where co-located workloads contend\nfor limited resources. Traditional load-balancing strategies are typically\nreactive and rely on outdated or coarse-grained metrics, often leading to\nsuboptimal routing decisions and increased tail latencies. This paper\ninvestigates the use of round-trip time (RTT) predictors to enhance request\nrouting by anticipating application latency. We develop lightweight and\naccurate RTT predictors that are trained on time-series monitoring data\ncollected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of\nhighly correlated monitoring metrics, our approach maintains low overhead while\nremaining adaptable to diverse co-location scenarios and heterogeneous\nhardware. The predictors achieve up to 95% accuracy while keeping the\nprediction delay within 10% of the application RTT. In addition, we identify\nthe minimum prediction accuracy threshold and key system-level factors required\nto ensure effective predictor deployment in resource-constrained clusters.\nSimulation-based evaluation demonstrates that performance-aware load balancing\ncan significantly reduce application RTT and minimize resource waste. These\nresults highlight the feasibility of integrating predictive load balancing into\nfuture production systems.", "AI": {"tldr": "This paper develops lightweight RTT predictors for performance-aware load balancing in edge/cloud environments, achieving 95% accuracy with minimal overhead to reduce application latency and resource waste.", "motivation": "Traditional load-balancing strategies are reactive and use outdated metrics, leading to suboptimal routing decisions and increased tail latencies in resource-constrained edge and cloud environments.", "method": "Developed lightweight RTT predictors trained on time-series monitoring data from Kubernetes-managed GPU clusters, using a reduced set of highly correlated metrics to maintain low overhead while adapting to diverse co-location scenarios.", "result": "Predictors achieved up to 95% accuracy with prediction delay within 10% of application RTT. Identified minimum accuracy thresholds and key system factors for effective deployment. Simulation showed significant reduction in application RTT and resource waste.", "conclusion": "Performance-aware load balancing using RTT predictors is feasible for future production systems, offering substantial improvements in latency reduction and resource efficiency."}}
