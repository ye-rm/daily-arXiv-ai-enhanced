<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe dynamically reconfigures LLM serving pipelines at runtime to handle variable workloads and GPU fragmentation, achieving 8.5x better resource efficiency and 38.3% lower latency than state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: Current LLM serving systems use static pipeline configurations that cannot adapt to dynamic workload conditions and suffer from severe resource fragmentation in serverless clusters, leading to substantial inefficiencies.

Method: FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity using: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation.

Result: Evaluation on an 82-GPU cluster shows FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.

Conclusion: FlexPipe successfully addresses the fundamental limitations of static pipeline configurations in LLM serving by enabling dynamic runtime reconfiguration, significantly improving resource utilization and performance in production environments.

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [2] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: This paper provides guidance for conducting cross-platform performance studies using single compute nodes as the base unit, offering templates for presenting scaling results and case studies.


<details>
  <summary>Details</summary>
Motivation: The increasing diversity of high-performance computing architectures has created a need for comparing code performance across different platforms, but there is a lack of available guidance on how to properly conduct such cross-platform studies.

Method: The authors propose using a single compute node on each platform as the natural base unit for computing studies, and provide guidance for setting up, running, and analyzing node-to-node scaling studies with templates for presenting results.

Result: The paper provides several case studies that highlight the benefits of this approach to cross-platform performance analysis.

Conclusion: This work offers practical guidance and templates for conducting meaningful cross-platform performance comparisons using node-to-node scaling studies as a standardized methodology.

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [3] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: GPU-accelerated process mapping algorithms for supercomputers that achieve significant speedups over CPU-based methods while maintaining competitive solution quality.


<details>
  <summary>Details</summary>
Motivation: To leverage the recent success of GPU-based graph partitioners for solving process mapping problems in supercomputers, where computational workload must be balanced while minimizing communication costs.

Method: Two GPU-accelerated approaches: 1) Hierarchical multisection that partitions task graphs alongside supercomputer hierarchy using GPU-based graph partitioners; 2) Integration of process mapping directly into multilevel graph partitioning pipeline with GPU-accelerated coarsening and refinement phases.

Result: Both methods achieve speedups exceeding 300x compared to state-of-the-art CPU algorithms. First method has ~10% higher communication costs but remains competitive. Second method achieves geometric mean speedup of 77.6x and peak speedup of 598x at cost of lower solution quality.

Conclusion: These are the first GPU-based algorithms for process mapping, demonstrating significant performance improvements while maintaining acceptable solution quality, with trade-offs between speed and optimization quality.

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [4] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: TALP-Pages is a framework that provides fast, in-repository performance feedback for HPC codes using fundamental performance and scaling metrics, integrated with CI workflows.


<details>
  <summary>Details</summary>
Motivation: To detect performance degradation early in HPC code development and provide meaningful insight into application scaling behavior during development.

Method: Uses TALP for on-the-fly collection of performance metrics, generates HTML reports with visualizations of performance factor regression and scaling-efficiency tables based on CI-friendly folder structure.

Result: TALP-Pages produces scaling-efficiency tables faster than tracing-based tools with lower overhead and tighter resource constraints. Successfully integrated with GENE-X CI setup with minimal changes.

Conclusion: The framework effectively enables performance monitoring in development workflows and can detect and explain performance improvements.

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [5] [Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness](https://arxiv.org/abs/2510.12274)
*Hao Jiang,Meng Qin,Ruijie Kuai,Dandan Liang*

Main category: cs.DC

TL;DR: Metronome is a network-aware, priority-aware scheduling mechanism for cloud native networks that uses time-division multiplexing and multi-objective optimization to improve bandwidth utilization and reduce job completion times.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of efficient resource coordination in cloud native networks, particularly coping with dynamic network bandwidth fluctuations in clusters, especially for jobs with periodic traffic patterns and dynamic bandwidth demands like distributed training.

Method: Uses time-division multiplexing approach leveraging job traffic characteristics to create elastic network resource allocation model; incorporates multi-objective optimization strategy considering latency and job priorities; adapts to dynamic environment through cluster monitoring and reconfiguration operations.

Result: Extensive experiments with 13 ML models show Metronome enhances cluster resource utilization while guaranteeing service performance. Reduces job completion time by up to 19.50% and improves average bandwidth utilization by up to 23.20% compared to existing Kubernetes scheduling mechanisms.

Conclusion: Metronome effectively addresses dynamic bandwidth allocation challenges in cloud native networks, demonstrating significant improvements in both job completion times and bandwidth utilization across multiple scenarios.

Abstract: With the rapid growth in computing power demand, cloud native networks have
emerged as a promising solution to address the challenges of efficient resource
coordination, particularly in coping with the dynamic fluctuations of network
bandwidth in clusters. We propose Metronome, a network-aware and priority-aware
scheduling mechanism for cloud native networks. This mechanism is designed to
support jobs that exhibit periodic traffic patterns and dynamic bandwidth
demands, particularly in the context of distributed training. Specifically,
Metronome employs a time-division multiplexing approach that leverages job
traffic characteristics to construct an elastic network resource allocation
model, enabling efficient bandwidth sharing across multiple jobs. In addition,
it incorporates a multi-objective optimization strategy, jointly considering
latency and job priorities to achieve globally optimal as well as dynamic
resource allocation. Finally, Metronome adapts to the dynamic environment by
monitoring the cluster and performing reconfiguration operations. Extensive
experiments with 13 common machine learning models demonstrate that Metronome
can enhance cluster resource utilization while guaranteeing service
performance. Compared with the existing Kubernetes scheduling mechanisms across
multiple scenarios, Metronome reduces job completion time by up to 19.50% while
improving average bandwidth utilization by up to 23.20%.

</details>


### [6] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: A Kubernetes-based tool that enables deferred, non-intrusive application of cloud design patterns to data-sharing pipelines without modifying service source code, while collecting energy metrics for energy-aware decisions.


<details>
  <summary>Details</summary>
Motivation: Traditional cloud design patterns compromise modularity and reusability when pre-defined in data-sharing pipelines, conflicting with their dynamic, consumer-driven nature.

Method: Developed a Kubernetes-based tool that supports automated pattern injection and energy consumption metrics collection without requiring changes to service source code.

Result: The tool enables deferred and non-intrusive application of selected cloud design patterns while preserving the flexible, composable structure of reusable data-sharing pipelines.

Conclusion: This approach allows developers to make energy-aware decisions while maintaining the modularity and reusability benefits of data mesh architectures in federated environments.

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [7] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: JLab and ESnet developed EJFAT, an FPGA-accelerated architecture for computational load balancing that handles compression, fragmentation, NAT, decompression and reassembly of streamed experimental data between edge and cluster computing.


<details>
  <summary>Details</summary>
Motivation: To address the need for high throughput and low latency processing of streamed experimental data in scientific facilities, supporting both time-critical data acquisition systems and data center workflows.

Method: Implemented FPGA acceleration architecture focusing on compression, fragmentation, UDP packet destination redirection (NAT), decompression and reassembly to seamlessly integrate edge and cluster computing.

Result: Successfully demonstrated integration with data sources at JLab, EJFAT load balancer at ESnet, and computational cluster resources at LBNL, showing synergy with DOE activities like Integrated Research Infrastructure.

Conclusion: EJFAT provides an effective solution for direct processing of streamed experimental data that benefits JLab's science program and future data centers requiring high throughput and low latency capabilities.

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: Proposed a descriptor-based DMA controller optimized for small unit transfers with lightweight descriptors and speculative prefetching, achieving 1.66x lower latency and up to 3.6x higher bus utilization while using fewer hardware resources.


<details>
  <summary>Details</summary>
Motivation: Classical descriptor-based DMACs are inefficient for arbitrary small transfers due to excessive descriptor size and serialized processing, leading to large static overheads in modern heterogeneous computing systems with demanding memory transfers.

Method: Implemented a lightweight descriptor format in AXI4-based DMAC with low-overhead speculative descriptor prefetching scheme, integrated into 64-bit Linux-capable RISC-V SoC and emulated on Kintex FPGA.

Result: Achieved 1.66x less latency launching transfers, up to 2.5x bus utilization in ideal memory system (3.6x in deep memory systems) with 64-byte transfers, using 11% fewer LUTs, 23% fewer FFs, no BRAMs, and synthesized at 1.44 GHz occupying 49.5 kGE.

Conclusion: The proposed DMAC efficiently handles arbitrary small transfers with significant performance improvements and reduced hardware overhead compared to off-the-shelf solutions.

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores](https://arxiv.org/abs/2510.12280)
*Yosuke Bando,Akinobu Mita,Kazuhiro Hiwada,Shintaro Sano,Tomoya Suzuki,Yu Nakanishi,Kazutaka Tomida,Hirotsugu Kajihara,Akiyuki Kaneko,Daisuke Taki,Yukimasa Miyamoto,Tomokazu Yoshida,Tatsuo Shiozawa*

Main category: cs.PF

TL;DR: SSD-based KV stores can effectively use microsecond-latency memory instead of expensive DRAM by leveraging software prefetching from user-level threads, achieving near-DRAM performance even with 5μs memory latency.


<details>
  <summary>Details</summary>
Motivation: To reduce costs by offloading in-memory data structures from expensive host DRAM to cheaper microsecond-latency secondary memory while maintaining performance in SSD-based key-value stores.

Method: Analyzed impact of microsecond memory latency on KV throughput, modeled interplay between prefetching and I/O, designed microbenchmarks, modified existing KV stores to use software prefetching from user-level threads, and tested on FPGA-based memory with adjustable latency.

Result: KV operation throughputs matched the model predictions, and modified KV stores achieved near-DRAM throughputs even with 5μs memory latency.

Conclusion: SSD-based KV stores can use microsecond-latency memory as a cost-effective DRAM alternative without needing new techniques, as existing prefetching methods combined with I/O provide sufficient latency tolerance.

Abstract: When key-value (KV) stores use SSDs for storing a large number of items,
oftentimes they also require large in-memory data structures including indices
and caches to be traversed to reduce IOs. This paper considers offloading most
of such data structures from the costly host DRAM to secondary memory whose
latency is in the microsecond range, an order of magnitude longer than those of
currently available DIMM-mounted or CXL memory devices. While emerging
microsecond-latency memory is likely to cost much less than DRAM, it can
significantly slow down SSD-based KV stores if naively employed. This paper
analyzes and evaluates the impact of microsecond-level memory latency on the KV
operation throughput. Our analysis finds that a well-known latency-hiding
technique of software prefetching for long-latency memory from user-level
threads is effective. The novelty of our analysis lies in modeling how the
interplay between prefetching and IO affects performance, from which we derive
an equation that well explains the throughput degradation due to long memory
latency. The model tells us that the presence of IO significantly enhances the
tolerance to memory latency, leading to a finding that SSD-based KV stores can
be made latency-tolerant without devising new techniques for
microsecond-latency memory. To confirm this, we design a microbenchmark as well
as modify existing SSD-based KV stores so that they issue prefetches from
user-level threads, and run them while placing most of in-memory data
structures on FPGA-based memory with adjustable microsecond latency. The
results demonstrate that their KV operation throughputs can be well explained
by our model, and the modified KV stores achieve near-DRAM throughputs for up
to a memory latency of 5 microseconds. This suggests the possibility that
SSD-based KV stores can use microsecond-latency memory as a cost-effective
alternative to the host DRAM.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [10] [Multi-objective Bayesian Optimization with Human-in-the-Loop for Flexible Neuromorphic Electronics Fabrication](https://arxiv.org/abs/2510.11727)
*Benius Dunn,Javier Meza-Arroyo,Armi Tiihonen,Mark Lee,Julia W. P. Hsu*

Main category: cs.ET

TL;DR: This paper presents a multi-objective Bayesian optimization (MOBO) approach with human-in-the-loop feedback to optimize photonic curing conditions for flexible metal-insulator-metal capacitors with aluminum oxide dielectric, accelerating the optimization process for neuromorphic electronics applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome processing constraints in fabricating flexible neuromorphic electronics using metal oxide materials, where traditional grid-search optimization is unfeasible due to the many input parameters involved in photonic curing.

Method: The method combines multi-objective Bayesian optimization (MOBO) with a human-in-the-loop (HITL) framework to optimize photonic curing conditions, incorporating failed experiments into the machine learning workflow to accelerate optimization.

Result: The framework successfully reduces the number of experimental rounds required for optimization and enables tuning of dielectric properties through analysis of different Pareto-optimal conditions, with insights provided by Shapley Additive exPlanations analysis.

Conclusion: The demonstrated MOBO with HITL framework can be adapted to various multi-objective experimental problems with interconnected inputs and high failure rates, generating usable results for machine learning models in neuromorphic electronics and beyond.

Abstract: Neuromorphic computing hardware enables edge computing and can be implemented
in flexible electronics for novel applications. Metal oxide materials are
promising candidates for fabricating flexible neuromorphic electronics, but
suffer from processing constraints due to the incompatibilities between oxides
and polymer substrates. In this work, we use photonic curing to fabricate
flexible metal-insulator-metal capacitors with solution-processible aluminum
oxide dielectric tailored for neuromorphic applications. Because photonic
curing outcomes depend on many input parameters, identifying an optimal
processing condition through a traditional grid-search approach is unfeasible.
Here, we apply multi-objective Bayesian optimization (MOBO) to determine
photonic curing conditions that optimize the trade-off between desired
electrical properties of large capacitance-frequency dispersion and low leakage
current. Furthermore, we develop a human-in-the-loop (HITL) framework for
incorporating failed experiments into the MOBO machine learning workflow,
demonstrating that this framework accelerates optimization by reducing the
number of experimental rounds required. Once optimization is concluded, we
analyze different Pareto-optimal conditions to tune the dielectrics properties
and provide insight into the importance of different inputs through Shapley
Additive exPlanations analysis. The demonstrated framework of combining MOBO
with HITL feedback can be adapted to a wide range of multi-objective
experimental problems that have interconnected inputs and high experimental
failure rates to generate usable results for machine learning models.

</details>


### [11] [Wireless Sensing of Temperature, Strain and Crack Growth in 3D-Printed Metal Structures via Magnetoelastic and Thermomagnetic Inclusions](https://arxiv.org/abs/2510.11730)
*Connor G. McMahan,Gavin Chang,Raymond Nguyen,Souren Soukiazian,David A. Smith,Tobias Schaedler,David Shahan*

Main category: cs.ET

TL;DR: First wireless strain and temperature sensing in 3D-printed metallic structures using electromagnetic inspection hardware, enabling need-based maintenance instead of scheduled teardowns.


<details>
  <summary>Details</summary>
Motivation: To establish need-based parts maintenance driven by accurate damage assessments rather than regularly scheduled maintenance teardowns, extending service intervals for structures in harsh environments.

Method: Encapsulate magnetoelastic and thermomagnetic materials inside microtubes and embed them during additive manufacturing. Mechanical/thermal stimuli affect magnetic permeability, modulating coil impedance placed near the printed part surface.

Result: Strain sensing accurate to +/-27x10-6 over at least 6x10-4 strain range, and temperature sensing accurate to +/-0.75°C over 70°C range (95% confidence). Demonstrated detection of plasticity onset and fatigue crack growth thousands of cycles before critical failure.

Conclusion: Extends non-destructive eddy-current damage detection to accurate, real-time strain and temperature monitoring within metallic structures, enabling predictive maintenance and extended service life.

Abstract: In this study, we demonstrate the first realization of wireless strain and
temperature sensing within 3D-printed metallic structures using standard
electromagnetic inspection hardware. This establishes a path toward need-based
parts maintenance driven by accurate damage assessments instead of relying on
regularly scheduled maintenance teardowns, extending the service intervals of
structures operating in harsh environments. To this end, we encapsulate
magnetoelastic and thermomagnetic materials inside microtubes and embed the
sensing elements during additive manufacturing. Mechanical and thermal stimuli
affect the magnetic permeability of the embedded materials, which modulates the
impedance of a coil placed on or near the surface of the printed part. We
demonstrate strain sensing accurate to +/-27x10-6 over at least a 6x10-4 strain
range, and temperature sensing accurate to +/-0.75oC over a 70oC range, both to
a 95% confidence interval. We highlight these sensors' capabilities by
detecting the onset of plasticity and fatigue-driven crack growth thousands of
cycles before critical failure. This extends non-destructive eddy-current
damage detection to accurate, real-time strain and temperature monitoring
within metallic structures.

</details>


### [12] [Quantum Annealing for Staff Scheduling in Educational Environments](https://arxiv.org/abs/2510.12278)
*Alessia Ciacco,Francesca Guerriero,Eneko Osaba*

Main category: cs.ET

TL;DR: Quantum annealing approach for staff allocation across educational levels with constraints on availability, competencies, and fairness, tested on real data from Italian schools.


<details>
  <summary>Details</summary>
Motivation: Address a real-world staff allocation problem in a public school system in Calabria, Italy, where staff must be distributed across kindergartens, primary, and secondary schools under multiple constraints.

Method: Developed an optimization model and investigated a solution approach based on quantum annealing.

Result: Computational experiments on real-world data show quantum annealing produces balanced assignments in short runtimes.

Conclusion: Demonstrates practical applicability of quantum optimization methods in educational scheduling and complex resource allocation tasks.

Abstract: We address a novel staff allocation problem that arises in the organization
of collaborators among multiple school sites and educational levels. The
problem emerges from a real case study in a public school in Calabria, Italy,
where staff members must be distributed across kindergartens, primary, and
secondary schools under constraints of availability, competencies, and
fairness. To tackle this problem, we develop an optimization model and
investigate a solution approach based on quantum annealing. Our computational
experiments on real-world data show that quantum annealing is capable of
producing balanced assignments in short runtimes. These results provide
evidence of the practical applicability of quantum optimization methods in
educational scheduling and, more broadly, in complex resource allocation tasks.

</details>
