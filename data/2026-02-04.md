<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [WritePolicyBench: Benchmarking Memory Write Policies under Byte Budgets](https://arxiv.org/abs/2602.02574)
*Edgard El Cham*

Main category: cs.PF

TL;DR: WritePolicyBench is a benchmark for evaluating memory write policies that manage what to store, merge, and evict under strict byte budgets while handling document/API drift in streaming data.


<details>
  <summary>Details</summary>
Motivation: There's a need for standardized evaluation of memory write policies that must operate under strict byte constraints while dealing with non-stationary data streams (document/API drift), requiring controlled benchmarks to measure both task success and budget efficiency.

Method: The benchmark provides: (i) task generators with controlled non-stationarity, (ii) explicit action interface for external memory operations, (iii) byte-accurate cost model for precise measurement, and (iv) standardized metrics for evaluation.

Result: The paper introduces a comprehensive benchmark framework that enables systematic evaluation of memory write policies under realistic constraints including byte budgets and data drift.

Conclusion: WritePolicyBench provides a standardized framework for evaluating memory management policies in streaming scenarios with budget constraints and data drift, enabling fair comparison and advancement of write policy research.

Abstract: We introduce WritePolicyBench, a benchmark for evaluating memory write policies: decision rules that choose what to store, merge, and evict under a strict byte budget while processing a stream with document/API drift. The benchmark provides (i) task generators with controlled non-stationarity, (ii) an explicit action interface for external memory, (iii) a byte-accurate cost model, and (iv) standardized metrics that measure both task success and budget efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)
*Ruihan Lin,Zezhen Ding,Zean Han,Jiheng Zhang*

Main category: cs.DC

TL;DR: The paper develops a stochastic control framework for scheduling heterogeneous LLM inference workloads across GPU clusters, addressing contention between compute-intensive prefill and memory-bound decode phases with state-dependent service rates.


<details>
  <summary>Details</summary>
Motivation: LLMs are becoming critical enterprise infrastructure, but their two-phase inference (prefill and decode) creates state-dependent contention when sharing GPU resources. Workload heterogeneity with varying input/output lengths further complicates scheduling across large GPU clusters.

Method: Formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates based on empirical measurements. Analyze fluid approximations, solve steady-state linear programs for optimal resource allocation, and design gate-and-route policies for prefill admission and decode routing.

Result: Proved asymptotic optimality of gate-and-route policies in many-GPU limit under bundled and separate token-pricing schemes. Extended framework to incorporate SLIs like latency and fairness. Numerical experiments with empirical data show policies outperform standard serving heuristics.

Conclusion: Developed a comprehensive stochastic control framework for scheduling heterogeneous LLM workloads that addresses state-dependent contention between prefill and decode phases, providing asymptotically optimal policies that outperform existing heuristics while accommodating various pricing schemes and service level requirements.

Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.

</details>


### [3] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: The paper introduces Last-K Preemption model for dynamic task graph scheduling, allowing selective rescheduling of recent tasks while preserving earlier allocations, showing moderate preemption achieves most benefits of full preemption with better fairness and lower overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional dynamic scheduling of task graphs typically doesn't revisit prior task allocations and focuses mainly on minimizing makespan, potentially missing opportunities for improved fairness, utilization, and runtime efficiency through controlled preemption.

Method: Introduces the Last-K Preemption model that selectively reschedules recent task graphs while preserving earlier allocations. Compares preemptive, non-preemptive, and partial-preemptive strategies using synthetic, RIoTBench, WFCommons, and adversarial workloads across multiple metrics.

Result: Results show that moderate preemption can achieve most of the makespan and utilization improvements of full preemption while maintaining better fairness and lower runtime overhead compared to full preemption strategies.

Conclusion: Controlled schedule preemption using the Last-K model offers a balanced approach that captures most performance benefits of full preemption while preserving fairness and minimizing computational overhead in dynamic task graph scheduling.

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [4] [Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization](https://arxiv.org/abs/2602.03246)
*Tamoghna Sarkar,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: Distributed algorithm for optimal rate allocation in multi-source multi-node networks with both access-path delays and service-node queueing delays, using congestion pricing to achieve system-optimal flow-weighted end-to-end delay minimization.


<details>
  <summary>Details</summary>
Motivation: Addresses the practical problem of rate allocation in networked systems where both access-path delays (rate-dependent, convex) and service-node queueing delays (load-dependent) must be jointly considered, which arises in many distributed systems.

Method: Formulates the problem as a convex program, shows it yields global optimum via KKT conditions, develops a lightweight distributed pricing-based algorithm where service nodes compute congestion prices from aggregate load and sources solve small separable convex allocation problems.

Result: The distributed algorithm converges to the centralized optimum, with numerical illustrations demonstrating convergence and highlighting trade-offs from jointly modeling access and service congestion.

Conclusion: The proposed distributed pricing mechanism effectively solves the joint access-service congestion problem, achieving system-optimal flow allocation through local computations and minimal message passing (scalar congestion prices).

Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.

</details>


### [5] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fern√°ndez Anta*

Main category: cs.DC

TL;DR: This paper examines how blockchain validators can exploit multi-core CPU parallelism for block construction and execution while preserving blockchain semantics, developing MILP formulations and heuristics for optimization.


<details>
  <summary>Details</summary>
Motivation: Blockchain validators need to reduce block processing time using multi-core CPUs, but must maintain deterministic execution that preserves transaction order, respects conflicts, and adheres to per-block runtime limits.

Method: The authors formalize two validator-side optimization problems: (1) executing an ordered block on p cores to minimize makespan while ensuring equivalence to sequential execution, and (2) selecting/scheduling mempool transactions under runtime limit B to maximize validator reward. They develop exact MILP formulations capturing conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads.

Result: Using Ethereum mainnet traces and including Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and simple reward-greedy baseline (RG) for block construction, the paper empirically quantifies trade-offs between optimality and runtime.

Conclusion: The research provides systematic approaches for validators to exploit multi-core parallelism during block construction and execution while preserving blockchain semantics, with practical solutions balancing optimality and computational efficiency.

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [6] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: Energy-efficient agreement algorithm with O(log f) active rounds per participant for crash-fault systems


<details>
  <summary>Details</summary>
Motivation: Traditional agreement protocols require all participants to be active in all rounds, consuming significant energy. Recent work introduced energy-efficient agreement to minimize participant energy costs by reducing the number of rounds each party must actively participate in.

Method: Developed a recursive agreement algorithm that achieves agreement while minimizing active participation. The algorithm ensures each participant only needs to be active for O(log f) rounds, where f < n is the maximum number of crash faults.

Result: Achieved agreement with O(log f) active rounds per participant, significantly reducing energy consumption compared to traditional agreement protocols that require linear or higher round participation.

Conclusion: The recursive algorithm provides an energy-efficient solution to the classic agreement problem, offering logarithmic active round complexity per participant in crash-fault systems, which is particularly valuable for energy-constrained distributed systems.

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [7] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: DALI is a workload-aware offloading framework for efficient Mixture of Experts (MoE) inference on local PCs that addresses CPU-GPU load imbalance, inaccurate prefetching, and poor cache hit rates through dynamic expert assignment, residual-based prefetching, and workload-aware cache replacement.


<details>
  <summary>Details</summary>
Motivation: MoE architectures increase LLM capacity without proportional computation cost but have huge parameter sizes. Offloading to host memory with CPU/GPU computation is promising for resource-constrained local PCs, but existing approaches mismatch the dynamic nature of expert workloads, causing CPU-GPU load imbalance, inaccurate prefetching, and poor GPU cache hit rates.

Method: DALI proposes three key techniques: 1) Dynamic expert assignment using 0-1 integer optimization solved with Greedy Assignment strategy at runtime; 2) Residual-Based Prefetching leveraging inter-layer residual information to predict high-workload experts; 3) Workload-Aware Cache Replacement policy exploiting temporal correlation in expert activations.

Result: DALI achieves significant speedups in both prefill and decoding phases over state-of-the-art offloading frameworks across various MoE models and settings.

Conclusion: DALI effectively addresses the fundamental inefficiencies in MoE offloading by matching workload dynamics, enabling efficient MoE inference on resource-constrained local PC platforms through workload-aware optimization techniques.

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [8] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: Synchronous SGD methods (including m-Synchronous SGD) are shown to be nearly optimal for many heterogeneous computation scenarios, contrary to common assumptions favoring asynchronous approaches.


<details>
  <summary>Details</summary>
Motivation: Despite recent progress in asynchronous optimization, most distributed optimization methods still rely on synchronous approaches. The paper aims to revisit synchronous methods and demonstrate their unexpected optimality in heterogeneous computation environments.

Method: The authors analyze Synchronous SGD and its robust variant m-Synchronous SGD under random computation times and adversarial partial participation of workers. They provide theoretical analysis of these methods' time complexities.

Result: Theoretical analysis shows that synchronous methods achieve optimal time complexities in many practical regimes (up to logarithmic factors), making them sufficient for many modern heterogeneous computation scenarios.

Conclusion: While asynchronous methods may be necessary for some tasks, synchronous methods are nearly optimal and sufficient for many heterogeneous computation scenarios, challenging the common preference for asynchronous approaches.

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [9] [ProphetKV: User-Query-Driven Selective Recomputation for Efficient KV Cache Reuse in Retrieval-Augmented Generation](https://arxiv.org/abs/2602.02579)
*Shihao Wang,Jiahao Chen,Yanqi Pan,Hao Huang,Yichen Hao,Xiangyu Zou,Wen Xia,Wentao Zhang,Haitao Wang,Junhong Li,Chongyang Qiu,Pengfei Wang*

Main category: cs.OS

TL;DR: ProphetKV is a KV cache reuse method for RAG that prioritizes query-relevant tokens to overcome the "crowding-out effect" in current token selection, achieving near-full accuracy with minimal recomputation overhead.


<details>
  <summary>Details</summary>
Motivation: Current RAG prefill methods suffer from computational bottlenecks. Existing approaches that reuse pre-calculated KV caches have a "crowding-out effect" where globally salient but query-irrelevant tokens dominate the limited recomputation budget, displacing essential query-relevant tokens and degrading accuracy.

Method: ProphetKV is a user-query-driven KV cache reuse method that dynamically prioritizes tokens based on semantic relevance to the user query. It employs a dual-stage recomputation pipeline that fuses layer-wise attention metrics to select a high-utility token set, ensuring the recomputation budget bridges the informational gap between retrieved context and user query.

Result: ProphetKV retains 96%-101% of full-prefill accuracy with only 20% recomputation ratio. It achieves accuracy improvements of 8.8%-24.9% on RULER and 18.6%-50.9% on LongBench over state-of-the-art approaches like CacheBlend, EPIC, and KVShare.

Conclusion: ProphetKV effectively addresses the crowding-out effect in RAG prefill by focusing recomputation resources on query-relevant tokens, enabling high-fidelity attention recovery with minimal computational overhead while significantly outperforming existing methods.

Abstract: The prefill stage of long-context Retrieval-Augmented Generation (RAG) is severely bottlenecked by computational overhead. To mitigate this, recent methods assemble pre-calculated KV caches of retrieved RAG documents (by a user query) and reprocess selected tokens to recover cross-attention between these pre-calculated KV caches. However, we identify a fundamental "crowding-out effect" in current token selection criteria: globally salient but user-query-irrelevant tokens saturate the limited recomputation budget, displacing the tokens truly essential for answering the user query and degrading inference accuracy.
  We propose ProphetKV, a user-query-driven KV Cache reuse method for RAG scenarios. ProphetKV dynamically prioritizes tokens based on their semantic relevance to the user query and employs a dual-stage recomputation pipeline to fuse layer-wise attention metrics into a high-utility set. By ensuring the recomputation budget is dedicated to bridging the informational gap between retrieved context and the user query, ProphetKV achieves high-fidelity attention recovery with minimal overhead. Our extensive evaluation results show that ProphetKV retains 96%-101% of full-prefill accuracy with only a 20% recomputation ratio, while achieving accuracy improvements of 8.8%-24.9% on RULER and 18.6%-50.9% on LongBench over the state-of-the-art approaches (e.g., CacheBlend, EPIC, and KVShare).

</details>
