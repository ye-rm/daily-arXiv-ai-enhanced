<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation](https://arxiv.org/abs/2602.15204)
*Kevin Garner,Polykarpos Thomadakis,Nikos Chrisochoides*

Main category: cs.DC

TL;DR: A distributed memory method for anisotropic mesh adaptation that avoids collective communication and global synchronization by separating meshing functionality from performance aspects using shared memory mesh generation software and a parallel runtime system.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable anisotropic mesh adaptation method for high-performance computing architectures that avoids the performance bottlenecks of collective communication and global synchronization techniques used in existing approaches.

Method: Separates meshing functionality from performance aspects using: 1) multicore cc-NUMA-based shared memory mesh generation software, and 2) a parallel runtime system. First adapts interface elements on a single node, then distributes subdomains across cluster nodes while keeping adapted interface elements frozen to maintain mesh conformity.

Result: The method successfully generates meshes with up to approximately 1 billion elements, achieving comparable quality and performance to existing state-of-the-art HPC meshing software.

Conclusion: The presented distributed memory approach effectively avoids collective communication and global synchronization bottlenecks while maintaining mesh quality and performance comparable to existing HPC meshing solutions, demonstrating the effectiveness of separating meshing functionality from performance optimization.

Abstract: This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software.

</details>


### [2] [Co-Design and Evaluation of a CPU-Free MPI GPU Communication Abstraction and Implementation](https://arxiv.org/abs/2602.15356)
*Patrick G. Bridges,Derek Schafer,Jack Lange,James B. White,Anthony Skjellum,Evan Suggs,Thomas Hines,Purushotham Bangalore,Matthew G. F. Dosanjh,Whit Schonbein*

Main category: cs.DC

TL;DR: MPI-based GPU communication API enables CPU-free communication with up to 50% latency reduction and 28% speedup on 8,192 GPUs.


<details>
  <summary>Details</summary>
Motivation: Current GPU communication APIs either rely on CPU involvement or impose heavy synchronization burdens on programmers, creating bottlenecks for efficient GPU-based ML and HPC applications.

Method: Design and implementation of an MPI-based GPU communication API that builds on existing MPI extensions and leverages HPE Slingshot 11 network card capabilities, enabling CPU-free communication.

Result: The API achieves up to 50% reduction in medium message latency for GPU ping-pong exchanges and 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs on the Frontier supercomputer.

Conclusion: The proposed MPI-based GPU communication API successfully enables easy-to-use, high-performance, CPU-free communication, demonstrating significant performance improvements for large-scale GPU applications.

Abstract: Removing the CPU from the communication fast path is essential to efficient GPU-based ML and HPC application performance. However, existing GPU communication APIs either continue to rely on the CPU for communication or rely on APIs that place significant synchronization burdens on programmers. In this paper we describe the design, implementation, and evaluation of an MPI-based GPU communication API enabling easy-to-use, high-performance, CPU-free communication. This API builds on previously proposed MPI extensions and leverages HPE Slingshot 11 network card capabilities. We demonstrate the utility and performance of the API by showing how the API naturally enables CPU-free gather/scatter halo exchange communication primitives in the Cabana/Kokkos performance portability framework, and through a performance comparison with Cray MPICH on the Frontier and Tuolumne supercomputers. Results from this evaluation show up to a 50% reduction in medium message latency in simple GPU ping-pong exchanges and a 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs of the Frontier supercomputer.

</details>


### [3] [FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations](https://arxiv.org/abs/2602.15379)
*Zhihao Shu,Md Musfiqur Rahman Sanim,Hangyu Zheng,Kunxiong Zhu,Miao Yin,Gagan Agrawal,Wei Niu*

Main category: cs.DC

TL;DR: FlashMem is a memory streaming framework for mobile GPUs that dynamically streams DNN weights on demand instead of preloading all parameters, achieving significant memory reduction and speedup for large models and multi-DNN workloads.


<details>
  <summary>Details</summary>
Motivation: Modern DNNs are too large and complex for mobile GPUs with limited resources. Existing frameworks preload all weights before execution, which is inefficient for large models and multi-DNN workloads where memory constraints become critical.

Method: FlashMem uses static determination of model loading schedules and dynamic on-demand streaming of weights. It leverages 2.5D texture memory to minimize data transformations and improve execution efficiency, avoiding full weight preloading.

Result: Experimental results on 11 models show FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.

Conclusion: FlashMem's memory streaming approach effectively addresses the challenges of executing large DNNs and multi-DNN workloads on mobile GPUs, significantly reducing memory consumption and improving inference latency compared to traditional weight preloading strategies.

Abstract: The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.

</details>


### [4] [Service Orchestration in the Computing Continuum: Structural Challenges and Vision](https://arxiv.org/abs/2602.15794)
*Boris Sedlak,Víctor Casamayor Pujol,Ildefons Magrans de Abril,Praveen Kumar Donta,Adel N. Toosi,Schahram Dustdar*

Main category: cs.DC

TL;DR: The paper analyzes structural challenges in Computing Continuum service orchestration, proposes an ideal autonomous solution, suggests Active Inference as one approach, and outlines research roadmap with focus on standardized simulation environments.


<details>
  <summary>Details</summary>
Motivation: The Computing Continuum's heterogeneous and dynamic infrastructure increases complexity for service orchestration compared to centralized architectures, requiring research into autonomous solutions.

Method: First summarizes structural problems of the CC, then envisions an ideal solution for autonomous service orchestration, and proposes Active Inference (a neuroscience concept) as one instantiation for self-organizing services.

Result: No existing solution achieves the vision of autonomous service orchestration across the CC. Research faces structural challenges, most notably the need for standardized simulation and evaluation environments.

Conclusion: The paper outlines a research roadmap toward resilient and scalable service orchestration in the Computing Continuum, emphasizing the need for standardized evaluation frameworks and autonomous orchestration mechanisms.

Abstract: The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166)
*Tanner Andrulis,Michael Gilbert,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: FFM is a mapper that quickly finds optimal fused mappings for tensor algebra accelerators by pruning suboptimal partial mappings and joining them, achieving linear scaling despite exponential mapspace growth.


<details>
  <summary>Details</summary>
Motivation: Existing mappers cannot find optimal fused mappings in feasible runtime because the search space grows exponentially with computation steps, limiting the ability to evaluate accelerator architecture potential.

Method: FFM shrinks the search space by pruning subsets of mappings (partial mappings) that are never part of optimal mappings, then joins partial mappings to construct optimal fused mappings.

Result: FFM's runtime scales approximately linearly with computation steps despite exponential mapspace growth, and is over 1000× faster than prior state-of-the-art approaches for Transformers.

Conclusion: FFM enables efficient exploration of fused mappings for tensor algebra accelerators, overcoming the exponential search complexity that previously limited optimal mapping discovery.

Abstract: The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.
  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.

</details>


### [6] [The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15172)
*Michael Gilbert,Tanner Andrulis,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: TCM is a fast mapper that guarantees optimal DNN accelerator mappings by introducing dataplacement concept and aggressive pruning, reducing search space by up to 32 orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Existing DNN accelerator mapping approaches use heuristics that can't guarantee optimal mappings, preventing proper hardware evaluation since performance differences could be due to suboptimal mapping rather than hardware changes.

Method: Introduces dataplacement concept (complementary to dataflow) to analyze mappings, identifies opportunities to prune redundant/suboptimal mappings, enabling full mapspace search with massive reduction in search space.

Result: TCM finds optimal mappings in under a minute, while prior mappers produce 21% higher energy-delay-product even with 1000x more runtime (>10 hours).

Conclusion: TCM is the first mapper capable of guaranteed optimal mapping discovery in feasible runtime, enabling proper accelerator evaluation and design by eliminating mapping uncertainty.

Abstract: The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.
  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.
  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\%$ higher than optimal) even when given $1000\times$ the runtime ($>10$ hours).

</details>


### [7] [Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis](https://arxiv.org/abs/2602.15336)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: LLMs (GPT, Gemini, Claude) perform poorly on technical digital logic problems despite students perceiving them as helpful, revealing a gap between perceived usefulness and actual correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as tutors by undergraduate students, but their reliability on circuit- and diagram-based digital logic problems remains unclear and needs evaluation.

Method: Human-AI study with 24 students comparing three LLMs on 10 digital logic questions, plus independent judge-based evaluation against official solutions using strict correctness criteria.

Result: For sequential problems (Q1-Q7), none of the LLMs matched official answers despite producing confident explanations. Students rated models favorably, showing gap between perceived helpfulness and formal correctness.

Conclusion: Without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

Abstract: Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

</details>


### [8] [Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification](https://arxiv.org/abs/2602.15388)
*Yonghao Wang,Jiaxin Zhou,Yang Yin,Hongqin Lyu,Zhiteng Chao,Wenchao Ding,Jing Ye,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: CoverAssert is an iterative framework that improves LLM-generated SystemVerilog assertions by analyzing functional coverage and providing targeted feedback to prioritize uncovered specifications.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based techniques for generating SystemVerilog assertions from natural language specifications suffer from poor assertion quality due to LLMs' limited understanding of IC design. There's a need to verify whether generated assertions effectively cover functional specifications and to design feedback mechanisms based on coverage analysis.

Method: CoverAssert uses a lightweight mechanism to match generated assertions with specific functional descriptions by clustering joint representations of semantic features from LLM-generated assertions and structural features from ASTs of related signals. It maps these back to specifications to analyze functional coverage quality, then constructs a feedback loop based on coverage to guide LLMs in prioritizing uncovered functional points iteratively.

Result: When integrated with state-of-the-art generators AssertLLM and Spec2Assertion, CoverAssert achieves average improvements of 9.57% in branch coverage, 9.64% in statement coverage, and 15.69% in toggle coverage across four open-source designs.

Conclusion: CoverAssert effectively addresses the limitations of single-pass LLM assertion generation by introducing an iterative framework that leverages functional coverage analysis to provide targeted feedback, significantly improving assertion quality across multiple coverage metrics.

Abstract: While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [9] [Pairwise XOR and XNOR Gates in Squeezed Instantaneous Noise Based Logic](https://arxiv.org/abs/2602.15032)
*Nasir Kenarangui,Laszlo B. Kish,Arthur Powalka*

Main category: cs.ET

TL;DR: This paper implements XOR and XNOR gates for squeezed instantaneous noise-based logic (INBL), extending previous symmetric INBL work to enable more complex algorithms in the squeezed scheme.


<details>
  <summary>Details</summary>
Motivation: To extend the XOR/XNOR gate operations from symmetric INBL to squeezed INBL scheme, enabling more complex INBL algorithms and advancing toward gate universality in this classical computing framework.

Method: Implemented pairwise XOR and XNOR gates for squeezed INBL scheme, working with hyperspace vectors (M-bit binary strings) and their superpositions while maintaining compatibility with squeezed reference system.

Result: Validated that squeezed-scheme XOR/XNOR gates have correct Boolean behavior for both bitwise and targeted M-bit strings, preserve instantaneous evaluation, and can be tailored from symmetric INBL toolkit.

Conclusion: The implementation advances INBL gate universality, strengthens INBL as a flexible classical framework emulating quantum advantages, and provides key gates for more complex squeezed INBL algorithms.

Abstract: Instantaneous noise-based logic (INBL) is a novel computing approach that encodes binary information using stochastic processes. It uses 2M orthogonal stochastic reference noises for M noise-bits to construct an exponentially large Hilbert space (hyperspace) of dimension 2^M. INBL offers a classical alternative to quantum-style parallelism for specific problems with exponential speedup compared to classical algorithms. Building on recent work that introduced pairwise XOR and XNOR operations defined for a symmetric INBL scheme, this paper implements these gates for a squeezed INBL scheme. Hyperspace vectors are product strings corresponding to M-bit long binary numbers. The proposed operations can apply pairwise on hyperspace vectors and their superpositions, while remaining compatible with the squeezed reference system. We validate that the squeezed-scheme XOR/XNOR gate operations have correct Boolean behavior over both bitwise and targeted M-bit strings and demonstrate that the operations preserve instantaneous evaluation. The results show that the XOR/XNOR toolkit, previously developed for symmetric INBL, can be tailored for the squeezed scheme. This development is a key part of the gate set needed for more complex INBL algorithms in the squeezed INBL scheme and advances the objective of gate universality in INBL. It further strengthens the case for INBL as a flexible, classical computing framework that can emulate some structural advantages of quantum computation.

</details>


### [10] [High Convergence Rates of CMOS Invertible Logic Circuits Based on Many-Body Hamiltonians](https://arxiv.org/abs/2602.15033)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.ET

TL;DR: CMOS invertible-logic circuits using three-body spin interactions achieve higher convergence rates than conventional two-body designs with minimal area overhead.


<details>
  <summary>Details</summary>
Motivation: To improve probabilistic computing by creating simpler Hamiltonian landscapes that are easier to optimize, overcoming limitations of conventional two-body interaction designs.

Method: Developed CMOS invertible-logic circuits based on many-body Hamiltonians with three-body spin interactions, using stochastic computing and annealing techniques to enable probabilistic forward/backward operations.

Result: Three-body CIL circuits achieved few-times higher convergence rates compared to conventional two-body designs with negligible area overhead on FPGA implementations.

Conclusion: Three-body interaction Hamiltonians provide simpler energy landscapes that facilitate reaching global minima, making them superior to conventional two-body designs for probabilistic computing applications.

Abstract: This paper introduces CMOS invertible-logic (CIL) circuits based on many-body Hamiltonians. CIL can realize probabilistic forward and backward operations of a function by annealing a corresponding Hamiltonian using stochastic computing. We have created a Hamiltonian that includes three-body interaction of spins (probabilistic nodes). It provides some degrees of freedom to design a simpler landscape of Hamiltonian (energy) than that of the conventional two-body Hamiltonian. The simpler landscape makes it easier to reach the global minimum energy. The proposed three-body CIL circuits are designed and evaluated with the conventional two-body CIL circuits, resulting in few-times higher convergence rates with negligible area overhead on FPGA.

</details>


### [11] [Full-Field Damage Monitoring in Architected Lattices Using In situ Electrical Impedance Tomography](https://arxiv.org/abs/2602.15048)
*Akash Deep,Andrea Samore,Alistair McEwan,Andrew McBride,Shanmugam Kumar*

Main category: cs.ET

TL;DR: First in-situ implementation of electrical impedance tomography (EIT) within tunable architected lattice materials for real-time 3D damage monitoring, enabling systematic exploration of lattice designs and early-stage fracture detection.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, full-field sensing modality for architected multifunctional materials that goes beyond conventional point measurements, enabling real-time monitoring of damage evolution including early-stage prefracture events in 3D printed lattice composites.

Method: Designed Voronoi-based branch-trunk-branch lattice motifs inspired by 2D wallpaper symmetries, fabricated using CNT-infused photocurable resins with nanoscale filler dispersion confirmed by FESEM. Implemented EIT with 16 peripheral electrodes during quasi-static tensile loading, using adjacent and across current injection schemes to reconstruct conductivity maps.

Result: EIT successfully resolved sequential ligament fracture with high temporal resolution, with localized conductivity loss quantitatively coinciding with fracture sites including regions remote from electrodes. Architectural tunability allowed systematic control of EIT imaging sensitivity to early-stage damage, and resistance discontinuities at failure corroborated spatial localization.

Conclusion: In-situ EIT establishes a scalable, full-field sensing modality for architected multifunctional materials, providing an experimentally validated pathway toward autonomous, intelligent materials and data-rich material states that can inform digital twin frameworks for structural, biomedical, and energy applications.

Abstract: Electrical impedance tomography (EIT) enables non-invasive, spatially continuous reconstruction of internal conductivity distributions, providing full field sensing beyond conventional point measurements. Here, we report the first in situ implementation of EIT within a tunable architected lattice materials framework, enabling systematic exploration across a broad lattice design space while achieving real time monitoring of damage evolution, including early stage, prefracture events, in 3D printed multifunctional lattice composites. Lattices are designed via Voronoi based branch trunk branch motifs inspired by 2D wallpaper symmetries and fabricated using CNT infused photocurable resins, with nanoscale filler dispersion confirmed by field emission scanning electron microscopy. Sixteen electrodes distributed along the lattice periphery enable EIT measurements during quasi static tensile loading. Conductivity maps reconstructed using adjacent and across current injection schemes resolve sequential ligament fracture with high temporal resolution, with localised conductivity loss quantitatively coinciding with fracture sites, including regions remote from electrodes. Architectural tunability allows systematic control of EIT imaging sensitivity to early stage damage, while pronounced resistance discontinuities at failure further corroborate spatial localisation; global end to end resistance measurements complement macroscopic stress strain responses. Collectively, these results establish in situ EIT as a scalable, full field sensing modality for architected multifunctional materials, providing an experimentally validated pathway toward autonomous, intelligent materials and data rich material states that can inform digital twin frameworks for structural, biomedical, and energy related applications.

</details>


### [12] [Quantum Optimization for Access Point Selection Under Budget Constraint](https://arxiv.org/abs/2602.15049)
*Mohamed Khalil Brik,Ahmed Shokry,Moustafa Youssef*

Main category: cs.ET

TL;DR: Quantum annealing-based AP selection algorithm reduces AP count by 96.1% while maintaining localization accuracy, achieving 61x speedup over classical methods.


<details>
  <summary>Details</summary>
Motivation: Optimal AP selection is crucial for indoor localization but constrained by budget, creating accuracy-cost trade-off. Classical approaches are computationally expensive for large-scale 3D environments.

Method: Formulate AP selection as QUBO problem suitable for quantum annealing. Use quantum annealing to identify most effective AP subset within budget constraints.

Result: 96.1% reduction in required APs while maintaining comparable 3D localization accuracy. 61x speedup (0.20s vs classical), 10% lower mean localization error. 73% floor accuracy vs 58.6% (classical) and 70.4% (full AP set).

Conclusion: Quantum AP selection algorithm shows promise for large-scale 3D localization by drastically reducing infrastructure requirements with negligible performance impact, outperforming classical methods in both accuracy and computational speed.

Abstract: Optimal Access Point (AP) selection is crucial for accurate indoor localization, yet it is constrained by budget, creating a trade-off between localization accuracy and deployment cost. Classical approaches to AP selection are often computationally expensive, hindering their application in large-scale 3D indoor environments.
  In this paper, we introduce a quantum APs selection algorithm under a budget constraint. The proposed algorithm leverages quantum annealing to identify the most effective subset of APs allowed within a given budget. We formulate the APs selection problem as a quadratic unconstrained binary optimization (QUBO) problem, making it suitable for quantum annealing solvers. The proposed technique can drastically reduce infrastructure requirements with a negligible impact on performance.
  We implement the proposed quantum algorithm and deploy it in a realistic 3D testbed. Our results show that the proposed approach can reduce the number of required APs by 96.1% while maintaining a comparable 3D localization accuracy. Furthermore, the proposed quantum approach outperforms classical AP selection algorithms in both accuracy and computational speed. Specifically, our technique achieves a time of 0.20 seconds, representing a speedup of 61 times over its classical counterpart, while reducing the mean localization error by 10% compared to the classical counterpart. For floor localization, the quantum approach achieves 73% floor accuracy, outperforming both the classical AP selection (58.6%) and even using the complete set of APs (70.4%). This highlights the promise of the proposed quantum APs selection algorithm for large-scale 3D localization.

</details>


### [13] [Quantum Computing for Healthcare Digital Twin Systems](https://arxiv.org/abs/2602.15477)
*Asma Taheri Monfared,Andrea Bombarda,Angelo Gargantini,Majid Haghparast*

Main category: cs.ET

TL;DR: This paper reviews Quantum Digital Twins (QDTs) for healthcare, analyzing key challenges hindering real-world adoption and outlining research directions for developing secure, reliable quantum digital twin systems.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems are becoming increasingly complex, requiring advanced computational models for real-time monitoring, secure data exchange, and intelligent decision-making. While Digital Twins provide virtual representations of healthcare entities, classical frameworks face scalability, computational efficiency, and security limitations. Quantum Digital Twins offer potential enhancements but face significant adoption barriers.

Method: The paper conducts a comprehensive review of Quantum Digital Twins for healthcare, with specific focus on identifying and analyzing key challenges. It examines existing QDT models and their limitations, then outlines critical research directions and enabling strategies.

Result: The review identifies fundamental challenges including quantum hardware limitations, hybrid classical-quantum system integration difficulties, cloud-based quantum access issues, scalability constraints, and lack of clinical trust. These factors currently hinder real-world adoption of QDTs in healthcare.

Conclusion: The paper concludes that while Quantum Digital Twins show promise for enhancing healthcare systems through quantum computing advantages, significant challenges must be addressed. It provides a roadmap for advancing development of secure, reliable, and clinically viable quantum digital twin systems for next-generation healthcare applications.

Abstract: The growing complexity of healthcare systems requires advanced computational models for real-time monitoring, secure data exchange, and intelligent decision-making. Digital Twins (DTs) provide virtual representations of physical healthcare entities, enabling continuous patient monitoring and personalized care. However, classical DT frameworks face limitations in scalability, computational efficiency, and security. Recent studies have introduced Quantum Digital Twins (QDTs) to enhance performance through quantum computing, addressing challenges such as quantum-resistant security and efficient task offloading in healthcare environments. Despite these advances, most existing QDT models remain constrained by fundamental challenges related to quantum hardware limitations, hybrid classical-quantum system integration, cloud-based quantum access, scalability, and clinical trust. This paper provides a comprehensive review of QDTs for healthcare, with a particular focus on identifying and analyzing the key challenges that currently hinder their real-world adoption. Furthermore, it outlines critical research directions and enabling strategies aimed at advancing the development of secure, reliable, and clinically viable quantum digital twin systems for next-generation healthcare applications.

</details>
