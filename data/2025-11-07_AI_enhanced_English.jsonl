{"id": "2511.03866", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.03866", "abs": "https://arxiv.org/abs/2511.03866", "authors": ["Arijit Bhattacharjee", "Ali TehraniJamsaz", "Le Chen", "Niranjan Hasabnis", "Mihai Capota", "Nesreen Ahmed", "Ali Jannesari"], "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly\naccelerated progress in code translation, enabling more accurate and efficient\ntransformation across programming languages. While originally developed for\nnatural language processing, LLMs have shown strong capabilities in modeling\nprogramming language syntax and semantics, outperforming traditional rule-based\nsystems in both accuracy and flexibility. These models have streamlined\ncross-language conversion, reduced development overhead, and accelerated legacy\ncode migration. In this paper, we introduce OMPILOT, a novel domain-specific\nencoder-decoder transformer tailored for translating C++ code into OpenMP,\nenabling effective shared-memory parallelization. OMPILOT leverages custom\npre-training objectives that incorporate the semantics of parallel constructs\nand combines both unsupervised and supervised learning strategies to improve\ncode translation robustness. Unlike previous work that focused primarily on\nloop-level transformations, OMPILOT operates at the function level to capture a\nwider semantic context. To evaluate our approach, we propose OMPBLEU, a novel\ncomposite metric specifically crafted to assess the correctness and quality of\nOpenMP parallel constructs, addressing limitations in conventional translation\nmetrics.", "AI": {"tldr": "OMPILOT is a domain-specific encoder-decoder transformer that translates C++ code to OpenMP for shared-memory parallelization, using custom pre-training objectives and operating at function level with a novel evaluation metric called OMPBLEU.", "motivation": "To address limitations in traditional rule-based code translation systems and enable more effective parallelization of C++ code through automated OpenMP translation, capturing wider semantic context than previous loop-level approaches.", "method": "Leverages custom pre-training objectives incorporating parallel construct semantics, combines unsupervised and supervised learning strategies, and operates at function level rather than just loop-level transformations.", "result": "Enables more accurate and efficient transformation of C++ code into OpenMP parallel constructs, outperforming traditional systems in both accuracy and flexibility for shared-memory parallelization.", "conclusion": "OMPILOT represents a significant advancement in code translation for parallel computing, providing robust function-level translation capabilities and introducing OMPBLEU as a specialized metric for evaluating parallel code quality."}}
{"id": "2511.03941", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03941", "abs": "https://arxiv.org/abs/2511.03941", "authors": ["Fabio Diniz Rossi"], "title": "Stochastic Modeling for Energy-Efficient Edge Infrastructure", "comment": "8 pages, 4 figures, 3 tables", "summary": "Edge Computing enables low-latency processing for real-time applications but\nintroduces challenges in power management due to the distributed nature of edge\ndevices and their limited energy resources. This paper proposes a stochastic\nmodeling approach using Markov Chains to analyze power state transitions in\nEdge Computing. By deriving steady-state probabilities and evaluating energy\nconsumption, we demonstrate the benefits of AI-driven predictive power scaling\nover conventional reactive methods. Monte Carlo simulations validate the model,\nshowing strong alignment between theoretical and empirical results. Sensitivity\nanalysis highlights how varying transition probabilities affect power\nefficiency, confirming that predictive scaling minimizes unnecessary\ntransitions and improves overall system responsiveness. Our findings suggest\nthat AI-based power management strategies significantly enhance energy\nefficiency by anticipating workload demands and optimizing state transitions.\nExperimental results indicate that AI-based power management optimizes workload\ndistribution across heterogeneous edge nodes, reducing energy consumption\ndisparities between devices, improving overall efficiency, and enhancing\nadaptive power coordination in multi-node environments.", "AI": {"tldr": "This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing, demonstrating that AI-driven predictive power scaling outperforms conventional reactive methods in energy efficiency and system responsiveness.", "motivation": "Edge Computing enables low-latency processing but faces power management challenges due to distributed edge devices with limited energy resources, requiring efficient power state management.", "method": "The paper uses Markov Chains for stochastic modeling of power state transitions, derives steady-state probabilities, evaluates energy consumption, and validates the model through Monte Carlo simulations and sensitivity analysis.", "result": "Experimental results show strong alignment between theoretical and empirical results, with AI-based power management reducing energy consumption disparities, improving overall efficiency, and enhancing adaptive power coordination in multi-node environments.", "conclusion": "AI-based power management strategies significantly enhance energy efficiency by anticipating workload demands and optimizing state transitions, minimizing unnecessary transitions and improving system responsiveness in Edge Computing environments."}}
{"id": "2511.04268", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04268", "abs": "https://arxiv.org/abs/2511.04268", "authors": ["Iker Mart\u00edn-\u00c1lvarez", "Jos\u00e9 I. Aliaga", "Maribel Castillo", "Sergio Iserte"], "title": "Parallel Spawning Strategies for Dynamic-Aware MPI Applications", "comment": "10 pages, 1 Table, 6 Figures, 8 Equations, 2 Listings", "summary": "Dynamic resource management is an increasingly important capability of High\nPerformance Computing systems, as it enables jobs to adjust their resource\nallocation at runtime. This capability has been shown to reduce workload\nmakespan, substantially decrease job waiting times and improve overall system\nutilization. In this context, malleability refers to the ability of\napplications to adapt to new resource allocations during execution. Although\nbeneficial, malleability incurs significant reconfiguration costs, making the\nreduction of these costs an important research topic.\n  Some existing methods for MPI applications respawn the entire application,\nwhich is an expensive solution that avoids the reuse of original processes.\nOther MPI methods reuse them, but fail to fully release unneeded processes when\nshrinking, since some ranks within the same communicator remain active across\nnodes, preventing the application from returning those nodes to the system.\nThis work overcomes both limitations by proposing a novel parallel spawning\nstrategy, in which all processes cooperate in spawning before redistribution,\nthereby reducing execution time. Additionally, it removes shrinkage\nlimitations, allowing better adaptation of parallel systems to workload and\nreducing their makespan. As a result, it preserves competitive expansion times\nwith at most a $1.25\\times$ overhead, while enabling fast shrink operations\nthat reduce their cost by at least $20\\times$. This strategy has been validated\non both homogeneous and heterogeneous systems and can also be applied in\nshared-resource environments.", "AI": {"tldr": "A novel parallel spawning strategy for MPI applications that enables efficient dynamic resource management by reducing reconfiguration costs during both expansion and shrinkage operations, with significant performance improvements.", "motivation": "Current MPI malleability methods either respawn entire applications (expensive) or fail to fully release unneeded processes during shrinkage, preventing optimal resource utilization in HPC systems.", "method": "Proposes a parallel spawning strategy where all processes cooperate in spawning before redistribution, enabling complete process release during shrinkage and better adaptation to workload changes.", "result": "Preserves competitive expansion times (max 1.25\u00d7 overhead) while enabling fast shrink operations (20\u00d7 cost reduction), validated on both homogeneous and heterogeneous systems.", "conclusion": "The strategy overcomes limitations of existing MPI malleability methods, significantly reducing reconfiguration costs and improving dynamic resource management capabilities in HPC environments."}}
{"id": "2511.04477", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04477", "abs": "https://arxiv.org/abs/2511.04477", "authors": ["Rongxiang Wang", "Kangyuan Shu", "Felix Xiaozhu Lin"], "title": "Enabling Dynamic Sparsity in Quantized LLM Inference", "comment": null, "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.", "AI": {"tldr": "This paper proposes techniques for efficient LLM deployment on resource-constrained devices by combining dynamic sparse inference with low-bit quantization, achieving up to 1.55x faster decoding throughput while maintaining accuracy.", "motivation": "Deploying LLMs on end-user devices is important for responsiveness, privacy, and cost, but limited GPU memory and compute capabilities make efficient execution challenging. Dynamic activation sparsity in LLMs could reduce computation but interacts poorly with group-wise quantization.", "method": "The method includes: (1) zigzag-patterned quantization layout that organizes weights consistently with activation sparsity and improves GPU memory locality; (2) specialized GEMV kernel for this layout to utilize parallel compute units; (3) compact runtime mechanism for gathering sparse indices with minimal overhead.", "result": "Across various model scales and hardware configurations, the approach achieves up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference.", "conclusion": "Structured sparsity and quantization can effectively coexist on commodity GPUs, enabling efficient LLM deployment on resource-constrained devices."}}
{"id": "2511.03747", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03747", "abs": "https://arxiv.org/abs/2511.03747", "authors": ["Ali Safa", "Farida Mohsen", "Zainab Ali", "Bo Wang", "Amine Bermak"], "title": "OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications", "comment": null, "summary": "Memristive crossbars enable in-memory multiply-accumulate and local\nplasticity learning, offering a path to energy-efficient edge AI. To this end,\nwe present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our\nknowledge, is the first fully open memristor interfacing system integrating (i)\na reproducible hardware interface for memristor crossbars with mixed-signal\nread-program-verify loops; (ii) a firmware-software stack with high-level APIs\nfor inference and on-device learning; and (iii) a Voltage-Incremental\nProportional-Integral (VIPI) method to program pre-trained weights into analog\nconductances, followed by chip-in-the-loop fine-tuning to mitigate device\nnon-idealities. OpenMENA is validated on digit recognition, demonstrating the\nflow from weight transfer to on-device adaptation, and on a real-world robot\nobstacle-avoidance task, where the memristor-based model learns to map\nlocalization inputs to motor commands. OpenMENA is released as open source to\ndemocratize memristor-enabled edge-AI research.", "AI": {"tldr": "Open-MENA is the first fully open memristor interfacing system for edge AI, featuring hardware interfaces, software APIs, and VIPI programming method for memristor crossbars, validated on digit recognition and robot obstacle-avoidance tasks.", "motivation": "To democratize memristor-enabled edge-AI research by providing a fully open system that enables energy-efficient in-memory computing and local plasticity learning for edge AI applications.", "method": "Developed Open-MENA with three key components: (1) reproducible hardware interface for memristor crossbars with mixed-signal read-program-verify loops, (2) firmware-software stack with high-level APIs for inference and on-device learning, and (3) VIPI (Voltage-Incremental Proportional-Integral) method for programming pre-trained weights followed by chip-in-the-loop fine-tuning.", "result": "Successfully validated on digit recognition tasks demonstrating the complete flow from weight transfer to on-device adaptation, and on real-world robot obstacle-avoidance where the memristor-based model learned to map localization inputs to motor commands.", "conclusion": "Open-MENA provides a comprehensive open-source platform that enables reproducible research in memristor-based edge AI, successfully demonstrating practical applications in both digit recognition and autonomous robot navigation."}}
{"id": "2511.03944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03944", "abs": "https://arxiv.org/abs/2511.03944", "authors": ["Tong Zhang", "Vikram Sharma Mailthody", "Fei Sun", "Linsen Ma", "Chris J. Newburn", "Teresa Zhang", "Yang Liu", "Jiangpeng Li", "Hao Zhong", "Wen-Mei Hwu"], "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies", "comment": "13 pages, 10 figures", "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy.", "AI": {"tldr": "The paper revisits the classic five-minute rule for data placement between DRAM and storage, updating it with modern considerations including host costs, DRAM bandwidth/capacity, and SSD performance models. It shows that for AI platforms with GPU hosts and high-IOPS SSDs, the caching threshold collapses from minutes to seconds, reframing NAND flash as an active data tier.", "motivation": "To modernize the classical five-minute rule by integrating previously excluded factors like host costs, feasibility limits, and workload behavior, especially for modern AI platforms where traditional storage-memory economics no longer apply.", "method": "Developed a constraint- and workload-aware framework from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded SSD performance models. Created MQSim-Next, a calibrated SSD simulator for validation and sensitivity analysis.", "result": "For modern AI platforms with GPU-centric hosts and ultra-high-IOPS SSDs, the DRAM-to-flash caching threshold collapses from minutes to a few seconds, fundamentally changing the memory hierarchy paradigm and reframing NAND flash as an active data tier.", "conclusion": "The work transforms a classical heuristic into an actionable, feasibility-aware analysis framework and opens a broad research space across the hardware-software stack for AI-era memory hierarchy design, with concrete case studies demonstrating the new design possibilities."}}
{"id": "2511.04523", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04523", "abs": "https://arxiv.org/abs/2511.04523", "authors": ["Silvia Bonomi", "Giovanni Farina", "Roy Friedman", "Eviatar B. Procaccia", "Sebastien Tixeuil"], "title": "A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems", "comment": null, "summary": "Modern distributed systems face growing security threats, as attackers\ncontinuously enhance their skills and vulnerabilities span across the entire\nsystem stack, from hardware to the application layer. In the system design\nphase, fault tolerance techniques can be employed to safeguard systems. From a\ntheoretical perspective, an attacker attempting to compromise a system can be\nabstracted by considering the presence of Byzantine processes in the system.\nAlthough this approach enhances the resilience of the distributed system, it\nintroduces certain limitations regarding the accuracy of the model in\nreflecting real-world scenarios. In this paper, we consider a self-protecting\ndistributed system based on the \\emph{Monitoring-Analyse-Plan-Execute over a\nshared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic\nMobile Byzantine Failure (MBF) that can be plugged into the Analysis component.\nOur new model captures the dynamics of evolving attacks and can be used to\ndrive the self-protection and reconfiguration strategy. We analyze\nmathematically the time that it takes until the number of Byzantine nodes\ncrosses given thresholds, or for the system to self-recover back into a safe\nstate, depending on the rates of Byzantine infection spreading \\emph{vs.} the\nrate of self-recovery. We also provide simulation results that illustrate the\nbehavior of the system under such assumptions.", "AI": {"tldr": "A new probabilistic Mobile Byzantine Failure model for self-protecting distributed systems that captures evolving attack dynamics and enables mathematical analysis of system behavior under attack vs recovery scenarios.", "motivation": "Modern distributed systems face increasing security threats across all layers, and existing Byzantine fault models have limitations in accurately reflecting real-world attack scenarios.", "method": "Proposed a probabilistic Mobile Byzantine Failure model integrated into the MAPE-K architecture's Analysis component, with mathematical analysis of Byzantine node thresholds and system recovery times.", "result": "Developed mathematical models to analyze time until Byzantine nodes cross thresholds and system self-recovery, with simulation results validating the system behavior under attack-recovery dynamics.", "conclusion": "The proposed probabilistic MBF model effectively captures evolving attack patterns and can drive self-protection strategies in distributed systems, providing analytical tools to understand system resilience."}}
{"id": "2511.04136", "categories": ["cs.ET", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.04136", "abs": "https://arxiv.org/abs/2511.04136", "authors": ["Neil Na", "Chih-Hao Cheng", "Shou-Chen Hsu", "Che-Fu Liang", "Chung-Chih Lin", "Nathaniel Y. Na", "Andrew I. Shieh", "Erik Chen", "Haisheng Rong", "Richard A. Soref"], "title": "Implementation of transformer-based LLMs with large-scale optoelectronic neurons on a CMOS image sensor platform", "comment": null, "summary": "The recent rapid deployment of datacenter infrastructures for performing\nlarge language models (LLMs) and related artificial intelligence (AI)\napplications in the clouds is predicted to incur an exponentially growing\nenergy consumption in the near-term future. In this paper, we propose and\nanalyze the implementation of the transformer model, which is the cornerstone\nof the modern LLMs, with novel large-scale optoelectronic neurons (OENs)\nconstructed over the commercially available complementary\nmetal-oxide-semiconductor (CMOS) image sensor (CIS) platform. With all of the\nrequired optoelectronic devices and electronic circuits integrated in a chiplet\nonly about 2 cm by 3 cm in size, 175 billon parameters in the case of GPT-3 are\nshown to perform inference at an unprecedented speed of 12.6 POPS using only a\n40 nm CMOS process node, along with a high power efficiency of 74 TOPS/W and a\nhigh area efficiency of 19 TOPS/mm2, both surpassing the related digital\nelectronics by roughly two orders of magnitude. The influence of the\nquantization formats and the hardware induced errors are numerically\ninvestigated, and are shown to have a minimal impact. Our study presents a new\nyet practical path toward analog neural processing units (NPUs) to complement\nexisting digital processing units.", "AI": {"tldr": "This paper proposes implementing transformer models using novel large-scale optoelectronic neurons (OENs) integrated on CMOS image sensor platforms, achieving unprecedented speed and power efficiency for LLM inference.", "motivation": "The rapid deployment of datacenter infrastructure for LLMs and AI applications is predicted to cause exponentially growing energy consumption, necessitating more efficient computing solutions.", "method": "Implementation of transformer models using novel large-scale optoelectronic neurons constructed over commercially available CMOS image sensor platforms, with all required optoelectronic devices and electronic circuits integrated in a small chiplet.", "result": "Achieved inference speed of 12.6 POPS for GPT-3's 175 billion parameters using only 40 nm CMOS process, with power efficiency of 74 TOPS/W and area efficiency of 19 TOPS/mm\u00b2 - both roughly two orders of magnitude better than digital electronics.", "conclusion": "The study presents a practical path toward analog neural processing units to complement existing digital processing units, with minimal impact from quantization formats and hardware-induced errors."}}
{"id": "2511.04036", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04036", "abs": "https://arxiv.org/abs/2511.04036", "authors": ["Yue Jiet Chong", "Yimin Wang", "Zhen Wu", "Xuanyao Fong"], "title": "PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration", "comment": null, "summary": "This paper presents a 3D-stacked chiplets based large language model (LLM)\ninference accelerator, consisting of non-volatile in-memory-computing\nprocessing elements (PEs) and Inter-PE Computational Network (IPCN),\ninterconnected via silicon photonic to effectively address the communication\nbottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling\nand workload mapping. Simulation results show it achieves $3.95\\times$ speedup\nand $30\\times$ efficiency improvement over the Nvidia A100 before chiplet\nclustering and power gating scheme (CCPG). Additionally, the system achieves\nfurther scalability and efficiency improvement with the implementation of CCPG\nto accommodate larger models, attaining $57\\times$ efficiency improvement over\nNvidia H100 at similar throughput.", "AI": {"tldr": "A 3D-stacked chiplet-based LLM inference accelerator using non-volatile in-memory-computing PEs and silicon photonic interconnects, achieving significant speedup and efficiency improvements over Nvidia GPUs.", "motivation": "To address communication bottlenecks in large language model inference through advanced 3D chiplet architecture and photonic interconnects.", "method": "Uses 3D-stacked chiplets with non-volatile in-memory-computing processing elements interconnected via silicon photonic Inter-PE Computational Network, plus a specialized LLM mapping scheme for hardware optimization.", "result": "Achieves 3.95\u00d7 speedup and 30\u00d7 efficiency improvement over Nvidia A100, with further improvements (57\u00d7 efficiency over H100) using chiplet clustering and power gating scheme.", "conclusion": "The proposed 3D-stacked chiplet architecture with photonic interconnects effectively overcomes communication bottlenecks and significantly improves LLM inference performance and efficiency."}}
{"id": "2511.04631", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04631", "abs": "https://arxiv.org/abs/2511.04631", "authors": ["Petr Kuznetsov", "Nathan Josia Schrodt"], "title": "Resolving Conflicts with Grace: Dynamically Concurrent Universality", "comment": null, "summary": "Synchronization is the major obstacle to scalability in distributed\ncomputing. Concurrent operations on the shared data engage in synchronization\nwhen they encounter a \\emph{conflict}, i.e., their effects depend on the order\nin which they are applied. Ideally, one would like to detect conflicts in a\n\\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it\nis very common that two concurrent operations conflict only in some rarely\noccurring states. In this paper, we define the notion of \\emph{dynamic\nconcurrency}: an operation employs strong synchronization primitives only if it\n\\emph{has} to arbitrate with concurrent operations, given the current system\nstate. We then present a dynamically concurrent universal construction.", "AI": {"tldr": "This paper introduces dynamic concurrency, where operations use strong synchronization only when necessary based on current system state, and presents a dynamically concurrent universal construction.", "motivation": "Synchronization is the major scalability bottleneck in distributed computing, particularly when operations conflict only in rare system states. The goal is to detect conflicts dynamically and minimize unnecessary synchronization.", "method": "The paper defines dynamic concurrency and presents a universal construction that employs strong synchronization primitives only when operations must arbitrate with concurrent operations given the current system state.", "result": "A dynamically concurrent universal construction is developed that adapts synchronization requirements to the actual system state rather than using static conservative approaches.", "conclusion": "Dynamic concurrency provides a more efficient approach to synchronization by reducing unnecessary coordination overhead and improving scalability in distributed systems."}}
{"id": "2511.04104", "categories": ["cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.04104", "abs": "https://arxiv.org/abs/2511.04104", "authors": ["Chao Guo", "Jiahe Xu", "Moshe Zukerman"], "title": "Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs", "comment": null, "summary": "Hardware disaggregation seeks to transform Data Center (DC) resources from\ntraditional server fleets into unified resource pools. Despite existing\nchallenges that may hinder its full realization, significant progress has been\nmade in both industry and academia. In this article, we provide an overview of\nthe motivations and recent advancements in hardware disaggregation. We further\ndiscuss the research challenges and opportunities associated with disaggregated\narchitectures, focusing on aspects that have received limited attention. We\nargue that hardware disaggregation has the potential to reshape the entire DC\necosystem, impacting application design, resource scheduling, hardware\nconfiguration, cooling, and power system optimization. Additionally, we present\na numerical study to illustrate several key aspects of these challenges.", "AI": {"tldr": "Overview of hardware disaggregation motivations, advancements, challenges, and opportunities in data centers, with a numerical study illustrating key aspects.", "motivation": "Transform data center resources from traditional server fleets into unified resource pools to improve resource utilization and efficiency.", "method": "Provides an overview of motivations and recent advancements, discusses research challenges and opportunities, and presents a numerical study.", "result": "Hardware disaggregation has the potential to reshape the entire data center ecosystem, impacting application design, resource scheduling, hardware configuration, cooling, and power system optimization.", "conclusion": "Despite existing challenges, significant progress has been made in hardware disaggregation, and it presents opportunities to transform data center architecture and operations."}}
{"id": "2511.04321", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04321", "abs": "https://arxiv.org/abs/2511.04321", "authors": ["Yuanpeng Zhang", "Xing Hu", "Xi Chen", "Zhihang Yuan", "Cong Li", "Jingchen Zhu", "Zhao Wang", "Chenguang Zhang", "Xin Si", "Wei Gao", "Qiang Wu", "Runsheng Wang", "Guangyu Sun"], "title": "AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM", "comment": "18 pages, 22 figures, accepted by ISCA 2025", "summary": "SRAM Processing-in-Memory (PIM) has emerged as the most promising\nimplementation for high-performance PIM, delivering superior computing density,\nenergy efficiency, and computational precision. However, the pursuit of higher\nperformance necessitates more complex circuit designs and increased operating\nfrequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly\ndegrade chip performance and even threaten reliability. Conventional\ncircuit-level IR-drop mitigation methods, such as back-end optimizations, are\nresource-intensive and often compromise power, performance, and area (PPA). To\naddress these challenges, we propose AIM, comprehensive software and hardware\nco-design for architecture-level IR-drop mitigation in high-performance PIM.\nInitially, leveraging the bit-serial and in-situ dataflow processing properties\nof PIM, we introduce Rtog and HR, which establish a direct correlation between\nPIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,\nenabling extensive exploration of architecture-level IR-drop mitigation while\nmaintaining computational accuracy through software optimization. Subsequently,\nwe develop IR-Booster, a dynamic adjustment mechanism that integrates\nsoftware-level HR information with hardware-based IR-drop monitoring to adapt\nthe V-f pairs of the PIM macro, achieving enhanced energy efficiency and\nperformance. Finally, we propose the HR-aware task mapping method, bridging\nsoftware and hardware designs to achieve optimal improvement. Post-layout\nsimulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up\nto 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement\nand 1.152x speedup.", "AI": {"tldr": "AIM is a comprehensive software-hardware co-design approach that mitigates IR-drop issues in high-performance SRAM Processing-in-Memory (PIM) through workload-aware optimization, achieving significant performance and energy efficiency improvements.", "motivation": "High-performance SRAM PIM faces severe IR-drop problems due to complex circuit designs and high operating frequencies, which degrade chip performance and reliability. Conventional circuit-level mitigation methods are resource-intensive and compromise power, performance, and area.", "method": "Proposes AIM framework with: 1) Rtog and HR to correlate PIM workloads with IR-drop, 2) LHR and WDS for architecture-level IR-drop mitigation via software optimization, 3) IR-Booster for dynamic V-f pair adjustment using software HR info and hardware monitoring, 4) HR-aware task mapping to bridge software-hardware designs.", "result": "Post-layout simulation on 7nm 256-TOPS PIM chip shows: 69.2% IR-drop mitigation, 2.29x energy efficiency improvement, and 1.152x speedup.", "conclusion": "AIM effectively addresses IR-drop challenges in high-performance PIM through comprehensive software-hardware co-design, achieving substantial performance and efficiency gains while maintaining computational accuracy."}}
{"id": "2511.04677", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04677", "abs": "https://arxiv.org/abs/2511.04677", "authors": ["Joaquin Tarraga-Moreno", "Daniel Barley", "Francisco J. Andujar Munoz", "Jesus Escudero-Sahuquillo", "Holger Froning", "Pedro Javier Garcia", "Francisco J. Quiles", "Jose Duato"], "title": "Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers", "comment": null, "summary": "The rapid growth of data-intensive applications such as generative AI,\nscientific simulations, and large-scale analytics is driving modern\nsupercomputers and data centers toward increasingly heterogeneous and tightly\nintegrated architectures. These systems combine powerful CPUs and accelerators\nwith emerging high-bandwidth memory and storage technologies to reduce data\nmovement and improve computational efficiency. However, as the number of\naccelerators per node increases, communication bottlenecks emerge both within\nand between nodes, particularly when network resources are shared among\nheterogeneous components.", "AI": {"tldr": "Modern supercomputers face communication bottlenecks due to increasing heterogeneity and tight integration of CPUs, accelerators, and high-bandwidth memory/storage technologies.", "motivation": "The growth of data-intensive applications like generative AI and scientific simulations is driving systems toward heterogeneous architectures to reduce data movement and improve efficiency.", "method": "Combining powerful CPUs with accelerators and emerging high-bandwidth memory/storage technologies in tightly integrated architectures.", "result": "As accelerator count per node increases, communication bottlenecks emerge both within and between nodes, especially when network resources are shared among heterogeneous components.", "conclusion": "Current heterogeneous architectures face significant communication challenges that need to be addressed as systems scale with more accelerators per node."}}
