<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Magnetic Field-Mediated Superconducting Logic](https://arxiv.org/abs/2602.07146)
*Alexander J. Edwards,Son T. Le,Nicholas W. G. Smith,Ebenezer C. Usih,Austin Thomas,Christopher J. K. Richardson,Nicholas A. Blumenschein,Aubrey T. Hanbicki,Adam L. Friedman,Joseph S. Friedman*

Main category: cs.ET

TL;DR: Novel superconducting switching device using spin-orbit torque-switched magnet proximity magnetization to control superconductor resistivity, enabling energy-efficient superconducting logic circuits.


<details>
  <summary>Details</summary>
Motivation: Superconductors are attractive for energy-efficient computing but face fundamental limitations in logic circuit integration that hinder scaling and increase energy consumption.

Method: Proposed and experimentally demonstrated a superconducting switching device that utilizes proximity magnetization from a spin-orbit torque-switched magnet to control superconductor resistivity. Also proposed a complete logic family comprised solely of these devices.

Result: Experimental demonstration of the novel superconducting switching device and proposal of a complete logic family based on this technology.

Conclusion: This novel implementation has the potential to drastically outperform existing superconducting logic families in terms of energy efficiency and scalability.

Abstract: While superconductors are highly attractive for energy-efficient computing, fundamental limitations in their logic circuit integration have hindered scaling and led to increased energy consumption. We therefore propose and experimentally demonstrate a novel superconducting switching device utilizing the proximity magnetization from a spin-orbit torque-switched magnet to control the resistivity of a superconductor. We further propose a complete logic family comprised solely of these devices. This novel implementation has the potential to drastically outperform existing superconducting logic families in terms of energy efficiency and scalability.

</details>


### [2] [Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units](https://arxiv.org/abs/2602.07518)
*Manuel Escudero,Mohamadreza Zolfagharinejad,Sjoerd van den Belt,Nikolaos Alachiotis,Wilfred G. van der Wiel*

Main category: cs.ET

TL;DR: Analog hardware implementation of Kolmogorov-Arnold Networks using reconfigurable nonlinear-processing units achieves significant energy and area efficiency for edge inference.


<details>
  <summary>Details</summary>
Motivation: While KANs offer promising neural computation with learnable nonlinear edge functions, their efficient hardware implementation remains challenging. The paper aims to develop a physical analog KAN architecture that can be implemented in silicon hardware for energy-efficient edge inference.

Method: The authors introduce a physical analog KAN architecture using reconfigurable nonlinear-processing units (RNPUs) - multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. These RNPUs are combined into edge processors and assembled into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing.

Result: The system demonstrates accurate function approximation across increasing task complexity with fewer or comparable trainable parameters than MLPs. Hardware measurements show energy per inference of ~250 pJ and latency of ~600 ns, representing 10²-10³× energy reduction and ~10× area reduction compared to digital fixed-point MLPs at similar error.

Conclusion: RNPUs serve as scalable, hardware-native nonlinear computing primitives, and analog KAN architectures provide a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware for edge inference applications.

Abstract: Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\sim$250 pJ and an end-to-end inference latency of $\sim$600 ns for a representative workload, corresponding to a $\sim$10$^{2}$-10$^{3}\times$ reduction in energy accompanied by a $\sim$10$\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.

</details>


### [3] [HoloGraph: All-Optical Graph Learning via Light Diffraction](https://arxiv.org/abs/2602.07724)
*Yingjie Li,Shanglin Zhou,Caiwen Ding,Cunxi Yu*

Main category: cs.ET

TL;DR: HoloGraph is the first monolithic free-space all-optical graph neural network system that enables light-speed optical message passing over graph structures using diffractive propagation and phase modulations.


<details>
  <summary>Details</summary>
Motivation: Existing physics-based neural networks like Diffractive Optical Neural Networks (DONNs) have shown advantages in computational speed and energy efficiency but have mostly focused on machine intelligence with limited capability for graph-structured tasks.

Method: Introduces a novel domain-specific message-passing mechanism with optical skip channels integrated into light propagation for all-optical graph learning, using diffractive propagation and phase modulations for optical message passing over graph structures.

Result: Experimental results using standard graph learning datasets Cora-ML and Citeseer show competitive or even superior classification performance compared to conventional digital graph neural networks.

Conclusion: HoloGraph represents a significant advancement in physics-based neural networks for graph-structured tasks, demonstrating the effectiveness of the proposed novel architecture and algorithmic methods through comprehensive ablation studies.

Abstract: As a representative of next-generation device/circuit technology beyond CMOS, physics-based neural networks such as Diffractive Optical Neural Networks (DONNs) have demonstrated promising advantages in computational speed and energy efficiency. However, existing DONNs and other physics-based neural networks have mostly focused on exploring their machine intelligence, with limited studies in handling graph-structured tasks. Thus, we introduce HoloGraph, the first monolithic free-space all-optical graph neural network system. It proposes a novel, domain-specific message-passing mechanism with optical skip channels integrated into light propagation for the all-optical graph learning. HoloGraph enables light-speed optical message passing over graph structures with diffractive propagation and phase modulations. Our experimental results with HoloGraph, conducted using standard graph learning datasets Cora-ML and Citeseer, show competitive or even superior classification performance compared to conventional digital graph neural networks. Comprehensive ablation studies demonstrate the effectiveness of the proposed novel architecture and algorithmic methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models](https://arxiv.org/abs/2602.08257)
*Antonis Psistakis,Burak Ocalan,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: HEAL is a low-overhead recovery scheme for non-transactional leaderless distributed systems that performs optimized online incremental recovery with minimal throughput impact.


<details>
  <summary>Details</summary>
Motivation: The need for lightweight recovery mechanisms in distributed systems that can recover from faults quickly with minimal impact on system throughput, especially for modern non-transactional leaderless systems.

Method: Proposes HEAL, a general recovery scheme with algorithms for Linearizable consistency and different memory persistency models, performing optimized online incremental recovery on node failures.

Result: HEAL recovers clusters in 120ms average with only 8.7% throughput reduction, compared to 360 seconds and 16.2% reduction for conventional leaderless recovery, and 20.7x faster recovery with 62.4% less throughput degradation than leader-based incremental recovery.

Conclusion: HEAL is an effective low-overhead recovery scheme for modern non-transactional leaderless distributed systems, significantly outperforming both conventional leaderless recovery and state-of-the-art leader-based incremental recovery schemes.

Abstract: Ensuring resilience in distributed systems has become an acute concern. In today's environment, it is crucial to develop light-weight mechanisms that recover a distributed system from faults quickly and with only a small impact on the live-system throughput. To address this need, this paper proposes a new low-overhead, general recovery scheme for modern non-transactional leaderless distributed systems. We call our scheme HEAL. On a node failure, HEAL performs an optimized online incremental recovery. This paper presents HEAL's algorithms for settings with Linearizable consistency and different memory persistency models. We implement HEAL on a 6-node Intel cluster. Our experiments running TAOBench workloads show that HEAL is very effective. HEAL recovers the cluster in 120 milliseconds on average, while reducing the throughput of the running workload by an average of 8.7%. In contrast, a conventional recovery scheme for leaderless systems needs 360 seconds to recover, reducing the throughput of the system by 16.2%. Finally, compared to an incremental recovery scheme for a state-of-the-art leader-based system, HEAL reduces the average recovery latency by 20.7x and the throughput degradation by 62.4%.

</details>


### [5] [PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping](https://arxiv.org/abs/2602.08747)
*Zhixin Zhao,Yitao Hu,Simin Chen,Mingfang Ji,Wei Yang,Yuhao Zhang,Laiping Zhao,Wenxin Li,Xiulong Liu,Wenyu Qu,Hao Wang*

Main category: cs.DC

TL;DR: PARD is a proactive dropping system for DNN inference pipelines that improves goodput by making timely dropping decisions and selecting appropriate requests to drop based on runtime information and workload intensity.


<details>
  <summary>Details</summary>
Motivation: Existing reactive dropping policies in DNN inference pipelines drop requests too late or choose wrong requests, leading to poor goodput. The paper proposes that proactive dropping can enhance goodput across the entire workload.

Method: PARD integrates: 1) a proactive dropping method using runtime pipeline information to decide when to drop requests, and 2) an adaptive request priority mechanism that selects which requests to drop based on remaining latency budgets and workload intensity.

Result: Evaluation on a 64-GPU cluster with real-world workloads shows PARD achieves 16%-176% higher goodput than state-of-the-art, reduces drop rate by 1.6×-17×, and reduces wasted computation resources by 1.5×-62×.

Conclusion: Proactive dropping with timely decisions and proper request selection significantly improves goodput in DNN inference pipelines compared to reactive approaches, demonstrating the value of PARD's design.

Abstract: Modern deep neural network (DNN) applications integrate multiple DNN models into inference pipelines with stringent latency requirements for customized tasks. To mitigate extensive request timeouts caused by accumulation, systems for inference pipelines commonly drop a subset of requests so the remaining ones can satisfy latency constraints. Since it is commonly believed that request dropping adversely affects goodput, existing systems only drop requests when they have to, which we call reactive dropping. However, this reactive policy can not maintain high goodput, as it neither makes timely dropping decisions nor identifies the proper set of requests to drop, leading to issues of dropping requests too late or dropping the wrong set of requests.
  We propose that the inference system should proactively drop certain requests in advance to enhance the goodput across the entire workload. To achieve this, we design an inference system PARD. It enhances goodput with timely and precise dropping decisions by integrating a proactive dropping method that decides when to drop requests using runtime information of the inference pipeline, and an adaptive request priority mechanism that selects which specific requests to drop based on remaining latency budgets and workload intensity. Evaluation on a cluster of 64 GPUs over real-world workloads shows that PARD achieves $16\%$-$176\%$ higher goodput than the state of the art while reducing the drop rate and wasted computation resources by $1.6\times$-$17\times$ and $1.5\times$-$62\times$ respectively.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [6] [Fork, Explore, Commit: OS Primitives for Agentic Exploration](https://arxiv.org/abs/2602.08199)
*Cong Wang,Yusheng Zheng*

Main category: cs.OS

TL;DR: Branch contexts: a new OS abstraction for AI agent exploration with isolated copy-on-write workspaces and atomic commit/rollback for both filesystem and process state.


<details>
  <summary>Details</summary>
Motivation: AI agents need to explore multiple solution paths in parallel, each potentially modifying files and spawning processes, requiring isolated environments with atomic commit and rollback semantics.

Method: Introduce branch contexts with four key features: copy-on-write isolation, structured lifecycle, first-commit-wins resolution, and nestable contexts. Implemented via BranchFS (FUSE-based filesystem) and a proposed branch() syscall for Linux.

Result: BranchFS achieves sub-350 μs branch creation independent of base filesystem size, and modification-proportional commit overhead (under 1 ms for small changes).

Conclusion: Branch contexts provide efficient isolation for AI agent exploration with minimal overhead, enabling parallel path exploration with reliable commit semantics.

Abstract: AI agents increasingly perform agentic exploration: pursuing multiple solution paths in parallel and committing only the successful one. Because each exploration path may modify files and spawn processes, agents require isolated environments with atomic commit and rollback semantics for both filesystem state and process state. We introduce the branch context, a new OS abstraction that provides: (1) copy-on-write state isolation with independent filesystem views and process groups, (2) a structured lifecycle of fork, explore, and commit/abort, (3) first-commit-wins resolution that automatically invalidates sibling branches, and (4) nestable contexts for hierarchical exploration. We realize branch contexts in Linux through two complementary components. First, BranchFS is a FUSE-based filesystem that gives each branch context an isolated copy-on-write workspace, with O(1) creation, atomic commit to the parent, and automatic sibling invalidation, all without root privileges. BranchFS is open sourced in https://github.com/multikernel/branchfs. Second, branch() is a proposed Linux syscall that spawns processes into branch contexts with reliable termination, kernel-enforced sibling isolation, and first-commit-wins coordination. Preliminary evaluation of BranchFS shows sub-350 us branch creation independent of base filesystem size, and modification-proportional commit overhead (under 1 ms for small changes).

</details>


### [7] [Equilibria: Fair Multi-Tenant CXL Memory Tiering At Scale](https://arxiv.org/abs/2602.08800)
*Kaiyang Zhao,Neha Gholkar,Hasan Maruf,Abhishek Dhanotia,Johannes Weiner,Gregory Price,Ning Sun,Bhavya Dwivedi,Stuart Clark,Dimitrios Skarlatos*

Main category: cs.OS

TL;DR: Equilibria is an OS framework for fair, multi-tenant CXL memory tiering at datacenter scale that addresses limitations of existing solutions by providing per-container controls, flexible fairness policies, and observability to prevent performance interference.


<details>
  <summary>Details</summary>
Motivation: Memory dominates datacenter costs and power, and CXL expansion offers cost-effective memory but requires software-level tiering. Existing solutions lack multi-tenancy support, have limited control-plane flexibility leading to fairness violations, and insufficient observability for diagnosing performance issues at scale.

Method: Equilibria provides per-container controls for memory fair-share allocation and fine-grained observability of tiered-memory usage. It enforces user-specified fairness policies through regulated promotion/demotion and mitigates noisy-neighbor interference by suppressing thrashing.

Result: Evaluated in a large hyperscaler fleet with production workloads and benchmarks, Equilibria helps meet SLOs while avoiding performance interference. It improves performance over state-of-the-art Linux solution TPP by up to 52% for production workloads and 1.7x for benchmarks.

Conclusion: Equilibria successfully addresses key limitations of existing CXL tiering solutions by providing fair, multi-tenant memory management with proper controls and observability, with all patches released to the Linux community.

Abstract: Memory dominates datacenter system cost and power. Memory expansion via Compute Express Link (CXL) is an effective way to provide additional memory at lower cost and power, but its effective use requires software-level tiering for hyperscaler workloads. Existing tiering solutions, including current Linux support, face fundamental limitations in production deployments. First, they lack multi-tenancy support, failing to handle stacked homogeneous or heterogeneous workloads. Second, limited control-plane flexibility leads to fairness violations and performance variability. Finally, insufficient observability prevents operators from diagnosing performance pathologies at scale.
  We present Equilibria, an OS framework enabling fair, multi-tenant CXL tiering at datacenter scale. Equilibria provides per-container controls for memory fair-share allocation and fine-grained observability of tiered-memory usage and operations. It further enforces flexible, user-specified fairness policies through regulated promotion and demotion, and mitigates noisy-neighbor interference by suppressing thrashing.
  Evaluated in a large hyperscaler fleet using production workloads and benchmarks, Equilibria helps workloads meet service level objectives (SLOs) while avoiding performance interference. It improves performance over the state-of-the-art Linux solution, TPP, by up to 52% for production workloads and 1.7x for benchmarks. All Equilibria patches have been released to the Linux community.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Investigating Energy Bounds of Analog Compute-in-Memory with Local Normalization](https://arxiv.org/abs/2602.08081)
*Brian Rojkov,Shubham Ranjan,Derek Wright,Manoj Sachdev*

Main category: cs.AR

TL;DR: GR-MAC enables energy-efficient analog CIM for floating-point LLMs by using local normalization to decouple hardware resolution from input dynamic range.


<details>
  <summary>Details</summary>
Motivation: Edge AI needs energy efficiency, LLMs use low-bit floating-point formats, but conventional CIM wastes hardware resolution on dynamic range rather than precision, with ADC dominating energy consumption.

Method: Introduces Gain-Ranging MAC (GR-MAC) with local normalization for each input, weight, and MAC output using low-power digital logic, keeping expensive MAC operations in energy-efficient analog domain.

Result: 4-bit increase in input dynamic range without energy cost at 35 dB SQNR; ADC resolution becomes invariant to input distribution with 1.5-bit reduction compared to conventional lower bound.

Conclusion: GR-MAC establishes pathway for analog CIM to achieve favorable energy scaling for modern AI workloads by decoupling hardware requirements from input dynamic range.

Abstract: Modern edge AI workloads demand maximum energy efficiency, motivating the pursuit of analog Compute-in-Memory (CIM) architectures. Simultaneously, the popularity of Large-Language-Models (LLMs) drives the adoption of low-bit floating-point formats which prioritize dynamic range. However, the conventional direct-accumulation CIM accommodates floating-points by normalizing them to a shared widened fixed-point scale. Consequently, hardware resolution is dictated by the input's dynamic range rather than its precision, and energy consumption is dominated by the ADC. We address this limitation by introducing local normalization for each input, weight, and multiply-accumulate (MAC) output via a Gain-Ranging MAC (GR-MAC). Normalization overhead is handled by low-power digital logic, enabling the computationally expensive MAC operation to remain in the energy-efficient low-precision analog regime. Energy modelling shows that the addition of a gain-ranging Stage to the MAC enables a 4-bit increase in input dynamic range without increased energy consumption at a 35 dB SQNR standard. Additionally, the ADC resolution requirement becomes invariant to input distribution assumptions, allowing construction of an upper bound with a 1.5-bit reduction compared to the conventional lower bound. These results establish a pathway towards unlocking favourable energy scaling trends of analog CIM for modern AI workloads.

</details>


### [9] [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](https://arxiv.org/abs/2602.08323)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: AFMTJs enable ultrafast switching with picosecond speeds and femtojoule energy, outperforming conventional MTJs by 8x lower latency and 9x lower energy, delivering 17.5x speedup and 20x energy savings in in-memory computing architectures.


<details>
  <summary>Details</summary>
Motivation: To develop antiferromagnetic tunnel junctions (AFMTJs) that overcome the limitations of conventional magnetic tunnel junctions (MTJs) by enabling ultrafast picosecond switching and ultra-low femtojoule write energy for next-generation computing systems.

Method: Created the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling, using SPICE-based simulations to evaluate performance metrics.

Result: AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into in-memory computing architectures, they deliver 17.5x average speedup and nearly 20x energy savings compared to CPU baselines, significantly outperforming MTJ-based IMC.

Conclusion: AFMTJs establish themselves as a compelling primitive for scalable, low-power computing due to their superior performance in speed and energy efficiency compared to conventional MTJ technology.

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

</details>


### [10] [karl. -- A Research Vehicle for Automated and Connected Driving](https://arxiv.org/abs/2602.08842)
*Jean-Pierre Busch,Lukas Ostendorf,Guido Linden,Lennart Reiher,Till Beemelmanns,Bastian Lampe,Timo Woopen,Lutz Eckstein*

Main category: cs.AR

TL;DR: The paper presents karl., a new research vehicle for automated and connected driving, designed to provide institutions with access to L4-capable testing platforms for independent research, engineering, and validation.


<details>
  <summary>Details</summary>
Motivation: There is a gap in access to L4-capable research vehicles for institutions outside major corporations, limiting independent research in automated and connected driving despite the importance of real-world testing alongside simulation.

Method: The authors designed and built karl., a flexible research vehicle platform for automated and connected driving, sharing the reasoning, design choices, and technical details to help bridge the access gap.

Result: The paper presents karl. as a completed research vehicle that enables real-world testing, validation, data collection, and demonstration of futuristic use cases for automated and connected driving research.

Conclusion: By sharing the design and technical details of karl., the authors aim to help other institutions overcome the barrier of limited access to L4-capable research vehicles, fostering more independent research in automated and connected driving.

Abstract: As highly automated driving is transitioning from single-vehicle closed-access testing to commercial deployments of public ride-hailing in selected areas (e.g., Waymo), automated driving and connected cooperative intelligent transport systems (C-ITS) remain active fields of research. Even though simulation is omnipresent in the development and validation life cycle of automated and connected driving technology, the complex nature of public road traffic and software that masters it still requires real-world integration and testing with actual vehicles. Dedicated vehicles for research and development allow testing and validation of software and hardware components under real-world conditions early on. They also enable collecting and publishing real-world datasets that let others conduct research without vehicle access, and support early demonstration of futuristic use cases. In this paper, we present karl., our new research vehicle for automated and connected driving. Apart from major corporations, few institutions worldwide have access to their own L4-capable research vehicles, restricting their ability to carry out independent research. This paper aims to help bridge that gap by sharing the reasoning, design choices, and technical details that went into making karl. a flexible and powerful platform for research, engineering, and validation in the context of automated and connected driving. More impressions of karl. are available at https://karl.ac.

</details>
