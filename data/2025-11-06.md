<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.ET](#cs.ET) [Total: 3]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)
*Andrei Ivanov,Siyuan Shen,Gioele Gottardo,Marcin Chrapek,Afif Boudaoud,Timo Schneider,Luca Benini,Torsten Hoefler*

Main category: cs.PF

TL;DR: PerfLLM is an automatic optimization methodology that uses Large Language Models and Reinforcement Learning to optimize machine learning performance across diverse hardware architectures without requiring hardware-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of ML models and hardware heterogeneity make manual optimization resource-intensive, while existing automatic approaches rely on complex hardware-specific heuristics that hinder performance portability.

Method: Uses LLMs and RL with PerfDojo environment that frames optimization as an RL game using human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations.

Result: Demonstrates significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

Conclusion: PerfLLM enables effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training for improved performance portability.

Abstract: The increasing complexity of machine learning models and the proliferation of
diverse hardware architectures (CPUs, GPUs, accelerators) make achieving
optimal performance a significant challenge. Heterogeneity in instruction sets,
specialized kernel requirements for different data types and model features
(e.g., sparsity, quantization), and architecture-specific optimizations
complicate performance tuning. Manual optimization is resource-intensive, while
existing automatic approaches often rely on complex hardware-specific
heuristics and uninterpretable intermediate representations, hindering
performance portability. We introduce PerfLLM, a novel automatic optimization
methodology leveraging Large Language Models (LLMs) and Reinforcement Learning
(RL). Central to this is PerfDojo, an environment framing optimization as an RL
game using a human-readable, mathematically-inspired code representation that
guarantees semantic validity through transformations. This allows effective
optimization without prior hardware knowledge, facilitating both human analysis
and RL agent training. We demonstrate PerfLLM's ability to achieve significant
performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [LogicSparse: Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators](https://arxiv.org/abs/2511.03079)
*Changhong Li,Biswajit Basu,Shreejith Shanker*

Main category: cs.AR

TL;DR: A framework that embeds unstructured sparsity into FPGA dataflow accelerators for QNNs, achieving high compression and throughput improvements without dedicated sparse engines.


<details>
  <summary>Details</summary>
Motivation: FPGAs show promise for QNN deployment but face limitations with modern deep-learning models on resource-constrained edge devices. Unstructured sparsity remains underexploited due to irregular memory access patterns.

Method: Introduces a framework that embeds unstructured sparsity into dataflow accelerators, eliminating dedicated sparse engines while preserving parallelism. Includes a hardware-aware pruning strategy for improved efficiency.

Result: On LeNet-5, achieves 51.6x compression and 1.23x throughput improvement using only 5.12% of LUTs, effectively exploiting unstructured sparsity.

Conclusion: The framework successfully enables efficient exploitation of unstructured sparsity for QNN acceleration on FPGAs without requiring specialized sparse hardware.

Abstract: FPGAs have been shown to be a promising platform for deploying Quantised
Neural Networks (QNNs) with high-speed, low-latency, and energy-efficient
inference. However, the complexity of modern deep-learning models limits the
performance on resource-constrained edge devices. While quantisation and
pruning alleviate these challenges, unstructured sparsity remains
underexploited due to irregular memory access. This work introduces a framework
that embeds unstructured sparsity into dataflow accelerators, eliminating the
need for dedicated sparse engines and preserving parallelism. A hardware-aware
pruning strategy is introduced to improve efficiency and design flow further.
On LeNet-5, the framework attains 51.6 x compression and 1.23 x throughput
improvement using only 5.12% of LUTs, effectively exploiting unstructured
sparsity for QNN acceleration.

</details>


### [3] [An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM](https://arxiv.org/abs/2511.03203)
*Deyang Yu,Chenchen Liu,Chuanjie Zhang,Xiao Fang,Weisheng Zhao*

Main category: cs.AR

TL;DR: A novel SOT-MRAM-based computing-in-memory macro using event-driven spiking processing achieves high energy efficiency of 243.6 TOPS/W through hybrid series-parallel cell structure and lightweight spike encoding circuits.


<details>
  <summary>Details</summary>
Motivation: Existing MRAM-based computing-in-memory designs suffer from high energy consumption due to complex analog circuits, creating a need for more energy-efficient solutions.

Method: Uses SOT-MRAM crossbar with hybrid series-parallel cell structure for matrix-vector multiplication, implements event-driven spiking processing with lightweight circuits for signal encoding/decoding instead of traditional analog circuits.

Result: Designed and evaluated in 28nm technology, achieves peak energy efficiency of 243.6 TOPS/W, significantly outperforming existing designs.

Conclusion: The proposed SOT-MRAM-based CIM macro with event-driven spiking processing provides a highly energy-efficient solution for computing-in-memory applications.

Abstract: The application of Magnetic Random-Access Memory (MRAM) in
computing-in-memory (CIM) has gained significant attention. However, existing
designs often suffer from high energy consumption due to their reliance on
complex analog circuits for computation. In this work, we present a Spin-Orbit-
Torque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking
processing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid
series-parallel cell structure to efficiently support matrix-vector
multiplication (MVM). Signal information is (en) decoded as spikes using
lightweight circuits, eliminating the need for conventional area- and
powerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in
28nm technology, and experimental results show that it achieves a peak energy
efficiency of 243.6 TOPS/W, significantly outperforming existing designs.

</details>


### [4] [Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics](https://arxiv.org/abs/2511.03427)
*Florentia Afentaki,Maha Shatta,Konstantinos Balaskas,Georgios Panagopoulos,Georgios Zervakis,Mehdi B. Tahoori*

Main category: cs.AR

TL;DR: Proposes a mixed-kernel, mixed-signal SVM design for flexible electronics that combines linear and RBF kernels across digital and analog domains to optimize the accuracy/hardware cost trade-off.


<details>
  <summary>Details</summary>
Motivation: Flexible electronics face limitations in integration density and power constraints that prevent ML circuit implementation. Existing SVM designs force a trade-off between hardware costs (linear kernels) and accuracy (RBF kernels).

Method: Co-optimization approach that trains mixed-kernel SVMs and maps binary classifiers to appropriate kernel (linear/RBF) and domain (digital/analog) to maximize accuracy while reducing costly RBF classifiers.

Result: 7.7% higher accuracy than state-of-the-art single-kernel linear SVMs, and 108x area reduction and 17x power reduction compared to digital RBF implementations.

Conclusion: The mixed-kernel, mixed-signal SVM design successfully balances the cost/accuracy trade-off in flexible electronics, enabling ML circuit implementation with significantly improved performance and reduced hardware overhead.

Abstract: Flexible Electronics (FE) have emerged as a promising alternative to
silicon-based technologies, offering on-demand low-cost fabrication,
conformality, and sustainability. However, their large feature sizes severely
limit integration density, imposing strict area and power constraints, thus
prohibiting the realization of Machine Learning (ML) circuits, which can
significantly enhance the capabilities of relevant near-sensor applications.
Support Vector Machines (SVMs) offer high accuracy in such applications at
relatively low computational complexity, satisfying FE technologies'
constraints. Existing SVM designs rely solely on linear or Radial Basis
Function (RBF) kernels, forcing a trade-off between hardware costs and
accuracy. Linear kernels, implemented digitally, minimize overhead but
sacrifice performance, while the more accurate RBF kernels are prohibitively
large in digital, and their analog realization contains inherent functional
approximation. In this work, we propose the first mixed-kernel and mixed-signal
SVM design in FE, which unifies the advantages of both implementations and
balances the cost/accuracy trade-off. To that end, we introduce a
co-optimization approach that trains our mixed-kernel SVMs and maps binary SVM
classifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),
aiming to maximize accuracy whilst reducing the number of costly RBF
classifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art
single-kernel linear SVMs, and reduce area and power by 108x and 17x on average
compared to digital RBF implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: This paper presents the EuroHPC JU Center of Excellence in Exascale CFD's experience in measuring and optimizing energy consumption across major European HPC systems using CFD applications, highlighting the benefits of accelerators and mixed-precision techniques for sustainable exascale computing.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency has become a central challenge for modern high-performance computing systems due to escalating computational demands and architectural complexity leading to significant energy footprints.

Method: The study uses case studies with representative CFD applications (waLBerla, FLEXI/GALÃ†XI, Neko, and NekRS) to evaluate energy-to-solution and time-to-solution metrics on diverse CPU- and GPU-based architectures across major European HPC systems including LUMI, MareNostrum5, MeluXina, and JUWELS Booster.

Result: Results highlight the advantages of accelerators and mixed-precision techniques for reducing energy consumption while maintaining computational accuracy.

Conclusion: The paper advocates for facilitating energy measurements on HPC systems to raise awareness, educate the community, and take actions toward more sustainable exascale computing.

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [6] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: The paper proposes a formal framework using multiagent atomic transactions to classify global digital platforms into four categories based on the cardinality of essential agents: centralised (1), decentralised (finite >1), federated (infinite but not universal), and grassroots (universal).


<details>
  <summary>Details</summary>
Motivation: To provide a mathematical framework for classifying global digital platforms that serve entire populations, enabling systematic analysis of existing and potential platforms.

Method: Introduces atomic transactions-based multiagent transition systems and protocols, defines essential agents as minimal sets whose removal prevents communication, and uses cardinality analysis to classify platforms.

Result: Shows that all global platforms can be partitioned into four classes based on essential agent cardinality, with formal specifications and proofs for a social network example.

Conclusion: Provides the first comprehensive mathematical framework for classifying any global platform by analyzing multiagent atomic-transaction specifications and essential agent cardinality, placing grassroots platforms in their proper formal context.

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [7] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM is a unified memory-affinity data layout and DRAM address mapping scheme that optimizes NPU-PIM co-execution for LLM inference on edge devices, achieving up to 3.0x faster TTFT and 2.18x faster TTLT.


<details>
  <summary>Details</summary>
Motivation: LLMs deployed on edge devices with NPUs face memory-intensive decode phases that limit performance, while NPU-PIM co-execution suffers from data layout mismatches, bandwidth loss, and redundant storage issues.

Method: Proposes UMDAM with column-major, tile-based data layout and configurable DRAM mapping strategy to ensure NPU compatibility while maximizing PIM efficiency without extra memory overhead.

Result: Evaluations on OPT models show UMDAM reduces time-to-first-token by up to 3.0x and time-to-last-token by 2.18x.

Conclusion: UMDAM significantly improves end-to-end LLM inference efficiency on edge devices by optimizing NPU-PIM co-execution through unified memory-affinity data layout and address mapping.

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [8] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: This paper evaluates three isolation strategies (cgroups/CPU pinning, Docker containers, Firecracker MicroVMs) for mitigating performance variability in duet benchmarking, finding that process isolation generally reduces false positives except for Docker containers which are more susceptible to noise interference.


<details>
  <summary>Details</summary>
Motivation: Duet benchmarking addresses performance variability in cloud environments from multi-tenant resource contention by running synchronized workloads, but requires additional isolation mechanisms to handle intra-VM contention between the synchronized workloads.

Method: The researchers evaluated three isolation strategies: cgroups with CPU pinning, Docker containers, and Firecracker MicroVMs. They compared these against an unisolated baseline by running benchmarks in a duet setup alongside a noise generator that intentionally degrades performance measurements.

Result: All experiments showed different latency distributions under noise generation. Process isolation generally lowered false positives, except for Docker containers which were more susceptible to performance degradation despite internally using cgroups and CPU pinning.

Conclusion: The paper recommends using process isolation for synchronized workloads in duet benchmarking, with the exception of Docker containers which should be avoided due to their higher susceptibility to performance degradation from noise interference.

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [9] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: The paper introduces a new topological encoding using spectral spaces for round-based full-information adversaries in distributed computing, providing a general characterization of solvability for colorless tasks against compact adversaries through Stone duality.


<details>
  <summary>Details</summary>
Motivation: To unify topological methods in distributed computing by developing a functorial presentation that can handle various message adversaries and provide a general computability theorem for colorless tasks.

Method: Uses spectral spaces and Alexandrov topology on faces posets, defines limit objects via projective limits in the category of spectral spaces, and applies Stone duality to derive computability conditions.

Result: Establishes that colorless tasks are solvable against compact adversaries if and only if there exists a compatible spectral map between the limit object and output space, revealing that colored and uncolored models have equivalent computability power.

Conclusion: The spectral space approach provides a unified topological framework for distributed computability, explaining previously known equivalences between colored and uncolored models through topological reasoning rather than algorithmic reductions.

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [10] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: This paper generalizes distributed computing task solvability characterizations to input-dependent adversaries, shows equivalence between core-resilient adversaries with crashes at different times, and provides necessary/sufficient conditions for solving k-Set Agreement with condition-based adversaries.


<details>
  <summary>Details</summary>
Motivation: To extend existing computability characterizations from fixed message adversaries to input-dependent adversaries, and to better understand the solvability of k-Set Agreement under various adversary models using topological methods.

Method: Uses geometric and topological framework from distributed computing, generalizing previous characterizations to input-dependent adversaries and analyzing core-resilient adversaries through geometric constructions.

Result: Proves that core-resilient adversaries have equivalent computability power regardless of crash timing, provides complete characterization for k-Set Agreement solvability with condition-based adversaries, and identifies four distinct settings for distributed task presentation.

Conclusion: The paper successfully generalizes computability characterizations to input-dependent adversaries, establishes equivalence results for core-resilient adversaries, and provides comprehensive conditions for k-Set Agreement solvability using geometric topological methods.

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [11] [NF-SecRIS: RIS-Assisted Near-Field Physical Layer Security via Secure Location Modulation](https://arxiv.org/abs/2511.02949)
*Zhendong Wang,Chenyang Meng,Jun Yang,Jiayuan Wang,Yin Li,Linshan Jiang,Jin Zhang*

Main category: cs.ET

TL;DR: The paper proposes NF-SecRIS, the first range-angle-dependent 2D physical layer security system for 6G near-field communications using ultra-large-scale RIS, enabling secure communication where only legitimate users can decode signals while eavesdroppers receive obfuscated constellations.


<details>
  <summary>Details</summary>
Motivation: 6G networks require high physical layer security, but existing solutions only provide one-dimensional security in angle dimension and cannot achieve security in range dimension, creating a gap in comprehensive protection.

Method: Proposed NF-SecRIS system with secure location modulation scheme that synthesizes near-field spatial-temporal coding patterns on RIS with low complexity, operating without synchronization requirements.

Result: Experimental results show legitimate users achieve BER below 10^{-4}, while eavesdroppers at different ranges or angles suffer from BER exceeding 40%, validating 2D PLS implementation.

Conclusion: NF-SecRIS successfully implements the first range-angle-dependent 2D physical layer security system for near-field communications, providing comprehensive protection against eavesdroppers in both range and angle dimensions.

Abstract: The 6G wireless networks impose extremely high requirements on physical layer
secure communication. However, the existing solutions usually can only achieve
one-dimensional physical layer security (PLS) in the angle dimension, and
cannot achieve PLS in the range dimension. In this paper, we propose the
NF-SecRIS system, the first range-angle-dependent (2D) PLS near-field
communication system based on ultra-large-scale reconfigurable intelligent
surface (RIS). We propose the secure location modulation scheme to synthesize
the near-field spatial-temporal coding pattern of RIS with extremely low
complexity. It ensures that only legitimate user can receive the raw
constellations, while potential eavesdroppers at other ranges or angles can
only receive the obfuscated constellations. NF-SecRIS operates without
requiring synchronization with either transmitter or receiver. We implement a
prototype of NF-SecRIS and conduct comprehensive experiments with multiple
modulation schemes. The results show that the bit error rate (BER) of
legitimate user is below 10^{-4}, while eavesdroppers at other ranges or angles
suffer from BER exceeding 40%. It validates the implementation of 2D PLS in
near-field communications.

</details>


### [12] [QAGT-MLP: An Attention-Based Graph Transformer for Small and Large-Scale Quantum Error Mitigation](https://arxiv.org/abs/2511.03119)
*Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.ET

TL;DR: QAGT-MLP is an attention-based graph transformer for quantum error mitigation that encodes circuits as graphs and uses dual-path attention to extract global structural and local lightcone contexts, achieving superior performance on large-scale quantum circuits.


<details>
  <summary>Details</summary>
Motivation: Existing quantum error mitigation techniques impose substantial execution or calibration overheads, while learning-based methods struggle to scale to large and deep circuits, creating a need for efficient and scalable QEM solutions.

Method: Encode quantum circuits as graphs with nodes representing gates and edges capturing connectivity. Use dual-path attention to extract global structural context and local lightcone context, then combine with circuit descriptors and noisy values in an MLP to predict mitigated values.

Result: Outperformed state-of-the-art learning baselines on 100-qubit TFIM circuits in terms of mean error and error variability, demonstrating strong validity and applicability under matched shot budgets.

Conclusion: QAGT-MLP achieves high mitigation quality without increasing noise scaling or resource demands, offering a scalable and practical path to quantum error mitigation for modern quantum workloads.

Abstract: Noisy quantum devices demand error-mitigation techniques to be accurate yet
simple and efficient in terms of number of shots and processing time. Many
established approaches (e.g., extrapolation and quasi-probability cancellation)
impose substantial execution or calibration overheads, while existing
learning-based methods have difficulty scaling to large and deep circuits. In
this research, we introduce QAGT-MLP: an attention-based graph transformer
tailored for small- and large-scale quantum error mitigation (QEM). QAGT-MLP
encodes each quantum circuit as a graph whose nodes represent gate instances
and whose edges capture qubit connectivity and causal adjacency. A dual-path
attention module extracts features around measured qubits at two scales or
contexts: 1) graph-wide global structural context; and 2) fine-grained local
lightcone context. These learned representations are concatenated with
circuit-level descriptor features and the circuit noisy expected values, then
they are passed to a lightweight MLP to predict the noise-mitigated values. On
large-scale 100-qubit Trotterized 1D Transverse-Field Ising Models -- TFIM
circuits -- the proposed QAGT-MLP outperformed state-of-the-art learning
baselines in terms of mean error and error variability, demonstrating strong
validity and applicability in real-world QEM scenarios under matched shot
budgets. By using attention to fuse global structures with local lightcone
neighborhoods, QAGT-MLP achieves high mitigation quality without the increasing
noise scaling or resource demand required by classical QEM pipelines, while
still offering a scalable and practical path to QEM in modern and future
quantum workloads.

</details>


### [13] [LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol](https://arxiv.org/abs/2511.03706)
*Yu-Erh Pan,Ayesha Siddika Nipu*

Main category: cs.ET

TL;DR: An LLM-enhanced Air Monitoring Interface (AMI) that integrates real-time sensor data with conversational interface using Model Context Protocol (MCP) to reduce hallucinations and improve reliability in environmental monitoring.


<details>
  <summary>Details</summary>
Motivation: Traditional air quality monitoring systems are difficult for non-expert users to interpret due to complex visualizations, limited interactivity, and high deployment costs, while LLMs offer accessibility but suffer from hallucinations in safety-critical domains.

Method: Combines Django-based backend, responsive user dashboard, and secure MCP server that exposes system functions as discoverable tools, allowing LLM to act as active operator grounded in live environmental data.

Result: Expert evaluation showed high factual accuracy (4.78/5), completeness (4.82/5), and minimal hallucinations (4.84/5), supported by inter-rater reliability analysis.

Conclusion: Combining LLMs with standardized tool protocols enables reliable, secure, and user-friendly interfaces for real-time environmental monitoring by grounding outputs in live data.

Abstract: Air quality monitoring is central to environmental sustainability and public
health, yet traditional systems remain difficult for non-expert users to
interpret due to complex visualizations, limited interactivity, and high
deployment costs. Recent advances in Large Language Models (LLMs) offer new
opportunities to make sensor data more accessible, but their tendency to
produce hallucinations limits reliability in safety-critical domains. To
address these challenges, we present an LLM-enhanced Air Monitoring Interface
(AMI) that integrates real-time sensor data with a conversational interface via
the Model Context Protocol (MCP). Our system grounds LLM outputs in live
environmental data, enabling accurate, context-aware responses while reducing
hallucination risk. The architecture combines a Django-based backend, a
responsive user dashboard, and a secure MCP server that exposes system
functions as discoverable tools, allowing the LLM to act as an active operator
rather than a passive responder. Expert evaluation demonstrated high factual
accuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a
scale of 5, supported by inter-rater reliability analysis. These results
highlight the potential of combining LLMs with standardized tool protocols to
create reliable, secure, and user-friendly interfaces for real-time
environmental monitoring.

</details>
