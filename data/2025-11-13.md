<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [3D Guard-Layer: An Integrated Agentic AI Safety System for Edge Artificial Intelligence](https://arxiv.org/abs/2511.08842)
*Eren Kurshan,Yuan Xie,Paul Franzon*

Main category: cs.AR

TL;DR: Proposes an agentic AI safety architecture using 3D integration to create a dedicated safety layer for edge AI systems, enabling dynamic threat detection and mitigation with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of edge AI adoption faces significant security vulnerabilities despite existing guardrails, creating barriers to practical deployment and safety of AI systems.

Method: Leverages 3D integration to embed a dedicated safety layer that uses co-location with edge computing hardware for continuous monitoring, detection, and proactive threat mitigation through local processing and learning capabilities.

Result: The architecture enhances resilience against network-based attacks while improving system reliability, modularity, and performance with minimal cost and integration overhead.

Conclusion: The proposed agentic AI safety infrastructure effectively addresses security challenges in edge AI deployment through adaptive, co-located threat mitigation that maintains system efficiency.

Abstract: AI systems have found a wide range of real-world applications in recent years. The adoption of edge artificial intelligence, embedding AI directly into edge devices, is rapidly growing. Despite the implementation of guardrails and safety mechanisms, security vulnerabilities and challenges have become increasingly prevalent in this domain, posing a significant barrier to the practical deployment and safety of AI systems. This paper proposes an agentic AI safety architecture that leverages 3D to integrate a dedicated safety layer. It introduces an adaptive AI safety infrastructure capable of dynamically learning and mitigating attacks against the AI system. The system leverages the inherent advantages of co-location with the edge computing hardware to continuously monitor, detect and proactively mitigate threats to the AI system. The integration of local processing and learning capabilities enhances resilience against emerging network-based attacks while simultaneously improving system reliability, modularity, and performance, all with minimal cost and 3D integration overhead.

</details>


### [2] [FsimNNs: An Open-Source Graph Neural Network Platform for SEU Simulation-based Fault Injection](https://arxiv.org/abs/2511.09131)
*Li Lu,Jianan Wen,Milos Krstic*

Main category: cs.AR

TL;DR: This paper presents an open-source platform using Spatio-Temporal Graph Neural Networks (STGNNs) to accelerate SEU fault simulation, addressing the computational cost issues of traditional simulation-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional simulation-based fault injection for Single Event Upsets (SEUs) becomes computationally expensive as circuit complexity increases, creating a need for more efficient approaches.

Method: Developed an open-source platform with three STGNN architectures incorporating Atrous Spatial Pyramid Pooling (ASPP) and attention mechanisms for improved spatio-temporal feature extraction. Built datasets from six open-source circuits of varying complexity.

Result: The platform successfully accelerates SEU fault simulation and provides comprehensive benchmarks. The STGNN models demonstrate predictive capability across multiple test cases and show generalization potential.

Conclusion: The STGNN-based approach offers an efficient alternative to traditional SEU fault simulation methods, with the open-source platform and datasets supporting reproducibility and further research in this domain.

Abstract: Simulation-based fault injection is a widely adopted methodology for assessing circuit vulnerability to Single Event Upsets (SEUs); however, its computational cost grows significantly with circuit complexity. To address this limitation, this work introduces an open-source platform that exploits Spatio-Temporal Graph Neural Networks (STGNNs) to accelerate SEU fault simulation. The platform includes three STGNN architectures incorporating advanced components such as Atrous Spatial Pyramid Pooling (ASPP) and attention mechanisms, thereby improving spatio-temporal feature extraction. In addition, SEU fault simulation datasets are constructed from six open-source circuits with varying levels of complexity, providing a comprehensive benchmark for performance evaluation. The predictive capability of the STGNN models is analyzed and compared on these datasets. Moreover, to further investigate the efficiency of the approach, we evaluate the predictive capability of STGNNs across multiple test cases and discuss their generalization capability. The developed platform and datasets are released as open-source to support reproducibility and further research on https://github.com/luli2021/FsimNNs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: First implementation of selective code offloading to FPGAs via OpenMP target directive within MLIR, combining OpenMP and HLS dialects for portable FPGA compilation.


<details>
  <summary>Details</summary>
Motivation: With Moore's Law slowing, heterogeneous computing platforms like FPGAs are gaining interest for accelerating HPC workloads, requiring better integration with existing compiler ecosystems.

Method: Combines MLIR OpenMP dialect with High-Level Synthesis (HLS) dialect to create portable compilation flow targeting FPGAs, supporting any MLIR-compatible front end like Flang.

Result: Successfully implemented FPGA code offloading via OpenMP target directives within MLIR, demonstrating composability benefits and reduced development effort using existing MLIR building blocks.

Conclusion: Establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem, supporting manual optimization through standard OpenMP directives.

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [4] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: DC flexibility through energy resource decoupling can significantly reduce carbon emissions by improving renewable energy absorption, with optimized distribution achieving 98% of potential carbon reduction using only 70% of decoupling capacity, and DC-grid cooperation providing 1.4x greater carbon reduction.


<details>
  <summary>Details</summary>
Motivation: The exploding power consumption of AI and cloud datacenters intensifies carbon footprint concerns, as their constant power needs conflict with volatile renewable generation required for grid decarbonization.

Method: Decouple datacenter power capacity from grid load using energy resources, then optimize distribution and management approaches considering site variation and datacenter-grid cooperation.

Result: Optimized distribution delivers >98% potential grid carbon reduction with 70% total decoupling need. DC-grid cooperation enables 1.4x grid carbon reduction compared to one-way information sharing.

Conclusion: Decoupling is economically viable as datacenters can achieve power cost and carbon emission benefits exceeding local decoupling costs, though grid intervention may be needed due to site variation.

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [5] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: This paper compares CPU performance and cost across four major cloud providers (AWS, Azure, GCP, OCI) using SPEC ACCEL workloads, finding AWS offers best performance but highest cost, while OCI is most economical despite slower speeds.


<details>
  <summary>Details</summary>
Motivation: To evaluate how HPC-style CPU performance and pricing vary across different cloud providers and instance types (Intel, AMD, ARM) under different pricing models, helping users make informed decisions based on their workload priorities.

Method: Used a subset of OpenMP workloads from SPEC ACCEL suite to test performance across AWS, Azure, GCP, and OCI cloud platforms, comparing Intel, AMD, and ARM instance types under both on-demand and one-year discounted pricing models.

Result: AWS consistently delivered shortest runtime across all three CPU types but charged premium prices. OCI was most economical across all CPU families despite slower performance. Azure showed mid-range performance and cost. GCP had mixed results with AMD performing well but ARM being significantly slower and more expensive. AWS's ARM instances outperformed its Intel and AMD counterparts by up to 49%.

Conclusion: Instance choices and provider selection significantly impact both runtime and cost, suggesting workload priorities (raw speed vs cost minimization) should guide decisions on cloud instance types and providers.

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [6] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: The paper presents APPFL, an enterprise-grade federated learning framework designed to bridge the gap between research prototypes and production deployment by providing scalable local simulation, seamless transition to distributed deployment, multi-level abstractions, and comprehensive privacy protection.


<details>
  <summary>Details</summary>
Motivation: Federated learning enables collaborative model training without centralized data sharing, which is crucial for scientific domains with privacy, ownership, and compliance constraints. However, building scalable and privacy-preserving enterprise-level FL frameworks remains challenging, especially when transitioning from local prototyping to distributed deployment across heterogeneous infrastructures.

Method: The paper proposes the Advanced Privacy-Preserving Federated Learning (APPFL) framework with key capabilities: scalable local simulation and prototyping, seamless transition to deployment, distributed deployment across diverse infrastructures (personal devices to HPC systems), multi-level abstractions balancing usability and flexibility, and comprehensive privacy protection using differential privacy, secure aggregation, authentication, and confidential computing.

Result: The paper presents architectural designs for realizing an enterprise-grade FL framework that can scale seamlessly across computing environments, though specific implementation results are not detailed in the abstract.

Conclusion: The APPFL framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for scientific applications by addressing key challenges in federated learning deployment.

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [7] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG is a software framework that replaces MIG's rigid one-to-one allocation with a one-to-many model, enabling shared-memory collectives across instances without hardware changes, improving GPU cluster utilization and reducing fragmentation.


<details>
  <summary>Details</summary>
Motivation: GPU clusters suffer from underutilization due to MIG's hardware rigidity and one-to-one allocation model, causing severe fragmentation and inefficient resource use in multi-tenant environments.

Method: Developed Flex-MIG as a software-only framework that implements a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without requiring hardware modifications.

Result: Eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, demonstrating substantial cluster efficiency improvements.

Conclusion: Rethinking MIG's operational model as a software-coordinated layer significantly enhances GPU cluster efficiency without hardware changes, making GPU-sharing more effective in multi-tenant settings.

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [8] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: The paper proposes Combine-and-Exchange Scheduling (CES), a novel scheduling approach for coroutines that keeps contended critical sections on the same thread while spreading parallelizable work across other threads, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current userspace synchronization primitives approach synchronization from a kernel-level scheduling perspective, introducing unnecessary delays and limiting throughput for coroutines and other userspace-scheduled tasks.

Method: Developed Combine-and-Exchange Scheduling (CES) that ensures contended critical sections remain on the same execution thread while distributing parallelizable work evenly across remaining threads.

Result: Achieved 3-fold performance improvements in application benchmarks and 8-fold performance improvements in microbenchmarks.

Conclusion: CES provides an effective scheduling approach for userspace-scheduled tasks like coroutines that can be applied to many existing languages and libraries, significantly improving performance by optimizing synchronization overhead.

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [9] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: CMP is a coordination-free lock-free queue that maintains strict FIFO ordering, unbounded capacity, and lock-free progress while being simpler than existing implementations by using bounded protection windows instead of infinite protection against reclamation hazards.


<details>
  <summary>Details</summary>
Motivation: Existing lock-free queue implementations are overly complex due to coordination mechanisms for preventing hazards like ABA and use-after-free, which dominate design and compromise FIFO ordering, capacity, or progress. These overheads become critical in AI workloads with hundreds to thousands of concurrent threads.

Method: Cyclic Memory Protection (CMP) uses bounded protection windows to provide practical reclamation guarantees without coordination overhead, preserving strict FIFO semantics and lock-free progress.

Result: CMP outperforms state-of-the-art lock-free queues by 1.72-4x under high contention while scaling to hundreds of threads, with proven strict FIFO and safety via linearizability and bounded reclamation analysis.

Conclusion: Highly concurrent queues can achieve fundamental simplicity without weakening queue semantics by using practical bounded protection instead of theoretically infinite protection that drives unnecessary complexity.

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [10] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA is a programming language for spatial dataflow architectures that provides high-level control over data placement and flow while abstracting low-level details, enabling complex parallel patterns with significantly reduced code and near-ideal scaling.


<details>
  <summary>Details</summary>
Motivation: Programming spatial dataflow architectures like Cerebras Wafer-Scale Engine is challenging due to the need for explicit orchestration of data movement and asynchronous computation, with existing models overlooking their unique capabilities for efficient dataflow over regular grids.

Method: Developed SPADA language with rigorous dataflow semantics framework defining routing correctness, data races, and deadlocks. Designed compiler targeting Cerebras CSL with multi-level lowering, serving as both programming interface and IR for DSLs.

Result: SPADA enables expressing complex parallel patterns in 6-8x less code than CSL with near-ideal weak scaling across three orders of magnitude, demonstrated with GT4Py stencil DSL.

Conclusion: SPADA advances both theoretical foundations and practical usability of spatial dataflow architectures by unifying programming under a single model that provides precise control while abstracting architecture-specific details.

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [11] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: Formal verification of the universal TDM communication algorithm in a Python federated learning testbed using CSP process algebra and model checking.


<details>
  <summary>Details</summary>
Motivation: To formally verify the third generic algorithm (universal TDM communication) in the Python Testbed for Federated Learning Algorithms, following previous verification of the first two algorithms.

Method: Two-phase approach: (1) construct CSP model as faithful representation of Python code, (2) use PAT model checker to automatically prove correctness through deadlock freeness (safety) and successful termination (liveness) properties.

Result: The third generic algorithm (universal TDM communication) was successfully verified to be correct through automated model checking.

Conclusion: The universal TDM communication algorithm in the federated learning testbed has been formally verified to be deadlock-free and successfully terminating, completing the verification of all three generic algorithms in the framework.

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [PANDA: Noise-Resilient Antagonist Identification in Production Datacenters](https://arxiv.org/abs/2511.08803)
*Sixiang Zhou,Nan Deng,Krzysiek Rzadca,Charlie Y. Hu,Xiaojun Lin*

Main category: cs.PF

TL;DR: PANDA is a noise-resilient framework for identifying performance-interfering jobs in datacenters that improves antagonist detection accuracy from 50-55% to 82.6% by using global historical knowledge and machine-level CPI metrics.


<details>
  <summary>Details</summary>
Motivation: Modern datacenters collocate multiple jobs on shared machines to improve resource utilization, but this leads to performance interference from antagonistic jobs that overconsume shared resources. Existing detection methods are either costly (offline profiling) or unreliable (sample-from-production approaches with noisy measurements).

Method: PANDA uses cycles per instruction (CPI) as its performance metric but differs by: (i) leveraging global historical knowledge across all machines to suppress sampling noise, and (ii) introducing a machine-level CPI metric that captures shared-resource contention among multiple co-located tasks.

Result: Evaluation on Google production trace shows PANDA ranks true antagonists far more accurately than prior methods - improving average suspicion percentile from 50-55% to 82.6%. It achieves consistent antagonist identification under multi-victim scenarios with negligible runtime overhead.

Conclusion: PANDA provides an effective and scalable solution for identifying performance-interfering jobs in production datacenters, overcoming limitations of existing approaches through noise-resilient techniques and machine-level resource contention analysis.

Abstract: Modern warehouse-scale datacenters commonly collocate multiple jobs on shared machines to improve resource utilization. However, such collocation often leads to performance interference caused by antagonistic jobs that overconsume shared resources. Existing antagonist-detection approaches either rely on offline profiling, which is costly and unscalable, or use a sample-from-production approach, which suffers from noisy measurements and fails under multi-victim scenarios. We present PANDA, a noise-resilient antagonist identification framework for production-scale datacenters. Like prior correlation-based methods, PANDA uses cycles per instruction (CPI) as its performance metric, but it differs by (i) leveraging global historical knowledge across all machines to suppress sampling noise and (ii) introducing a machine-level CPI metric that captures shared-resource contention among multiple co-located tasks. Evaluation on a recent Google production trace shows that PANDA ranks true antagonists far more accurately than prior methods -- improving average suspicion percentile from 50-55% to 82.6% -- and achieves consistent antagonist identification under multi-victim scenarios, all with negligible runtime overhead.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [13] [Reservoir Computing-Based Detection for Molecular Communications](https://arxiv.org/abs/2511.08762)
*Abdulkadir Bilge,Eren Akyol,Murat Kuscu*

Main category: cs.ET

TL;DR: Proposes a low-complexity Reservoir Computing detector for mobile molecular communication that effectively handles severe inter-symbol interference without explicit channel modeling, achieving superior performance with minimal trainable parameters and ultra-low latency.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based Molecular Communication faces severe ISI that worsens in mobile scenarios with time-varying channels. Traditional model-based detection is intractable, while deep learning methods are too complex for resource-constrained micro/nanodevices.

Method: Uses a fixed, recurrent non-linear reservoir to project time-varying received signals into high-dimensional state space, transforming complex temporal detection into simple linear classification without explicit channel modeling or complex retraining.

Result: Significantly outperforms classical detectors and achieves superior performance compared to complex ML methods (LSTM, CNN, MLP) under severe ISI, with only 300 trainable parameters vs. up to 264k for MLP, and ultra-low latency inference of approximately 1 μs per symbol.

Conclusion: Reservoir Computing provides an effective low-complexity solution for mobile molecular communication detection, capturing ISI dynamics efficiently while being suitable for resource-constrained devices.

Abstract: Diffusion-based Molecular Communication (MC) is inherently challenged by severe inter-symbol interference (ISI). This is significantly amplified in mobile scenarios, where the channel impulse response (CIR) becomes time-varying and stochastic. Obtaining accurate Channel State Information (CSI) for traditional model-based detection is intractable in such dynamic environments. While deep learning (DL) offers adaptability, its complexity is unsuitable for resource-constrained micro/nanodevices. This paper proposes a low-complexity Reservoir Computing (RC) based detector. The RC architecture utilizes a fixed, recurrent non-linear reservoir to project the time-varying received signal into a high-dimensional state space. This effectively transforms the complex temporal detection problem into a simple linear classification task, capturing ISI dynamics without explicit channel modeling or complex retraining. Evaluated in a realistic 3D mobile MC simulation environment (Smoldyn), our RC detector significantly outperforms classical detectors and achieves superior performance compared to complex ML methods (LSTM, CNN, MLP) under severe ISI. Importantly, RC achieves this with significantly fewer trainable parameters (e.g., 300 vs. up to 264k for MLP) and ultra-low latency inference (approx. 1 $μ$s per symbol).

</details>


### [14] [Modeling Closed-loop Analog Matrix Computing Circuits with Interconnect Resistance](https://arxiv.org/abs/2511.09151)
*Mu Zhou,Junbin Long,Yubiao Luo,Zhong Sun*

Main category: cs.ET

TL;DR: The paper presents fast solving algorithms for analog matrix computing circuits with interconnect resistance, achieving orders of magnitude acceleration over SPICE simulations while maintaining accuracy, and develops a bias-based compensation strategy that significantly reduces interconnect-induced errors.


<details>
  <summary>Details</summary>
Motivation: As matrix size grows in analog matrix computing circuits based on RRAM, interconnect resistance degrades computational accuracy and limits circuit scalability. Traditional SPICE simulators become prohibitively slow for large-scale circuits due to quadratic growth of nodes and feedback connections.

Method: Model AMC circuits with interconnect resistance for matrix inversion and eigenvector computation, and propose fast solving algorithms that exploit the sparsity of the Jacobian matrix. Extend the approach to open-loop matrix-vector multiplication circuits.

Result: The algorithms achieve several orders of magnitude acceleration compared to SPICE while maintaining high accuracy. A bias-based compensation strategy reduces interconnect-induced errors by over 50% for INV and 70% for EGV circuits.

Conclusion: The proposed fast solvers enable efficient modeling of large-scale AMC circuits and reveal scaling behavior of optimal bias with respect to matrix size and interconnect resistance, providing effective mitigation strategies for interconnect-induced errors.

Abstract: Analog matrix computing (AMC) circuits based on resistive random-access memory (RRAM) have shown strong potential for accelerating matrix operations. However, as matrix size grows, interconnect resistance increasingly degrades computational accuracy and limits circuit scalability. Modeling and evaluating these effects are therefore critical for developing effective mitigation strategies. Traditional SPICE (Simulation Program with Integrated Circuit Emphasis) simulators, which rely on modified nodal analysis, become prohibitively slow for large-scale AMC circuits due to the quadratic growth of nodes and feedback connections. In this work, we model AMC circuits with interconnect resistance for two key operations-matrix inversion (INV) and eigenvector computation (EGV), and propose fast solving algorithms tailored for each case. The algorithms exploit the sparsity of the Jacobian matrix, enabling rapid and accurate solutions. Compared to SPICE, they achieve several orders of magnitude acceleration while maintaining high accuracy. We further extend the approach to open-loop matrix-vector multiplication (MVM) circuits, demonstrating similar efficiency gains. Finally, leveraging these fast solvers, we develop a bias-based compensation strategy that reduces interconnect-induced errors by over 50% for INV and 70% for EGV circuits. It also reveals the scaling behavior of the optimal bias with respect to matrix size and interconnect resistance.

</details>


### [15] [RIoT Digital Twin: Modeling, Deployment, and Optimization of Reconfigurable IoT System with Optical-Radio Wireless Integration](https://arxiv.org/abs/2511.09303)
*Alaa Awad Abdellatif,Sergio Silva,Eduardo Baltazar,Bruno Oliveira,Senhui Qiu,Mohammud J. Bocus,Kerstin Eder,Robert J. Piechocki,Nuno T. Almeida,Helder Fontes*

Main category: cs.ET

TL;DR: An optimized RIoT framework integrating optical and radio wireless technologies with digital twin modeling for energy efficiency, scalability, and adaptability in 6G IoT networks.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of hybrid optical-radio environments in IoT and improve energy efficiency, scalability, and adaptability for sustainable 6G networks.

Method: Developed a high-fidelity Digital Twin within NS-3 platform modeling RF communication, optical wireless communication, and energy harvesting mechanisms. Implemented proactive cross-layer optimization with dynamic reconfiguration of transmission rates, wake/sleep scheduling, and access technology selection.

Result: The framework substantially enhances performance, resilience, and sustainability of 6G IoT networks through accurate physical behavior representation and runtime optimization.

Conclusion: The proposed RIoT framework successfully combines digital twin technology, hybrid optical-radio integration, and data-driven energy modeling to create more efficient and sustainable IoT networks for 6G applications.

Abstract: This paper proposes an optimized Reconfigurable Internet of Things (RIoT) framework that integrates optical and radio wireless technologies with a focus on energy efficiency, scalability, and adaptability. To address the inherent complexity of hybrid optical-radio environments, a high-fidelity Digital Twin (DT) is developed within the Network Simulator 3 (NS-3) platform. The DT models deploy subsystems of the RIoT architecture, including radio frequency (RF) communication, optical wireless communication (OWC), and energy harvesting and consumption mechanisms that enable autonomous operation. Real-time energy and power measurements from target hardware platforms are also incorporated to ensure accurate representation of physical behavior and enable runtime analysis and optimization. Building on this foundation, a proactive cross-layer optimization strategy is devised to balance energy efficiency and quality of service (QoS). The strategy dynamically reconfigures RIoT nodes by adapting transmission rates, wake/sleep scheduling, and access technology selection. Results demonstrate that the proposed framework, combining digital twin technology, hybrid optical-radio integration, and data-driven energy modeling, substantially enhances the performance, resilience, and sustainability of 6G IoT networks.

</details>
