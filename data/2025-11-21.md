<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference](https://arxiv.org/abs/2511.15950)
*Michael V. DeBole,Rathinakumar Appuswamy,Neil McGlohon,Brian Taba,Steven K. Esser,Filipp Akopyan,John V. Arthur,Arnon Amir,Alexander Andreopoulos,Peter J. Carlson,Andrew S. Cassidy,Pallab Datta,Myron D. Flickner,Rajamohan Gandhasri,Guillaume J. Garreau,Megumi Ito,Jennifer L. Klamo,Jeffrey A. Kusnitz,Nathaniel J. McClatchey,Jeffrey L. McKinstry,Tapan K. Nayak,Carlos Ortega Otero,Hartmut Penner,William P. Risk,Jun Sawada,Jay Sivagnaname,Daniel F. Smith,Rafael Sousa,Ignacio Terrizzano,Takanori Ueda,Trent Gray-Donald,David Cox,Dharmendra S. Modha*

Main category: cs.DC

TL;DR: A vertically integrated system with 288 NorthPole neural accelerator cards delivers 115 peta-ops at 4-bit precision, consuming 30 kW power in a compact 42U rack. It can run 3 simultaneous instances of an 8B-parameter model with 28 users and 2.8ms latency.


<details>
  <summary>Details</summary>
Motivation: To create a scalable and efficient cloud inference service for enterprise AI applications that can handle various model sizes and context lengths while maintaining low power consumption and compact footprint.

Method: Combines 288 NorthPole neural inference accelerator cards with offline training algorithms, high-performance runtime stack, and containerized inference pipeline in a vertically integrated system across 18 2U servers.

Result: Achieves 115 peta-ops at 4-bit integer precision, 3.7 PB/s memory bandwidth, 30 kW power consumption, 730 kg weight in 0.67 m² footprint. Supports 3 simultaneous 8B-parameter model instances with 28 users and 2.8ms latency.

Conclusion: The system is scalable, modular, and reconfigurable, ideal for deploying agentic workflows in existing data center environments, supporting various configurations from 18 instances of 3B-parameter models to single 70B-parameter model instances.

Abstract: A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.

</details>


### [2] [Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows](https://arxiv.org/abs/2511.15977)
*Daniel Mas Montserrat,Ray Verma,Míriam Barrabés,Francisco M. de la Vega,Carlos D. Bustamante,Alexander G. Ioannidis*

Main category: cs.DC

TL;DR: The paper proposes adaptive RAM-efficient parallelization methods for chromosome-level bioinformatics workflows, including symbolic regression for memory estimation, dynamic scheduling with polynomial regression, and static scheduling to optimize chromosome processing order, achieving faster execution and reduced memory overruns.


<details>
  <summary>Details</summary>
Motivation: Large-scale genomic workflows process massive datasets (tens to hundreds of GB per sample) leading to high memory spikes, intensive disk I/O, and task failures from out-of-memory errors. Static resource allocation struggles with variable per-chromosome RAM demands, resulting in poor resource utilization and long runtimes.

Method: 1) Symbolic regression model to estimate per-chromosome memory consumption with interpolating bias to minimize over-allocation; 2) Dynamic scheduler using polynomial regression to predict RAM usage and treat task packing as a Knapsack problem for optimal job batching; 3) Static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput.

Result: The proposed methods, evaluated on simulations and real-world genomic pipelines, reduce memory overruns and balance load across threads, achieving faster end-to-end execution.

Conclusion: The adaptive RAM-efficient parallelization mechanisms successfully optimize large-scale genomic workflows by addressing memory variability and improving resource utilization.

Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.

</details>


### [3] [Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT](https://arxiv.org/abs/2511.15957)
*Nasit S Sony,Xianzhong Ding*

Main category: cs.DC

TL;DR: Slim-HBBFT is an atomic broadcast protocol that reduces communication complexity by O(n) by only considering requests from a fraction of parties, using a prioritized provable-broadcast protocol to generate proof of broadcast only for selected parties.


<details>
  <summary>Details</summary>
Motivation: Traditional asynchronous Byzantine agreement protocols are expensive because they require every party to broadcast all requests, even when requests are duplicated. This inefficiency is not justified when requests don't vary significantly.

Method: Uses a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties, and builds Slim-HBBFT atomic broadcast protocol on top of this core design.

Result: Achieves improved communication complexity by a factor of O(n) compared to conventional protocols, while maintaining security properties.

Conclusion: Slim-HBBFT provides an efficient atomic broadcast solution that reduces communication overhead while satisfying the properties of Asynchronous Common Subset protocol through comprehensive security analysis.

Abstract: Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\langle n-f \rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.

</details>


### [4] [Can Asymmetric Tile Buffering Be Beneficial?](https://arxiv.org/abs/2511.16041)
*Chengyue Wang,Wesley Pang,Xinrui Wu,Gregory Jun,Luis Romero,Endri Taka,Diana Marculescu,Tony Nowatzki,Pranathi Vasireddy,Joseph Melber,Deming Chen,Jason Cong*

Main category: cs.DC

TL;DR: Asymmetric Tile Buffering (ATB) decouples input and output tile dimensions in GEMM operations, achieving up to 4.54x speedup on AMD XDNA2 AI Engine by increasing arithmetic intensity despite higher kernel switching costs.


<details>
  <summary>Details</summary>
Motivation: Conventional symmetric tile buffering in GEMM limits performance by matching input and output tile sizes. The authors aim to improve GEMM efficiency by exploring asymmetric tile buffering strategies.

Method: Developed asymmetric tile buffering (ATB) technique that decouples buffered tile dimensions of input and output operands. Created a performance model that considers both ATB benefits (higher arithmetic intensity) and overheads (kernel switching costs). Applied ATB to AMD XDNA2 AI Engine as a case study.

Result: Achieved up to 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16-BF16 GEMM, establishing a new performance record for XDNA2 AI Engine.

Conclusion: ATB is both practical and highly beneficial for GEMM operations, providing significant performance improvements by optimizing tile buffering strategies while managing associated overheads.

Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.

</details>


### [5] [Mitigating Shared Storage Congestion Using Control Theory](https://arxiv.org/abs/2511.16177)
*Thomas Collignon,Kouds Halitim,Raphaël Bleuse,Sophie Cerf,Bogdan Robu,Éric Rutten,Lionel Seinturier,Alexandre van Kempen*

Main category: cs.DC

TL;DR: A self-adaptive control theory approach for dynamically regulating client-side I/O rates in HPC systems to mitigate congestion and improve performance stability.


<details>
  <summary>Details</summary>
Motivation: Traditional I/O optimizations in HPC systems are workload-specific, require deep expertise, and don't address performance unpredictability caused by resource congestion in shared environments.

Method: Proposed a control theory-based approach using runtime system load metrics to dynamically regulate client-side I/O rates, implemented and tested on a multi-node cluster with representative workloads.

Result: The method effectively mitigated I/O congestion, reducing total runtime by up to 20% and lowering tail latency while maintaining stable performance.

Conclusion: The control theory-based self-adaptive approach provides an effective solution for managing I/O congestion and improving performance stability in shared HPC environments.

Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.

</details>


### [6] [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)
*Rongxin Cheng,Kai Zhou,Xingda Wei,Siyuan Liu,Mingcong Han,Mingjing Ai,Yeju Zhou,Baoquan Zhong,Wencong Xiao,Xin Liu,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: SpecActor accelerates LLM post-training rollout using speculative decoding with dynamic decoupled speculation and Best-of-N methods, achieving 1.3-1.7x speedup over baselines.


<details>
  <summary>Details</summary>
Motivation: Rollout dominates training time in LLM post-training, and speculative decoding can accelerate unparallelizable generation while maintaining correctness through verification.

Method: Uses speculative decoding with dynamic decoupled speculation for GPU efficiency in large batches, and dynamic Best-of-N speculation to select optimal drafting methods during rollout.

Result: Achieves 1.3-1.7x faster than common post-training baselines and 1.3-1.5x faster than naive speculative decoding adoption.

Conclusion: SpecActor effectively addresses speculative rollout challenges and significantly accelerates LLM post-training without requiring extra computational resources.

Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.

</details>


### [7] [Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming](https://arxiv.org/abs/2511.16450)
*Ziyue Xu,Zhihong Zhang,Holger R. Roth,Chester Chen,Yan Cheng,Andrew Feng*

Main category: cs.DC

TL;DR: The paper presents NVIDIA FLARE's enhanced federated learning capabilities for Large Language Models, addressing communication overhead and resource constraints through quantization and streaming techniques.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces critical challenges with communication overhead and local resource constraints when dealing with Large Language Models with billions of parameters, requiring efficient transmission and processing for practical deployment.

Method: Enhanced FL workflows for LLMs using two key techniques: message quantization to reduce message size, and container/file streaming for efficient memory management, improving scalability and integration with existing workflows.

Result: The advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

Conclusion: NVIDIA FLARE's improved communication capabilities effectively address the challenges of federated learning with Large Language Models, making FL more practical and scalable for real-world applications.

Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

</details>


### [8] [Distributed MIS Algorithms for Rational Agents using Games](https://arxiv.org/abs/2511.16533)
*Nithin Salevemula,Shreyas Pai*

Main category: cs.DC

TL;DR: The paper studies distributed Maximal Independent Set computation in networks with rational agents, proposing two algorithms that ensure no unilateral deviations are beneficial while maintaining correctness and fairness.


<details>
  <summary>Details</summary>
Motivation: Classical distributed algorithms assume nodes follow protocols, but this fails when nodes are strategic agents who may deviate to maximize utility. Standard MIS algorithms rely on randomness or identifiers that can be manipulated or create unfairness.

Method: Proposed two algorithms using pairwise interactions between neighbors as simple games for randomness generation, where no single node can unilaterally affect outcomes. This enables symmetry breaking compatible with rational behavior.

Result: Both algorithms ensure no agent can increase expected utility through unilateral deviations at any execution stage. All nodes have positive probability of joining MIS, and final output is correct MIS. Algorithms terminate in O(log n) rounds with high probability under mild assumptions.

Conclusion: The proposed algorithms provide strong game-theoretic guarantees (stronger than Trembling-Hand Perfect Equilibrium) for distributed MIS computation with rational agents, ensuring fairness, correctness, and efficient termination.

Abstract: We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.
  Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.
  For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\log n)$ rounds with high probability.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [9] [Interfacial and bulk switching MoS2 memristors for an all-2D reservoir computing framework](https://arxiv.org/abs/2511.16557)
*Asmita S. Thool,Sourodeep Roy,Prahalad Kanti Barman,Kartick Biswas,Pavan Nukala,Abhishek Misra,Saptarshi Das,and Bhaswar Chakrabarti*

Main category: cs.ET

TL;DR: A reservoir computing network using Au/Ti/MoS2/Au memristive devices with engineered memory dynamics - volatile switching in monolayer MoS2 for reservoir states and non-volatile switching in multilayer MoS2 for readout synapses, achieving 89.56% precision in spoken-digit recognition.


<details>
  <summary>Details</summary>
Motivation: To design an efficient reservoir computing network by exploiting different memory dynamics in memristive devices, combining volatile short-term memory for reservoir states and non-volatile analog switching for readout synapses.

Method: Engineered temporal dynamics by controlling CVD MoS2 film thickness: monolayer for volatile switching (short-term memory) and multilayer for non-volatile switching with analog conductance tuning. Used volatile memristors for 4-bit reservoir states and nonvolatile memristor array for readout layer.

Result: Multilayer MoS2 devices showed excellent uniformity and analog behavior in conductance tuning with trap-assisted SCLC mechanism. The RC network achieved 89.56% precision in spoken-digit recognition and successfully analyzed nonlinear time series equations.

Conclusion: The hybrid approach combining volatile and non-volatile memristive devices enables efficient reservoir computing with high performance in pattern recognition tasks, demonstrating the potential of engineered MoS2-based memristors for neuromorphic computing applications.

Abstract: In this study, we design a reservoir computing (RC) network by exploiting short- and long-term memory dynamics in Au/Ti/MoS$_2$/Au memristive devices. The temporal dynamics is engineered by controlling the thickness of the Chemical Vapor Deposited (CVD) MoS$_2$ films. Devices with a monolayer (1L)-MoS$_2$ film exhibit volatile (short-term memory) switching dynamics. We also report non-volatile resistance switching with excellent uniformity and analog behavior in conductance tuning for the multilayer (ML) MoS$_2$ memristive devices. We correlate this performance with trap-assisted space-charge limited conduction (SCLC) mechanism, leading to a bulk-limited resistance switching behavior. Four-bit reservoir states are generated using volatile memristors. The readout layer is implemented with an array of nonvolatile synapses. This small RC network achieves 89.56\% precision in a spoken-digit recognition task and is also used to analyze a nonlinear time series equation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [CIMinus: Empowering Sparse DNN Workloads Modeling and Exploration on SRAM-based CIM Architectures](https://arxiv.org/abs/2511.16368)
*Yingjie Qi,Jianlei Yang,Rubing Yang,Cenlin Duan,Xiaolin He,Ziyan He,Weitao Pan,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMinus is a framework for cost modeling of sparse DNN workloads on compute-in-memory architectures, providing energy and latency analysis to bridge theoretical design with practical implementation.


<details>
  <summary>Details</summary>
Motivation: Compute-in-memory (CIM) systems face challenges in exploiting sparsity due to rigid array structures, with complex dataflow design and mapping strategies needed for diverse sparsity patterns in multi-macro CIM systems. There's currently no unified systematic approach for modeling sparse DNN workloads in CIM.

Method: Proposed CIMinus framework provides detailed energy consumption analysis at component level and overall workload latency assessment for sparse DNN workloads on CIM architectures.

Result: Validated against contemporary CIM architectures and demonstrated applicability in two use-cases, providing insights into sparsity pattern impacts and mapping strategy effectiveness.

Conclusion: CIMinus bridges the gap between theoretical design and practical implementation for sparse DNN workloads in CIM systems, offering valuable analysis of energy consumption and latency.

Abstract: Compute-in-memory (CIM) has emerged as a pivotal direction for accelerating workloads in the field of machine learning, such as Deep Neural Networks (DNNs). However, the effective exploitation of sparsity in CIM systems presents numerous challenges, due to the inherent limitations in their rigid array structures. Designing sparse DNN dataflows and developing efficient mapping strategies also become more complex when accounting for diverse sparsity patterns and the flexibility of a multi-macro CIM structure. Despite these complexities, there is still an absence of a unified systematic view and modeling approach for diverse sparse DNN workloads in CIM systems. In this paper, we propose CIMinus, a framework dedicated to cost modeling for sparse DNN workloads on CIM architectures. It provides an in-depth energy consumption analysis at the level of individual components and an assessment of the overall workload latency. We validate CIMinus against contemporary CIM architectures and demonstrate its applicability in two use-cases. These cases provide valuable insights into both the impact of sparsity patterns and the effectiveness of mapping strategies, bridging the gap between theoretical design and practical implementation.

</details>


### [11] [Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts](https://arxiv.org/abs/2511.16374)
*Abdelrahman Helaly,Nourhan Sakr,Kareem Madkour,Ilhami Torunoglu*

Main category: cs.AR

TL;DR: A hybrid workflow for multipatterning layout decomposition that combines GNN-based initial predictions with refinement strategies to achieve conflict-free decomposition and color balancing.


<details>
  <summary>Details</summary>
Motivation: Existing heuristic-based backtracking and SAT solvers struggle to handle both complex constraints and secondary objectives simultaneously in multipatterning decomposition.

Method: Hybrid workflow that treats multipatterning as constrained graph coloring, using unsupervised GNN-based agent for initial color predictions followed by refinement strategies (GNN-based heuristic and simulated annealing).

Result: Experimental evaluation shows complete conflict-free decomposition and consistent color balancing on both proprietary and open source layout datasets.

Conclusion: The framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.

Abstract: Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.

</details>
