<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPILOT is a domain-specific encoder-decoder transformer that translates C++ code to OpenMP for shared-memory parallelization, using custom pre-training objectives and operating at function level with a novel evaluation metric called OMPBLEU.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional rule-based code translation systems and enable more effective parallelization of C++ code through automated OpenMP translation, capturing wider semantic context than previous loop-level approaches.

Method: Leverages custom pre-training objectives incorporating parallel construct semantics, combines unsupervised and supervised learning strategies, and operates at function level rather than just loop-level transformations.

Result: Enables more accurate and efficient transformation of C++ code into OpenMP parallel constructs, outperforming traditional systems in both accuracy and flexibility for shared-memory parallelization.

Conclusion: OMPILOT represents a significant advancement in code translation for parallel computing, providing robust function-level translation capabilities and introducing OMPBLEU as a specialized metric for evaluating parallel code quality.

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [2] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing, demonstrating that AI-driven predictive power scaling outperforms conventional reactive methods in energy efficiency and system responsiveness.


<details>
  <summary>Details</summary>
Motivation: Edge Computing enables low-latency processing but faces power management challenges due to distributed edge devices with limited energy resources, requiring efficient power state management.

Method: The paper uses Markov Chains for stochastic modeling of power state transitions, derives steady-state probabilities, evaluates energy consumption, and validates the model through Monte Carlo simulations and sensitivity analysis.

Result: Experimental results show strong alignment between theoretical and empirical results, with AI-based power management reducing energy consumption disparities, improving overall efficiency, and enhancing adaptive power coordination in multi-node environments.

Conclusion: AI-based power management strategies significantly enhance energy efficiency by anticipating workload demands and optimizing state transitions, minimizing unnecessary transitions and improving system responsiveness in Edge Computing environments.

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [3] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: A novel parallel spawning strategy for MPI applications that enables efficient dynamic resource management by reducing reconfiguration costs during both expansion and shrinkage operations, with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current MPI malleability methods either respawn entire applications (expensive) or fail to fully release unneeded processes during shrinkage, preventing optimal resource utilization in HPC systems.

Method: Proposes a parallel spawning strategy where all processes cooperate in spawning before redistribution, enabling complete process release during shrinkage and better adaptation to workload changes.

Result: Preserves competitive expansion times (max 1.25× overhead) while enabling fast shrink operations (20× cost reduction), validated on both homogeneous and heterogeneous systems.

Conclusion: The strategy overcomes limitations of existing MPI malleability methods, significantly reducing reconfiguration costs and improving dynamic resource management capabilities in HPC environments.

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [4] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: This paper proposes techniques for efficient LLM deployment on resource-constrained devices by combining dynamic sparse inference with low-bit quantization, achieving up to 1.55x faster decoding throughput while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on end-user devices is important for responsiveness, privacy, and cost, but limited GPU memory and compute capabilities make efficient execution challenging. Dynamic activation sparsity in LLMs could reduce computation but interacts poorly with group-wise quantization.

Method: The method includes: (1) zigzag-patterned quantization layout that organizes weights consistently with activation sparsity and improves GPU memory locality; (2) specialized GEMV kernel for this layout to utilize parallel compute units; (3) compact runtime mechanism for gathering sparse indices with minimal overhead.

Result: Across various model scales and hardware configurations, the approach achieves up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference.

Conclusion: Structured sparsity and quantization can effectively coexist on commodity GPUs, enabling efficient LLM deployment on resource-constrained devices.

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [5] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: A new probabilistic Mobile Byzantine Failure model for self-protecting distributed systems that captures evolving attack dynamics and enables mathematical analysis of system behavior under attack vs recovery scenarios.


<details>
  <summary>Details</summary>
Motivation: Modern distributed systems face increasing security threats across all layers, and existing Byzantine fault models have limitations in accurately reflecting real-world attack scenarios.

Method: Proposed a probabilistic Mobile Byzantine Failure model integrated into the MAPE-K architecture's Analysis component, with mathematical analysis of Byzantine node thresholds and system recovery times.

Result: Developed mathematical models to analyze time until Byzantine nodes cross thresholds and system self-recovery, with simulation results validating the system behavior under attack-recovery dynamics.

Conclusion: The proposed probabilistic MBF model effectively captures evolving attack patterns and can drive self-protection strategies in distributed systems, providing analytical tools to understand system resilience.

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [6] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: This paper introduces dynamic concurrency, where operations use strong synchronization only when necessary based on current system state, and presents a dynamically concurrent universal construction.


<details>
  <summary>Details</summary>
Motivation: Synchronization is the major scalability bottleneck in distributed computing, particularly when operations conflict only in rare system states. The goal is to detect conflicts dynamically and minimize unnecessary synchronization.

Method: The paper defines dynamic concurrency and presents a universal construction that employs strong synchronization primitives only when operations must arbitrate with concurrent operations given the current system state.

Result: A dynamically concurrent universal construction is developed that adapts synchronization requirements to the actual system state rather than using static conservative approaches.

Conclusion: Dynamic concurrency provides a more efficient approach to synchronization by reducing unnecessary coordination overhead and improving scalability in distributed systems.

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: The paper revisits the classic five-minute rule for data placement between DRAM and storage, updating it with modern considerations including host costs, DRAM bandwidth/capacity, and SSD performance models. It shows that for AI platforms with GPU hosts and high-IOPS SSDs, the caching threshold collapses from minutes to seconds, reframing NAND flash as an active data tier.


<details>
  <summary>Details</summary>
Motivation: To modernize the classical five-minute rule by integrating previously excluded factors like host costs, feasibility limits, and workload behavior, especially for modern AI platforms where traditional storage-memory economics no longer apply.

Method: Developed a constraint- and workload-aware framework from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded SSD performance models. Created MQSim-Next, a calibrated SSD simulator for validation and sensitivity analysis.

Result: For modern AI platforms with GPU-centric hosts and ultra-high-IOPS SSDs, the DRAM-to-flash caching threshold collapses from minutes to a few seconds, fundamentally changing the memory hierarchy paradigm and reframing NAND flash as an active data tier.

Conclusion: The work transforms a classical heuristic into an actionable, feasibility-aware analysis framework and opens a broad research space across the hardware-software stack for AI-era memory hierarchy design, with concrete case studies demonstrating the new design possibilities.

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [8] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: A 3D-stacked chiplet-based LLM inference accelerator using non-volatile in-memory-computing PEs and silicon photonic interconnects, achieving significant speedup and efficiency improvements over Nvidia GPUs.


<details>
  <summary>Details</summary>
Motivation: To address communication bottlenecks in large language model inference through advanced 3D chiplet architecture and photonic interconnects.

Method: Uses 3D-stacked chiplets with non-volatile in-memory-computing processing elements interconnected via silicon photonic Inter-PE Computational Network, plus a specialized LLM mapping scheme for hardware optimization.

Result: Achieves 3.95× speedup and 30× efficiency improvement over Nvidia A100, with further improvements (57× efficiency over H100) using chiplet clustering and power gating scheme.

Conclusion: The proposed 3D-stacked chiplet architecture with photonic interconnects effectively overcomes communication bottlenecks and significantly improves LLM inference performance and efficiency.

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [9] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: Overview of hardware disaggregation motivations, advancements, challenges, and opportunities in data centers, with a numerical study illustrating key aspects.


<details>
  <summary>Details</summary>
Motivation: Transform data center resources from traditional server fleets into unified resource pools to improve resource utilization and efficiency.

Method: Provides an overview of motivations and recent advancements, discusses research challenges and opportunities, and presents a numerical study.

Result: Hardware disaggregation has the potential to reshape the entire data center ecosystem, impacting application design, resource scheduling, hardware configuration, cooling, and power system optimization.

Conclusion: Despite existing challenges, significant progress has been made in hardware disaggregation, and it presents opportunities to transform data center architecture and operations.

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [10] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: AIM is a comprehensive software-hardware co-design approach that mitigates IR-drop issues in high-performance SRAM Processing-in-Memory (PIM) through workload-aware optimization, achieving significant performance and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: High-performance SRAM PIM faces severe IR-drop problems due to complex circuit designs and high operating frequencies, which degrade chip performance and reliability. Conventional circuit-level mitigation methods are resource-intensive and compromise power, performance, and area.

Method: Proposes AIM framework with: 1) Rtog and HR to correlate PIM workloads with IR-drop, 2) LHR and WDS for architecture-level IR-drop mitigation via software optimization, 3) IR-Booster for dynamic V-f pair adjustment using software HR info and hardware monitoring, 4) HR-aware task mapping to bridge software-hardware designs.

Result: Post-layout simulation on 7nm 256-TOPS PIM chip shows: 69.2% IR-drop mitigation, 2.29x energy efficiency improvement, and 1.152x speedup.

Conclusion: AIM effectively addresses IR-drop challenges in high-performance PIM through comprehensive software-hardware co-design, achieving substantial performance and efficiency gains while maintaining computational accuracy.

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


### [11] [Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers](https://arxiv.org/abs/2511.04677)
*Joaquin Tarraga-Moreno,Daniel Barley,Francisco J. Andujar Munoz,Jesus Escudero-Sahuquillo,Holger Froning,Pedro Javier Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.AR

TL;DR: Modern supercomputers face communication bottlenecks due to increasing heterogeneity and tight integration of CPUs, accelerators, and high-bandwidth memory/storage technologies.


<details>
  <summary>Details</summary>
Motivation: The growth of data-intensive applications like generative AI and scientific simulations is driving systems toward heterogeneous architectures to reduce data movement and improve efficiency.

Method: Combining powerful CPUs with accelerators and emerging high-bandwidth memory/storage technologies in tightly integrated architectures.

Result: As accelerator count per node increases, communication bottlenecks emerge both within and between nodes, especially when network resources are shared among heterogeneous components.

Conclusion: Current heterogeneous architectures face significant communication challenges that need to be addressed as systems scale with more accelerators per node.

Abstract: The rapid growth of data-intensive applications such as generative AI,
scientific simulations, and large-scale analytics is driving modern
supercomputers and data centers toward increasingly heterogeneous and tightly
integrated architectures. These systems combine powerful CPUs and accelerators
with emerging high-bandwidth memory and storage technologies to reduce data
movement and improve computational efficiency. However, as the number of
accelerators per node increases, communication bottlenecks emerge both within
and between nodes, particularly when network resources are shared among
heterogeneous components.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [12] [OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications](https://arxiv.org/abs/2511.03747)
*Ali Safa,Farida Mohsen,Zainab Ali,Bo Wang,Amine Bermak*

Main category: cs.ET

TL;DR: Open-MENA is the first fully open memristor interfacing system for edge AI, featuring hardware interfaces, software APIs, and VIPI programming method for memristor crossbars, validated on digit recognition and robot obstacle-avoidance tasks.


<details>
  <summary>Details</summary>
Motivation: To democratize memristor-enabled edge-AI research by providing a fully open system that enables energy-efficient in-memory computing and local plasticity learning for edge AI applications.

Method: Developed Open-MENA with three key components: (1) reproducible hardware interface for memristor crossbars with mixed-signal read-program-verify loops, (2) firmware-software stack with high-level APIs for inference and on-device learning, and (3) VIPI (Voltage-Incremental Proportional-Integral) method for programming pre-trained weights followed by chip-in-the-loop fine-tuning.

Result: Successfully validated on digit recognition tasks demonstrating the complete flow from weight transfer to on-device adaptation, and on real-world robot obstacle-avoidance where the memristor-based model learned to map localization inputs to motor commands.

Conclusion: Open-MENA provides a comprehensive open-source platform that enables reproducible research in memristor-based edge AI, successfully demonstrating practical applications in both digit recognition and autonomous robot navigation.

Abstract: Memristive crossbars enable in-memory multiply-accumulate and local
plasticity learning, offering a path to energy-efficient edge AI. To this end,
we present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our
knowledge, is the first fully open memristor interfacing system integrating (i)
a reproducible hardware interface for memristor crossbars with mixed-signal
read-program-verify loops; (ii) a firmware-software stack with high-level APIs
for inference and on-device learning; and (iii) a Voltage-Incremental
Proportional-Integral (VIPI) method to program pre-trained weights into analog
conductances, followed by chip-in-the-loop fine-tuning to mitigate device
non-idealities. OpenMENA is validated on digit recognition, demonstrating the
flow from weight transfer to on-device adaptation, and on a real-world robot
obstacle-avoidance task, where the memristor-based model learns to map
localization inputs to motor commands. OpenMENA is released as open source to
democratize memristor-enabled edge-AI research.

</details>


### [13] [Implementation of transformer-based LLMs with large-scale optoelectronic neurons on a CMOS image sensor platform](https://arxiv.org/abs/2511.04136)
*Neil Na,Chih-Hao Cheng,Shou-Chen Hsu,Che-Fu Liang,Chung-Chih Lin,Nathaniel Y. Na,Andrew I. Shieh,Erik Chen,Haisheng Rong,Richard A. Soref*

Main category: cs.ET

TL;DR: This paper proposes implementing transformer models using novel large-scale optoelectronic neurons (OENs) integrated on CMOS image sensor platforms, achieving unprecedented speed and power efficiency for LLM inference.


<details>
  <summary>Details</summary>
Motivation: The rapid deployment of datacenter infrastructure for LLMs and AI applications is predicted to cause exponentially growing energy consumption, necessitating more efficient computing solutions.

Method: Implementation of transformer models using novel large-scale optoelectronic neurons constructed over commercially available CMOS image sensor platforms, with all required optoelectronic devices and electronic circuits integrated in a small chiplet.

Result: Achieved inference speed of 12.6 POPS for GPT-3's 175 billion parameters using only 40 nm CMOS process, with power efficiency of 74 TOPS/W and area efficiency of 19 TOPS/mm² - both roughly two orders of magnitude better than digital electronics.

Conclusion: The study presents a practical path toward analog neural processing units to complement existing digital processing units, with minimal impact from quantization formats and hardware-induced errors.

Abstract: The recent rapid deployment of datacenter infrastructures for performing
large language models (LLMs) and related artificial intelligence (AI)
applications in the clouds is predicted to incur an exponentially growing
energy consumption in the near-term future. In this paper, we propose and
analyze the implementation of the transformer model, which is the cornerstone
of the modern LLMs, with novel large-scale optoelectronic neurons (OENs)
constructed over the commercially available complementary
metal-oxide-semiconductor (CMOS) image sensor (CIS) platform. With all of the
required optoelectronic devices and electronic circuits integrated in a chiplet
only about 2 cm by 3 cm in size, 175 billon parameters in the case of GPT-3 are
shown to perform inference at an unprecedented speed of 12.6 POPS using only a
40 nm CMOS process node, along with a high power efficiency of 74 TOPS/W and a
high area efficiency of 19 TOPS/mm2, both surpassing the related digital
electronics by roughly two orders of magnitude. The influence of the
quantization formats and the hardware induced errors are numerically
investigated, and are shown to have a minimal impact. Our study presents a new
yet practical path toward analog neural processing units (NPUs) to complement
existing digital processing units.

</details>
