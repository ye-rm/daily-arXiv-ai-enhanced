<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks](https://arxiv.org/abs/2511.11320)
*Jiaqi Lin,Yi Jiang,Abhronil Sengupta*

Main category: cs.ET

TL;DR: The paper proposes a stochastic Equilibrium Propagation (EP) framework that integrates probabilistic spiking neurons to enable scalable learning in deep convolutional spiking convergent recurrent neural networks, addressing biological plausibility and training stability issues in existing SNN training methods.


<details>
  <summary>Details</summary>
Motivation: Current SNN training methods like BPTT with surrogate gradients achieve strong performance but lack biological plausibility, while existing EP frameworks struggle with spiking neuron discontinuities and scaling beyond simple tasks. The stochastic nature of biological spiking mechanisms and recent hardware trends motivate a more biologically grounded approach.

Method: The authors develop a stochastic EP framework that incorporates probabilistic spiking neurons into the EP paradigm, which smoothens the optimization landscape and stabilizes training. This approach enables learning in deep convolutional spiking CRNNs with theoretical guarantees under mean-field theory.

Result: The proposed stochastic EP framework narrows the performance gap with BPTT-trained SNNs and EP-trained non-spiking CRNNs on vision benchmarks while maintaining the locality property, demonstrating improved scalability and training stability.

Conclusion: Stochastic EP represents a promising direction for neuromorphic and on-chip learning by providing a biologically plausible, scalable training method for spiking neural networks that preserves locality and achieves competitive performance on vision tasks.

Abstract: Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and biologically grounded alternative. However, existing EP frameworks, primarily based on deterministic neurons, either require complex mechanisms to handle discontinuities in spiking dynamics or fail to scale beyond simple visual tasks. Inspired by the stochastic nature of biological spiking mechanism and recent hardware trends, we propose a stochastic EP framework that integrates probabilistic spiking neurons into the EP paradigm. This formulation smoothens the optimization landscape, stabilizes training, and enables scalable learning in deep convolutional spiking convergent recurrent neural networks (CRNNs). We provide theoretical guarantees showing that the proposed stochastic EP dynamics approximate deterministic EP under mean-field theory, thereby inheriting its underlying theoretical guarantees. The proposed framework narrows the gap to both BPTT-trained SNNs and EP-trained non-spiking CRNNs in vision benchmarks while preserving locality, highlighting stochastic EP as a promising direction for neuromorphic and on-chip learning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: The FengHuang Platform proposes a disaggregated AI infrastructure with multi-tier shared-memory architecture to overcome GPU memory and communication scaling limitations for AI inference, achieving significant resource savings while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Traditional GPU-centric architectures face scalability challenges for AI inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling, which hinder the growth of large language models and generative AI systems.

Method: FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations.

Result: Simulations demonstrate up to 93% local memory capacity reduction, 50% GPU compute savings, 16x to 70x faster inter-GPU communication, and up to 50% GPU reductions while maintaining performance across workloads like GPT-3, Grok-1, and QWEN3-235B.

Conclusion: FengHuang provides a scalable, flexible, cost-effective rack-level AI infrastructure solution with open, heterogeneous design that eliminates vendor lock-in and enables significant infrastructure and power cost reductions.

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [3] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester is a multi-agent LLM framework that automates unit test generation for HPC software using OpenMP and MPI, addressing challenges like non-deterministic behavior and synchronization through specialized agents in a collaborative workflow.


<details>
  <summary>Details</summary>
Motivation: Traditional unit testing methods struggle with HPC applications due to parallelism, complex algorithms, and diverse hardware, often failing to handle non-deterministic behavior and synchronization issues effectively.

Method: Uses a multi-agent LLM framework with specialized agents (Recipe Agent and Test Agent) that collaborate through iterative critique loops to generate and refine context-aware unit tests targeting parallel execution constructs and communication patterns.

Result: Successfully generates compilable and functionally correct tests for OpenMP and MPI primitives, identifying subtle bugs missed by conventional techniques, with significantly improved test compilation rates and correctness compared to standalone LLMs.

Conclusion: HPCAgentTester provides a robust and scalable solution for ensuring parallel software reliability, effectively addressing HPC-specific testing challenges through its collaborative multi-agent architecture.

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


### [4] [UFO$^3$: Weaving the Digital Agent Galaxy](https://arxiv.org/abs/2511.11332)
*Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.DC

TL;DR: UFO$^3$ is a system that unifies heterogeneous devices into a single orchestration fabric using TaskConstellations - distributed DAGs of atomic subtasks that enable asynchronous execution, adaptive recovery, and dynamic optimization across multiple devices.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered agent frameworks are confined to single OS or devices, making cross-device workflows brittle and manual. There's a need to dissolve boundaries between devices and platforms to enable seamless agent collaboration.

Method: Models user requests as mutable TaskConstellations (distributed DAGs of atomic subtasks with control/data dependencies), uses Constellation Orchestrator for safe asynchronous execution with dynamic DAG updates, and Agent Interaction Protocol for persistent low-latency task dispatch.

Result: Achieves 83.3% subtask completion, 70.9% task success rate, exposes parallelism with average width of 1.72, reduces end-to-end latency by 31% vs sequential baseline, and demonstrates graceful degradation under agent failures.

Conclusion: UFO$^3$ enables accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric for ubiquitous computing.

Abstract: Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.
  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.

</details>


### [5] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: The paper introduces a novel algorithm that enables high-performance physical system simulations, achieving 1.6 million time steps per second and 84 PFLOP/s with 90% peak performance utilization on both single-node and clustered systems.


<details>
  <summary>Details</summary>
Motivation: Existing domain decomposition methods fail to deliver high simulation rates or high utilization in network computing environments, with Exascale systems achieving only a small fraction of their peak performance for these workloads.

Method: The paper introduces the novel algorithm that overcomes limitations of traditional domain decomposition methods, enabling efficient simulation of physical systems in networked computing environments.

Result: The implementation achieves simulations running at 1.6 million time steps per second and 84 PFLOP/s, reaching 90% of peak performance in both single-node and clustered environments. Demonstrated with shallow-water equations modeling a tsunami from asteroid impact at 460m-resolution on planetary scale using 64 Cerebras CS-3 systems.

Conclusion: The novel algorithm successfully addresses the performance limitations of traditional domain decomposition methods, enabling highly efficient physical system simulations that achieve near-peak performance on modern computing systems.

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: This paper proposes simplifying ESD protection and inter-chiplet signaling in 2.5D/3D packaging technologies to enable further chiplet miniaturization below 100 mm² by reducing conventional I/O circuitry overhead.


<details>
  <summary>Details</summary>
Motivation: Advanced packaging technologies enable larger-scale VLSI systems with better energy efficiency, but conventional I/O circuitry (ESD protection and signaling) creates significant area overhead that limits chiplet size reduction below 100 mm².

Method: Revisited reliability requirements from chiplet interface design perspective, conducted parasitic extraction and SPICE simulations to demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies.

Result: Demonstrated that ESD protection and inter-chiplet signaling can be significantly simplified in advanced packaging technologies, reducing the area overhead that previously constrained chiplet miniaturization.

Conclusion: Simplification of ESD protection and signaling circuitry paves the way for further chiplet miniaturization and improves the composability and reusability of tiny chiplets in 2.5D/3D heterogeneous integration systems.

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [7] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: MMA-Sim is the first bit-accurate reference model that reveals undocumented arithmetic behaviors of matrix multiplication accelerators (MMAs) in modern GPUs, addressing numerical imprecision and inconsistency issues in DNN training and inference.


<details>
  <summary>Details</summary>
Motivation: Modern GPUs integrate matrix multiplication accelerators (MMAs) like NVIDIA Tensor Cores and AMD Matrix Cores, but their undocumented arithmetic specifications cause numerical imprecision and inconsistency that compromise DNN stability and reproducibility.

Method: Developed MMA-Sim through targeted and randomized tests to derive nine arithmetic algorithms that simulate floating-point matrix multiplication behaviors of MMAs from ten GPU architectures (8 NVIDIA, 2 AMD), achieving bitwise equivalence with real hardware.

Result: Large-scale validation confirmed bitwise equivalence between MMA-Sim and real hardware. The study identified undocumented behaviors that could lead to significant errors and investigated arithmetic behaviors affecting DNN training stability.

Conclusion: MMA-Sim provides the first comprehensive analysis of MMA arithmetic behaviors, revealing critical undocumented specifications that impact DNN numerical stability and reproducibility, enabling better understanding and mitigation of precision issues in GPU-accelerated deep learning.

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


### [8] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: The paper proposes T-MAC, a method using table lookup to accelerate LLM inference on NPUs by overcoming hardware limitations through fused two-level table-based dequantization and concurrency-hierarchy-guided tiling.


<details>
  <summary>Details</summary>
Motivation: LLM inference on NPUs is slower than CPUs due to poor performance on non-GEMM computations like dequantization, and existing solutions either split prefill/decoding between NPUs/CPUs or sacrifice accuracy.

Method: Uses table lookup to subsume unsupported hardware operations, with fused two-level table-based dequantization and concurrency-hierarchy-guided tiling to create a unified table layout, implementing prefill via three-stage pipeline and mapping table-lookup-based decoding to NPU vector units.

Result: Achieves 1.4x speedup for prefill, 3.1x speedup for decoding, and 84% energy savings compared to baseline NPU methods.

Conclusion: T-MAC effectively accelerates LLM inference on NPUs by leveraging table lookup techniques to overcome hardware limitations while maintaining performance and energy efficiency.

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>
