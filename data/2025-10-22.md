<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: Framework for migrating large-scale atmospheric and oceanic AI models from PyTorch to MindSpore and optimizing for Chinese chips, achieving comparable accuracy while reducing hardware dependencies and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Current AI models for climate and weather research heavily depend on GPUs, limiting hardware independence especially for Chinese domestic hardware and frameworks, creating a need for alternative solutions.

Method: Developed a framework focusing on software-hardware adaptation, memory optimization, and parallelism to migrate models from PyTorch to MindSpore and optimize for Chinese chips.

Result: Experimental results show the migration preserves original model accuracy while significantly reducing system dependencies and improving operational efficiency using Chinese chips as viable alternatives.

Conclusion: The work provides valuable insights and practical guidance for using Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [2] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: This paper proposes DSL-OTA, an over-the-air analog aggregation method for distributed swarm learning that enhances communication efficiency, enables effective cooperation, and ensures privacy preservation in wireless networks.


<details>
  <summary>Details</summary>
Motivation: Limited transmission resources and complex communication environments are significant bottlenecks for efficient collaboration among edge devices in large-scale wireless networks, despite advances in distributed learning systems.

Method: The paper proposes DSL-OTA, which incorporates multi-worker selection strategy with over-the-air analog aggregation, making standard DSL more federated and securing aggregation from data leakage risks.

Result: Theoretical analyses show fast convergence rate and low communication costs. Simulations demonstrate superior learning performance under both homogeneous and heterogeneous dataset settings compared to existing methods.

Conclusion: DSL-OTA effectively addresses communication bottlenecks in distributed swarm learning while maintaining privacy and achieving better learning performance across different dataset conditions.

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [3] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: An end-to-end parallel framework for efficient analysis of large-scale GPU traces in HPC systems, achieving 67% scalability improvement.


<details>
  <summary>Details</summary>
Motivation: Large-scale GPU traces are crucial for identifying performance bottlenecks in HPC architectures, but their volume and complexity make analysis computationally expensive and time-consuming.

Method: The framework partitions and processes trace data concurrently, employing causal graph methods and parallel coordinating charts to expose performance variability and dependencies across execution flows.

Result: Experimental results demonstrate a 67% improvement in scalability, showing effectiveness for analyzing multiple traces independently.

Conclusion: The proposed parallel performance analysis framework efficiently handles multiple large-scale GPU traces and significantly improves scalability for HPC performance analysis.

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [4] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: SLICE is a novel scheduling system for LLM inference services on edge devices that addresses differentiated Service Level Objectives (SLOs) for latency-sensitive applications, achieving significantly higher SLO attainment compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: Current LLM scheduling systems prioritize throughput maximization but fail to address diverse SLO requirements for edge devices, leading to high violation rates for latency-sensitive applications like machine control and navigation planning.

Method: SLICE combines a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates to optimize LLM inference services for differentiated SLO requirements.

Result: Experimental results show SLICE achieves up to 35x higher SLO attainment and 3.4x better task completion time compared to state-of-the-art solutions Orca and FastServe.

Conclusion: SLICE effectively addresses the limitations of existing LLM scheduling systems by providing differentiated SLO support for edge computing scenarios, significantly improving service quality for latency-sensitive applications.

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [5] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake is a KV-Cache-centric serving framework that optimizes scheduling and memory management for LLMs in multi-agent applications with function calls, reducing latency by 47.06% and improving GPU memory utilization by 16.9% compared to vLLM.


<details>
  <summary>Details</summary>
Motivation: LLMs deployed in multi-agent applications with external function calls face severe KV Cache performance challenges including space contention causing cache eviction and time underutilization leaving GPU memory idle during tool call stalls.

Method: Tokencake uses agent-aware design with Space Scheduler for dynamic memory partitioning to shield critical agents from contention, and Time Scheduler with proactive offload and predictive upload to repurpose GPU memory during function call stalls.

Result: Evaluation on multi-agent benchmarks shows Tokencake reduces end-to-end latency by over 47.06% and improves effective GPU memory utilization by up to 16.9% compared to vLLM.

Conclusion: Tokencake successfully addresses KV Cache performance challenges in multi-agent LLM applications through co-optimized scheduling and memory management, significantly improving both latency and GPU utilization.

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [6] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: New communication-efficient distributed interactive proofs for planarity with O(log* n)-round protocols and small proof sizes.


<details>
  <summary>Details</summary>
Motivation: To design efficient distributed interactive proofs for planarity that minimize prover-verifier communication while maintaining verification capabilities.

Method: Develop DIP protocols with multiple interaction rounds and adjustable proof sizes based on the number of rounds, specifically O(r)-round protocols for planarity with proof sizes O(log^(r)n) and O(log^(r)n + logΔ/r).

Result: Achieved O(log* n)-round DIP protocols for embedded planarity with O(1) proof size and for planarity with O(⌈log Δ/log* n⌉) proof size.

Conclusion: The paper presents communication-efficient distributed interactive proofs for planarity that offer flexible trade-offs between interaction rounds and proof sizes, significantly improving upon previous approaches.

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [7] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: Performance regression detection in CI/CD systems faces challenges due to resource-intensive benchmarking. This vision paper identifies three key challenges: composability of optimization strategies, automated result evaluation, and practical CI/CD integration.


<details>
  <summary>Details</summary>
Motivation: Performance regressions in large-scale software systems cause significant resource inefficiencies, making early detection critical. Current benchmark optimization techniques aren't practically integrated into real-world CI/CD pipelines despite their importance for maintaining service-level agreements.

Method: The paper presents a conceptual cloud-based benchmarking framework designed to handle identified challenges transparently. It analyzes the current state of benchmark optimization and identifies gaps preventing broader adoption.

Result: Three central challenges are identified: (a) composability of benchmark optimization strategies, (b) automated evaluation of benchmarking results, and (c) usability and complexity of applying these strategies in CI/CD systems.

Conclusion: The paper aims to stimulate research toward making performance regression detection in CI/CD systems more practical and effective by presenting these open problems and proposing a conceptual framework to address them.

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [8] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: PCMS is a GPU-accelerated coupling framework for multimodel simulations on supercomputers, supporting up to 5D field mapping and demonstrated with plasma physics code couplings achieving 85% weak scaling efficiency on 2,080 GPUs.


<details>
  <summary>Details</summary>
Motivation: To enable efficient coupling of multiple simulation codes on leadership class supercomputers, particularly for complex plasma physics applications requiring GPU acceleration and high-dimensional field mapping capabilities.

Method: Developed a parallel coupler framework with distributed control and field mapping methods for up to five dimensions, utilizing discretization and field information to accommodate physics constraints. Demonstrated with couplings between gyrokinetic codes (XGC, GTC), Monte Carlo neutral transport code (DEGAS2), and energetic particle transport code (GNET).

Result: Successfully demonstrated couplings between XGC-DEGAS2 and GNET-GTC codes. Achieved weak scaling on up to 2,080 GPUs of Frontier supercomputer with 85% efficiency.

Conclusion: PCMS provides an effective GPU-accelerated coupling framework for multimodel simulations on leadership class systems, enabling complex plasma physics applications with high scalability and efficiency.

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Metrics and evaluations for computational and sustainable AI efficiency](https://arxiv.org/abs/2510.17885)
*Hongyuan Liu,Xinyang Liu,Guosheng Hu*

Main category: cs.PF

TL;DR: A unified methodology for evaluating AI model inference that integrates computational performance, energy consumption, and carbon emissions across diverse hardware and software platforms.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluation methods are fragmented and fail to provide holistic comparisons across different hardware, software stacks, and numeric precisions, making it difficult to optimize systems and assess environmental impact.

Method: Proposes a reproducible framework that systematically measures latency, throughput distributions, energy consumption, and location-adjusted carbon emissions while maintaining matched accuracy constraints. Applied to multi-precision models across diverse hardware (from data-center accelerators to consumer GPUs) and software stacks (PyTorch, TensorRT, ONNX Runtime).

Result: Establishes a rigorous benchmarking framework that produces decision-ready Pareto frontiers, clarifying trade-offs between accuracy, latency, energy, and carbon emissions.

Conclusion: The methodology enables evidence-based decisions for sustainable AI deployment, with open-source code provided for independent verification and adoption by researchers and practitioners.

Abstract: The rapid advancement of Artificial Intelligence (AI) has created
unprecedented demands for computational power, yet methods for evaluating the
performance, efficiency, and environmental impact of deployed models remain
fragmented. Current approaches often fail to provide a holistic view, making it
difficult to compare and optimise systems across heterogeneous hardware,
software stacks, and numeric precisions. To address this gap, we propose a
unified and reproducible methodology for AI model inference that integrates
computational and environmental metrics under realistic serving conditions. Our
framework provides a pragmatic, carbon-aware evaluation by systematically
measuring latency and throughput distributions, energy consumption, and
location-adjusted carbon emissions, all while maintaining matched accuracy
constraints for valid comparisons. We apply this methodology to multi-precision
models across diverse hardware platforms, from data-centre accelerators like
the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream
software stacks including PyTorch, TensorRT, and ONNX Runtime. By
systematically categorising these factors, our work establishes a rigorous
benchmarking framework that produces decision-ready Pareto frontiers,
clarifying the trade-offs between accuracy, latency, energy, and carbon. The
accompanying open-source code enables independent verification and facilitates
adoption, empowering researchers and practitioners to make evidence-based
decisions for sustainable AI deployment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ is a speculative decoding method that uses part of the full-model weight bits to create a quantized draft model, eliminating extra training/storage overhead while achieving significant speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Large language models have high inference latency due to their large parameter sizes, and existing quantization methods often cause performance degradation while speculative decoding incurs extra overheads.

Method: Algorithm-hardware co-design that uses part of full-model weight bits to form quantized draft model, with reconfigurable processing element array for efficient execution of both draft and verification passes.

Result: Achieves speedups of 2.07x, 1.53x, and 1.45x compared to FP16, Olive, and Tender respectively across 15 LLMs and tasks.

Conclusion: SPEQ provides an effective solution for reducing LLM inference latency without performance degradation or additional overhead through its co-designed speculative decoding approach.

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>
