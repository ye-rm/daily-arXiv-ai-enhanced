<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.ET](#cs.ET) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign is a GPU-accelerated SPHINCS+ implementation using hierarchical tuning and compiler optimizations to speed up post-quantum hash-based signatures, achieving 1.24-3.13x throughput improvements over state-of-the-art GPU implementations.


<details>
  <summary>Details</summary>
Motivation: SPHINCS+ is a stateless hash-based post-quantum signature scheme, but its signature generation is slow due to intensive hash computations. While GPUs offer massive parallelism potential, existing GPU optimizations fail to fully exploit SPHINCS+'s Merkle tree parallelism or lack fine-grained compiler-level customization across its diverse computational kernels.

Method: HERO Sign employs hierarchical tuning and compiler optimizations: 1) Tree Fusion strategy for FORS with automated Tree Tuning search algorithm that adapts to GPU architectures, 2) Adaptive compilation strategy selecting between PTX and native code paths for different kernels (FORS Sign, TREE Sign, WOTS+ Sign), 3) Task graph-based construction for batched signature generation to reduce multi-stream idle time and kernel launch overhead.

Result: HERO Sign achieves throughput improvements of 1.28-3.13x, 1.28-2.92x, and 1.24-2.60x under SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains on A100, H100, and GTX 2080, with two orders of magnitude reduction in kernel launch latency compared to state-of-the-art GPU implementations.

Conclusion: HERO Sign demonstrates that hierarchical tuning and compiler-level optimizations can effectively accelerate SPHINCS+ signature generation on GPUs, addressing the limitations of existing GPU implementations and providing significant performance improvements across multiple GPU architectures.

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: Agentic Cloud Data Engineering is a policy-aware control architecture using AI agents to autonomously manage cloud data pipelines, reducing recovery time by 45%, cutting costs by 25%, and decreasing manual intervention by 70%.


<details>
  <summary>Details</summary>
Motivation: Current cloud data pipelines rely on static configurations and reactive operations, leading to slow recovery, inefficient resource use, and high manual overhead despite dynamic workloads, evolving schemas, cost constraints, and governance requirements.

Method: A policy-aware control architecture integrates bounded AI agents into pipeline governance. Specialized agents analyze telemetry/metadata, reason over declarative cost/compliance policies, and propose constrained operational actions like adaptive resource reconfiguration, schema reconciliation, and automated failure recovery, with all actions validated against governance policies.

Result: Experimental evaluation with representative batch/streaming workloads shows: 45% reduction in mean pipeline recovery time, ~25% lower operational costs, over 70% decrease in manual intervention events, while maintaining data freshness and policy compliance.

Conclusion: Policy-bounded agentic control provides an effective, practical approach for governing cloud data pipelines in enterprise environments, addressing dynamic workloads and governance requirements while improving efficiency and reducing manual overhead.

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [3] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: A container-based resource management framework for edge servers that uses profiling data to model performance and optimizes latency and power consumption through convex optimization and greedy refinement.


<details>
  <summary>Details</summary>
Motivation: Edge computing needs efficient resource orchestration for latency-critical applications, but faces challenges from task heterogeneity and limited resources on edge servers.

Method: Profiling experiments create nonlinear fitting model for CPU/memory-latency relationships; formulate MINLP problem for joint latency-power optimization; decompose into convex subproblems solved via two-stage CRMS combining convex optimization and greedy refinement.

Result: CRMS reduces latency by over 14% and improves energy efficiency compared to heuristic and search-based baselines, with polynomial-time complexity supporting quasi-dynamic execution.

Conclusion: The proposed container-based resource management scheme offers a practical, scalable solution for heterogeneous edge environments with dynamic workloads, enabling efficient intra-node optimization on edge servers.

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [4] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: Proposes a joint client selection and resource allocation (CSRA) approach for federated learning in wireless networks to address data heterogeneity, minimizing latency and energy while constraining generalization error.


<details>
  <summary>Details</summary>
Motivation: Federated learning efficiency in wireless networks is limited by communication/computational constraints and significant data heterogeneity among clients, which causes repeated training cycles, increased energy consumption, and prolonged latency.

Method: First presents theoretical analysis of client data heterogeneity impact on global model generalization error, formulates optimization problem to jointly minimize learning latency and energy consumption while constraining generalization error, then proposes joint client selection and resource allocation (CSRA) approach using convex optimization and relaxation techniques.

Result: Extensive simulations show the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that don't account for data heterogeneity.

Conclusion: The CSRA approach effectively addresses data heterogeneity challenges in federated learning, improving overall system efficiency in wireless networks through joint optimization of client selection and resource allocation.

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [5] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV is a KV cache compression framework for LLMs that reduces memory usage by 153-180% compared to quantization methods while improving throughput by 76-172% on GPUs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs face significant memory challenges with long-context inference due to the substantial KV cache requirements that can scale to gigabytes, limiting practical deployment.

Method: PackKV introduces novel lossy compression techniques specifically tailored to KV cache characteristics, with careful co-design of compression algorithms and system architecture that supports dynamically growing KV caches while maintaining computational efficiency.

Result: Under the same accuracy drop as state-of-the-art quantization methods, PackKV achieves 153.2% higher memory reduction for K cache and 179.6% for V cache, with throughput improvements of 75.7% for K and 171.7% for V across A100 and RTX Pro 6000 GPUs compared to cuBLAS kernels.

Conclusion: PackKV provides an efficient KV cache management framework that significantly reduces memory requirements while improving computational throughput, making long-context LLM inference more practical and scalable.

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [6] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: This paper addresses I/O bottlenecks in LLM checkpointing by developing microbenchmarks to evaluate liburing's effectiveness, finding that file system-aware aggregation and coalescing strategies can achieve up to 7.6× higher write throughput compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale with 3D parallelism, checkpoint/restore becomes a big-data I/O problem with bottlenecks across storage tiers. Kernel-accelerated I/O libraries like liburing may help but their effectiveness for LLM checkpointing remains underexplored.

Method: Developed microbenchmarks to quantify trade-offs when using liburing, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. Focused on file system-aware strategies.

Result: Uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Achieved up to 3.9× higher write throughput than DataStates-LLM and 7.6× higher than TorchSnapshot.

Conclusion: The results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends, demonstrating significant performance improvements over state-of-the-art LLM checkpointing engines.

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [7] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: RABO/RAFBO: Resource-adaptive distributed bilevel optimization framework with second-order free hypergradient estimator for low-resource clients, achieving asymptotically optimal convergence rate O(1/√(C_x*Q)).


<details>
  <summary>Details</summary>
Motivation: Traditional distributed bilevel optimization algorithms cannot be applied to low-resource clients due to excessive computation requirements for optimizing both lower- and upper-level functions simultaneously.

Method: Propose resource-adaptive distributed bilevel optimization framework with second-order free hypergradient estimator, allowing clients to optimize submodels adapted to available resources. Address challenges of partial outer/inner parameter coupling and local partial training.

Result: Theoretical analysis shows both RABO and RAFBO achieve asymptotically optimal convergence rate O(1/√(C_x*Q)), dominated by minimum coverage of outer parameter C_x*. Experiments demonstrate effectiveness and computation efficiency.

Conclusion: The proposed resource-adaptive framework enables distributed bilevel optimization on low-resource clients with provable convergence guarantees and practical efficiency.

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [8] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: AI-driven framework for proactive resource optimization in multi-cluster cloud systems using predictive learning and policy-aware decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing reactive, cluster-centric resource management approaches are inefficient for modern multi-cluster cloud deployments, leading to poor resource utilization, slow adaptation, and high operational overhead across distributed environments.

Method: AI-driven framework integrating predictive learning, policy-aware decision-making, and continuous feedback to enable proactive, coordinated resource management across clusters by analyzing cross-cluster telemetry and historical execution patterns.

Result: Prototype demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches.

Conclusion: Intelligent, self-adaptive infrastructure management is a key enabler for scalable and resilient cloud platforms, with the framework showing effectiveness in optimizing system-wide behavior.

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [9] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL is a fault-tolerant communication library that provides lossless, low-overhead failover for large-scale ML training and inference by exploiting multi-NIC hardware to handle network faults without job termination.


<details>
  <summary>Details</summary>
Motivation: Modern ML training and inference span tens to tens of thousands of GPUs, where network faults waste 10-15% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference.

Method: R²CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. It exploits multi-NIC hardware to provide lossless, low-overhead failover.

Result: R²CCL is highly robust to NIC failures, incurring less than 1% training and less than 3% inference overheads. It outperforms baselines AdapCC and DejaVu by 12.18× and 47×, respectively.

Conclusion: R²CCL provides an effective fault-tolerant communication solution for large-scale ML systems, significantly reducing GPU hour waste and improving system resilience against network failures.

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [10] [MSched: GPU Multitasking via Proactive Memory Scheduling](https://arxiv.org/abs/2512.24637)
*Weihang Shen,Yinqiu Chen,Rong Chen,Haibo Chen*

Main category: cs.OS

TL;DR: MSched is an OS-level scheduler that predicts GPU memory access patterns and proactively prepares working sets to avoid expensive page faults, achieving up to 57.88x speedup over demand paging for memory-oversubscribed GPU workloads.


<details>
  <summary>Details</summary>
Motivation: Limited GPU HBM capacity bottlenecks hosting larger-scale GPU tasks. Demand paging causes up to 78x slowdown due to massive working sets and poor locality in GPU workloads. However, GPU memory access patterns are predictable via kernel launch arguments and asynchronous execution.

Method: MSched extends GPU context switching with proactive working set preparation, coalescing fragmented page faults into single efficient migrations. Uses template-based approach for near-perfect working set prediction and co-designs task scheduler with memory manager for globally optimal page placement policy.

Result: MSched outperforms demand paging by up to 11.05x for scientific and deep learning workloads, and 57.88x for LLM under memory oversubscription.

Conclusion: Proactive working set preparation through predictable GPU memory access patterns and OS-level scheduling co-design significantly improves performance for memory-oversubscribed GPU workloads compared to traditional demand paging approaches.

Abstract: The limited HBM capacity has become the primary bottleneck for hosting an increasing number of larger-scale GPU tasks. While demand paging extends capacity via host DRAM, it incurs up to 78x slowdown due to the massive working sets and poor locality of GPU workloads. We observe, however, that GPU memory access patterns are inherently predictable via kernel launch arguments and their asynchronous execution nature. Leveraging this, we propose MSched, an OS-level scheduler that extends GPU context switching to include proactive working set preparation, thereby coalescing fragmented, eventual, and expensive page faults into a single efficient migration. MSched employs a template-based approach to predict working sets with near-perfect accuracy and proposes a co-design between task scheduler and memory manager to enforce a globally optimal page placement policy. Evaluation demonstrates that MSched outperforms demand paging by up to 11.05x for scientific and deep learning workloads, and 57.88x for LLM under memory oversubscription.

</details>


### [11] [Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search](https://arxiv.org/abs/2512.25065)
*Rohit Dwivedula,Divyanshu Saxena,Sujay Yadalam,Daehyeok Kim,Aditya Akella*

Main category: cs.OS

TL;DR: Vulcan uses LLMs to automatically generate instance-optimal heuristics for resource management tasks, outperforming human-designed algorithms by up to 69%.


<details>
  <summary>Details</summary>
Motivation: Current resource management in systems relies on hand-designed heuristics that are expensive to create and need constant redesign due to changing hardware, workloads, and environments.

Method: Vulcan separates policy and mechanism through LLM-friendly interfaces, then uses evolutionary search over LLM-generated code to synthesize specialized heuristics for specific workloads and hardware.

Result: Synthesized heuristics for cache eviction and memory tiering outperform all human-designed state-of-the-art algorithms by up to 69% and 7.9% respectively.

Conclusion: LLM-based code generation can automatically create instance-optimal heuristics that outperform human-designed solutions, offering a scalable alternative to manual heuristic design.

Abstract: Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.
  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.
  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [12] [An Electronic Ising Machine](https://arxiv.org/abs/2512.23720)
*Matt Bowring,Ben Anderdson,Ben Tiffany*

Main category: cs.ET

TL;DR: Custom PCB accelerator uses analog coupled oscillators to solve NP-hard graph problems via annealing, achieving low-power, high-speed computation through physics-based analog computing.


<details>
  <summary>Details</summary>
Motivation: To develop a specialized hardware accelerator that can efficiently solve NP-hard graph problems using analog computing principles, overcoming limitations of traditional digital approaches in terms of power consumption and speed.

Method: Design a custom printed circuit board with coupled nonlinear electronic oscillators based on annealing principles. The system uses an energy-based representation of graph problems, allowing the analog architecture to naturally follow gradients toward stable phase alignments that encode solutions.

Result: The paper presents a functional PCB accelerator that demonstrates low-power and high-speed operation for solving NP-hard graph problems through analog computing, with detailed circuit design, simulations, and experimental validation.

Conclusion: The work demonstrates the feasibility and potential of physics-based analog computing devices as efficient accelerators for computationally hard problems, contributing to the emerging field of novel computing architectures beyond traditional digital approaches.

Abstract: We develop a custom printed circuit board (PCB) as a low-power and high-speed accelerator for NP-Hard graph problems. Based on the annealing principle, it uses an analog computing architecture of coupled nonlinear electronic oscillators. Using an energy-based representation of the input problem, the system is shown to naturally follow the gradient towards stable phase alignments that encode solutions. We introduce the motivational theory, give an overview of our detailed circuit design, simulations, and experiments, and provide insight on the emerging development of novel physics-based computing devices.

</details>


### [13] [Biochemical Computing Mode for Sequential Logic](https://arxiv.org/abs/2512.23734)
*Han Huang,Chengzhi Ma,Yuxin Zhao,Qingyao Wang,Xinglong Xiao,Xiulin Shu,Zhifeng Hao*

Main category: cs.ET

TL;DR: The paper proposes a biochemical computing mode using enzyme-driven molecular logic gates that can achieve sequential logic circuits, enabling general-purpose biochemical computers comparable to electronic computers.


<details>
  <summary>Details</summary>
Motivation: While various next-generation computing modes (optical, quantum, DNA-based) have been proposed, sequential logic circuits - essential for continuous computation and memory storage - are often overlooked due to implementation difficulties. The paper aims to address this gap for biochemical computing.

Method: The authors use enzyme control of enzymatic reactions to design a logic gate model composed of small molecules driven by enzymes. They mathematically analyze static and dynamic input-output properties of biochemical logic gate components and prove the system satisfies sequential mapping similar to electronic computers.

Result: The biochemical computing mode satisfies sequential mapping similar to electronic computers. When combined with the storage characteristics of NOT-AND gates, it can realize sequential logic circuits, providing a theoretical foundation for general-purpose biochemical computers.

Conclusion: The proposed enzyme-driven biochemical computing mode can achieve sequential logic circuits comparable to electronic computers, establishing theoretical groundwork for developing general-purpose biochemical computers as part of next-generation computing paradigms.

Abstract: Recent years have witnessed the growing scholarly interest in the next-generation general-purpose computers. Various innovative computing modes have been proposed, such as optical, quantum phenomena, and DNA-based modes. Sequential logic circuits are a critical factor that enables these modes to function as general-purpose computers, given their essential role in facilitating continuous computation and memory storage through their ability to store states. However, compared to computability, it is often overlooked due to the difficulty of its implementation. In this paper, we first demonstrate sequential mapping, a crucial necessary condition for electronic computers to realize sequential logic circuits, and highlight this distinctive property of general-purpose computers in the context of logic gate circuits. To achieve computational functionalities comparable to those of electronic computers, we utilize the control effect of enzymes on enzymatic reactions to design a logic gate model that is composed of small molecules and driven by enzymes, subsequently propose a biochemical computing mode. Furthermore, we mathematically analyze the static and dynamic input-output properties of biochemical logic gate components and prove that the biochemical computing mode satisfies sequential mapping similar to electronic computers. When combined with the storage characteristics of NOT-AND gates, it can realize sequential logic circuits. The findings can serve as a theoretical foundation for developing general-purpose biochemical computers.

</details>


### [14] [Ovonic switches enable energy-efficient dendrite-like computing](https://arxiv.org/abs/2512.23736)
*Unhyeon Kang,Jaesang Lee,Seungmin Oh,Hanchan Song,Jongkil Park,Jaewook Kim,Seongsik Park,Hyun Jae Jang,Sangbum Kim,Su-in Yi,Suhas Kumar,Suyoun Lee*

Main category: cs.ET

TL;DR: A single two-terminal Ovonic threshold switching device demonstrates self-sustained dynamics, universal Boolean logic, XOR operations, and edge detection capabilities, offering energy-efficient neuromorphic computing inspired by biological dendrites.


<details>
  <summary>Details</summary>
Motivation: Biological dendrites exhibit complex computational capabilities beyond simple pooling, including temporal dynamics, logic operations, and signal processing. Mimicking this functional density could provide powerful primitives for neuromorphic computing to replace aging digital paradigms.

Method: Using electrically driven Ovonic threshold switching in Sb-Te-doped GeSe to create a single two-terminal component that exhibits self-sustained dynamics and computational capabilities. The device demonstrates universal Boolean logic, XOR operations, and edge detection for images.

Result: The single component achieves self-sustained dynamics, universal Boolean logic, XOR operations (traditionally requiring multiple components), and edge gradient detection. Networks of these switches exhibit half-adder and full-adder properties with inhibitory/excitatory signal discrimination, offering orders of magnitude better energy efficiency than digital solutions.

Conclusion: This work demonstrates a computational primitive that emulates dendrite-like functionality in a single device, paving the way for energy-efficient post-digital neuromorphic computing with significantly simpler hardware compared to conventional digital circuits.

Abstract: Over the last decade, dendrites within individual biological neurons, which were previously thought to generally perform information pooling and networking, have now been shown to express complex temporal dynamics, Boolean-like logic, arithmetic, signal discrimination, and edge detection for image and sound recognition. Mimicking this rich functional density could offer a powerful primitive for neuromorphic computing, which has sought to replace the aging digital computing paradigms using biological inspirations. Here, using electrically driven Ovonic threshold switching in Sb-Te-doped GeSe, we demonstrate a single two-terminal component capable of self-sustained dynamics and universal Boolean logic, in addition to XOR operations (which is traditionally thought to require a network of active components). We then employ logic-driven dynamics in a single component to detect and estimate the gradients of edges in images, a task that otherwise requires elaborate circuits. A network of Ovonic switches exhibits properties of a half adder and a full adder, in addition to discriminative logic accommodating inhibitory and excitatory signals. We show that this computational primitive is not only seemingly simpler, but also offers many orders of magnitude improved energy efficiency compared to prevailing digital solutions. As such, this work paves the path for potentially emulating dendrites for efficient post-digital neuromorphic computing.

</details>


### [15] [Exploring the Potential of Spiking Neural Networks in UWB Channel Estimation](https://arxiv.org/abs/2512.23975)
*Youdong Zhang,Xu He,Xiaolin Meng*

Main category: cs.ET

TL;DR: Unsupervised Spiking Neural Network for UWB channel estimation achieves 80% accuracy with lower complexity than deep learning methods, suitable for neuromorphic edge devices.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based UWB channel estimation methods are computationally intensive and clash with resource constraints of low-cost edge devices, creating a need for more efficient solutions.

Method: Developed a fully unsupervised Spiking Neural Network (SNN) solution for UWB channel estimation, with comprehensive comparative strategies evaluated on a public benchmark.

Result: The unsupervised SNN approach achieves 80% test accuracy, comparable to several supervised deep learning strategies, while offering drastically reduced model complexity.

Conclusion: SNNs are promising for UWB channel estimation as they maintain competitive accuracy while being inherently suited to neuromorphic deployment and offering significant complexity reduction for edge devices.

Abstract: Although existing deep learning-based Ultra-Wide Band (UWB) channel estimation methods achieve high accuracy, their computational intensity clashes sharply with the resource constraints of low-cost edge devices. Motivated by this, this letter explores the potential of Spiking Neural Networks (SNNs) for this task and develops a fully unsupervised SNN solution. To enable a comprehensive performance analysis, we devise an extensive set of comparative strategies and evaluate them on a compelling public benchmark. Experimental results show that our unsupervised approach still attains 80% test accuracy, on par with several supervised deep learning-based strategies. Moreover, compared with complex deep learning methods, our SNN implementation is inherently suited to neuromorphic deployment and offers a drastic reduction in model complexity, bringing significant advantages for future neuromorphic practice.

</details>
