<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 11]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io is a log-based rollback recovery and fine-grain data lineage system for serverless distributed data pipelines that supports non-deterministic operators and external interactions while enabling independent operator recovery and dynamic scaling.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for correct rollback recovery and fine-grained data lineage capture in distributed data pipelines, particularly for serverless scalable architectures that handle non-deterministic operations and external system interactions.

Method: LOG.io uses a log-based rollback recovery protocol that supports general programming models including non-deterministic operators, external system interactions, and custom code. It is non-blocking, allows independent operator recovery, leverages data parallelization, and supports dynamic operator scaling during pipeline execution.

Result: Performance evaluation in SAP Data Intelligence shows: 1) With straggler operators and moderate throughput (1 event/100ms), LOG.io matches ABS during normal processing and outperforms it during recovery; 2) Otherwise, ABS performs better; 3) Data parallelization reduces LOG.io overhead while ABS doesn't improve; 4) Data lineage capture overhead is marginal (<1.5%).

Conclusion: LOG.io provides an effective solution for rollback recovery and fine-grained data lineage in distributed pipelines, particularly beneficial in scenarios with straggler operators and moderate event throughput, while maintaining low overhead for data lineage capture.

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [2] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA enables multipath data transfer between GPU and host memory to overcome PCIe bandwidth bottlenecks in LLM serving, achieving 4.62x bandwidth improvement and reducing TTFT by up to 2.38x.


<details>
  <summary>Details</summary>
Motivation: PCIe bandwidth bottleneck limits LLM performance for prefix cache fetching and model switching, causing underutilized intra-server bandwidth despite theoretical multipath possibilities.

Method: Proposes Multipath Memory Access (MMA) scheme enabling efficient multipath data transfer between GPU and host memory via dynamic library injection for seamless deployment without code modification.

Result: Achieves 245 GB/s peak bandwidth (4.62x speedup over native single-path), reduces TTFT by 1.14x-2.38x, and decreases model-switching latency in vLLM by 1.12x-2.48x.

Conclusion: MMA effectively addresses PCIe bandwidth bottleneck in LLM serving through multipath data transfer, significantly improving performance metrics without requiring application modifications.

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [3] [Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin](https://arxiv.org/abs/2512.16058)
*Yifei Qiu,Tianle Liao,Xin Jin,Shaohua Wu,Dusit Niyato,Qinyu Zhang*

Main category: cs.DC

TL;DR: This survey paper proposes GOST (Goal-Oriented Semantic Twin) as a lightweight alternative to conventional Digital Twins for SAGSIN networks, prioritizing utility over fidelity to overcome computational and synchronization limitations.


<details>
  <summary>Details</summary>
Motivation: Conventional Digital Twins face fundamental limitations in SAGSIN (space-air-ground-sea integrated networks) including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps, despite being initially promising for real-time situational awareness and intelligent operational maintenance.

Method: Proposes GOST framework with three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Uses semantic technologies and goal-oriented principles to construct lightweight, task-specific representations rather than full physical mirroring.

Result: GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking in remote satellite-UAV networks case study. Provides comprehensive tutorial on GOST construction and multidimensional evaluation framework.

Conclusion: GOST establishes a transformative twinning paradigm for SAGSIN development, prioritizing utility over fidelity to overcome limitations of conventional Digital Twins in complex integrated networks.

Abstract: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.

</details>


### [4] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: Researchers analyze cold-start latency in serverless computing through developer issue reports, create taxonomies of problems and solutions, develop SCABENCH benchmark and INITSCOPE analysis framework, achieving significant improvements in localization accuracy and diagnostic efficiency.


<details>
  <summary>Details</summary>
Motivation: Cold-start latency remains a major performance bottleneck in serverless computing, but prior work treats mitigation as black-box optimization rather than addressing it as a developer-visible design problem that needs systematic analysis and evidence-driven solutions.

Method: Analyzed 81 adjudicated issue reports across open-source serverless systems to derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges. Built SCABENCH as a reproducible benchmark and INITSCOPE as a lightweight analysis framework that links what code is loaded with what is executed.

Result: INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools on SCABENCH. A developer study showed higher task accuracy and faster diagnosis, demonstrating practical effectiveness.

Conclusion: The research advances evidence-driven, performance-aware practices for cold-start mitigation in serverless design by providing systematic analysis, reproducible benchmarks, and effective diagnostic tools that address cold starts as developer-visible design problems rather than black-box optimizations.

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [5] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: An online scheduling framework for NVIDIA MIG GPUs that addresses resource contention and fragmentation through conditional load balancing, dynamic partitioning, and job migration, improving makespan by up to 35%.


<details>
  <summary>Details</summary>
Motivation: Modern GPU workloads need efficient resource sharing as many jobs don't require full GPU capacity. NVIDIA's MIG offers hardware-level partitioning but introduces challenges: persistent resource contention from shared components like PCIe bandwidth, and GPU fragmentation due to MIG's limited valid configurations and rigid profile placement constraints.

Method: Proposes an online scheduling framework integrating three techniques: 1) Conditional load balancing to minimize resource contention, 2) Dynamic partitioning to adapt GPU allocations, and 3) Job migration to reorganize allocations and combat both internal and external fragmentation.

Result: Experimental results show significant system efficiency improvements. When all techniques are applied, the makespan improves by up to 35% compared to baseline approaches.

Conclusion: The proposed online scheduling framework effectively addresses the unique challenges of NVIDIA MIG GPU sharing, including resource contention and fragmentation, through integrated load balancing, dynamic partitioning, and migration techniques, achieving substantial performance gains.

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [6] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: SBS scheduling reduces TTFT by 30-40% and improves throughput by 15-20% in DP+EP LLM serving by buffering requests to form optimal batches and balancing load across DP units.


<details>
  <summary>Details</summary>
Motivation: DP+EP LLM serving architectures have high internal synchronization costs that cause immediate request dispatching to create queuing bubbles and degrade Time-to-First-Token performance.

Method: Proposed Staggered Batch Scheduling (SBS) that buffers requests to form optimal execution batches, plus Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases.

Result: Deployed on H800 cluster serving Deepseek-V3, reduces TTFT by 30-40% and improves throughput by 15-20% compared to state-of-the-art immediate scheduling baselines.

Conclusion: SBS effectively addresses scheduling challenges in DP+EP LLM serving by eliminating internal queuing bubbles through temporal decoupling while maintaining throughput.

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [7] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus is a scalable distributed transaction system for disaggregated memory that disaggregates locks from data to eliminate RDMA NIC bottlenecks, improving throughput by up to 2.1× and reducing latency by up to 49.4%.


<details>
  <summary>Details</summary>
Motivation: Existing distributed transaction systems on disaggregated memory suffer from performance bottlenecks at memory node RDMA NICs due to high volumes of one-sided atomic operations for locks, hindering scalability.

Method: Lotus disaggregates locks from data, executing all locks on compute nodes. It uses application-aware lock management leveraging workload locality, a lock-first transaction protocol for early conflict detection, and lock-rebuild-free recovery for CN failures.

Result: Experimental results show Lotus improves transaction throughput by up to 2.1× and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on disaggregated memory.

Conclusion: Lock disaggregation effectively addresses RDMA NIC bottlenecks in disaggregated memory systems, enabling scalable distributed transaction processing with significant performance improvements.

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [8] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: FlexKV is a memory-disaggregated key-value store that improves performance by dynamically offloading indexes to compute nodes, addressing issues of one-sided atomic operations and inefficient compute-side caches in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing memory-disaggregated KV stores suffer from poor performance due to overdependence on one-sided atomic operations in index processing and constrained efficiency in compute-side caches, limiting the potential benefits of disaggregated memory architectures.

Method: FlexKV introduces index proxying to dynamically offload indexes to compute nodes, with three key components: 1) rank-aware hotness detection for load balancing, 2) two-level compute node memory optimization, and 3) RPC-aggregated cache management to reduce coherence overhead.

Result: FlexKV improves throughput by up to 2.94× and reduces latency by up to 85.2% compared to state-of-the-art memory-disaggregated KV stores.

Conclusion: FlexKV demonstrates that dynamic index proxying effectively addresses performance bottlenecks in memory-disaggregated KV stores, enabling better utilization of compute node resources while maintaining efficient cache coherence management.

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [9] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: A federated compute platform for AI in scientific workloads that provides reproducible deployments, distributed infrastructure access, and full ML lifecycle support from development to deployment.


<details>
  <summary>Details</summary>
Motivation: To support Artificial Intelligence in scientific workloads by providing consistent, transparent access to distributed e-Infrastructures with reproducible deployments and comprehensive ML lifecycle support.

Method: Developed a federated compute platform with reproducible deployments, comprehensive service catalogue, integrated user experience covering full ML lifecycle (development with interactive environments, training with GPU resources and federated learning, deployment across Cloud Continuum), plus tools for traceability, reproducibility, and ecosystem integration.

Result: The platform delivers consistent access to distributed e-Infrastructures, offers integrated ML lifecycle support, provides traceability/reproducibility tools, integrates with AI model providers/datasets/storage, and is customizable for external communities.

Conclusion: The platform successfully supports AI in scientific workloads through federated infrastructure access, comprehensive ML lifecycle management, and customizable features that lower adoption barriers for scientific communities.

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [10] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: A CPU-GPU collaborative inference framework for MoE models that uses expert caching on GPU and CPU offloading for cache misses to improve performance on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: LLMs have high computational demands that challenge deployment on consumer hardware. MoE models reduce computation but still require substantial memory beyond typical GPU capacities. Traditional offloading methods cause latency issues during inference.

Method: Novel CPU-GPU collaborative inference framework with expert caching mechanism on GPU to reduce data transfer. Computations are offloaded to CPU for cache miss handling, leveraging CPU multithreading optimizations.

Result: The framework demonstrates performance improvements and shows potential for CPU-GPU collaboration to maximize hardware utilization for single-request inference on consumer-grade systems.

Conclusion: CPU-GPU collaborative inference with expert caching enables efficient deployment of MoE models on consumer hardware by reducing data transfer requirements and leveraging both GPU and CPU resources effectively.

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


### [11] [Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint](https://arxiv.org/abs/2512.16792)
*Endar Suprih Wihidayat,Sieteng Soh,Kwan-Wu Chin,Duc-son Pham*

Main category: cs.DC

TL;DR: Proposes Multi-stage Edge Server Upgrade (M-ESU) problem for long-term MEC planning, with MILP optimal solution for small networks and efficient heuristic (M-ESU/H) for large networks achieving near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Existing MEC systems need long-term planning to handle growing computational demands, stricter delay requirements, and budget constraints over multiple years, requiring systematic upgrade strategies rather than one-time deployment.

Method: Two solutions: (1) Mixed Integer Linear Programming (MILP) model for optimal solution in small networks, (2) M-ESU/H heuristic algorithm for large-scale networks that considers both server deployment and capacity upgrades across multiple stages with budget constraints.

Result: For small networks, M-ESU/H achieves solutions within 1.25% of optimal while running orders of magnitude faster. For large networks, M-ESU/H yields up to 21.57% improvement in task satisfaction compared to alternative heuristics under identical budget and demand growth conditions.

Conclusion: M-ESU framework provides scalable and practical solution for long-term MEC system planning, balancing server deployment, capacity upgrades, and task offloading to maximize delay-constrained task satisfaction across multiple budget-constrained stages.

Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [12] [Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution](https://arxiv.org/abs/2512.16238)
*Yifeng Cai,Zhida An,Yuhan Meng,Houqian Liu,Pengli Wang,Yao Guo,Ding Li*

Main category: cs.OS

TL;DR: PKUS is a system that enables secure, efficient use of professional knowledge in LLMs by separating knowledge as compact adapters executed in trusted hardware environments, achieving near-full fine-tuning performance with 8-12x speedup.


<details>
  <summary>Details</summary>
Motivation: Professional knowledge providers face unfair tradeoffs - they receive little value share but retain copyright/privacy liability, making them reluctant to contribute to LLM services. Existing methods lack trustworthy, controllable ways to use professional knowledge as they combine knowledge with LLM backbones and keep providers in the dark.

Method: PKUS treats professional knowledge as separable artifacts: keeps backbone model on GPUs, encodes each provider's contribution as compact adapters executed only inside attested Trusted Execution Environments (TEEs). Uses hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling.

Result: On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching accuracy and F1 of full fine-tuning and plain LoRA. Achieves lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.

Conclusion: PKUS provides a practical, secure solution for professional knowledge utilization in LLMs, enabling fair participation of knowledge providers while maintaining performance and efficiency through hardware-based trust mechanisms.

Abstract: Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.
  In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: This paper introduces a workload characterization methodology for branch prediction, proposing two new accuracy identifiers (branch working set size and branch predictability) that correlate with misprediction rates of modern predictors like TAGE and perceptron.


<details>
  <summary>Details</summary>
Motivation: Accurate branch prediction is crucial for instruction-level parallelism extraction, performance improvement, and energy consumption reduction. The paper aims to develop better workload characterization methods to understand branch prediction accuracy across different workloads.

Method: The authors propose a workload characterization methodology using two new identifiers: branch working set size (group of most frequently occurring branch contexts) and branch predictability. They analyze 2,451 workload traces, defining branch working set as a 3-part tuple of branch address, global history, and local history.

Result: The study characterized 2,451 workload traces into seven branch working set size categories and nine predictability categories. The proposed parameters show high correlation with misprediction rates of modern branch prediction schemes like TAGE and perceptron.

Conclusion: The paper presents new workload-driven branch prediction accuracy identifiers that provide insights into prediction accuracy sources and identify favored workload categories for modern branch predictors, enabling better understanding and optimization of branch prediction performance.

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [14] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: This paper presents a complete system architecture view of the Aria2 wearable contextual AI system, emphasizing the importance of end-to-end system modeling for power optimization in always-on, spatially-aware wearable devices.


<details>
  <summary>Details</summary>
Motivation: Next-generation human-oriented computing requires always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives for contextual AI personal assistants, but designing such systems is challenging due to complexity and stringent power constraints.

Method: The authors provide the first complete system architecture view of the Aria2 wearable contextual AI system, detailing system modeling and design space exploration processes to understand how to guide design for such complex systems.

Result: The study shows that an end-to-end full system model view is vitally important because no single component overwhelmingly dominates system power, requiring long-range design decisions and power optimizations to be made in full system context to avoid bottlenecks.

Conclusion: The paper reflects on lessons and insights for enabling all-day, wearable, contextual AI systems, emphasizing that comprehensive system modeling is essential to address power constraints and avoid bottlenecks in complex wearable AI architectures.

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human- relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [15] [Human-like Working Memory from Artificial Intrinsic Plasticity Neurons](https://arxiv.org/abs/2512.15829)
*Jingli Liu,Huannan Zheng,Bohao Zou,Kezhou Yang*

Main category: cs.ET

TL;DR: IPNet is a neuromorphic architecture that implements human-like working memory using magnetic tunnel junction neurons with intrinsic plasticity, achieving superior performance on dynamic vision tasks with dramatically reduced energy consumption compared to conventional neural networks.


<details>
  <summary>Details</summary>
Motivation: To create energy-efficient artificial working memory that mimics biological systems, overcoming the high energy costs and noise sensitivity of traditional recurrent or parallel architectures in artificial networks.

Method: Hardware-software co-designed neuromorphic architecture using Magnetic Tunnel Junction (MTJ) neurons that exploit Joule-heating dynamics to physically emulate biological memory volatility through neuronal intrinsic plasticity.

Result: Achieved 99.65% accuracy on 11-class DVS gesture datasets, 99.48% on 22-class time-reversed benchmark, outperforming RNN/LSTM/CNN baselines; reduced steering prediction error by 14.4% for autonomous driving; demonstrated 2,874x memory power reduction vs LSTMs and 90,920x vs 3D-CNNs; compact ~1.5μm² footprint.

Conclusion: Implementing human-like working memory via intrinsic neuronal plasticity in neuromorphic hardware provides superior dynamic vision processing with minimal metabolic cost, validating a bio-plausible near-sensor processing paradigm.

Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.

</details>


### [16] [Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil](https://arxiv.org/abs/2512.16071)
*Yixuan Gao,Tanvir Ahmed,Mikhail Mohammed,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.ET

TL;DR: SoilScanner is a wireless RF-based system that detects lead contamination in soil by analyzing how different salts affect radio signal propagation at various frequencies, achieving 72% accuracy in classifying soil as low or high Pb.


<details>
  <summary>Details</summary>
Motivation: Widespread lead contamination in urban soils threatens food safety, public health, and city greening efforts, but current detection methods are labor-intensive and expensive.

Method: The system uses radio frequency signals to detect Pb based on the discovery that different salts (NaCl and Pb(NO3)2) affect signal propagation differently. Controlled experiments with manually added salts showed distinct reflection patterns at different frequencies. Field validation used machine learning on uncontrolled samples.

Result: SoilScanner achieved 72% accuracy in classifying soil samples into low-Pb (<200 ppm) and high-Pb (≥200 ppm) categories. No samples with >500 ppm Pb were misclassified.

Conclusion: It's feasible to develop portable, affordable lead detection devices using wireless technology for soil screening, offering a practical alternative to traditional labor-intensive methods.

Abstract: Widespread Pb (lead) contamination of urban soil significantly impacts food safety and public health and hinders city greening efforts. However, most existing technologies for measuring Pb are labor-intensive and costly. In this study, we propose SoilScanner, a radio frequency-based wireless system that can detect Pb in soils. This is based on our discovery that the propagation of different frequency band radio signals is affected differently by different salts such as NaCl and Pb(NO3)2 in the soil. In a controlled experiment, manually adding NaCl and Pb(NO3)2 in clean soil, we demonstrated that different salts reflected signals at different frequencies in distinct patterns. In addition, we confirmed the finding using uncontrolled field samples with a machine learning model. Our experiment results show that SoilScanner can classify soil samples into low-Pb and high-Pb categories (threshold at 200 ppm) with an accuracy of 72%, with no sample with > 500 ppm of Pb being misclassified. The results of this study show that it is feasible to build portable and affordable Pb detection and screening devices based on wireless technology.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [17] [XTC, A Research Platform for Optimizing AI Workload Operators](https://arxiv.org/abs/2512.16512)
*Pompougnac Hugo,Guillon Christophe,Noiry Sylvain,Dutilleul Alban,Iooss Guillaume,Rastello Fabrice*

Main category: cs.PF

TL;DR: XTC is a platform that provides a unified interface for scheduling specification and performance evaluation across different compiler ecosystems, enabling portable experimentation and fair comparison of optimization strategies.


<details>
  <summary>Details</summary>
Motivation: Existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across different frameworks. There's no unified interface that decouples scheduling specification from code generation and measurement.

Method: Introduces XTC platform with a common API and reproducible measurement framework that unifies scheduling and performance evaluation across compilers.

Result: XTC enables portable experimentation and accelerates research on optimization strategies by providing a platform for fair comparison and reuse of scheduling specifications across different compiler frameworks.

Conclusion: XTC addresses the fragmentation in compiler ecosystems by providing a unified scheduling interface that decouples specification from implementation, enabling better research and development of AI operator optimizations.

Abstract: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.

</details>


### [18] [An Upper Bound on the M/M/k Queue With Deterministic Setup Times](https://arxiv.org/abs/2512.16854)
*Jalani Williams,Weina Wang,Mor Harchol-Balter*

Main category: cs.PF

TL;DR: First closed-form characterization of waiting time in finite-server systems with deterministic setup times, providing upper/lower bounds and accurate approximation.


<details>
  <summary>Details</summary>
Motivation: Modern systems regularly turn servers on/off to reduce costs, but setup times cause queueing problems. Existing models assume exponential setup times, which is unrealistic and underestimates setup time harm. Need to understand deterministic setup times in multiserver systems.

Method: Analyzes M/M/k/Setup-Deterministic system using new Method of Intervening Stopping Times (MIST) technique for analyzing random time integrals. Derives upper and lower bounds on average waiting time.

Result: First closed-form characterization of waiting time in any finite-server system with setup times. Bounds are within multiplicative constant of each other. Combined bounds yield simple, accurate approximation for average waiting time.

Conclusion: Provides comprehensive understanding of deterministic setup times in multiserver systems, addressing limitations of exponential-setup assumption. MIST technique enables analysis of previously intractable problems.

Abstract: In many systems, servers do not turn on instantly; instead, a setup time must pass before a server can begin work. These "setup times" can wreak havoc on a system's queueing; this is especially true in modern systems, where servers are regularly turned on and off as a way to reduce operating costs (energy, labor, CO2, etc.). To design modern systems which are both efficient and performant, we need to understand how setup times affect queues.
  Unfortunately, despite successes in understanding setup in a single-server system, setup in a multiserver system remains poorly understood. To circumvent the main difficulty in analyzing multiserver setup, all existing results assume that setup times are memoryless, i.e. distributed Exponentially. However, in most practical settings, setup times are close to Deterministic, and the widely used Exponential-setup assumption leads to unrealistic model behavior and a dramatic underestimation of the true harm caused by setup times.
  This paper provides a comprehensive characterization of the average waiting time in a multiserver system with Deterministic setup times, the M/M/k/Setup-Deterministic. In particular, we derive upper and lower bounds on the average waiting time in this system, and show these bounds are within a multiplicative constant of each other. These bounds are the first closed-form characterization of waiting time in any finite-server system with setup times. Further, we demonstrate how to combine our upper and lower bounds to derive a simple and accurate approximation for the average waiting time. These results are all made possible via a new technique for analyzing random time integrals that we named the Method of Intervening Stopping Times, or MIST.

</details>
