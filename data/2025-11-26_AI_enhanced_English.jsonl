{"id": "2511.19991", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2511.19991", "abs": "https://arxiv.org/abs/2511.19991", "authors": ["Meng-Chia Lee", "Wen Sheng Lim", "Yuan-Hao Chang", "Tei-Wei Kuo"], "title": "SARA: A Stall-Aware Memory Allocation Strategy for Mixed-Criticality Systems", "comment": null, "summary": "The memory capacity in edge devices is often limited due to constraints on cost, size, and power. Consequently, memory competition leads to inevitable page swapping in memory-constrained mixed-criticality edge devices, causing slow storage I/O and thus performance degradation. In such scenarios, inefficient memory allocation disrupts the balance between application performance, causing soft real-time (soft RT) tasks to miss deadlines or preventing non-real-time (non-RT) applications from optimizing throughput. Meanwhile, we observe unpredictable, long system-level stalls (called long stalls) under high memory and I/O pressure, which further degrade performance. In this work, we propose a Stall-Aware Real-Time Memory Allocator (SARA), which discovers opportunities for performance balance by allocating just enough memory to soft RT tasks to meet deadlines and, at the same time, optimizing the remaining memory for non-RT applications. To minimize the memory usage of soft RT tasks while meeting real-time requirements, SARA leverages our insight into how latency, caused by memory insufficiency and measured by our proposed PSI-based metric, affects the execution time of each soft RT job, where a job runs per period and a soft RT task consists of multiple periods. Moreover, SARA detects long stalls using our definition and proactively drops affected jobs, minimizing stalls in task execution. Experiments show that SARA achieves an average of 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with memory capacity limited to 60% of peak demand.", "AI": {"tldr": "SARA is a stall-aware real-time memory allocator that balances performance between soft real-time tasks and non-real-time applications in memory-constrained edge devices by allocating just enough memory to meet deadlines while optimizing remaining memory for throughput.", "motivation": "Memory constraints in edge devices cause page swapping and performance degradation, leading to soft RT tasks missing deadlines and non-RT applications having suboptimal throughput. Unpredictable long system-level stalls further worsen performance under high memory and I/O pressure.", "method": "SARA allocates minimal memory to soft RT tasks to meet deadlines while optimizing remaining memory for non-RT applications. It uses PSI-based metrics to measure latency from memory insufficiency and proactively detects/drops jobs affected by long stalls to minimize execution disruptions.", "result": "SARA achieves 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with only 60% of peak memory demand.", "conclusion": "SARA effectively balances performance between soft RT and non-RT applications in memory-constrained edge devices by intelligently managing memory allocation and proactively handling long stalls, demonstrating significant improvements in both real-time reliability and throughput."}}
{"id": "2511.19791", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.19791", "abs": "https://arxiv.org/abs/2511.19791", "authors": ["Sen Zhang", "Lingjun Xiong", "Yipie Liu", "Brian L. Mark", "Lei Yang", "Zebo Yang", "Weiwen Jiang"], "title": "An End-to-End Distributed Quantum Circuit Simulator", "comment": null, "summary": "Quantum computing has made substantial progress in recent years; however, its scalability remains constrained on a monolithic quantum processing unit (QPU). Distributed quantum computing (DQC) offers a pathway by coordinating multiple QPUs to execute large-scale circuits. Yet, DQC still faces practical barriers, as its realization depends on advances in hardware-level components such as quantum transducers and high-fidelity entanglement-distribution modules. While these technologies continue to improve, mature DQC platforms remain unavailable. In the meantime, researchers need to assess the benefits of DQC and evaluate emerging DQC designs, but the software ecosystem lacks a circuit-level simulator that models heterogeneous backends, noisy connections, and distributed execution. To fill this gap, this paper proposes SimDisQ, the first end-to-end circuit-level DQC simulator, composed of a set of novel DQC-oriented automated simulation toolkits and communication noise models that can interoperate with existing toolkits in mainstream quantum software ecosystems. Leveraging circuit-level simulation capabilities, SimDisQ enables quantitative exploration of architectural design trade-offs, communication fidelity constraints, and new circuit optimization challenges introduced by DQC, providing a foundation for future research in this promising direction. Benchmarking experiments using SimDisQ respond to a couple of open questions in the community; for example, noisy simulation of superconducting and trapped-ion qubits, with a reasonable entanglement- distribution fidelity, reveal that heterogeneous QPUs can indeed yield higher execution fidelity.", "AI": {"tldr": "SimDisQ is the first end-to-end circuit-level distributed quantum computing simulator that models heterogeneous backends, noisy connections, and distributed execution to enable quantitative exploration of DQC design trade-offs.", "motivation": "Distributed quantum computing faces practical barriers due to hardware limitations, and researchers need tools to assess DQC benefits and evaluate emerging designs, but current software lacks circuit-level simulation capabilities for heterogeneous backends and noisy connections.", "method": "Proposes SimDisQ - an end-to-end circuit-level DQC simulator with novel DQC-oriented automated simulation toolkits and communication noise models that interoperate with existing quantum software ecosystems.", "result": "Benchmarking experiments using SimDisQ reveal that heterogeneous QPUs can yield higher execution fidelity, responding to open questions in the community about distributed quantum computing performance.", "conclusion": "SimDisQ provides a foundation for future DQC research by enabling quantitative exploration of architectural design trade-offs, communication fidelity constraints, and new circuit optimization challenges introduced by distributed quantum computing."}}
{"id": "2511.19740", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19740", "abs": "https://arxiv.org/abs/2511.19740", "authors": ["Tergel Molom-Ochir", "Benjamin F. Morris", "Mark Horton", "Chiyue Wei", "Cong Guo", "Brady Taylor", "Peter Liu", "Shan X. Wang", "Deliang Fan", "Hai Helen Li", "Yiran Chen"], "title": "CAMformer: Associative Memory is All You Need", "comment": "7 pages, 10 figures", "summary": "Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.", "AI": {"tldr": "CAMformer is a novel accelerator that reinterprets attention as associative memory using Binary Attention Content Addressable Memory (BA-CAM) to achieve constant-time similarity search through analog charge sharing, overcoming the quadratic cost of traditional attention.", "motivation": "Transformers face scalability challenges due to the quadratic computational cost of attention mechanisms, which require dense similarity computations between queries and keys.", "method": "Proposes CAMformer accelerator that uses voltage-domain BA-CAM to compute attention scores via analog charge sharing, replacing digital arithmetic with physical similarity sensing. Implements hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization.", "result": "Achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators while maintaining near-lossless accuracy on BERT and Vision Transformer workloads.", "conclusion": "CAMformer successfully addresses transformer scalability challenges by reinterpreting attention as associative memory operations, demonstrating significant improvements in efficiency and performance while preserving model accuracy."}}
{"id": "2511.19445", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.19445", "abs": "https://arxiv.org/abs/2511.19445", "authors": ["Luca Accorsi", "Demetrio Lagan\u00e0", "Federico Michelotto", "Roberto Musmanno", "Daniele Vigo"], "title": "Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution", "comment": null, "summary": "We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.", "AI": {"tldr": "FILO2^x is a parallel shared-memory adaptation of the FILO2 algorithm for Capacitated Vehicle Routing Problems that enables concurrent asynchronous optimization of multiple solution areas with minimal synchronization, significantly reducing resolution time while maintaining solution quality.", "motivation": "To improve the computational efficiency of solving large-scale Capacitated Vehicle Routing Problem instances by better exploiting available computing resources through parallelization, while avoiding the need for explicit decomposition and minimizing synchronization overhead.", "method": "Developed FILO2^x as a single-trajectory parallel adaptation of FILO2 algorithm using iteration-based parallelism, where multiple solvers concurrently and asynchronously optimize different areas of the same solution, leveraging the locality of FILO2 optimization applications.", "result": "FILO2^x greatly enhances resolution time compared to the original single-threaded FILO2 approach while maintaining similar final solution quality, demonstrated on instances ranging from hundreds to hundreds of thousands of customers.", "conclusion": "The proposed parallel shared-memory schema successfully enables efficient cooperative optimization of large-scale vehicle routing problems with minimal synchronization, achieving significant speedup without compromising solution quality."}}
{"id": "2511.19973", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.19973", "abs": "https://arxiv.org/abs/2511.19973", "authors": ["Hoa Nguyen", "Pongstorn Maidee", "Jason Lowe-Power", "Alireza Kaviani"], "title": "Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher", "comment": "13 pages, 13 figures", "summary": "Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.", "AI": {"tldr": "The Pickle Prefetcher is a programmable LLC prefetcher that uses software-defined strategies instead of hardware prediction to handle irregular memory access patterns, achieving significant performance improvements over traditional prefetchers.", "motivation": "Traditional prediction-based prefetchers struggle with irregular memory access patterns common in modern applications, while large LLCs can increase latency for such workloads. There's a need for prefetchers that can effectively handle these irregular patterns without complex hardware prediction logic.", "method": "Pickle Prefetcher uses a programmable approach where software defines prefetching strategies through a simple interface without ISA expansion. It trades hardware prediction complexity for software programmability, allowing adaptation to various access patterns while dedicating hardware resources to scheduling and issuing prefetch requests.", "result": "Evaluation on gem5 full-system simulations shows Pickle Prefetcher significantly outperforms traditional techniques, achieving up to 1.74x speedup on GAPBS BFS implementation over baseline, and up to 1.40x speedup when combined with private cache prefetchers over systems using only private cache prefetchers.", "conclusion": "The programmable approach of Pickle Prefetcher effectively handles irregular memory access patterns, demonstrating that software-defined prefetching strategies can provide substantial performance benefits over traditional hardware-based prediction methods for modern applications."}}
{"id": "2511.19450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19450", "abs": "https://arxiv.org/abs/2511.19450", "authors": ["M. Zeeshan Haider", "Tayyaba Noreen", "M. D. Assuncao", "Kaiwen Zhang"], "title": "AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains", "comment": null, "summary": "Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\\% lower latency, and 20\\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.", "AI": {"tldr": "PSAP is a predictive shard allocation protocol that uses workload forecasting and reinforcement learning to dynamically assign accounts and transactions to shards, improving blockchain scalability by reducing workload skew and cross-shard communication.", "motivation": "Static or heuristic shard allocation in blockchain sharding leads to workload skew, congestion, and excessive cross-shard communication, diminishing the scalability benefits of sharding.", "method": "PSAP integrates Temporal Workload Forecasting (TWF) with safety-constrained reinforcement learning (Safe-PPO) for multi-block-ahead prediction and adaptive shard reconfiguration, using synchronized quantized runtime and safety gates for deterministic inference.", "result": "Experimental evaluation shows up to 2x throughput improvement, 35% lower latency, and 20% reduced cross-shard overhead compared to existing dynamic sharding baselines on heterogeneous datasets.", "conclusion": "Predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems."}}
{"id": "2511.20090", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20090", "abs": "https://arxiv.org/abs/2511.20090", "authors": ["Zizhang Luo", "Fan Cui", "Kexing Zhou", "Runlin Guo", "Mile Xia", "Hongyuan Hou", "Yun Lian"], "title": "R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation", "comment": null, "summary": "Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.", "AI": {"tldr": "R3A is an LLM-based automatic RTL program repair framework that uses stochastic Tree-Of-Thoughts and multi-agent fault localization to improve reliability in fixing hardware design bugs, achieving 90.6% bug fix rate.", "motivation": "Traditional automatic program repair methods for RTL rely on fixed templates and handle limited bugs, while LLMs suffer from unreliable outcomes due to randomness and long input contexts of RTL code and waveforms.", "method": "Proposes R3A framework with stochastic Tree-Of-Thoughts method to control patch generation agent, sampling search states using heuristic function to balance exploration-exploitation, and multi-agent fault localization to find fault candidates.", "result": "R3A fixes 90.6% of bugs in RTL-repair dataset within time limit, covering 45% more bugs than traditional methods and other LLM approaches, with 86.7% pass@5 rate on average.", "conclusion": "R3A demonstrates high reliability in RTL bug repair, significantly outperforming traditional methods and other LLM-based approaches through its innovative stochastic Tree-Of-Thoughts and multi-agent fault localization techniques."}}
{"id": "2511.19453", "categories": ["cs.DC", "cs.DB", "cs.OS", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19453", "abs": "https://arxiv.org/abs/2511.19453", "authors": ["Yuxin Wang", "Yuankai He", "Weisong Shi"], "title": "AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.", "AI": {"tldr": "AVS is a hierarchical storage system for autonomous vehicles that co-designs computation with modality-aware reduction, hot-cold tiering, and lightweight metadata indexing to handle massive heterogeneous data (14TB/day) while enabling efficient querying for third-party applications.", "motivation": "Current vehicle data loggers and storage stacks fail to efficiently handle the massive heterogeneous data generated by autonomous vehicles (14TB/day) and support querying for emerging third-party applications, necessitating a general-purpose onboard storage system.", "method": "AVS uses a hierarchical design with: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The system was benchmarked on AV data covering SSD/HDD filesystems and embedded indexing, and validated on embedded hardware with real L4 autonomous driving traces.", "result": "The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets, demonstrating efficient data storage and querying capabilities for autonomous vehicle applications.", "conclusion": "AVS successfully addresses the storage challenges in autonomous vehicles and motivates storage as a first-class component in AV stacks, with observations and next steps outlined for more scalable and longer deployments."}}
{"id": "2511.19456", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.19456", "abs": "https://arxiv.org/abs/2511.19456", "authors": ["Anton Reinhard", "Simeon Ehrig", "Ren\u00e9 Widera", "Michael Bussmann", "Uwe Hernandez Acosta"], "title": "Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED", "comment": null, "summary": "Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.", "AI": {"tldr": "A Julia framework for automatically generating statically scheduled and compiled code from computational DAGs, with domain-specific optimizations applied to quantum electrodynamics matrix element computations.", "motivation": "Complex scientific computations often have subtasks with distinct hardware requirements, requiring optimal scheduling across different devices while considering parallelism, dependencies, and data transfer speeds.", "method": "Developed a Julia framework that uses directed acyclic graphs (DAGs) to represent computational problems, automatically produces statically scheduled and compiled code, and incorporates domain-specific information for enhanced optimizations.", "result": "Implemented the framework with an example application computing matrix elements for scattering processes with many external particles in quantum electrodynamics, demonstrating the theoretical concepts.", "conclusion": "The framework enables automatic and dynamic generation of optimized code for complex computational problems, achieving better hardware utilization through domain-specific DAG scheduling and compilation."}}
{"id": "2511.19457", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19457", "abs": "https://arxiv.org/abs/2511.19457", "authors": ["Ziyang Zhang", "Jie Liu", "Luca Mottola"], "title": "SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference", "comment": "14 pages, 12 figures", "summary": "The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\\%-16\\% less energy than the SOTA co-execution baseline.", "AI": {"tldr": "SparOA is a CPU-GPU hybrid inference framework that optimizes DNN performance on edge devices by leveraging sparsity and computational intensity for operator scheduling, achieving significant speedups and energy savings.", "motivation": "Deep neural networks face performance challenges on resource-constrained edge devices, where existing solutions like model compression sacrifice accuracy and specialized hardware is costly. Hybrid inference methods often ignore how operator characteristics impact performance.", "method": "SparOA uses three components: (1) threshold predictor for optimal sparsity and computational intensity thresholds, (2) reinforcement learning-based scheduler for dynamic resource allocation based on real-time hardware states, and (3) hybrid inference engine with asynchronous execution and batch size optimization.", "result": "SparOA achieves 1.22-1.31x average speedup compared to baselines, outperforms CPU-Only by up to 50.7x, and consumes 7%-16% less energy than state-of-the-art co-execution baselines.", "conclusion": "SparOA effectively addresses DNN performance challenges on edge devices through intelligent operator scheduling based on sparsity and computational intensity, delivering significant performance improvements and energy efficiency."}}
{"id": "2511.19460", "categories": ["cs.DC", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19460", "abs": "https://arxiv.org/abs/2511.19460", "authors": ["Sofiane Ben Amor", "Guillaume Guerard", "Loup-No\u00e9 Levy"], "title": "Systemic approach for modeling a generic smart grid", "comment": null, "summary": "Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.", "AI": {"tldr": "The paper presents a backbone model for smart grid simulation that integrates multiple systems and uses distributed optimization for production and consumption scheduling.", "motivation": "Smart grid advances create complex interdisciplinary modeling problems that traditional computational methods struggle to solve, requiring systemic approaches to integrate power systems, energy markets, demand-side management, and other grid resources.", "method": "Developed a backbone model for smart grid simulation that uses distributed optimization of subsystems to achieve production and consumption scheduling while maintaining flexibility and scalability.", "result": "The tool successfully simulates disparate systems to validate assumptions before human-scale modeling, enabling testing of alternative grid scenarios.", "conclusion": "The proposed backbone model provides an effective approach for smart grid simulation through distributed optimization, addressing the complex interdisciplinary challenges of modern power grid modeling."}}
{"id": "2511.19464", "categories": ["cs.DC", "cs.AI", "cs.CR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.19464", "abs": "https://arxiv.org/abs/2511.19464", "authors": ["Marcio Pohlmann", "Alex Severo", "Geft\u00e9 Almeida", "Diego Kreutz", "Tiago Heinrich", "Louren\u00e7o Pereira"], "title": "Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments", "comment": "5 pages, 3 figures, 2 tables, submitted to ERRC/WRSeg 2025", "summary": "SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.", "AI": {"tldr": "Local small language models (SLMs) can effectively automate incident categorization for SOCs/CSIRTs, with performance depending more on model size and GPU capacity than temperature settings.", "motivation": "Address the need for automated incident categorization while avoiding costs, latency, and confidentiality risks of cloud-based LLMs by exploring locally executed SLMs.", "method": "Evaluated 21 SLMs (1B-20B parameters) with varying temperature settings, measuring execution time and precision across different architectures.", "result": "Temperature has minimal impact on performance; model size (parameters) and GPU capacity are the key factors determining effectiveness.", "conclusion": "Locally executed SLMs are viable for incident categorization, with larger models and adequate GPU resources providing optimal performance."}}
{"id": "2511.19463", "categories": ["cs.DC", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.19463", "abs": "https://arxiv.org/abs/2511.19463", "authors": ["Aldo Canfora", "Eleonora Bergamaschi", "Riccardo Mioli", "Federico Battini", "Mirko Degli Esposti", "Giorgio Pedrazzi", "Chiara Dellacasa"], "title": "Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna", "comment": "Preprint submitted for publication", "summary": "Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.", "AI": {"tldr": "A UBEM pipeline integrating EnergyPlus, HPC, and open geospatial data to estimate building energy demand in Bologna, Italy, simulating 25,000 buildings in under 30 minutes.", "motivation": "Urban Building Energy Modeling is crucial for understanding and forecasting city-scale energy consumption, requiring efficient methods to handle large building datasets.", "method": "Integrated EnergyPlus simulations with HPC using open geospatial datasets; obtained geometric data from Bologna Open Data portal and LiDAR, non-geometric attributes from regional regulations and TABULA database.", "result": "Successfully simulated approximately 25,000 buildings in under 30 minutes using the Leonardo supercomputer at Cineca.", "conclusion": "The developed UBEM pipeline demonstrates efficient large-scale building energy simulation capabilities by leveraging HPC and open data sources."}}
{"id": "2511.19468", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19468", "abs": "https://arxiv.org/abs/2511.19468", "authors": ["Blaise Ag\u00fcera y Arcas", "Travis Beals", "Maria Biggs", "Jessica V. Bloom", "Thomas Fischbacher", "Konstantin Gromov", "Urs K\u00f6ster", "Rishiraj Pravahan", "James Manyika"], "title": "Towards a future space-based, highly scalable AI infrastructure system design", "comment": "19 pages, 4 figures", "summary": "If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\\lesssim$\\$200/kg by the mid-2030s.", "AI": {"tldr": "This paper proposes a scalable AI compute system in space using satellite fleets with solar power, optical inter-satellite links, and Google TPU accelerators, designed for efficient machine learning operations in orbit.", "motivation": "As AI becomes a foundational technology with growing compute and energy demands, the Sun represents the largest energy source in our solar system, making space-based AI infrastructure an efficient way to tap into solar power.", "method": "The system uses fleets of satellites equipped with solar arrays, free-space optical inter-satellite links, and Google TPU accelerator chips. Satellites fly in close proximity for high-bandwidth, low-latency communication, demonstrated through an 81-satellite cluster with 1 km radius formation. ML-based models control large-scale constellations, and Trillium TPUs are radiation-tested for space durability.", "result": "Radiation testing shows Trillium TPUs survive total ionizing dose equivalent to 5-year missions without permanent failures, with characterization for bit-flip errors. Launch cost analysis suggests LEO launch costs may reach \u2264$200/kg by mid-2030s.", "conclusion": "Space-based AI compute infrastructure using satellite fleets with solar power and optical communication is technically feasible and could become economically viable as launch costs decrease, providing a scalable solution for future AI compute demands."}}
{"id": "2511.19479", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19479", "abs": "https://arxiv.org/abs/2511.19479", "authors": ["Sangam Ghimire", "Paribartan Timalsina", "Nirjal Bhurtel", "Bishal Neupane", "Bigyan Byanju Shrestha", "Subarna Bhattarai", "Prajwal Gaire", "Jessica Thapa", "Sudan Jha"], "title": "Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments", "comment": null, "summary": "As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.", "AI": {"tldr": "A federated learning framework designed for mixed HPC and cloud environments that addresses system heterogeneity, communication overhead, and resource scheduling while maintaining model accuracy and data privacy.", "motivation": "Growing demand for scalable and privacy-aware AI systems, with FL emerging as a solution for decentralized training without moving raw data, combined with the complexities of HPC and cloud infrastructure dealing with heterogeneous hardware and non-uniform data.", "method": "Developed a federated learning framework specifically built to run efficiently across mixed HPC and cloud environments, addressing key challenges including system heterogeneity, communication overhead, and resource scheduling.", "result": "Demonstrated strong performance in scalability, fault tolerance, and convergence through experiments on a hybrid testbed, even under non-IID data distributions and varied hardware conditions.", "conclusion": "Federated learning shows potential as a practical approach for building scalable AI systems in modern distributed computing settings, effectively handling the challenges of mixed HPC and cloud environments."}}
{"id": "2511.19832", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19832", "abs": "https://arxiv.org/abs/2511.19832", "authors": ["Aurelio Vivas", "Harold Castro"], "title": "Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures", "comment": null, "summary": "Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.", "AI": {"tldr": "nFlows is a NUMA-aware workflow execution runtime system designed for data-intensive workflows on HPC systems with non-uniform memory access architectures, enabling modeling, execution, simulation, and validation of scheduling algorithms.", "motivation": "Workflow scheduling on HPC infrastructures is challenging due to NUMA architectures that create variable data-access latency and complicate task/data placement, yet most existing scheduling strategies lack NUMA-aware considerations.", "method": "Developed nFlows, a runtime system that supports modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems.", "result": "The system enables studies of NUMA effects on scheduling, design of NUMA-aware algorithms, analysis of data-movement behavior, identification of performance bottlenecks, and exploration of in-memory workflow execution.", "conclusion": "nFlows addresses the gap in NUMA-aware workflow scheduling for HPC systems by providing a comprehensive framework for developing and evaluating scheduling strategies that account for modern NUMA architectures with heterogeneous memory and accelerators."}}
{"id": "2511.19847", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19847", "abs": "https://arxiv.org/abs/2511.19847", "authors": ["Jinghang Xu", "Kun Guo", "Wei Teng", "Chenxi Liu", "Wei Feng"], "title": "Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks", "comment": null, "summary": "Artificial intelligence-generated content (AIGC) service provisioning in wireless edge networks involves two phases: content generation on edge servers and content transmission to mobile devices. In this paper, we take image generation as a representative application and propose a batch denoising framework, followed by a joint optimization of content generation and transmission, with the objective of maximizing the average AIGC service quality under an end-to-end service delay constraint. Motivated by the empirical observations that (i) batch denoising effectively reduces per-step denoising delay by enhancing parallelism and (ii) early denoising steps have a greater impact on generation quality than later steps, we develop the STACKING algorithm to optimize batch denoising. The STACKING operates independently of any specific form of the content quality function and achieves lower computational complexity. Building on the batch solution, we further optimize bandwidth allocation across AIGC services. Simulation results demonstrate the superior performance of our algorithm in delivering high-quality, lower-latency AIGC services.", "AI": {"tldr": "The paper proposes a joint optimization framework for AIGC service provisioning in wireless edge networks, focusing on batch denoising and bandwidth allocation to maximize service quality under delay constraints.", "motivation": "To address the challenge of providing high-quality AIGC services with low latency in wireless edge networks, where content generation and transmission phases need coordinated optimization.", "method": "Developed STACKING algorithm for batch denoising optimization that reduces per-step delay through enhanced parallelism, and jointly optimizes content generation and transmission with bandwidth allocation.", "result": "Simulation results show superior performance in delivering high-quality, lower-latency AIGC services compared to existing approaches.", "conclusion": "The proposed framework effectively balances AIGC service quality and latency constraints through optimized batch denoising and resource allocation strategies."}}
{"id": "2511.19880", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19880", "abs": "https://arxiv.org/abs/2511.19880", "authors": ["Prabhat Kumar Chand", "Anisur Rahaman Molla"], "title": "Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents", "comment": null, "summary": "Mobile agents have emerged as a powerful framework for solving fundamental graph problems in distributed settings in recent times. These agents, modelled as autonomous physical or software entities, possess local computation power, finite memory and have the ability to traverse a graph, offering efficient solutions to a range of classical problems. In this work, we focus on the problem of computing a \\emph{minimal dominating set} (mDS) in anonymous graphs using mobile agents. Building on the recently proposed optimal dispersion algorithm on the synchronous mobile agent model, we design two new algorithms that achieve a \\emph{linear-time} solution for this problem in the synchronous setting. Specifically, given a connected $n$-node graph with $n$ agents initially placed in either rooted or arbitrary configurations, we show that an mDS can be computed in $O(n)$ rounds using only $O(\\log n)$ bits of memory per agent, without using any prior knowledge of any global parameters. This improves upon the best-known complexity results in the literature over the same model. In addition, as natural by-products of our methodology, our algorithms also construct a spanning tree and elect a unique leader in $O(n)$ rounds, which are also important results of independent interest in the mobile-agent framework.", "AI": {"tldr": "The paper presents linear-time algorithms for computing minimal dominating sets in anonymous graphs using mobile agents with O(log n) memory, improving upon previous complexity results while also constructing spanning trees and electing leaders as by-products.", "motivation": "Mobile agents offer efficient solutions for distributed graph problems, and there is a need for improved algorithms for computing minimal dominating sets in anonymous graphs using limited memory and without global knowledge.", "method": "Two new algorithms building on optimal dispersion techniques that work in synchronous settings with n agents initially placed in rooted or arbitrary configurations, using only O(log n) bits of memory per agent.", "result": "Achieves O(n) round complexity for computing minimal dominating sets, which improves upon best-known results, while also constructing spanning trees and electing unique leaders in the same time complexity.", "conclusion": "The proposed algorithms provide efficient linear-time solutions for minimal dominating set computation in mobile agent frameworks with limited memory, offering significant improvements over existing approaches."}}
{"id": "2511.19949", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.19949", "abs": "https://arxiv.org/abs/2511.19949", "authors": ["Qingda Hu", "Xinjun Yang", "Feifei Li", "Junru Li", "Ya Lin", "Yuqi Zhou", "Yicong Zhu", "Junwei Zhang", "Rongbiao Xie", "Ling Zhou", "Bin Wu", "Wenchao Zhou"], "title": "PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases", "comment": "13 pages, accepted by FAST'26", "summary": "In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.", "AI": {"tldr": "PolarStore is a compressed shared storage system for cloud-native RDBMSs that combines hardware and software compression to reduce storage costs by 60% while maintaining performance comparable to uncompressed clusters.", "motivation": "Cloud-native RDBMSs face storage cost challenges despite elastic computing resources. Existing compression approaches present trade-offs: software-based methods have performance overheads while hardware-based solutions lack flexibility for diverse database workloads.", "method": "PolarStore employs a dual-layer compression mechanism combining in-storage compression in PolarCSD hardware with lightweight compression in software. It includes database-oriented optimizations for critical I/O paths, hardware improvements for host-level stability, and compression-aware scheduling for cluster-level space efficiency.", "result": "Deployed on thousands of storage servers within PolarDB managing over 100 PB of data, PolarStore achieves a compression ratio of 3.55, reduces storage costs by approximately 60%, while maintaining performance comparable to uncompressed clusters.", "conclusion": "PolarStore successfully addresses the storage cost challenge in cloud-native RDBMSs through its dual-layer compression approach, achieving significant cost savings without compromising performance, as demonstrated by its large-scale deployment in production environments."}}
{"id": "2511.19978", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.19978", "abs": "https://arxiv.org/abs/2511.19978", "authors": ["Junru Li", "Qing Wang", "Zhe Yang", "Shuo Liu", "Jiwu Shu", "Youyou Lu"], "title": "SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility", "comment": "12 pages, accepted by ICDE'26", "summary": "Distributed storage systems typically maintain strong consistency between data nodes and metadata nodes by adopting ordered writes: 1) first installing data; 2) then updating metadata to make data visible.We propose SwitchDelta to accelerate ordered writes by moving metadata updates out of the critical path. It buffers in-flight metadata updates in programmable switches to enable data visibility in the network and retain strong consistency. SwitchDelta uses a best-effort data plane design to overcome the resource limitation of switches and designs a novel metadata update protocol to exploit the benefits of in-network data visibility. We evaluate SwitchDelta in three distributed in-memory storage systems: log-structured key-value stores, file systems, and secondary indexes. The evaluation shows that SwitchDelta reduces the latency of write operations by up to 52.4% and boosts the throughput by up to 126.9% under write-heavy workloads.", "AI": {"tldr": "SwitchDelta accelerates distributed storage ordered writes by buffering metadata updates in programmable switches, enabling data visibility in the network while maintaining strong consistency.", "motivation": "Traditional distributed storage systems use ordered writes that require data installation before metadata updates, creating performance bottlenecks in the critical path.", "method": "Uses programmable switches to buffer in-flight metadata updates, implements best-effort data plane design to overcome switch resource limitations, and designs novel metadata update protocol for in-network data visibility.", "result": "Reduces write latency by up to 52.4% and boosts throughput by up to 126.9% under write-heavy workloads across three distributed in-memory storage systems.", "conclusion": "SwitchDelta successfully accelerates ordered writes by moving metadata updates out of the critical path using programmable switches while maintaining strong consistency."}}
{"id": "2511.20100", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20100", "abs": "https://arxiv.org/abs/2511.20100", "authors": ["Xinguo Zhu", "Shaohui Peng", "Jiaming Guo", "Yunji Chen", "Qi Guo", "Yuanbo Wen", "Hang Qin", "Ruizhi Chen", "Qirui Zhou", "Ke Gao", "Yanjun Wu", "Chen Zhao", "Ling Li"], "title": "QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation", "comment": "9 pages, 2 figures, accepted by AAAI 2026", "summary": "Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.", "AI": {"tldr": "MTMC is a hierarchical framework that decouples optimization strategy from implementation details for GPU kernel generation, using reinforcement learning for high-level strategy and LLMs for incremental implementation, achieving superior accuracy and performance over existing methods.", "motivation": "Developing high-performance GPU kernels is challenging due to reliance on expert crafting and poor portability. Existing LLM-based approaches directly generate entire optimized programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes, leading to correctness and efficiency limitations.", "method": "Proposes Macro Thinking Micro Coding (MTMC) framework: Macro Thinking employs reinforcement learning to guide lightweight LLMs in exploring semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement stepwise optimization proposals, avoiding full-kernel generation errors.", "result": "On KernelBench: achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% improvement than SOTA LLMs, with up to 7.3x speedup over LLMs and 2.2x over expert-optimized PyTorch Eager kernels. On TritonBench: attains up to 59.64% accuracy and 34x speedup.", "conclusion": "MTMC effectively navigates the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation with superior performance in both accuracy and running time compared to existing approaches."}}
{"id": "2511.20172", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20172", "abs": "https://arxiv.org/abs/2511.20172", "authors": ["Xinjun Yang", "Qingda Hu", "Junru Li", "Feifei Li", "Yuqi Zhou", "Yicong Zhu", "Qiuru Lin", "Jian Dai", "Yang Kong", "Jiayu Zhang", "Guoqiang Xu", "Qiang Liu"], "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management", "comment": "13 pages, accepted by SIGMOD'26", "summary": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.", "AI": {"tldr": "Beluga is a novel memory architecture using CXL switches to enable GPUs and CPUs to access shared large-scale memory pools, addressing memory bottlenecks in LLM inference by providing near-local memory latency and reducing programming complexity compared to RDMA-based solutions.", "motivation": "The rapid growth in LLM model sizes and demand for long-context inference creates memory bottlenecks in GPU-accelerated serving systems, where limited GPU HBM capacity forces reliance on host memory, which is constrained by CPU memory channels. Current RDMA-based disaggregated memory solutions suffer from high latency, complex protocols, and synchronization overhead.", "method": "Proposes Beluga architecture that enables GPUs and CPUs to access shared large-scale memory pools through CXL switches using native load/store access semantics. Includes systematic characterization of commercial CXL switch-based memory pools and design guidelines. Implements Beluga-KVCache system specifically for managing large-scale KVCache in LLM inference.", "result": "Beluga-KVCache achieves 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in vLLM inference engine compared to RDMA-based solutions.", "conclusion": "Beluga is the first system enabling GPUs to directly access large-scale memory pools through CXL switches, representing a significant advancement toward low-latency, shared access to vast memory resources by GPUs for LLM inference workloads."}}
{"id": "2511.20391", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20391", "abs": "https://arxiv.org/abs/2511.20391", "authors": ["Anton Ivashkevich", "Matija Pi\u0161korec", "Claudio J. Tessone"], "title": "Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi", "comment": "2 pages, 1 figure. Published in: 2025 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)", "summary": "We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers. The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity. It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience. For example, a functioning PoW consensus is easily visible from the LCD screens, as well as consensus degradation which might arise from various factors, including peer-to-peer topology and communication latency - all parameters which can be configured from the central web-based interface.", "AI": {"tldr": "A prototype Ethereum PoW blockchain network running on Raspberry Pi computers with LCD screens for educational visualization of blockchain concepts and consensus mechanisms.", "motivation": "To create an easy-to-setup, standalone blockchain system for educational purposes that demonstrates fundamental blockchain concepts and PoW consensus to a wide audience.", "method": "Developed a prototype using multiple Raspberry Pi computers connected via local WiFi router, featuring LCD screens for real-time visualization of blockchain state and consensus processes.", "result": "Successfully created a functioning PoW blockchain network that visually demonstrates consensus mechanisms, including consensus degradation from factors like peer-to-peer topology and communication latency.", "conclusion": "The Raspberry Pi-based prototype provides an effective educational tool for demonstrating blockchain fundamentals and PoW consensus through visual feedback on LCD screens, with configurable parameters via web interface."}}
