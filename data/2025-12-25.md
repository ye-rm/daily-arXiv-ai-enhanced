<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Hardware-Algorithm Co-Design for Hyperdimensional Computing Based on Memristive System-on-Chip](https://arxiv.org/abs/2512.20808)
*Yi Huang,Alireza Jaberi Rad,Qiangfei Xia*

Main category: cs.ET

TL;DR: Hardware-algorithm co-design approach implementing Hyperdimensional Computing (HDC) on memristive System-on-Chip for energy-efficient edge AI applications.


<details>
  <summary>Details</summary>
Motivation: HDC is suitable for resource-constrained edge AI, and memristive in-memory computing offers energy-efficient hardware. Combining both can create efficient AI deployments for edge devices.

Method: Hardware-algorithm co-design: using memristive crossbar randomness for encoding and analog IMC for classification, plus hardware-aware encoding techniques to map data features into hyperdimensional vectors.

Result: Experimental hardware results show 90.71% accuracy in language classification task, demonstrating the approach's effectiveness.

Conclusion: The proposed co-design approach shows potential for achieving energy-efficient AI deployments on edge devices by combining memristive IMC hardware with HDC algorithms.

Abstract: Hyperdimensional computing (HDC), utilizing a parallel computing paradigm and efficient learning algorithm, is well-suited for resource-constrained artificial intelligence (AI) applications, such as in edge devices. In-memory computing (IMC) systems based on memristive devices complement this by offering energy-efficient hardware solutions. To harness the advantages of both memristive IMC hardware and HDC algorithms, we propose a hardware-algorithm co-design approach for implementing HDC on a memristive System-on-Chip (SoC). On the hardware side, we utilize the inherent randomness of memristive crossbar arrays for encoding and employ analog IMC for classification. At the algorithm level, we develop hardware-aware encoding techniques that map data features into hyperdimensional vectors, optimizing the classification process within the memristive SoC. Experimental results in hardware demonstrate 90.71% accuracy in the language classification task, highlighting the potential of our approach for achieving energy-efficient AI deployments on edge devices.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)
*Aymen Alsaadi,Mason Hooten,Mariya Goliyad,Andre Merzky,Andrew Shao,Mikhail Titov,Tianle Wang,Yian Chen,Maria Kalantzi,Kent Lee,Andrew Park,Indira Pimpalkhare,Nick Radcliffe,Colin Wahl,Pete Mendygral,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RHAPSODY is a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions, allowing simulation codes, inference services, and agentic workflows to coexist within single job allocations on HPC platforms.


<details>
  <summary>Details</summary>
Motivation: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and agent-driven control, imposing heterogeneous and conflicting requirements on runtime systems. Existing systems only address subsets of these requirements, limiting support for emerging AI-HPC applications at scale.

Method: RHAPSODY is a multi-runtime middleware that provides uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, it composes and coordinates them (e.g., Dragon and vLLM) to enable concurrent execution of heterogeneous AI-HPC workloads within single job allocations.

Result: Evaluation on multiple HPC systems shows RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and enables data- and control-efficient coupling between AI and HPC tasks in agentic workflows.

Conclusion: RHAPSODY successfully addresses the limitations of existing systems by enabling concurrent execution of heterogeneous AI-HPC workloads through runtime composition and coordination, supporting emerging AI-HPC applications at scale on leadership-class HPC platforms.

Abstract: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.

</details>


### [3] [Stochastic well-structured transition systems](https://arxiv.org/abs/2512.20939)
*James Aspnes*

Main category: cs.DC

TL;DR: The paper extends well-structured transition systems with probabilistic scheduling to define stochastic well-structured transition systems, analyzes phase clock limitations, characterizes computational power of augmented systems as BPP, and unaugmented systems as symmetric languages in BPL.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework that encompasses various stochastic distributed systems (population protocols, chemical reaction networks, gossip models) and analyze their computational capabilities when augmented with ordering or equivalence relations on agents.

Method: Extends well-structured transition systems with probabilistic scheduling rules to define stochastic well-structured transition systems. Analyzes phase clock implementations and terminating computations in these systems, examining both unaugmented systems and those augmented with total order or equivalence relations on agents.

Result: Shows that phase clocks in these systems either stop or tick too fast after polynomially many expected steps. Terminating computations finish or fail in expected polynomial time. Augmented systems (with total order or equivalence relations) compute exactly languages in BPP, while unaugmented systems compute only symmetric languages in BPL.

Conclusion: Provides a unified framework for analyzing stochastic distributed systems and precisely characterizes their computational power based on whether they have access to ordering/equivalence information about agents, establishing a clear hierarchy between augmented and unaugmented systems.

Abstract: Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.
  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.

</details>


### [4] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: AutoHet is a system that automatically optimizes 3D parallelism for distributed LLM training on heterogeneous GPUs, addressing challenges like asymmetric tensor parallelism and efficient gradient synchronization, achieving up to 1.79× training speedup and 4.38× faster recovery.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs and diverse GPU hardware creates demand for efficient distributed training across heterogeneous GPU environments, requiring solutions for asymmetric parallelism structures and workload balancing.

Method: AutoHet automatically identifies optimal parallelism plans for heterogeneous GPUs, supports asymmetric 3D parallelism, uses theoretical optimization model for device grouping and load balancing to minimize training time, and implements efficient recovery strategy for spot instance preemption.

Result: AutoHet outperforms existing systems like Megatron-LM and Whale, achieving up to 1.79× speedup in training throughput and 4.38× faster recovery speed compared to spot instance baseline, evaluated on three large-scale models with three GPU types.

Conclusion: AutoHet effectively addresses heterogeneous GPU training challenges through automated parallelism optimization and efficient recovery mechanisms, significantly improving training efficiency and resilience in distributed environments.

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [5] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: The paper proposes an online scheduling framework for fine-tuning large foundation models using a mix of spot and on-demand GPU instances, addressing volatile spot market dynamics through prediction-based algorithms and adaptive policy selection.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large foundation models is expensive, and while GPU spot instances offer cost savings, their volatile prices and availability make deadline-aware scheduling challenging, necessitating intelligent mixed-instance scheduling approaches.

Method: Formulates integer programming for mixed-instance scheduling under price/availability dynamics, proposes prediction-based online algorithm using commitment level control, complementary algorithm without predictions, and policy selection algorithm that learns best policy from parameterized pool.

Result: Prediction-based algorithm achieves tighter performance bounds with decreasing prediction error, policy selection algorithm has O(√T) regret bound, and experimental results show framework adapts to varying market dynamics, outperforming baselines by up to 54.8% utility improvement.

Conclusion: The proposed online scheduling framework effectively addresses spot market volatility for foundation model fine-tuning through adaptive policy selection and prediction-based approaches, demonstrating significant cost efficiency improvements over existing methods.

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [6] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: Mesh-Attention is a new distributed attention algorithm that uses 2D tile assignments to GPUs instead of 1D rows/columns, achieving better communication-computation ratio and scalability than Ring-Attention.


<details>
  <summary>Details</summary>
Motivation: Distributed attention is crucial for scaling context windows in LLMs, but current state-of-the-art Ring-Attention suffers from scalability limitations due to excessive communication traffic.

Method: Proposes Mesh-Attention with a matrix-based model that assigns 2D computation tiles to GPUs, includes a greedy algorithm for efficient scheduling within tiles, and allows tuning of communication-computation ratio through different tile shapes.

Result: Achieves up to 3.4x speedup (2.9x average) and reduces communication volume by up to 85.4% (79.0% average) on 256 GPUs, with superior scalability as system scales.

Conclusion: Mesh-Attention substantially reduces overhead in large-scale deployments and demonstrates clear advantages over existing distributed attention algorithms through both theoretical analysis and extensive experiments.

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>


### [7] [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)
*S. M. Shovan,Arindam Khanda,Sanjukta Bhowmick,Sajal K. Das*

Main category: cs.DC

TL;DR: ESCHER: GPU-centric parallel data structure for efficient dynamic hypergraph analysis, achieving up to 473.7x speedup in triad counting.


<details>
  <summary>Details</summary>
Motivation: Real-world networks have higher-order interactions requiring hypergraph modeling, but large dynamic hypergraphs lack specialized software and data structures for efficient analysis.

Method: Propose ESCHER (Efficient and Scalable Hypergraph Evolution Representation) - a GPU-centric parallel data structure for managing large-scale hypergraph dynamics, with a triad-count update framework minimizing redundant computation.

Result: Outperforms state-of-the-art methods with speedups up to 104.5x (hyperedge-based), 473.7x (incident-vertex-based), and 112.5x (temporal triad types) on real-world and synthetic datasets.

Conclusion: ESCHER enables efficient analysis of large dynamic hypergraphs, addressing computational challenges in higher-order network analysis with significant performance improvements.

Abstract: Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: NotSoTiny is a new benchmark for evaluating LLMs on RTL code generation, built from real hardware designs to address limitations of existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RTL benchmarks for LLMs are limited in scale, skewed toward trivial designs, offer minimal verification rigor, and are vulnerable to data contamination, making it challenging to properly evaluate LLM capabilities in realistic hardware design scenarios.

Method: The authors built NotSoTiny from hundreds of actual hardware designs from the Tiny Tapeout community, using an automated pipeline that removes duplicates, verifies correctness, and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout's release schedule.

Result: Evaluation shows that NotSoTiny tasks are more challenging than prior benchmarks, demonstrating its effectiveness in overcoming current limitations of LLMs applied to hardware design.

Conclusion: NotSoTiny provides a better benchmark for assessing LLMs on structurally rich and context-aware RTL generation, guiding improvement of LLM technology for hardware design applications.

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


### [9] [ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update](https://arxiv.org/abs/2512.21153)
*Zhe Su,Giacomo Indiveri*

Main category: cs.AR

TL;DR: ElfCore is a 28nm digital spiking neural network processor for event-driven sensory processing, featuring integrated self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates, achieving significant improvements in power, memory, and capacity efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient spiking neural network processor for event-driven sensory processing that addresses limitations in existing solutions by integrating advanced learning capabilities while reducing power consumption and memory requirements.

Method: Developed a 28nm digital SNN processor with three key innovations: 1) local online self-supervised learning engine for multi-layer temporal learning without labeled inputs, 2) dynamic structured sparse training engine for high-accuracy sparse-to-sparse learning, and 3) activity-dependent sparse weight update mechanism that selectively updates weights based on input activity and network dynamics.

Result: Demonstrated on gesture recognition, speech, and biomedical signal processing tasks, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.

Conclusion: ElfCore represents a significant advancement in SNN processor design, successfully integrating advanced learning capabilities while achieving substantial improvements in power efficiency, memory usage, and network capacity for event-driven sensory processing applications.

Abstract: In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.

</details>
