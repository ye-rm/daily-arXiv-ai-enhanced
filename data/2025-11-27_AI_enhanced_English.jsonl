{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.", "AI": {"tldr": "This paper presents a methodology using Stochastic Petri Nets to analyze the availability of Nextcloud file servers in private cloud environments, evaluating four redundancy strategies to improve system reliability.", "motivation": "As cloud-based storage platforms become increasingly important for organizations seeking alternatives to public cloud services, assessing the dependability and reliability of these systems has become crucial, especially for private cloud deployments.", "method": "The methodology uses Stochastic Petri Nets (SPNs) modeling approach to evaluate different redundancy strategies for Nextcloud file servers hosted in Apache CloudStack private cloud environments. Four architectural configurations were analyzed: baseline, host-level redundancy, VM redundancy, and combined host-VM redundancy.", "result": "The results demonstrate that implementing redundancy at both host and virtual machine levels significantly improves system availability and reduces expected downtime compared to the baseline configuration.", "conclusion": "The proposed SPN-based approach provides an effective method for evaluating private cloud availability and supports infrastructure design decisions by quantifying the impact of different redundancy strategies on system reliability."}}
{"id": "2511.20834", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20834", "abs": "https://arxiv.org/abs/2511.20834", "authors": ["Dionysios Adamopoulos", "Anastasia Poulopoulou", "Georgios Goumas", "Christina Giannoula"], "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks", "comment": null, "summary": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.", "AI": {"tldr": "Spira is a new sparse convolution engine for 3D point clouds that leverages voxel coordinate properties (integer, bounded, continuous) to achieve significant performance improvements over prior methods through optimized kernel map construction and execution.", "motivation": "Prior sparse convolution engines fail to fully exploit key properties of voxel coordinates - they are integer-valued, bounded within spatial ranges, and geometrically continuous. This leads to high preprocessing and post-processing overheads during kernel map construction.", "method": "Spira introduces: (1) one-shot search algorithm for kernel map construction with no preprocessing and high memory locality, (2) packed-native processing scheme for low-cost voxel coordinate access, (3) dual-dataflow execution mechanism adapting to layer characteristics, and (4) network-wide parallelization building kernel maps for all layers concurrently.", "result": "Spira outperforms prior sparse convolution engines by 1.71x on average (up to 2.31x) for end-to-end inference, and by 2.13x on average (up to 3.32x) for layer-wise execution across diverse configurations.", "conclusion": "Spira demonstrates that exploiting voxel coordinate properties through specialized algorithms and execution mechanisms enables significant performance improvements in sparse convolution for 3D point cloud processing."}}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "AI": {"tldr": "Aragog is a system that dynamically adapts LLM configurations during workflow execution to optimize serving costs and performance, addressing the limitations of static configuration approaches in agentic workflows.", "motivation": "Agentic workflows require many LLM inferences, making them computationally expensive to serve at scale. Existing configuration selection approaches bind decisions before execution, which becomes suboptimal due to fluctuating system loads during lengthy workflow executions.", "method": "Aragog decouples the problem into two elements: a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using real-time system observations, with novel strategies to accelerate each component.", "result": "Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0-217.0% and reduces median latency by 32.5-78.9% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "conclusion": "Aragog demonstrates that progressive configuration adaptation throughout workflow execution can significantly improve serving efficiency and performance while preserving accuracy, making agentic workflows more scalable and cost-effective."}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "DOPD is a dynamic LLM inference system that adjusts instance allocations to achieve optimal prefill-to-decoding ratios based on real-time load monitoring, improving system performance and SLO attainment.", "motivation": "Current LLM systems decouple prefill and decoding stages on separate GPUs, but workload heterogeneity causes producer-consumer imbalance between instance types in disaggregated architectures.", "method": "Proposes DOPD system with dynamic instance allocation adjustment to achieve optimal P/D ratio using real-time load monitoring and appropriate request-scheduling policy.", "result": "Compared to vLLM and DistServe, DOPD improves system goodput by up to 1.5X, reduces P90 TTFT by up to 67.5%, and reduces P90 TPOT by up to 22.8%. Achieves over 99% SLO attainment with less additional resources.", "conclusion": "DOPD effectively resolves prefill-decoding instance imbalances and resource allocation mismatches under high concurrency through dynamic P/D ratio adjustment and proactive reconfiguration."}}
{"id": "2511.21046", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.21046", "abs": "https://arxiv.org/abs/2511.21046", "authors": ["Ahmet Efe", "H\u00fcsrev C\u0131lasun", "Abhimanyu Kumar", "Nafisa Sadaf Prova", "Ziqing Zeng", "Tahmida Islam", "Ruihong Yin", "Chaohui Li", "Peter Kreye", "Chris Kim", "Sachin S. Sapatnekar", "Ulya R. Karpuzcu"], "title": "On Solving Structured SAT on Ising Machines: A Semiprime Factorization Study", "comment": null, "summary": "Ising machines are emerging as a new technology for solving various classes of computationally hard problems of practical importance, yet their limits on structured SAT workloads, representative of numerous real-world applications, remain unexplored. We present the first systematic study of such problems, using semiprime factorization as a representative case. Our results show that highly restrictive, 'tight' constraints, when mapped into optimization form, fundamentally distort Ising dynamics, and that these distortions are amplified when problems are decomposed to fit within limited hardware. We propose a hybrid approach that offloads constraint-heavy components to classical preprocessing while reserving the computationally challenging part for the Ising machine. Structured SAT represents a crucial step toward real-world applications, which remain out of reach today due to Ising machine limitations. Our findings reveal that constraint handling is a central obstacle and highlight hybrid hardware-software approaches as the path forward to unlocking the long-term potential of Ising machines. We conduct our evaluation on the manufactured Ising chips and demonstrate that our flow more than doubles the solvable problem size on a 45-spin all-to-all Ising chip, from 8-bit (94 variables) to 11-bit (190 variables), without hardware changes.", "AI": {"tldr": "This paper presents the first systematic study of Ising machines' limitations on structured SAT problems, using semiprime factorization as a case study. It reveals that tight constraints distort Ising dynamics and proposes a hybrid approach that combines classical preprocessing with Ising computation to overcome hardware limitations.", "motivation": "To understand the fundamental limitations of Ising machines on structured SAT workloads, which represent numerous real-world applications, and to develop methods to overcome these limitations since current Ising machines cannot handle practical problems due to constraint handling issues.", "method": "The authors use semiprime factorization as a representative structured SAT problem. They analyze how tight constraints distort Ising dynamics when mapped to optimization form, and propose a hybrid approach that offloads constraint-heavy components to classical preprocessing while reserving computationally challenging parts for the Ising machine.", "result": "The hybrid approach more than doubles the solvable problem size on a 45-spin all-to-all Ising chip, from 8-bit (94 variables) to 11-bit (190 variables), without requiring hardware changes. This demonstrates that constraint handling is a central obstacle for Ising machines.", "conclusion": "Structured SAT problems represent a crucial step toward real-world applications of Ising machines, but constraint handling remains a major limitation. Hybrid hardware-software approaches are identified as the path forward to unlock the long-term potential of Ising machines for practical applications."}}
{"id": "2511.21235", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21235", "abs": "https://arxiv.org/abs/2511.21235", "authors": ["Daniel Berend", "Shlomi Dolev", "Sweta Kumari", "Dhruv Mishra", "Marina Kogan-Sadetsky", "Archit Somani"], "title": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing", "comment": "19 pages, 11 figures, 3 tables, Patented", "summary": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.", "AI": {"tldr": "DynamicAdaptiveClimb is a novel cache replacement policy that adapts promotion distances and cache sizes dynamically, achieving up to 29% higher hit ratios than FIFO and outperforming other state-of-the-art algorithms by 10-15% in fluctuating workloads.", "motivation": "Traditional cache replacement policies like LRU and CLIMB lack adaptability to changing access patterns and workload demands, leading to suboptimal performance in dynamic caching environments.", "method": "AdaptiveClimb dynamically adjusts promotion distances based on hit/miss patterns with one tunable parameter. DynamicAdaptiveClimb extends this by automatically tuning cache size in response to workload demands.", "result": "Evaluation across 1067 real-world traces shows DynamicAdaptiveClimb achieves up to 29% hit ratio improvement over FIFO baseline and 10-15% better performance than AdaptiveClimb and SIEVE, especially in environments with fluctuating working set sizes.", "conclusion": "The proposed approach effectively delivers efficient performance in modern dynamic caching environments through lightweight adaptation mechanisms that outperform traditional and state-of-the-art cache replacement policies."}}
{"id": "2511.21232", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21232", "abs": "https://arxiv.org/abs/2511.21232", "authors": ["Muhammed Yildirim", "Ozcan Ozturk"], "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI", "comment": "13 pages, 7 tables, 14 figures", "summary": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.", "AI": {"tldr": "A novel hardware accelerator with fused pixel-wise dataflow eliminates intermediate buffers in Depthwise Separable Convolutions, reducing data movement by 87% and achieving 59.3x speedup over software execution.", "motivation": "To address the memory wall bottleneck in Edge AI and TinyML applications caused by transferring intermediate feature maps in layer-by-layer execution of lightweight CNNs like MobileNetV2.", "method": "Introduces a hardware accelerator architecture with fused pixel-wise dataflow implemented as a Custom Function Unit for RISC-V processors, computing single output pixels to completion across all DSC stages without intermediate buffers.", "result": "Achieves 87% reduction in data movement, 59.3x speedup over baseline software execution on FPGA, and compact ASIC footprints of 0.284 mm\u00b2 (28nm) and 1.20 mm\u00b2 (40nm) with low power consumption.", "conclusion": "The work confirms feasibility of zero-buffer dataflow within TinyML resource constraints, offering an effective strategy to overcome memory wall in edge AI accelerators."}}
{"id": "2511.21267", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.21267", "abs": "https://arxiv.org/abs/2511.21267", "authors": ["Luca Fehlings", "Nihal Raut", "Md. Hanif Ali", "Francesco M. Puglisi", "Andrea Padovani", "Veeresh Deshpande", "Erika Covi"], "title": "Defect-Aware Physics-Based Compact Model for Ferroelectric nvCap: From TCAD Calibration to Circuit Co-Design", "comment": null, "summary": "Ferroelectric non-volatile capacitance-based memories enable non-destructive readout and low-power in-memory computing with 3D stacking potential. However, their limited memory window (1-10 fF/\u03bcm) requires material-device-circuit co-optimization. Existing compact models fail to capture the physics of small-signal capacitance, device variability, and cycling degradation, which are critical parameters for circuit design. In non-volatile capacitance devices, the small-signal capacitance difference of the polarization states is the key metric. The majority of the reported compact models do not incorporate any physical model of the capacitance as a function of voltage and polarization. We present a physics-based compact model that captures small-signal capacitance, interface and bulk defect contributions, and device variations through multi-scale modeling combining experimental data, TCAD simulations, and circuit validation. Based on this methodology, we show optimized memory read-out with +/- 5 mV sense margin and impact of device endurance at the circuit level. This work presents a comprehensive compact model which enables the design of selector-less arrays and 3D-stacked memories for compute-in-memory and storage memory.", "AI": {"tldr": "A physics-based compact model for ferroelectric non-volatile capacitance memories that captures small-signal capacitance, device variability, and cycling degradation, enabling optimized memory design with 3D stacking potential.", "motivation": "Existing compact models fail to capture critical physics of small-signal capacitance, device variability, and cycling degradation in ferroelectric capacitance memories, limiting circuit design optimization despite their advantages in non-destructive readout and low-power computing.", "method": "Multi-scale modeling combining experimental data, TCAD simulations, and circuit validation to develop a physics-based compact model that incorporates small-signal capacitance, interface/bulk defect contributions, and device variations.", "result": "The model enables optimized memory read-out with +/- 5 mV sense margin and demonstrates impact of device endurance at circuit level, supporting design of selector-less arrays and 3D-stacked memories.", "conclusion": "This comprehensive compact model addresses previous limitations and enables practical design of ferroelectric capacitance-based memories for compute-in-memory and storage applications with 3D stacking capability."}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "Integration of vLLM, Slurm and Kubernetes on supercomputer RAMSES to serve LLMs efficiently with low latency overhead for concurrent requests.", "motivation": "Address the mismatch between classical HPC operating models and the requirements of synchronous, user-facing dynamic AI inference workloads in higher education.", "method": "Proposed architecture combining vLLM, Slurm and Kubernetes on the RAMSES supercomputer to serve large language models.", "result": "The architecture scales efficiently for 100, 500 and 1000 concurrent requests with only ~500ms end-to-end latency overhead.", "conclusion": "The solution successfully adapts HPC infrastructure for dynamic AI inference workloads with minimal performance impact."}}
{"id": "2511.21346", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21346", "abs": "https://arxiv.org/abs/2511.21346", "authors": ["Mohamed Shahawy", "Julien de Castelnau", "Paolo Ienne"], "title": "Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration", "comment": null, "summary": "Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.", "AI": {"tldr": "Bombyx is a compiler toolchain that converts OpenCilk programs to Cilk-1-style code for efficient FPGA implementation, using explicit continuation-passing instead of costly context switching, with optimizations for memory-compute overlap.", "motivation": "Existing FPGA approaches for task-level parallelism often use OpenCilk's implicit task model which requires expensive context switching in hardware, making them inefficient for spatial architectures.", "method": "Developed Bombyx compiler that lowers OpenCilk programs to Cilk-1-inspired intermediate representation, supports multiple compilation targets including OpenCilk-compatible runtime and synthesizable PE generator for HLS tools, and implements decoupled access-execute optimization.", "result": "Enables efficient mapping of CPU-oriented TLP applications to FPGA spatial architectures, improves memory-compute overlap and overall throughput through automatic generation of high-performance processing elements.", "conclusion": "Bombyx successfully bridges the gap between CPU-oriented task parallelism and FPGA spatial architectures by leveraging Cilk-1's explicit continuation-passing model, providing an efficient compilation framework for FPGA-based TLP applications."}}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.", "AI": {"tldr": "This paper introduces data redundancy to improve memory locality for the near-field P2P operator in MLFMA on GPUs, achieving up to 7X kernel speedup but limited end-to-end application speedup (1.04X) due to data restructuring overhead.", "motivation": "The near-field P2P operator in MLFMA is a performance bottleneck on GPUs due to poor memory locality caused by memory access dispersion.", "method": "Introduces data redundancy to improve spatial locality by reducing memory access dispersion, with an analytical locality model to predict speedup trends without hardware profiling. Validated on two MLFMA applications: electromagnetic solver (DBIM-MLFMA) and stellar dynamics code (PhotoNs-2.0).", "result": "Achieved up to 7X kernel speedup due to improved cache behavior, but increased data volume raised overheads in data restructuring, limiting end-to-end application speedup to 1.04X. The model reliably captures performance trends across different problem sizes and densities.", "conclusion": "Data redundancy can enhance GPU performance for P2P operator when locality gains outweigh data movement costs. The technique is injectable into existing implementations with minimal code changes and the analytical model effectively predicts performance trends."}}
{"id": "2511.21451", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21451", "abs": "https://arxiv.org/abs/2511.21451", "authors": ["Flurin Arquint", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).", "AI": {"tldr": "First ASIC implementation of jammer-resilient multi-antenna time synchronization that mitigates jamming attacks using multi-antenna processing.", "motivation": "To provide robust time synchronization that is resilient to jamming attacks, particularly smart jammers with multiple transmit antennas.", "method": "Implemented a recent algorithm using multi-antenna processing in a 65 nm ASIC design supporting single-antenna transmitter and 16-antenna receiver configuration.", "result": "Fabricated ASIC has 2.87 mm\u00b2 core area, consumes 310 mW power, and supports 1.28 MS/s sampling rate while mitigating jammers with up to two transmit antennas.", "conclusion": "Successfully demonstrated the first hardware implementation of jammer-resilient multi-antenna time synchronization with practical performance metrics."}}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "AI": {"tldr": "MemFine is a memory-aware fine-grained scheduling framework that addresses memory bottlenecks in large-scale Mixture of Experts (MoE) training by decomposing token distribution and expert computation into chunks with optimized recomputation strategies.", "motivation": "Large-scale MoE models face critical memory bottlenecks due to load imbalance from dynamic token routing, causing memory overflow on GPUs with limited capacity and constraining model scalability. Existing load balancing methods that cap expert capacity compromise accuracy and fail on memory-constrained hardware.", "method": "MemFine decomposes token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput.", "result": "Experiments show MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "conclusion": "MemFine successfully addresses the memory bottleneck in MoE training through fine-grained scheduling and optimized recomputation, enabling scalable training on memory-constrained hardware while maintaining efficiency."}}
{"id": "2511.21461", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21461", "abs": "https://arxiv.org/abs/2511.21461", "authors": ["Jonas Elmiger", "Fabian Stuber", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\\times$ higher per-user throughput and 4.5$\\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.", "AI": {"tldr": "First SIMO receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection using MAED algorithm, achieving 100 Mb/s throughput with 223 mW power in 22 nm FD-SOI.", "motivation": "To develop a single-chip solution that can simultaneously handle smart jammers, channel estimation, and data detection for wireless communication systems under jamming attacks.", "method": "Implements MAED algorithm that uses spatial filtering through nonlinear optimization to unify jammer estimation/nulling, channel estimation, and data detection. Supports 8 receive antennas and mitigates both smart and barrage jammers.", "result": "Fabricated in 22 nm FD-SOI with 0.32 mm\u00b2 core area, achieves 100 Mb/s throughput at 223 mW power consumption. Delivers 3\u00d7 higher per-user throughput and 4.5\u00d7 higher area efficiency than state-of-the-art jammer-resilient detectors.", "conclusion": "The ASIC successfully demonstrates joint jammer mitigation, channel estimation, and data detection with superior performance metrics compared to existing solutions, enabling robust wireless communication under jamming conditions."}}
{"id": "2511.21549", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21549", "abs": "https://arxiv.org/abs/2511.21549", "authors": ["Jason Yik", "Walter Gallego Gomez", "Andrew Cheng", "Benedetto Leto", "Alessandro Pierro", "Noah Pacik-Nelson", "Korneel Van den Berghe", "Vittorio Fra", "Andreea Danielescu", "Gianvito Urgese", "Vijay Janapa Reddi"], "title": "Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators", "comment": null, "summary": "Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.", "AI": {"tldr": "This paper presents the first comprehensive performance analysis of neuromorphic accelerators, identifying three bottleneck states and proposing a floorline performance model with optimization methodology that achieves up to 3.86x runtime improvement and 3.38x energy reduction.", "motivation": "Neuromorphic accelerators have unique architectural characteristics that create different performance dynamics from conventional accelerators, but existing optimization approaches rely on aggregate metrics whose actual performance impact remains unknown.", "method": "The authors conducted theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators (Brainchip AKD1000, Synsense Speck, and Intel Loihi 2), established three bottleneck states, and developed the floorline performance model with optimization methodology combining sparsity-aware training and floorline-informed partitioning.", "result": "The study identified three distinct accelerator bottleneck states (memory-bound, compute-bound, and traffic-bound) and revealed shortcomings of conventional metrics. The proposed optimization methodology achieved substantial improvements: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations at iso-accuracy.", "conclusion": "The floorline performance model effectively identifies performance bounds and guides workload optimization for neuromorphic accelerators, demonstrating that understanding specific bottleneck states and workload configuration features is crucial for achieving optimal performance in these unique architectures."}}
