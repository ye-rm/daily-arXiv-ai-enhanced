<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [WIRE: Write Energy Reduction via Encoding in Phase Change Main Memories (PCM)](https://arxiv.org/abs/2511.04928)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia,Sherrene Bogle*

Main category: cs.ET

TL;DR: WIRE is a new coding mechanism for Phase Change Memory (PCM) that reduces write energy by ensuring most write operations require only one-bit flip, achieved through assigning codewords with hamming distance of one to frequent values.


<details>
  <summary>Details</summary>
Motivation: PCM faces significant energy challenges during write operations despite its advantages in scalability and standby energy efficiency over DRAM. Existing techniques for reducing write energy have limitations including bit-wise comparison overheads, increased write cycles, and poor endurance.

Method: The WIRE coding mechanism assigns codewords to frequent values such that they have a hamming distance of one, enabling most write operations to require only one-bit flip. This is augmented with block-level wear-leveling and rotating difference bits to improve PCM lifetime.

Result: Experimental evaluation with multi-threaded and multi-programmed workloads showed significant improvements in PCM lifetime, write energy reduction, and bit flip reduction compared to existing mechanisms.

Conclusion: WIRE coding mechanism effectively addresses PCM's write energy challenges by minimizing bit flips during writes, while also improving memory lifetime through wear-leveling techniques, making it a promising solution for PCM main memory applications.

Abstract: Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic
Random-Access Memory (DRAM) in terms of scalability and standby energy
efficiency. Altering a PCM cell's state during writes demands substantial
energy, posing a significant challenge to PCM's role as the primary main
memory. Prior research has explored methods to reduce write energy consumption,
including the elimination of redundant writes, minimizing cell writes, and
employing compact row buffers for filtering PCM main memory accesses. However,
these techniques had certain drawbacks like bit-wise comparison of the stored
values, preemptive updates increasing write cycles, and poor endurance. In this
paper, we propose WIRE, a new coding mechanism through which most write
operations force a maximum of one-bit flip. In this coding-based data storage
method, we look at the frequent value stack and assign a code word to the most
frequent values such that they have a hamming distance of one. In most of the
write accesses, writing a value needs one or fewer bit flips which can save
considerable write energy. This technique can be augmented with a wear-leveling
mechanism at the block level, and rotating the difference bit in the assigned
codes, increasing the lifetime of the PCM array at a low cost. Using a
full-system evaluation of our method and comparing it to the existing
mechanisms, our experimental results for multi-threaded and multi-programmed
workloads revealed considerable improvement in lifetime and write energy as
well as bit flip reduction.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette is a C++17 library that enables flexible, efficient, and portable data structure definitions for hardware acceleration on heterogeneous platforms like GPUs, with minimal runtime overhead through compile-time abstractions.


<details>
  <summary>Details</summary>
Motivation: Adapting large, object-oriented C++ codebases for hardware acceleration is extremely challenging, especially for heterogeneous platforms such as GPUs.

Method: Decouples data layout from interface description, supports multiple memory management strategies, provides efficient data transfers and conversions across devices, and allows interfaces to be augmented with arbitrary functions using compile-time abstractions.

Result: Enables flexible, efficient, and portable data structure definitions with minimal runtime overhead, maintains compatibility with existing code, and supports both straightforward and advanced use cases.

Conclusion: Marionette provides an effective solution for adapting C++ codebases to heterogeneous platforms, as demonstrated by a CUDA-based case study showing its efficiency and flexibility.

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [3] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: This paper proposes custom GPU instructions for RISC-V GPUs to accelerate hybrid HDC-CNN computing, achieving up to 56.2x performance improvement in microbenchmarks.


<details>
  <summary>Details</summary>
Motivation: Neural networks have high energy consumption, while Hyperdimensional Computing (HDC) offers lightweight alternatives but suffers from lower accuracy. Hybrid HDC-CNN accelerators face limitations in generalizability and programmability, creating an opportunity for RISC-V GPU customization.

Method: Design and implementation of custom GPU instructions optimized for HDC operations on RISC-V-based GPUs, enabling efficient processing of hybrid HDC-CNN workloads with four types of custom instructions.

Result: Experimental results show performance improvement of up to 56.2 times in microbenchmark tests using the custom HDC instructions.

Conclusion: RISC-V GPUs with custom instructions demonstrate significant potential for energy-efficient, high-performance computing of hybrid HDC-CNN workloads.

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [4] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: This paper proposes a method to estimate GPU stress using online telemetry parameters and hardware performance counters, focusing on measuring throughput, issued instructions, and stall events to predict reliability concerns from sustained workloads.


<details>
  <summary>Details</summary>
Motivation: GPUs in data centers and HPC systems face reliability issues due to sustained workloads that stress components and cause faults, potentially corrupting computations and leading to incorrect results. Estimating this stress is crucial for predicting reliability, especially aging effects.

Method: The researchers combine online telemetry parameters and hardware performance counters to assess GPU stress. They focus on performance counters that measure throughput, amount of issued instructions, and stall events to evaluate resource usage efficiency.

Result: Experimental results show that stress induced by parallel workloads can be estimated by combining telemetry data and performance counters that reveal resource usage efficiency of the target workload.

Conclusion: The combination of telemetry parameters and specific performance counters (throughput, issued instructions, stall events) provides an effective approach for estimating GPU stress and predicting reliability concerns in sustained workload scenarios.

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: Manhattan Distance Mapping (MDM) is a post-training weight mapping technique for memristive CIM crossbars that reduces parasitic resistance effects by optimizing active-memristor placement, improving accuracy by 3.6% on average in ResNets.


<details>
  <summary>Details</summary>
Motivation: Parasitic resistance limits crossbar efficiency by forcing DNN matrices to be mapped into small tiles, which reduces CIM speedup and increases digital synchronization overhead, ADC conversions, latency, I/O pressure, and chip area.

Method: MDM exploits bit-level structured sparsity by feeding activations from the denser low-order side and reordering rows according to Manhattan distance to relocate active cells toward regions less affected by parasitic resistance.

Result: Applied to DNN models on ImageNet-1k, MDM reduces nonideality factor by up to 46% and improves accuracy under analog distortion by an average of 3.6% in ResNets.

Conclusion: MDM provides a lightweight, spatially informed method for scaling CIM DNN accelerators by effectively mitigating parasitic resistance nonidealities.

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


### [6] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: The paper introduces the Load-Balance-Longest-Path (LBLP) algorithm for optimizing CNN deployment on In-Memory Computing hardware with multiple Processing Units, achieving improved processing rates and reduced latency through dynamic task allocation.


<details>
  <summary>Details</summary>
Motivation: To address data movement bottlenecks in deep learning acceleration and leverage the parallelism of In-Memory Computing (IMC) for efficient CNN deployment on multi-processing systems.

Method: Developed the LBLP algorithm that dynamically assigns CNN nodes to available IMC Processing Units, using an IMC Emulator (IMCE) with multiple PUs to investigate performance impacts.

Result: Experimental benchmarking against alternative scheduling strategies demonstrated that LBLP effectively maximizes processing rate and minimizes latency through efficient resource utilization across various CNN models.

Conclusion: The proposed LBLP algorithm provides an effective dynamic task allocation strategy for optimizing CNN performance on IMC-based hardware systems.

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


### [7] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: RAS is a hardware acceleration system for rANS algorithm that achieves significant speedups (121.2x encode, 70.9x decode) over software implementations while maintaining high compression ratios with neural probability models.


<details>
  <summary>Details</summary>
Motivation: Data centers need efficient lossless compression but current probabilistic model-based methods are computationally slow, creating a need for hardware acceleration.

Method: Hardware architecture integrating rANS algorithm with BF16 format distributions, unified division/modulo datapath, two-stage rANS update with byte-level re-normalization, prediction-guided decoding with speculative CDF search, and multi-lane organization with clock gating.

Result: Achieved 121.2x encode and 70.9x decode speedups over Python baseline, reduced decoder binary-search steps from 7.00 to 3.15 (55% fewer), sustained higher compression ratios than classical codecs, and outperformed CPU/GPU implementations.

Conclusion: RAS provides a practical approach to fast neural lossless compression by eliminating bottlenecks in rANS algorithm through specialized hardware architecture.

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [8] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: SilentZNS is a new zone mapping approach for ZNS SSDs that eliminates device-level write amplification and wear by dynamically allocating blocks to zones instead of using fixed physical zones, reducing dummy writes by up to 20x and improving performance by 3.7x.


<details>
  <summary>Details</summary>
Motivation: Current ZNS SSD implementations suffer from device-level write amplification, increased wear, and interference with host I/O due to fixed physical zones and full-zone operations that cause excessive physical writes.

Method: Proposes SilentZNS - a flexible zone allocation scheme that dynamically allocates available blocks to zones, departing from traditional logical-to-physical zone mapping, while maintaining wear-leveling and read performance constraints.

Result: Eliminates dummy writes by up to 20x, reduces device-level write amplification by 86% at 10% zone occupancy, decreases overall wear by up to 76.9%, and accelerates workload execution by up to 3.7x.

Conclusion: SilentZNS effectively addresses the limitations of current ZNS implementations by providing flexible zone allocation that minimizes unnecessary writes and wear while maintaining performance.

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [9] [MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference](https://arxiv.org/abs/2511.05321)
*Maximilian Kirschner,Konstantin Dudzik,Ben Krusekamp,Jürgen Becker*

Main category: cs.AR

TL;DR: A new multi-core vector processor architecture with predictable cores and local scratchpad memories addresses the gap between performance and predictability in real-time systems using neural networks, outperforming single-core designs while maintaining low execution time fluctuations.


<details>
  <summary>Details</summary>
Motivation: Real-time systems using neural networks need high-performance hardware with predictable timing, but current solutions either have limited resources or lack predictability due to memory interference in AI accelerators.

Method: Proposed a multi-core vector processor with predictable cores featuring local scratchpad memories, orchestrated by a central management core that follows a statically determined schedule for shared external memory access.

Result: Configurations with more smaller cores achieved better performance due to increased effective memory bandwidth and higher clock frequencies, while maintaining very low execution time fluctuations.

Conclusion: The architecture successfully bridges the performance-predictability gap for real-time neural network systems, demonstrating that multi-core designs with proper memory management can provide both high performance and timing predictability.

Abstract: Real-time systems, particularly those used in domains like automated driving,
are increasingly adopting neural networks. From this trend arises the need for
high-performance hardware exhibiting predictable timing behavior. While
state-of-the-art real-time hardware often suffers from limited memory and
compute resources, modern AI accelerators typically lack the crucial
predictability due to memory interference.
  We present a new hardware architecture to bridge this gap between performance
and predictability. The architecture features a multi-core vector processor
with predictable cores, each equipped with local scratchpad memories. A central
management core orchestrates access to shared external memory following a
statically determined schedule.
  To evaluate the proposed hardware architecture, we analyze different variants
of our parameterized design. We compare these variants to a baseline
architecture consisting of a single-core vector processor with large vector
registers. We find that configurations with a larger number of smaller cores
achieve better performance due to increased effective memory bandwidth and
higher clock frequencies. Crucially for real-time systems, execution time
fluctuation remains very low, demonstrating the platform's time predictability.

</details>
