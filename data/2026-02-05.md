<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [A-Graph: A Unified Graph Representation for At-Will Simulation across System Stacks](https://arxiv.org/abs/2602.04847)
*Daniel Price,Prabhu Vellaisamy,Patricia Gonzalez,George Michelogiannakis,John P. Shen,Di Wu*

Main category: cs.PF

TL;DR: Agraph is a unified graph representation for cross-stack design space exploration, implemented in the Archx framework for flexible, user-friendly simulation of performance and cost across technologies, architectures, and applications.


<details>
  <summary>Details</summary>
Motivation: The design space for computer systems has become increasingly complex and diverse across technologies, architectures, and applications. Existing EDA tools provide accurate results but don't perform design space exploration, while simulation-based approaches are often domain-specific, inflexible, and require high domain expertise.

Method: Introduces Architecture-Graph (Agraph), a unified graph representation that abstracts system components across application, software, architecture, and circuit stacks. Implements Agraph in the Archx framework with two key features: 1) easy-to-use programming interface for automatic design point generation and sweeping under constraints, and 2) scope-based metric retrieval for analysis at any hierarchy level.

Result: Case studies demonstrate Agraph's generalization across technologies, architectures, and applications with high simulation accuracy. The framework enables flexible exploration of the design space across system stacks.

Conclusion: Agraph and Archx provide a foundation for simulating both performance and cost across diverse system stacks, addressing limitations of existing domain-specific approaches and enabling more comprehensive design space exploration.

Abstract: As computer systems continue to diversify across technologies, architectures, applications, and beyond, the relevant design space has become larger and more complex. Given such trends, design space exploration (DSE) at early stages is critical to ensure agile development towards optimal performance and cost. Industry-grade EDA tools directly take in RTL code and report accurate results, but do not perform DSE. Recent works have attempted to explore the design space via simulation. However, most of these works are domain-specific and constrain the space that users are allowed to explore, offering limited flexibility between technologies, architecture, and applications. Moreover, they often demand high domain expertise to ensure high accuracy. To enable simulation that is agnostic to technology, architecture, and application at any granularity, we introduce Architecture-Graph (Agraph), a graph that unifies the system representation surrounding any arbitrary application, software, architecture, and circuit. Such a unified representation distinguishes Agraph from prior works, which focus on a single stack, allowing users to freely explore the design space across system stacks. To fully unleash the potential of Agraph, we further present Archx, a framework that implements Agraph. Archx is user-friendly in two ways. First, Archx has an easy-to-use programming interface to automatically generate and sweep design points under user constraints, boosting the programmability. Second, Archx adopts scope-based metric retrieval to analyze and understand each design point at any user-preferred hierarchy, enhancing the explainability. We conduct case studies that demonstrate Agraph's generalization across technologies, architecture, and applications with high simulation accuracy. Overall, we argue that Agraph and Archx serve as a foundation to simulate both performance and cost at will.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [2] [Horizon-LM: A RAM-Centric Architecture for LLM Training](https://arxiv.org/abs/2602.04816)
*Zhengqing Yuan,Lichao Sun,Yanfang,Ye*

Main category: cs.OS

TL;DR: Horizon-LM is a memory-centric training system that treats host memory as the primary parameter store and uses GPUs only as transient compute engines, enabling training of 120B parameter models on a single H200 GPU with 1.5TB host RAM.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs has outpaced single-GPU hardware evolution, making model scale constrained by memory rather than computation. Current GPU-centric approaches tie scaling to multi-GPU clusters and create barriers for node-scale post-training workloads like instruction tuning and alignment.

Method: Horizon-LM introduces a CPU-master, GPU-template execution model that eliminates persistent GPU-resident modules and autograd graphs. It uses explicit recomputation with manual gradient propagation and a pipelined double-buffered execution engine to decouple model scale from GPU count.

Result: On a single H200 GPU with 1.5TB host RAM, Horizon-LM trains models up to 120B parameters. On a standard single A100 machine, it achieves up to 12.2× higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness.

Conclusion: Horizon-LM demonstrates that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training, enabling high device utilization and predictable memory growth across platforms and scales.

Abstract: The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [3] [A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective](https://arxiv.org/abs/2602.04035)
*Thomas Neuner,Henriette Padberg,Lior Kornblum,Eilam Yalon,Pedram Khalili Amiri,Shahar Kvatinsky*

Main category: cs.ET

TL;DR: Review of recent advances in stateful and non-stateful logic techniques for processing-in-memory using emerging nonvolatile memory technologies like RRAM, PCM, and MRAM, examining both experimental and simulated designs with focus on reliability and device optimization.


<details>
  <summary>Details</summary>
Motivation: Data-intensive applications are straining conventional computing systems due to the memory wall problem, creating a need for processing-in-memory (PIM) paradigms that minimize data transfer between memory and processing units.

Method: Comprehensive review methodology examining both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies (RRAM, PCM, MRAM). Analysis includes experimentally demonstrated and simulated logic designs, with examination of logic families, memristive device types, reliability metrics, and how each logic family leverages device properties.

Result: Critical examination reveals key challenges in reliability and emphasizes the role of device-level optimization in enabling scalable and commercially viable PIM systems. Comparative analysis of representative device stacks and performance parameters illustrates trade-offs and quality indicators.

Conclusion: The review supports the development of optimized, robust memristive devices for next-generation PIM applications through comprehensive analysis of logic techniques and their implementation using emerging memory technologies.

Abstract: As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported.

</details>


### [4] [The Dynamics of Attention across Automated and Manual Driving Modes: A Driving Simulation Study](https://arxiv.org/abs/2602.04164)
*Yuan Cai,Mustafa Demir,Farzan Sasangohar,Mohsen Zare*

Main category: cs.ET

TL;DR: Study examines how driver visual attention shifts between road, mirrors, HMI, and speedometer across automated, manual, and transition driving modes using eye-tracking in a driving simulator.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in autonomous vehicles regarding driver re-engagement during mode transitions, as past accidents highlight risks of overreliance on automation and the need to understand dynamic attention allocation.

Method: High-fidelity driving simulation with eye-tracking technology measuring fixation duration, fixation count, and time to first fixation across automated, manual, and transition driving modes to assess attention allocation to different areas of interest.

Result: Driver attention varies significantly across modes: manual mode focuses on road, automated mode shows prolonged HMI fixation, and transition phases involve dynamic shifts between environmental and technological elements.

Conclusion: Attention allocation is mode-dependent, informing adaptive HMI designs that present context-relevant information to enhance driver-vehicle interaction, support effective transitions, and improve safety in autonomous vehicles.

Abstract: This study aims to explore the dynamics of driver attention to various zones, including the road, the central mirror, the embedded Human-Machine Interface (HMI), and the speedometer, across different driving modes in AVs. The integration of autonomous vehicles (AVs) into transportation systems has introduced critical safety concerns, particularly regarding driver re-engagement during mode transitions. Past accidents underscore the risks of overreliance on automation and highlight the need to understand dynamic attention allocation to support safety in autonomous driving. A high-fidelity driving simulation was conducted. Eye-tracking technology was used to measure fixation duration, fixation count, and time to first fixation across distinct driving modes (automated, manual, and transition), which were then used to assess how drivers allocated attention to various areas of interest (AOIs). Findings show that drivers' attention varies significantly across driving modes. In manual mode, attention consistently focuses on the road, while in automated mode, prolonged fixation on the embedded HMI was observed. During the handover and takeover phases, attention shifts dynamically between environmental and technological elements. The study reveals that driver attention allocation is mode-dependent. These findings inform the design of adaptive HMIs in AVs that align with drivers' attention patterns. By presenting relevant information according to the driving context, such systems can enhance driver-vehicle interaction, support effective transitions, and improve overall safety. Systematic analysis of visual attention dynamics across driving modes is gaining prominence, as it informs adaptive HMI designs and driver readiness interventions. The GLMM findings can be directly applied to the design of adaptive HMIs or driver training programs to enhance attention and improve safety.

</details>


### [5] [Self-evolving Embodied AI](https://arxiv.org/abs/2602.04411)
*Tongtong Feng,Xin Wang,Wenwu Zhu*

Main category: cs.ET

TL;DR: This paper introduces "self-evolving embodied AI" as a new paradigm where agents autonomously adapt through memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, moving beyond current fixed-embodiment approaches to achieve continual adaptive intelligence.


<details>
  <summary>Details</summary>
Motivation: Current embodied AI systems are limited to human-crafted settings with fixed embodiments and static environments, failing to handle real-world scenarios with variable embodiments and dynamic open environments. The paper aims to address this limitation by proposing a more adaptive approach.

Method: The paper introduces the self-evolving embodied AI paradigm, defining its framework, components, and mechanisms. It includes memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution. The authors systematically review state-of-the-art works for realized components.

Result: The paper presents a comprehensive framework for self-evolving embodied AI, reviews existing works that implement various components of this paradigm, discusses practical applications, and identifies future research directions for achieving autonomous evolution in AI systems.

Conclusion: Self-evolving embodied AI represents a promising new paradigm that enables agents to autonomously learn and interact with environments in a human-like manner, potentially providing a pathway toward general artificial intelligence through continual adaptation and evolution.

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence.

</details>


### [6] [Quantum-Based Resilient Routing in Networks: Minimizing Latency Under Dual-Link Failures](https://arxiv.org/abs/2602.04495)
*Maher Harb,Nader Foroughi,Matt Stehman,Bob Lutz,Nati Erez,Erik Garcell*

Main category: cs.ET

TL;DR: This paper formulates Layer 3 routing optimization as a graph problem to minimize latency and maximize resiliency against dual-link failures, then solves it using QAOA on quantum systems, validating the approach on a 5-vertex test topology.


<details>
  <summary>Details</summary>
Motivation: Network optimization problems have exponentially growing search spaces that become computationally intensive for large networks. The paper aims to address the latency-resilient Layer 3 routing optimization problem in telecommunications networks with predefined optical links, which is challenging due to combinatorial complexity.

Method: The problem is formulated as a graph-based optimization with objectives to minimize latency, create vertex-disjoint paths from each site to the internet backbone, and maximize resiliency against dual-link failures. The authors frame it as finding two disjoint shortest paths with a resiliency component, then adapt the formulation for QAOA (quantum approximate optimization algorithm). They test QAOA on both quantum simulator and quantum hardware using a toy graph with 5 vertices and 7 edges, considering both independent link failures and highly correlated failure scenarios.

Result: Both explored scenarios (independent link failures and highly correlated failure for one pair of edges) produced optimal network designs corresponding to valid solutions with highest frequency of occurrence and minimum energy state. This validates the proposed formulation for optimizing Layer 3 routing on quantum systems.

Conclusion: The paper successfully demonstrates that quantum computing approaches like QAOA can solve complex network optimization problems, specifically Layer 3 routing optimization with latency and resiliency constraints. The validation on a small test topology shows promise for future quantum systems to handle larger-scale telecommunications network optimization problems.

Abstract: Network optimization problems represent large combinatorial search spaces that grow exponentially with network size, making them computationally intensive to solve. This paper addresses the latency-resilient Layer 3 routing optimization problem in telecommunications networks with predefined Layer 1 optical links. We formulate this problem as a graph-based optimization problem with the objective of minimizing latency, creating vertex-disjoint paths from each site to the internet backbone, and maximizing overall resiliency by limiting the impact of dual-link failures. By framing the problem as finding two disjoint shortest paths, coupled together with a resiliency component to the objective function, we establish a single formulation to produce optimal path design. The mathematical formulation was adapted to solve the problem using quantum approximate optimization algorithm (QAOA) executed over both quantum simulator and quantum hardware. QAOA was tested on a toy graph topology with 5 vertices and 7 edges and considering two limiting scenarios respectively representing independent (uncorrelated) link failures and highly correlated failure for one pair of edges. Both explored scenarios produced the optimal network design-corresponding to the valid solution with highest frequency of occurrence and minimum energy state, hence, validating the proposed formulation for optimizing Layer 3 routing on quantum systems of the future.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: Conflict-obstruction-freedom generalizes obstruction-freedom by allowing progress when only commuting operations contend, but conflict-obstruction-free universal constructions are impossible in asynchronous read-write memory, revealing fundamental synchronization costs for conflicting operations.


<details>
  <summary>Details</summary>
Motivation: Motivated by the observation that commuting operations can execute in parallel, the paper aims to develop progress conditions that leverage commutativity to allow more parallelism while maintaining linearizability.

Method: Introduces conflict-obstruction-freedom: a process completes if it runs long enough without encountering step contention with conflicting (non-commuting) operations. Studies this condition in the context of commutativity-aware, linearizable implementations of shared objects.

Result: Proves that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model, exposing a fundamental limitation where conflicting operations impose synchronization costs.

Conclusion: The impossibility result reveals that conflict-aware universal constructions face inherent synchronization costs when conflicting operations are invoked, requiring eventual resolution of pending conflicts for progress.

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [8] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: GPU offloading of 5G LDPC decoding achieves 6× speedup over CPU, keeping within 0.5ms slot requirements while consuming minimal additional power.


<details>
  <summary>Details</summary>
Motivation: LDPC decoding is computationally intensive in 5G NR and must complete within 0.5ms. Many systems still use general-purpose CPUs, risking missed-slot events and limited scalability as bandwidth and user multiplexing increase.

Method: Empirical evaluation using NVIDIA Sionna PHY/SYS on TensorFlow to construct NR-like link-level chain with LDPC5G encoder/decoder, 16-QAM modulation, and AWGN. Swept parallel codewords and belief-propagation iterations, measuring decoding phase timing, CPU/GPU utilization, and power.

Result: Average 6× GPU/CPU throughput speedup. CPU latency reached ≈0.71ms at 20 iterations (exceeding 0.5ms slot), while GPU remained within 6-24% of slot. CPU decoding consumed ~10 Grace cores, GPU decoding added only 10-15W over idle while leaving CPU capacity for higher-layer tasks.

Conclusion: GPU offloading provides significant performance benefits for 5G LDPC decoding, meeting timing requirements with minimal power overhead. Results are conservative lower bounds using high-level Sionna layers, providing reusable methodology for evaluating physical-layer kernels on future platforms.

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [9] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: CONFINE is a secrecy-preserving approach for inter-organizational process mining using Trusted Execution Environments (TEEs) to securely analyze multi-party event logs while maintaining data confidentiality across organizational boundaries.


<details>
  <summary>Details</summary>
Motivation: Real-world business processes often span multiple independent organizations, but inter-organizational process mining faces confidentiality challenges. Organizations may not consent to disclose sensitive process data to each other or third-party service providers, limiting the applicability of traditional process mining techniques.

Method: CONFINE uses Trusted Execution Environments (TEEs) to deploy trusted applications that securely process multi-party event logs. It employs a four-stage protocol for protected data exchange and processing, with a segmentation-based strategy to handle memory limitations by transmitting event logs in smaller batches.

Result: The approach successfully handles realistic workloads with logarithmic memory growth relative to event log size and linear growth with the number of organizations. Formal verification confirms correctness, and security analysis validates TEE core guarantees.

Conclusion: CONFINE enables secure inter-organizational process mining while preserving data secrecy, demonstrating scalability and providing a practical solution for confidential multi-party process analysis with opportunities for further optimization.

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM is a new cache prefetching approach that combines online-learning access-map patterns with confidence-throttled speculative lookahead to address limitations of SPP and AMPM prefetchers.


<details>
  <summary>Details</summary>
Motivation: The performance gap between processor speed and memory system limits workload performance. Existing prefetchers like SPP and AMPM have limitations: SPP is susceptible to reference reordering in higher-level caches and OoO cores, while AMPM cannot speculate beyond its region patterns.

Method: SPPAM uses online-learning to build a set of access-map patterns, which are then used in a speculative lookahead mechanism that is throttled by a confidence metric. It targets the second-level cache.

Result: SPPAM alongside state-of-the-art prefetchers Berti and Bingo improves system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

Conclusion: SPPAM successfully addresses limitations of prior prefetching approaches by combining pattern learning with confidence-throttled speculation, achieving significant performance improvements over existing state-of-the-art prefetchers.

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [11] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia is an algorithm-hardware co-design framework that enables all-layer block floating point (BFP) activations for LLMs, achieving significant efficiency gains through systematic BFP configuration exploration, asymmetric bit allocation for KV-cache compression, and dedicated hardware components.


<details>
  <summary>Details</summary>
Motivation: LLMs have high memory and computation costs. While INT weights and FP activations are common, prior BFP approaches for activations only work in linear layers and fail in attention layers due to accuracy degradation, limiting overall efficiency improvements.

Method: 1) Systematic exploration of BFP configurations for better accuracy-compression trade-off across all layers; 2) Asymmetric bit-allocation strategy with hybrid offline-online outlier smoothing for aggressive KV-cache compression; 3) Dedicated hardware design with reconfigurable PE supporting mixed formats, real-time FP16-to-BFP converter, and tiling-aware dataflow.

Result: Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average compared to prior works across eight widely used LLMs, with only 0.3% average accuracy loss from aggressive KV-cache compression.

Conclusion: Harmonia successfully enables all-layer BFP activations through algorithm-hardware co-design, overcoming previous limitations in attention layers and achieving substantial efficiency improvements for LLM deployment while maintaining accuracy.

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>
