{"id": "2601.08025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08025", "abs": "https://arxiv.org/abs/2601.08025", "authors": ["Adiba Masud", "Nicholas Foley", "Pragathi Durga Rajarajan", "Palden Lama"], "title": "Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference", "comment": null, "summary": "The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior research has largely focused on single-objective optimization, such as minimizing latency or maximizing throughput. This paper challenges that view by reframing DNN partitioning as a multi-objective optimization problem. We argue that in real-world scenarios, a complex trade-off between latency and throughput exists, which is further complicated by network variability. To address this, we introduce ParetoPipe, an open-source framework that leverages Pareto front analysis to systematically identify optimal partitioning strategies that balance these competing objectives.\n  Our contributions are threefold: we benchmark pipeline partitioned inference on a heterogeneous testbed of Raspberry Pis and a GPU-equipped edge server; we identify Pareto-optimal points to analyze the latency-throughput trade-off under varying network conditions; and we release a flexible, open-source framework to facilitate distributed inference and benchmarking. This toolchain features dual communication backends, PyTorch RPC and a custom lightweight implementation, to minimize overhead and support broad experimentation.", "AI": {"tldr": "ParetoPipe is an open-source framework that treats DNN partitioning as a multi-objective optimization problem, balancing latency and throughput trade-offs using Pareto front analysis for deployment on heterogeneous edge devices.", "motivation": "Existing DNN partitioning approaches focus on single-objective optimization (e.g., minimizing latency OR maximizing throughput), ignoring the complex real-world trade-offs between these competing objectives, especially under variable network conditions on resource-constrained edge devices.", "method": "Introduces ParetoPipe framework that uses Pareto front analysis to systematically identify optimal partitioning strategies. Features dual communication backends (PyTorch RPC and custom lightweight implementation) to minimize overhead, and benchmarks pipeline partitioned inference on heterogeneous testbed of Raspberry Pis and GPU-equipped edge server.", "result": "Benchmarks show pipeline partitioned inference performance on heterogeneous edge devices; identifies Pareto-optimal points that reveal latency-throughput trade-offs under varying network conditions; provides flexible open-source framework for distributed inference and benchmarking.", "conclusion": "DNN partitioning should be treated as a multi-objective optimization problem rather than single-objective. ParetoPipe provides a practical framework for systematically exploring latency-throughput trade-offs in real-world edge deployments with network variability."}}
{"id": "2601.08277", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08277", "abs": "https://arxiv.org/abs/2601.08277", "authors": ["Yizhuo Rao", "Xingjian Cui", "Jiabin Xie", "Shangzhi Pang", "Guangnan Feng", "Jinhui Wei", "Zhiguang Chen", "Yutong Lu"], "title": "Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations", "comment": "Accepted for publication at EuroSys 2026", "summary": "Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.\n  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.\n  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\\times$ over the baseline and $2.0\\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\\%$ of theoretical CPU peak performance, nearly $2.8\\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.", "AI": {"tldr": "MatrixPIC redesigns PIC simulation deposition using CPU Matrix Processing Units, achieving up to 2.63\u00d7 speedup and 8.7\u00d7 kernel acceleration over baselines.", "motivation": "PIC simulations bottleneck on particle-grid interactions with atomic updates on CPUs. New CPU architectures with Matrix Processing Units (MPUs) offer opportunities to overcome this limitation through matrix outer-product operations.", "method": "MatrixPIC co-designs deposition kernel, data layout, and incremental particle sorting for hybrid MPU-VPU SIMD model. Features: (1) block-matrix formulation mapping to MPU outer-product primitives, (2) hybrid execution pipeline combining MPU accumulation with VPU data preparation, (3) O(1)-amortized incremental sorter using gapped packed-memory array.", "result": "Achieves up to 2.63\u00d7 total runtime speedup in LWFA simulations. Core kernel accelerated 8.7\u00d7 over baseline, 2.0\u00d7 over best hand-optimized VPU implementation. Reaches 83.08% of theoretical CPU peak performance, nearly 2.8\u00d7 higher than optimized CUDA kernel on data center GPU.", "conclusion": "Matrix-oriented co-design effectively accelerates PIC simulations on emerging CPU architectures with integrated MPUs, demonstrating significant performance gains over traditional approaches."}}
{"id": "2601.08374", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.08374", "abs": "https://arxiv.org/abs/2601.08374", "authors": ["Dali Chang", "Chong Zhang", "Kaiqi Zhang", "Mingguan Yang", "Huiyuan Li", "Weiqiang Kong"], "title": "Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity", "comment": null, "summary": "In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance \"sweet spot\" to anomalously remain at the low order of $p \\approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance \"sweet spot\" to the higher-order region of $p \\ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.", "AI": {"tldr": "Optimized matrix-free operator for high-order elasticity FEM shifts performance sweet spot from p\u22482 to p\u22656 via tensor factorization, Voigt symmetry, and kernel fusion, achieving 7-83x kernel speedups and 3.6-16.8x end-to-end improvements.", "motivation": "Existing matrix-free methods for high-order elasticity FEM fail to exploit modern CPU architectures and tensor-product structures, causing performance sweet spots to remain at low orders (p\u22482), limiting the potential of high-order methods.", "method": "Multi-level optimization strategy: 1) Replace O(p\u2076) generic algorithm with efficient O(p\u2074) tensor factorization, 2) Exploit Voigt symmetry to reduce redundant computations for elasticity, 3) Employ macro-kernel fusion to enhance data locality and break memory bandwidth bottleneck, implemented within MFEM framework with Geometric Multigrid preconditioner integration.", "result": "Successfully shifts performance sweet spot to higher-order region (p\u22656). Optimized core operator achieves 7x to 83x speedups over MFEM baseline, translating to 3.6x to 16.8x end-to-end performance improvement in complete solution process on mainstream x86 and ARM architectures.", "conclusion": "Provides validated and efficient practical path for large-scale, high-order elasticity simulations on mainstream CPU hardware by overcoming memory bottlenecks and fully exploiting modern CPU architectures through algorithmic and implementation optimizations."}}
{"id": "2601.08099", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.08099", "abs": "https://arxiv.org/abs/2601.08099", "authors": ["Andrew Adamatzky"], "title": "Directional Electrical Spiking, Bursting, and Information Propagation in Oyster Mycelium Recorded with a Star-Shaped Electrode Array", "comment": null, "summary": "Electrical activity in fungal mycelium has been reported in numerous species and experimental contexts, yet its spatial organisation and propagation remain insufficiently characterised. In this study we investigate the spatiotemporal structure of electrical potential dynamics in oyster mushroom (\\textit{Pleurotus ostreatus}) mycelium colonising a wood-shavings substrate. Electrical signals were recorded using an eight-channel star-shaped differential electrode array providing angular resolution around a central region of colonised substrate. We analyse spike statistics, bursting behaviour, inter-channel correlations, and event-based propagation delays. The results reveal strong directional heterogeneity in spiking frequency and amplitude, clustered bursting dynamics, partial and localised coupling between channels, and reproducible propagation patterns across spatial sectors. Electrical bursts originate preferentially in specific directions and recruit other regions with with characteristic delays ranging from seconds to minutes to hours. These findings support the interpretation of fungal mycelium as a spatially extended excitable medium capable of slow, distributed electrical signalling and signal integration.", "AI": {"tldr": "Fungal mycelium shows organized electrical activity with directional heterogeneity, clustered bursting, and reproducible propagation patterns, suggesting it functions as an extended excitable medium for slow electrical signaling.", "motivation": "To characterize the spatial organization and propagation of electrical activity in fungal mycelium, which has been reported but insufficiently studied in terms of spatiotemporal structure.", "method": "Used an eight-channel star-shaped differential electrode array to record electrical signals from oyster mushroom mycelium colonizing wood-shavings substrate, analyzing spike statistics, bursting behavior, inter-channel correlations, and event-based propagation delays.", "result": "Revealed strong directional heterogeneity in spiking frequency/amplitude, clustered bursting dynamics, partial localized coupling between channels, and reproducible propagation patterns across spatial sectors with characteristic delays from seconds to hours.", "conclusion": "Fungal mycelium functions as a spatially extended excitable medium capable of slow, distributed electrical signaling and signal integration, with organized propagation patterns."}}
{"id": "2601.08368", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.08368", "abs": "https://arxiv.org/abs/2601.08368", "authors": ["Marie Bolzer", "S\u00e9bastien Duval", "Marine Minier"], "title": "A New Tool to Find Lightweight (And, Xor) Implementations of Quadratic Vectorial Boolean Functions up to Dimension 9", "comment": null, "summary": "The problem of finding a minimal circuit to implement a given function is one of the oldest in electronics. It is known to be NP-hard. Still, many tools exist to find sub-optimal circuits to implement a function. In electronics, such tools are known as synthesisers. However, these synthesisers aim to implement very large functions (a whole electronic chip). In cryptography, the focus is on small functions, hence the necessity for new dedicated tools for small functions. Several tools exist to implement small functions. They differ by their algorithmic approach (some are based on Depth-First-Search as introduced by Ullrich in 2011, some are based on SAT-solvers like the tool desgined by Stoffelen in 2016, some non-generic tools use subfield decomposition) and by their optimisation criteria (some optimise for circuit size, others for circuit depth, and some for side-channel-protected implementations). However, these tools are limited to functions operating on less than 5 bits, sometimes 6 bits for quadratic functions, or to very simple functions. The limitation lies in a high computing time. We propose a new tool (The tool is provided alongside the IEEE article with CodeOcean and at https://github.com/seduval/implem-quad-sbox) to implement quadratic functions up to 9 bits within AND-depth 1, minimising the number of AND gates. This tool is more time-efficient than previous ones, allowing to explore larger implementations than others on 6 bits or less and allows to reach larger sizes, up to 9 bits.", "AI": {"tldr": "A new tool for implementing quadratic functions up to 9 bits with AND-depth 1, minimizing AND gates, overcoming previous size limitations of 5-6 bits.", "motivation": "Existing circuit synthesis tools for cryptography are limited to small functions (5-6 bits) due to high computational time, creating a need for dedicated tools that can handle larger quadratic functions efficiently.", "method": "Developed a new tool (available on GitHub and CodeOcean) that implements quadratic functions up to 9 bits within AND-depth 1 while minimizing the number of AND gates, using a more time-efficient approach than previous methods.", "result": "The tool can explore larger implementations than previous tools (6 bits or less) and can handle functions up to 9 bits, making it more time-efficient and capable of reaching larger sizes than existing solutions.", "conclusion": "The proposed tool addresses the limitation of existing synthesis tools for small cryptographic functions by enabling efficient implementation of quadratic functions up to 9 bits with AND-depth 1 optimization."}}
{"id": "2601.08539", "categories": ["cs.PF", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08539", "abs": "https://arxiv.org/abs/2601.08539", "authors": ["Jeffrey Spaan", "Kuan-Hsun Chen", "Ana-Lucia Varbanescu"], "title": "Reducing Compute Waste in LLMs through Kernel-Level DVFS", "comment": null, "summary": "The rapid growth of AI has fueled the expansion of accelerator- or GPU-based data centers. However, the rising operational energy consumption has emerged as a critical bottleneck and a major sustainability concern. Dynamic Voltage and Frequency Scaling (DVFS) is a well-known technique used to reduce energy consumption, and thus improve energy-efficiency, since it requires little effort and works with existing hardware. Reducing the energy consumption of training and inference of Large Language Models (LLMs) through DVFS or power capping is feasible: related work has shown energy savings can be significant, but at the cost of significant slowdowns. In this work, we focus on reducing waste in LLM operations: i.e., reducing energy consumption without losing performance. We propose a fine-grained, kernel-level, DVFS approach that explores new frequency configurations, and prove these save more energy than previous, pass- or iteration-level solutions. For example, for a GPT-3 training run, a pass-level approach could reduce energy consumption by 2% (without losing performance), while our kernel-level approach saves as much as 14.6% (with a 0.6% slowdown). We further investigate the effect of data and tensor parallelism, and show our discovered clock frequencies translate well for both. We conclude that kernel-level DVFS is a suitable technique to reduce waste in LLM operations, providing significant energy savings with negligible slow-down.", "AI": {"tldr": "This paper proposes a kernel-level DVFS approach for LLMs that achieves significant energy savings (up to 14.6%) with minimal performance loss, outperforming previous pass-level methods.", "motivation": "The rapid growth of AI and GPU-based data centers has led to unsustainable operational energy consumption. While DVFS can reduce energy consumption, existing approaches for LLMs achieve only modest savings with significant slowdowns. The authors aim to reduce energy waste in LLM operations without sacrificing performance.", "method": "The authors propose a fine-grained, kernel-level DVFS approach that explores new frequency configurations. This method operates at a more granular level than previous pass- or iteration-level solutions, allowing for more precise energy optimization during LLM training and inference operations.", "result": "The kernel-level DVFS approach achieves up to 14.6% energy savings for GPT-3 training with only 0.6% slowdown, significantly outperforming pass-level approaches that achieve only 2% savings. The method also translates well across different data and tensor parallelism configurations.", "conclusion": "Kernel-level DVFS is an effective technique for reducing energy waste in LLM operations, providing substantial energy savings with negligible performance impact, making it suitable for sustainable AI infrastructure."}}
{"id": "2601.08800", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08800", "abs": "https://arxiv.org/abs/2601.08800", "authors": ["Bowen Zhou", "Jinrui Jia", "Wenhao He", "Yong Zhang", "Fang Dong"], "title": "MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm", "comment": "Submitted to ICDCS 2026", "summary": "The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.\n  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.", "AI": {"tldr": "MixServe is an automatic distributed serving system for Mixture of Experts (MoE) models that uses a novel TP-EP hybrid parallelism with fused AR-A2A communication to optimize inference performance by reducing communication bottlenecks.", "motivation": "Current distributed MoE models face communication bottlenecks: tensor parallelism (TP) has low inter-node efficiency, and expert parallelism (EP) suffers from load imbalance, especially at high parallel degrees. These limitations hinder efficient deployment of large MoE models across multi-GPU/multi-node systems.", "method": "MixServe automatically evaluates communication overhead of parallel strategies based on model hyperparameters and hardware configurations, then selects optimal strategy. It introduces TP-EP hybrid parallelism with fused all-reduce/all-to-all communication that overlaps intra-node AR and inter-node A2A communication.", "result": "Experiments on DeepSeek-R1 and Qwen3 models show MixServe achieves 1.08~3.80x acceleration in time to first token, 1.03~1.66x acceleration in inter-token latency, and 5.2%~50.3% throughput improvement compared to existing approaches.", "conclusion": "MixServe provides an effective solution for distributed serving of MoE models by addressing communication bottlenecks through automatic strategy selection and innovative hybrid parallelism with fused communication, significantly improving inference performance across key metrics."}}
{"id": "2601.08687", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.08687", "abs": "https://arxiv.org/abs/2601.08687", "authors": ["Marco Tonnarelli", "Filippo Scaramuzza", "Simon Harrer", "Linus W. Dietz"], "title": "Data Product MCP: Chat with your Enterprise Data", "comment": "7 pages, preprint", "summary": "Computational data governance aims to make the enforcement of governance policies and legal obligations more efficient and reliable. Recent advances in natural language processing and agentic AI offer ways to improve how organizations share and use data. But many barriers remain. Today's tools require technical skills and multiple roles to discover, request, and query data. Automating data access using enterprise AI agents is limited by the means to discover and autonomously access distributed data. Current solutions either compromise governance or break agentic workflows through manual approvals. To close this gap, we introduce Data Product MCP integrated in a data product marketplace. This data marketplace, already in use at large enterprises, enables AI agents to find, request, and query enterprise data products while enforcing data contracts in real time without lowering governance standards. The system is built on the Model Context Protocol (MCP) and links the AI-driven marketplace with cloud platforms such as Snowflake, Databricks, and Google Cloud Platform. It supports semantic discovery of data products based on business context, automates access control by validating generated queries against approved business purposes using AI-driven checks, and enforces contracts in real time by blocking unauthorized queries before they run. We assessed the system with feedback from $n=16$ experts in data governance. Our qualitative evaluation demonstrates effectiveness through enterprise scenarios such as customer analytics. The findings suggest that Data Product MCP reduces the technical burden for data analysis without weakening governance, filling a key gap in enterprise AI adoption.", "AI": {"tldr": "Data Product MCP integrates data product marketplace with AI agents to automate data discovery and access while maintaining governance standards through real-time contract enforcement.", "motivation": "Current data governance tools require technical skills and manual approvals, creating barriers for AI agents to autonomously access distributed data while maintaining governance standards.", "method": "Built on Model Context Protocol (MCP), integrates AI-driven data marketplace with cloud platforms (Snowflake, Databricks, GCP), enabling semantic discovery, automated access control with AI-driven checks, and real-time contract enforcement.", "result": "Qualitative evaluation with 16 data governance experts shows effectiveness in enterprise scenarios like customer analytics, reducing technical burden without weakening governance.", "conclusion": "Data Product MCP fills a key gap in enterprise AI adoption by enabling AI agents to autonomously access data while maintaining governance standards through automated, real-time enforcement."}}
