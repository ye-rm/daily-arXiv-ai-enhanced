{"id": "2510.19765", "categories": ["cs.OS", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19765", "abs": "https://arxiv.org/abs/2510.19765", "authors": ["Vinay Banakar", "Suli Yang", "Kan Wu", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau", "Kimberly Keeton"], "title": "Tidying Up the Address Space", "comment": null, "summary": "Memory tiering in datacenters does not achieve its full potential due to\nhotness fragmentation -- the intermingling of hot and cold objects within\nmemory pages. This fragmentation prevents page-based reclamation systems from\ndistinguishing truly hot pages from pages containing mostly cold objects,\nfundamentally limiting memory efficiency despite highly skewed accesses. We\nintroduce address-space engineering: dynamically reorganizing application\nvirtual address spaces to create uniformly hot and cold regions that any\npage-level tiering backend can manage effectively. HADES demonstrates this\nfrontend/backend approach through a compiler-runtime system that tracks and\nmigrates objects based on access patterns, requiring minimal developer\nintervention. Evaluations across ten data structures achieve up to 70% memory\nreduction with 3% performance overhead, showing that address space engineering\nenables existing reclamation systems to reclaim memory aggressively without\nperformance degradation.", "AI": {"tldr": "HADES introduces address-space engineering to solve hotness fragmentation in memory tiering by reorganizing virtual address spaces to create uniformly hot/cold regions, enabling existing page-level tiering systems to reclaim memory more effectively.", "motivation": "Current memory tiering in datacenters suffers from hotness fragmentation - the mixing of hot and cold objects within memory pages, which prevents page-based reclamation systems from accurately identifying and managing truly hot pages, limiting memory efficiency despite skewed access patterns.", "method": "HADES uses a compiler-runtime system that implements address-space engineering by dynamically reorganizing application virtual address spaces. It tracks and migrates objects based on access patterns to create uniformly hot and cold regions, requiring minimal developer intervention.", "result": "Evaluations across ten data structures show up to 70% memory reduction with only 3% performance overhead, demonstrating that address space engineering enables existing reclamation systems to reclaim memory aggressively without performance degradation.", "conclusion": "Address-space engineering effectively solves hotness fragmentation in memory tiering, allowing page-level tiering backends to manage memory more efficiently by creating uniformly hot and cold regions, achieving significant memory savings with minimal performance impact."}}
{"id": "2510.19660", "categories": ["cs.ET", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.19660", "abs": "https://arxiv.org/abs/2510.19660", "authors": ["Andreas Mershin", "Nikolas Stefanou", "Adan Rotteveel", "Matthew Kung", "George Kung", "Alexandru Dan", "Howard Kivell", "Zoia Okulova", "Zoi Kountouri", "Paul Pu Liang"], "title": "Machine Olfaction and Embedded AI Are Shaping the New Global Sensing Industry", "comment": "23 pages, 116 citations, combination tech review/industry\n  roadmap/white paper on the rise of machine olfaction as an essential AI\n  modality", "summary": "Machine olfaction is rapidly emerging as a transformative capability, with\napplications spanning non-invasive medical diagnostics, industrial monitoring,\nagriculture, and security and defense. Recent advances in stabilizing mammalian\nolfactory receptors and integrating them into biophotonic and bioelectronic\nsystems have enabled detection at near single-molecule resolution thus placing\nmachines on par with trained detection dogs. As this technology converges with\nmultimodal AI and distributed sensor networks imbued with embedded AI, it\nintroduces a new, biochemical layer to a sensing ecosystem currently dominated\nby machine vision and audition. This review and industry roadmap surveys the\nscientific foundations, technological frontiers, and strategic applications of\nmachine olfaction making the case that we are currently witnessing the rise of\na new industry that brings with it a global chemosensory infrastructure. We\ncover exemplary industrial, military and consumer applications and address some\nof the ethical and legal concerns arising. We find that machine olfaction is\npoised to bring forth a planet-wide molecular awareness tech layer with the\npotential of spawning vast emerging markets in health, security, and\nenvironmental sensing via scent.", "AI": {"tldr": "Machine olfaction is emerging as a transformative technology with applications in medical diagnostics, industrial monitoring, agriculture, and security, potentially creating a global chemosensory infrastructure.", "motivation": "The motivation is to survey the scientific foundations, technological frontiers, and strategic applications of machine olfaction, highlighting its potential to create a new industry and global sensing ecosystem.", "method": "This is a review and industry roadmap that analyzes recent advances in stabilizing mammalian olfactory receptors and integrating them into biophotonic and bioelectronic systems, along with convergence with multimodal AI and distributed sensor networks.", "result": "The technology has enabled detection at near single-molecule resolution, placing machines on par with trained detection dogs, and is poised to create a planet-wide molecular awareness tech layer.", "conclusion": "Machine olfaction is positioned to spawn vast emerging markets in health, security, and environmental sensing via scent, though ethical and legal concerns need to be addressed."}}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure.", "AI": {"tldr": "CodeCRDT enables parallel LLM agent coordination through shared state monitoring using CRDTs instead of explicit messaging, achieving up to 21.1% speedup with 100% convergence but showing variable performance depending on task structure.", "motivation": "Multi-agent LLM systems currently fail to achieve parallel speedups due to expensive coordination overhead from explicit message passing between agents.", "method": "CodeCRDT uses observation-driven coordination with Conflict-Free Replicated Data Types (CRDTs), allowing agents to monitor shared state with observable updates and deterministic convergence rather than explicit message passing.", "result": "Evaluation across 600 trials showed up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, with 100% convergence and zero merge failures. Semantic conflict rates were 5-10%.", "conclusion": "Observation-driven coordination with CRDTs enables lock-free, conflict-free concurrent code generation with strong eventual consistency, but performance varies based on task structure, revealing quality-performance tradeoffs."}}
{"id": "2510.19577", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.19577", "abs": "https://arxiv.org/abs/2510.19577", "authors": ["Zuoming Fu", "Alex Manley", "Mohammad Alian"], "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration", "comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea", "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction.", "AI": {"tldr": "This paper presents gem5 Co-Pilot, an AI agent that uses Large Language Models to automate design space exploration for computer architecture using the gem5 simulator, featuring a GUI interface and database system for efficient parameter optimization.", "motivation": "Computer architecture design space exploration is complex and time-consuming, requiring analysis of numerous parameters and simulation statistics. The emergence of LLMs offers potential to accelerate text analysis and decision-making in this domain.", "method": "Built gem5 Co-Pilot with webpage GUI, implemented a design space exploration language and Design Space Database (DSDB), creating a Retrieval Augmented Generation system for gem5 optimization.", "result": "The system successfully identified optimal parameters for specific design constraints across four cost ranges, outperforming two baseline models with limited user interaction required.", "conclusion": "gem5 Co-Pilot effectively automates design space exploration using LLMs, enabling quick identification of optimal parameters based on performance and cost constraints while reducing user effort."}}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators.", "AI": {"tldr": "AI-driven distributed systems policy design using LLM-generated code and deterministic verification in a domain-specific simulator, applied to FaaS scheduler optimization.", "motivation": "To automate and scale the design of distributed systems policies by combining the creative generation capabilities of LLMs with rigorous verification methods.", "method": "Iterative generate-and-verify loop: LLM proposes Python policies, simulator evaluates on standardized traces, structured feedback guides subsequent generations using Bauplan FaaS runtime and Eudoxia simulator.", "result": "Preliminary results show throughput improvements across multiple models, demonstrating the approach's effectiveness for scheduler optimization.", "conclusion": "The methodology shows promise for automated policy design and AI could help bootstrap new simulators to scale this approach further."}}
{"id": "2510.19012", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed.", "AI": {"tldr": "Comparative analysis of Apache Spark performance across Java, Python, and Scala for ETL workflows with Apache Iceberg, showing language-dependent performance variations based on data size and operation complexity.", "motivation": "To address the limited comprehensive comparisons of full ETL workflows across programming languages using Apache Iceberg, and to understand how programming language choice affects Apache Spark performance for different data processing scenarios.", "method": "Executed comparative analysis by running ETL operations including data download from CSV files, transformation, and loading into Apache Iceberg tables using Apache Spark in Java, Python, and Scala programming languages, testing with different data sizes (5MB and 1.6GB CSV files) and complex operations combining multiple files.", "result": "Python performed best with small data (5MB: 6.71s vs Scala 9.13s, Java 9.62s), all languages showed similar performance with large data (1.6GB: Python 46.34s, Scala 47.72s, Java 50.56s), and Scala was fastest for complex operations (374.42s vs Java 379.8s, Python 398.32s).", "conclusion": "Programming language significantly impacts Apache Spark efficiency: Python excels with small datasets, while Scala and Java are more productive for large data volumes and complex operations, providing optimization guidance based on specific performance requirements and data processing needs."}}
{"id": "2510.19151", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19151", "abs": "https://arxiv.org/abs/2510.19151", "authors": ["Seri Khoury", "Manish Purohit", "Aaron Schild", "Joshua Wang"], "title": "On the Randomized Locality of Matching Problems in Regular Graphs", "comment": "DISC 2025. Abstract modified for arXiv", "summary": "The main goal in distributed symmetry-breaking is to understand the locality\nof problems; i.e., the radius of the neighborhood that a node needs to explore\nin order to arrive at its part of a global solution. In this work, we study the\nlocality of matching problems in the family of regular graphs, which is one of\nthe main benchmarks for establishing lower bounds on the locality of\nsymmetry-breaking problems, as well as for obtaining classification results.\nFor approximate matching, we develop randomized algorithms to show that $(1 +\n\\epsilon)$-approximate matching in regular graphs is truly local; i.e., the\nlocality depends only on $\\epsilon$ and is independent of all other graph\nparameters. Furthermore, as long as the degree $\\Delta$ is not very small\n(namely, as long as $\\Delta \\geq \\text{poly}(1/\\epsilon)$), this dependence is\nonly logarithmic in $1/\\epsilon$. This stands in sharp contrast to maximal\nmatching in regular graphs which requires some dependence on the number of\nnodes $n$ or the degree $\\Delta$. We show matching lower bounds for both\nresults. For maximal matching, our techniques further allow us to establish a\nstrong separation between the node-averaged complexity and worst-case\ncomplexity of maximal matching in regular graphs, by showing that the former is\nonly $O(1)$. Central to our main technical contribution is a novel\nmartingale-based analysis for the $\\approx 40$-year-old algorithm by Luby. In\nparticular, our analysis shows that applying one round of Luby's algorithm on\nthe line graph of a $\\Delta$-regular graph results in an almost\n$\\Delta/2$-regular graph.", "AI": {"tldr": "The paper studies the locality of matching problems in regular graphs, showing that (1+\u03b5)-approximate matching has locality depending only on \u03b5 (logarithmic in 1/\u03b5 for sufficiently large \u0394), while maximal matching requires dependence on n or \u0394. It establishes a strong separation between node-averaged and worst-case complexity for maximal matching, with the former being O(1).", "motivation": "To understand the locality of symmetry-breaking problems in distributed computing, particularly matching problems in regular graphs, which serve as benchmarks for establishing lower bounds and classification results.", "method": "Developed randomized algorithms for approximate matching and used novel martingale-based analysis of Luby's algorithm (applied to line graphs of regular graphs) to show that one round produces an almost \u0394/2-regular graph.", "result": "Showed that (1+\u03b5)-approximate matching in regular graphs has locality depending only on \u03b5 (logarithmic in 1/\u03b5 when \u0394 \u2265 poly(1/\u03b5)), while maximal matching requires dependence on n or \u0394. Established that node-averaged complexity of maximal matching is O(1) vs worst-case complexity.", "conclusion": "There is a fundamental difference between approximate and maximal matching in regular graphs regarding locality requirements, with approximate matching being truly local while maximal matching requires global parameters. The node-averaged complexity of maximal matching is significantly better than worst-case complexity."}}
{"id": "2510.19225", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19225", "abs": "https://arxiv.org/abs/2510.19225", "authors": ["Yongji Wu", "Xueshen Liu", "Haizhong Zheng", "Juncheng Gu", "Beidi Chen", "Z. Morley Mao", "Arvind Krishnamurthy", "Ion Stoica"], "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources.", "AI": {"tldr": "RLBoost is a hybrid architecture that efficiently uses preemptible GPU resources for RL training by dynamically offloading rollout workloads, enabling cost-efficient training with 28%-49% cost savings and 1.51x-1.97x throughput improvement.", "motivation": "Existing RL frameworks struggle with resource tension between rollout (scales efficiently) and training (requires tightly-coupled GPUs), while preemptible GPU resources offer cost-saving opportunities if efficiently utilized for rollout.", "method": "RLBoost uses a hybrid architecture with three key techniques: adaptive rollout offload to dynamically adjust workloads, pull-based weight transfer for quick instance provisioning, and token-level response collection/migration for preemption handling and load balancing.", "result": "Extensive experiments show RLBoost increases training throughput by 1.51x-1.97x while improving cost efficiency by 28%-49% compared to using only on-demand GPU resources.", "conclusion": "RLBoost successfully addresses the resource tension in RL workflows by efficiently harvesting preemptible GPU resources for rollout, achieving significant cost savings and throughput improvements."}}
{"id": "2510.19262", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19262", "abs": "https://arxiv.org/abs/2510.19262", "authors": ["Heng Xu", "Zhiwei Yu", "Chengze Du", "Ying Zhou", "Letian Li", "Haojie Wang", "Weiqiang Cheng", "Jialong Li"], "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training", "comment": null, "summary": "Training Mixture-of-Experts (MoE) models introduces sparse and highly\nimbalanced all-to-all communication that dominates iteration time. Conventional\nload-balancing methods fail to exploit the deterministic topology of Rail\narchitectures, leaving multi-NIC bandwidth underutilized. We present RailS, a\ndistributed load-balancing framework that minimizes all-to-all completion time\nin MoE training. RailS leverages the Rail topology's symmetry to prove that\nuniform sending ensures uniform receiving, transforming global coordination\ninto local scheduling. Each node independently executes a Longest Processing\nTime First (LPT) spraying scheduler to proactively balance traffic using local\ninformation. RailS activates N parallel rails for fine-grained, topology-aware\nmultipath transmission. Across synthetic and real-world MoE workloads, RailS\nimproves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For\nMixtral workloads, it shortens iteration time by 18%--40% and achieves\nnear-optimal load balance, fully exploiting architectural parallelism in\ndistributed training.", "AI": {"tldr": "RailS is a distributed load-balancing framework that optimizes all-to-all communication in MoE training by leveraging Rail topology symmetry and using local LPT scheduling to achieve near-optimal load balance and significantly reduce completion time.", "motivation": "MoE training suffers from sparse and imbalanced all-to-all communication that dominates iteration time, and conventional load-balancing methods fail to exploit Rail architecture topology, leaving multi-NIC bandwidth underutilized.", "method": "RailS leverages Rail topology symmetry to prove uniform sending ensures uniform receiving, transforming global coordination into local scheduling. Each node independently executes LPT spraying scheduler and activates N parallel rails for fine-grained, topology-aware multipath transmission.", "result": "RailS improves bus bandwidth by 20%-78%, reduces completion time by 17%-78%, shortens iteration time by 18%-40% for Mixtral workloads, and achieves near-optimal load balance.", "conclusion": "RailS fully exploits architectural parallelism in distributed MoE training by providing efficient topology-aware load balancing that significantly improves communication performance and reduces training iteration time."}}
{"id": "2510.19301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19301", "abs": "https://arxiv.org/abs/2510.19301", "authors": ["Ziheng Deng", "Xue Liu", "Jiantong Jiang", "Yankai Li", "Qingxu Deng", "Xiaochun Yang"], "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems", "comment": "Accepted for ICDE 2026", "summary": "The Viterbi algorithm is a key operator for structured sequence inference in\nmodern data systems, with applications in trajectory analysis, online\nrecommendation, and speech recognition. As these workloads increasingly migrate\nto resource-constrained edge platforms, standard Viterbi decoding remains\nmemory-intensive and computationally inflexible. Existing methods typically\ntrade decoding time for space efficiency, but often incur significant runtime\noverhead and lack adaptability to various system constraints. This paper\npresents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly\nViterbi decoding operator that enhances adaptability and resource efficiency.\nFLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning\nand parallelization techniques to enhance both time and memory efficiency,\nmaking it well-suited for resource-constrained data systems.To further decouple\nspace complexity from the hidden state space size, we present FLASH-BS Viterbi,\na dynamic beam search variant built on a memory-efficient data structure. Both\nproposed algorithms exhibit strong adaptivity to diverse deployment scenarios\nby dynamically tuning internal parameters.To ensure practical deployment on\nedge devices, we also develop FPGA-based hardware accelerators for both\nalgorithms, demonstrating high throughput and low resource usage. Extensive\nexperiments show that our algorithms consistently outperform existing baselines\nin both decoding time and memory efficiency, while preserving adaptability and\nhardware-friendly characteristics essential for modern data systems. All codes\nare publicly available at https://github.com/Dzh-16/FLASH-Viterbi.", "AI": {"tldr": "FLASH Viterbi is a fast, lightweight, adaptive Viterbi decoding operator that improves time and memory efficiency for resource-constrained edge platforms through non-recursive divide-and-conquer, pruning, and parallelization techniques.", "motivation": "Standard Viterbi decoding is memory-intensive and computationally inflexible for edge platforms, while existing methods trade decoding time for space efficiency with significant runtime overhead and lack adaptability to system constraints.", "method": "Combines non-recursive divide-and-conquer strategy with pruning and parallelization techniques. Also presents FLASH-BS Viterbi variant with dynamic beam search using memory-efficient data structure to decouple space complexity from hidden state space size.", "result": "Extensive experiments show consistent outperformance of existing baselines in both decoding time and memory efficiency, with FPGA-based hardware accelerators demonstrating high throughput and low resource usage.", "conclusion": "FLASH Viterbi algorithms preserve adaptability and hardware-friendly characteristics essential for modern data systems, making them well-suited for resource-constrained edge deployments."}}
{"id": "2510.19470", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19470", "abs": "https://arxiv.org/abs/2510.19470", "authors": ["Weihao Yang", "Hao Huang", "Donglei Wu", "Ningke Li", "Yanqi Pan", "Qiyang Zheng", "Wen Xia", "Shiyi Li", "Qiang Wang"], "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.", "AI": {"tldr": "HybridEP is a framework that optimizes Expert Parallelism (EP) for Mixture-of-Experts models in cross-data center training by dynamically transforming expert placement to reduce communication overhead under constrained bandwidth.", "motivation": "The rapid scaling of MoE models outpaces single DC training capabilities, requiring cross-DC training. However, EP faces severe scalability issues due to limited cross-DC bandwidth, as existing communication-computation overlap optimizations are ineffective in low-bandwidth scenarios.", "method": "HybridEP uses a stream-based model to determine optimal transmission ratios, incorporates domain-based partition to map hybrid patterns to GPU-level communication topology, and employs parameter-efficient migration to reduce expert transmission overhead and enlarge domain size.", "result": "HybridEP outperforms state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth, and achieves up to 1.45x speedup with 1k DCs in large-scale simulations across different bandwidths.", "conclusion": "HybridEP provides a more general EP approach with better scalability by dynamically optimizing expert placement and communication patterns, effectively addressing the bandwidth constraints in cross-DC MoE model training."}}
{"id": "2510.19617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19617", "abs": "https://arxiv.org/abs/2510.19617", "authors": ["Eric Ding"], "title": "Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud", "comment": null, "summary": "Collaborative Machine Learning is a paradigm in the field of distributed\nmachine learning, designed to address the challenges of data privacy,\ncommunication overhead, and model heterogeneity. There have been significant\nadvancements in optimization and communication algorithm design and ML hardware\nthat enables fair, efficient and secure collaborative ML training. However,\nless emphasis is put on collaborative ML infrastructure development. Developers\nand researchers often build server-client systems for a specific collaborative\nML use case, which is not scalable and reusable. As the scale of collaborative\nML grows, the need for a scalable, efficient, and ideally multi-tenant resource\nmanagement system becomes more pressing. We propose a novel system, Propius,\nthat can adapt to the heterogeneity of client machines, and efficiently manage\nand control the computation flow between ML jobs and edge resources in a\nscalable fashion. Propius is comprised of a control plane and a data plane. The\ncontrol plane enables efficient resource sharing among multiple collaborative\nML jobs and supports various resource sharing policies, while the data plane\nimproves the scalability of collaborative ML model sharing and result\ncollection. Evaluations show that Propius outperforms existing resource\nmanagement techniques and frameworks in terms of resource utilization (up to\n$1.88\\times$), throughput (up to $2.76$), and job completion time (up to\n$1.26\\times$).", "AI": {"tldr": "Propius is a novel system for collaborative machine learning that addresses infrastructure challenges through efficient resource management and scalable data handling, outperforming existing frameworks in resource utilization, throughput, and job completion time.", "motivation": "Current collaborative ML systems are often built as specific server-client implementations that lack scalability and reusability. As collaborative ML scales, there's a pressing need for scalable, efficient multi-tenant resource management systems that can handle heterogeneous client machines.", "method": "Propius uses a two-plane architecture: a control plane for efficient resource sharing among multiple collaborative ML jobs with various sharing policies, and a data plane for scalable model sharing and result collection. The system adapts to client machine heterogeneity and manages computation flow between ML jobs and edge resources.", "result": "Evaluations show Propius outperforms existing resource management techniques and frameworks with improvements in resource utilization (up to 1.88\u00d7), throughput (up to 2.76\u00d7), and job completion time (up to 1.26\u00d7).", "conclusion": "Propius provides an effective solution for scalable collaborative ML infrastructure that efficiently manages heterogeneous resources and improves system performance metrics significantly compared to existing approaches."}}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.", "AI": {"tldr": "A serverless GPU-based BDaaS blueprint using TabNet achieves 4.5x higher throughput, 98x lower latency, and 90% lower cost than Spark baselines while maintaining compliance through interpretability features.", "motivation": "Traditional distributed frameworks like Spark introduce coordination complexity and auditing overheads that are unsuitable for moderate-scale, latency-sensitive inference in regulated environments. Serverless GPUs and interpretable models like TabNet enable new deployment approaches.", "method": "Developed a production-oriented BDaaS blueprint integrating single-node serverless GPU runtime with TabNet, leveraging GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for compliance.", "result": "GPU pipelines achieved 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines. Compliance mechanisms added only ~5.7 ms latency with p99 < 22 ms, and interpretability remained stable under peak load.", "conclusion": "The approach provides a practical, secure, interpretable, and cost-efficient serverless GPU analytics solution for regulated enterprise and government settings, with a reproducible Helm-packaged blueprint and decision framework."}}
{"id": "2510.19805", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.19805", "abs": "https://arxiv.org/abs/2510.19805", "authors": ["Carl-Johan Fauvelle Munck af Rosensch\"old", "Feras M. Awaysheh", "Ahmad Awad"], "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond", "comment": "10 pages, 5 figures, 2 algorithms, 4 tables", "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development.", "AI": {"tldr": "This paper benchmarks Redis alternatives (Valkey, KeyDB, Garnet) in Kubernetes deployments, evaluating performance metrics and long-term viability trade-offs.", "motivation": "In-memory key-value datastores are crucial for cloud-native infrastructures but face scalability, compatibility, and sustainability constraints. The literature lacks experimental evaluation of state-of-the-art tools in this domain.", "method": "Systematic benchmarking of Valkey, KeyDB, and Garnet under realistic workloads within Kubernetes deployments, measuring throughput, tail latency, CPU/memory efficiency, and migration complexity.", "result": "The results demonstrate clear trade-offs among the benchmarked data systems, with comprehensive performance and viability assessments showing differences in metrics across the evaluated tools.", "conclusion": "The study highlights trade-offs between performance, compatibility, and long-term viability including project maturity, community support, and sustained development for emerging in-memory key-value stores."}}
