<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 18]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Energy Efficient Federated Learning with Hyperdimensional Computing (HDC)](https://arxiv.org/abs/2602.22290)
*Yahao Ding,Yinchao Yang,Jiaxiang Wang,Zhonghao Liu,Zhaohui Yang,Mingzhe Chen,Mohammad Shikh-Bahaei*

Main category: cs.DC

TL;DR: FL-HDC-DP framework combines hyperdimensional computing and differential privacy to minimize energy consumption in secure federated learning over wireless edge networks.


<details>
  <summary>Details</summary>
Motivation: To address high computational costs and privacy challenges in processing large-scale distributed data with conventional neural networks in federated learning settings, especially in energy-constrained wireless edge networks.

Method: Proposes FL-HDC-DP framework where edge devices use hyperdimensional computing for lightweight local training and apply differential privacy noise to protect model updates. Jointly optimizes HDC dimension, transmit power, and CPU frequency to minimize total energy consumption using a hybrid algorithm with outer enumeration search for HDC dimensions and inner one-dimensional search for resource allocation.

Result: Achieves up to 83.3% energy reduction compared to baseline schemes while maintaining high accuracy and faster convergence.

Conclusion: The proposed FL-HDC-DP framework effectively minimizes energy consumption for secure federated learning in wireless edge networks by combining lightweight hyperdimensional computing with privacy protection through differential privacy.

Abstract: This paper investigates the problem of minimizing total energy consumption for secure federated learning (FL) in wireless edge networks, a key paradigm for decentralized big data analytics. To tackle the high computational cost and privacy challenges of processing large-scale distributed data with conventional neural networks, we propose an FL with hyperdimensional computing and differential privacy (FL-HDC-DP) framework. Each edge device employs hyperdimensional computing (HDC) for lightweight local training and applies differential privacy (DP) noise to protect transmitted model updates. The total energy consumption is minimized through a joint optimization of the HDC dimension, transmit power, and CPU frequency. An efficient hybrid algorithm is developed, combining an outer enumeration search for HDC dimensions with an inner one-dimensional search for resource allocation. Simulation results show that the proposed framework achieves up to 83.3% energy reduction compared with baseline schemes, while maintaining high accuracy and faster convergence.

</details>


### [2] [Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges](https://arxiv.org/abs/2602.22350)
*Paul Borrill*

Main category: cs.DC

TL;DR: The paper introduces "engineered simultaneity" - a system design requiring comparison of spacelike-separated events using implicit simultaneity conventions presented as objective. The US NBBO is shown to be an instance, with its value being frame-dependent due to light-travel delays between exchanges, creating windows where no frame-independent price ordering exists. HFT firms exploit this via faster data feeds, extracting ~$5B annually.


<details>
  <summary>Details</summary>
Motivation: To analyze how financial market infrastructure like the NBBO creates systematic information asymmetries by relying on simultaneity concepts that have no frame-independent meaning in distributed systems with significant light-travel delays between exchanges.

Method: Introduces the concept of "engineered simultaneity," applies it to analyze the NBBO system, proves its frame-dependence mathematically, measures light-travel times between exchanges (43-1,180 km, 143-3,940 μs), compares latency differences between direct feeds (~tens of μs) and consolidated SIP (~1,128 μs), and demonstrates the resulting information asymmetry using Ryle's category mistake framework.

Result: The NBBO is frame-dependent with no frame-independent price ordering during windows created by light-travel delays. HFT firms exploit the ~50:1 latency advantage of direct feeds over SIP, creating information asymmetry that extracts approximately $5 billion annually from other market participants.

Conclusion: The NBBO constitutes a category mistake by applying simultaneity concepts where they lack frame-independent meaning. This engineered simultaneity creates systematic information asymmetries that benefit HFT firms at the expense of other market participants, raising fundamental questions about market fairness and infrastructure design.

Abstract: We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which "current" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of "simultaneity" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.

</details>


### [3] [DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System](https://arxiv.org/abs/2602.22392)
*Md Hasanur Rashid,Xinyi Li,Youbiao He,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: DIAL is a decentralized I/O autotuning system that uses local client metrics and machine learning to improve parallel file system performance without global monitoring overhead.


<details>
  <summary>Details</summary>
Motivation: Existing PFS autotuning approaches rely on extensive global runtime metrics and accurate modeling of application I/O patterns, creating heavy overheads that limit fine-grained, dynamic tuning in practical systems.

Method: DIAL takes a decentralized approach where each I/O client is treated as an independent unit, tuning configurations using only locally observable metrics. Machine learning models enable multiple tunable units to make independent but collective decisions.

Result: DIAL enables timely reaction to global storage system changes and achieves better global I/O performance for applications by reducing monitoring overhead and enabling decentralized decision-making.

Conclusion: The decentralized approach using local metrics and machine learning provides a more practical and efficient solution for PFS autotuning compared to traditional global monitoring methods.

Abstract: Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.

</details>


### [4] [AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage](https://arxiv.org/abs/2602.22409)
*Md Hasanur Rashid,Dong Dai*

Main category: cs.DC

TL;DR: AdapTBF: Adaptive bandwidth control for HPC storage that improves I/O efficiency by allowing applications to borrow/lend bandwidth during bursty phases, preventing small jobs from blocking large-scale ones while maintaining high storage utilization.


<details>
  <summary>Details</summary>
Motivation: Current HPC systems face I/O fairness issues where small jobs with disproportionate storage bandwidth consumption can block larger jobs, causing resource waste. Traditional proportional bandwidth limits (Token Bucket Filter) reduce overall I/O efficiency by not accommodating bursty I/O patterns typical in HPC applications.

Method: AdapTBF extends Token Bucket Filter in parallel file systems (like Lustre) with a decentralized bandwidth control approach using adaptive borrowing and lending. Applications can temporarily borrow unused bandwidth from others and later return it, accommodating bursty I/O patterns while maintaining fairness.

Result: Implementation in Lustre shows AdapTBF effectively manages I/O bandwidth while maintaining high storage utilization, even under extreme conditions. It prevents small jobs from blocking large-scale ones and accommodates bursty I/O patterns better than strict proportional limits.

Conclusion: AdapTBF provides a practical solution for I/O fairness in HPC storage systems by balancing per-application performance, overall storage efficiency, and fairness through adaptive bandwidth control that accommodates real-world bursty I/O patterns.

Abstract: Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.
  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.
  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.

</details>


### [5] [CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems](https://arxiv.org/abs/2602.22423)
*Md Hasanur Rashid,Nathan R. Tallent,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: CARAT is an ML-guided framework for scalable online tuning of parallel file system client-side parameters using only local metrics, achieving up to 3x performance improvement.


<details>
  <summary>Details</summary>
Motivation: Tuning parallel file systems in HPC is challenging due to complex I/O paths, diverse patterns, and dynamic conditions. Existing autotuning frameworks lack scalability, adaptivity, and online operation capabilities.

Method: CARAT uses machine learning to co-tune client-side RPC and caching parameters of parallel file systems, enabling each client to make independent tuning decisions online based on locally observable metrics without global coordination.

Result: CARAT achieved up to 3x performance improvement over default or static configurations across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments.

Conclusion: CARAT's scalable and lightweight approach enables effective online tuning of parallel file systems, with potential for wide deployment to benefit various data-intensive applications.

Abstract: Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.

</details>


### [6] [GetBatch: Distributed Multi-Object Retrieval for ML Data Loading](https://arxiv.org/abs/2602.22434)
*Alex Aizman,Abhishek Gaikwad,Piotr Żelasko*

Main category: cs.DC

TL;DR: GetBatch is a new object store API that replaces individual GET requests with batch retrieval, achieving up to 15x throughput improvement for small objects and significantly reducing latency in ML training workloads.


<details>
  <summary>Details</summary>
Motivation: ML training pipelines consume data in batches from distributed storage, but issuing thousands of individual GET requests incurs per-request overhead that dominates data transfer time, creating a bottleneck in training efficiency.

Method: Introduces GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution.

Result: GetBatch achieves up to 15x throughput improvement for small objects, reduces P95 batch retrieval latency by 2x, and reduces P99 per-object tail latency by 3.7x compared to individual GET requests in production training workloads.

Conclusion: Batch retrieval should be a first-class storage operation to optimize ML training pipelines, and GetBatch demonstrates significant performance improvements by reducing per-request overhead through deterministic, fault-tolerant streaming execution.

Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.

</details>


### [7] [CCCL: Node-Spanning GPU Collectives with CXL Memory Pooling](https://arxiv.org/abs/2602.22457)
*Dong Xu,Han Meng,Xinyu Chen,Dengcheng Zhu,Wei Tang,Fei Liu,Liguang Xie,Wu Xiang,Rui Shi,Yue Li,Henry Hu,Hui Zhang,Jianping Jiang,Dong Li*

Main category: cs.DC

TL;DR: A CXL-based collective communication library called \name that enables cross-node GPU operations using shared memory pools instead of traditional RDMA networking, achieving significant performance improvements and cost savings for LLM training.


<details>
  <summary>Details</summary>
Motivation: Large language model training/inference across multiple nodes creates high pressure on GPU memory and interconnect bandwidth. Traditional RDMA-based networking has limitations that CXL shared memory pools could address to reduce over-provisioning and improve resource utilization.

Method: Proposed \name, a collective communication library leveraging CXL shared memory pools to support cross-node GPU operations. The design addresses synchronization, data interleaving, and communication parallelization challenges specific to using CXL shared memory for collective communications.

Result: Evaluation on multiple nodes with TITAN-II CXL switch and six Micron CZ120 memory cards shows \name achieves average performance improvements: 1.34× for AllGather, 1.84× for Broadcast, 1.94× for Gather, and 1.04× for Scatter compared to RDMA-based implementation over 200 Gbps InfiniBand. LLM training case shows 1.11× speedup with 2.75× hardware cost savings.

Conclusion: CXL shared memory pools offer a scalable solution for GPU communication, demonstrating CXL's potential for memory-centric GPU communication that can outperform traditional RDMA-based networking while reducing hardware costs.

Abstract: Large language models (LLMs) training or inference across multiple nodes introduces significant pressure on GPU memory and interconnect bandwidth. The Compute Express Link (CXL) shared memory pool offers a scalable solution by enabling memory sharing across nodes, reducing over-provisioning and improving resource utilization. We propose \name, a collective communication library, leveraging the CXL shared memory pool to support cross-node GPU operations without relying on traditional RDMA-based networking. Our design addresses the challenges on synchronization, data interleaving, and communication parallelization faced by using the CXL shared memory pool for collective communications. Evaluating on multiple nodes with a TITAN-II CXL switch and six Micron CZ120 memory cards, we show that \name achieves highly efficient collective operations across hosts, demonstrating CXL's potential for scalable, memory-centric GPU communication. Our evaluation demonstrates that \name achieves average performance improvements of 1.34$\times$ for AllGather, 1.84$\times$ for Broadcast, 1.94$\times$ for Gather, and 1.04$\times$ for Scatter, compared to the original RDMA-based implementation over 200 Gbps InfiniBand. \textcolor{dong}{In addition, the evaluation with a case of LLM training shows 1.11$\times$ speedup compared with the InfiniBand while saving production cost by $2.75\times$ in hardware.}

</details>


### [8] [veScale-FSDP: Flexible and High-Performance FSDP at Scale](https://arxiv.org/abs/2602.22437)
*Zezhou Wang,Youjie Li,Zhiqi Lin,Jiacheng Yang,Cong Xie,Guanyu Feng,Zheng Zhong,Ziyue Huang,Hongyu Zhu,Zhi Zhang,Yanghua Peng,Xin Liu*

Main category: cs.DC

TL;DR: veScale-FSDP is a redesigned FSDP system that addresses limitations of current FSDP in supporting structure-aware training methods and non-element-wise optimizers through flexible sharding formats and structure-aware planning, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current FSDP systems struggle with structure-aware training methods (like block-wise quantized training) and non-element-wise optimizers (like Shampoo and Muon) used in cutting-edge models. They also have communication and memory efficiency issues that limit scaling to tens of thousands of GPUs.

Method: Introduces veScale-FSDP with a flexible sharding format called RaggedShard coupled with a structure-aware planning algorithm. This enables native support for efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers.

Result: veScale-FSDP achieves 5-66% higher throughput and 16-30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

Conclusion: veScale-FSDP successfully addresses the limitations of current FSDP systems by providing both flexibility for advanced training methods and superior performance at scale, making it suitable for training cutting-edge large-scale models.

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

</details>


### [9] [Fault-tolerant Reduce and Allreduce operations based on correction](https://arxiv.org/abs/2602.22445)
*Martin Kuettler,Hermann Haertig*

Main category: cs.DC

TL;DR: A fault-tolerant Reduce algorithm using correction-then-tree approach, extended to Allreduce by combining with Broadcast


<details>
  <summary>Details</summary>
Motivation: To create fault-tolerant Reduce operations by applying correction-based approaches similar to those used in Broadcast algorithms

Method: Two-phase approach: correction-like communication phase followed by tree-based phase for Reduce; then combined with Broadcast for Allreduce

Result: A fault-tolerant Reduce algorithm with proven semantics, extended to create a fault-tolerant Allreduce implementation

Conclusion: The correction-then-tree approach successfully provides fault tolerance for Reduce operations, enabling robust Allreduce implementations

Abstract: Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.
  Based on these results, Broadcast and Reduce are combined to provide Allreduce.

</details>


### [10] [FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud](https://arxiv.org/abs/2602.22580)
*Yuhao Lin,Zhipeng Tang,Jiayan Tong,Junqing Xiao,Bin Lu,Yuhang Li,Chao Li,Zhiguo Zhang,Junhua Wang,Hao Luo,James Cheng,Chuang Hu,Jiawei Jiang,Xiao Yan*

Main category: cs.DC

TL;DR: FuxiShuffle is a general data shuffle service designed for ultra-large production environments that dynamically adapts to job characteristics and cluster conditions while providing efficient failure resilience through active multi-replica failover and incremental recovery.


<details>
  <summary>Details</summary>
Motivation: Existing shuffle systems fail to adapt to highly dynamic job characteristics and cluster resource conditions in ultra-large clusters, and their passive fault tolerance mechanisms are inefficient when failures are inevitable in production environments.

Method: FuxiShuffle dynamically selects shuffle mode based on runtime information, conducts progress-aware scheduling for downstream workers, automatically determines backup strategies for shuffle data chunks, ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs incremental recovery without losing computation progress.

Result: Experiments show FuxiShuffle significantly reduces both end-to-end job completion time and aggregate resource consumption compared to baseline systems, with micro experiments confirming effectiveness in improving adaptability and failure resilience.

Conclusion: FuxiShuffle successfully addresses limitations of existing shuffle systems by providing good adaptability to dynamic conditions and efficient failure resilience, making it suitable for ultra-large production environments like Alibaba Cloud MaxCompute platform.

Abstract: Shuffle exchanges intermediate results between upstream and downstream operators in distributed data processing and is usually the bottleneck due to factors such as small random I/Os and network contention. Several systems have been designed to improve shuffle efficiency, but from our experiences of running ultra-large clusters at Alibaba Cloud MaxCompute platform, we observe that they can not adapt to highly dynamic job characteristics and cluster resource conditions, and their fault tolerance mechanisms are passive and inefficient when failures are inevitable. To tackle their limitations, we design and implement FuxiShuffle as a general data shuffle service for the ultra-large production environment of MaxCompute, featuring good adaptability and efficient failure resilience. Specifically, to achieve good adaptability, FuxiShuffle dynamically selects the shuffle mode based on runtime information, conducts progress-aware scheduling for the downstream workers, and automatically determines the most suitable backup strategy for each shuffle data chunk. To make failure resilience efficient, FuxiShuffle actively ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs an incremental recovery mechanism that does not lose computation progress. Our experiments show that, compared to baseline systems, FuxiShuffle significantly reduces not only end-to-end job completion time but also aggregate resource consumption. Micro experiments suggest that our designs are effective in improving adaptability and failure resilience.

</details>


### [11] [FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving](https://arxiv.org/abs/2602.22593)
*Shouwei Gao,Junqi Yin,Feiyi Wang,Wenqian Dong*

Main category: cs.DC

TL;DR: Flying Serving is a vLLM-based system that enables online switching between data parallelism (DP) and tensor parallelism (TP) without restarting workers, improving LLM serving performance under varying loads and request requirements.


<details>
  <summary>Details</summary>
Motivation: Production LLM serving needs to balance high throughput, low latency, and context capacity under dynamic traffic and mixed request requirements. Existing systems use static parallelism configurations that can't adapt efficiently to bursts, priorities, or long-context requests without disruptive restarts.

Method: Flying Serving virtualizes state to avoid data movement during DP-TP switching: (1) zero-copy Model Weights Manager for on-demand TP shard views, (2) KV Cache Adaptor preserving request state across layouts, (3) eagerly initialized Communicator Pool to amortize collective setup, and (4) deadlock-free scheduler coordinating safe transitions under execution skew.

Result: Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to 4.79× under high load and 3.47× under low load while supporting both latency- and memory-driven requests.

Conclusion: Flying Serving enables practical online DP-TP switching in LLM serving systems, allowing dynamic adaptation to varying workloads and request requirements without the performance penalties of traditional static configurations.

Abstract: Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\times$ under high load and $3.47\times$ under low load while supporting latency- and memory-driven requests.

</details>


### [12] [Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study](https://arxiv.org/abs/2602.22760)
*Philipp Wiesner,Soeren Becker,Brett Cornick,Dominik Scheinert,Alexander Acker,Odej Kao*

Main category: cs.DC

TL;DR: Curtailment-aware federated learning system trains LLMs using excess renewable energy across geo-distributed GPU clusters, reducing emissions to 5-12% of baseline.


<details>
  <summary>Details</summary>
Motivation: Large language model training consumes substantial energy while renewable energy sources often produce excess electricity that gets curtailed (wasted). Aligning training with these curtailment windows enables using clean, cheap electricity.

Method: Developed a system for full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows. Uses elastic switching between local single-site training and federated multi-site synchronization based on site availability. Implemented with Flower federated learning framework and real-world marginal carbon intensity traces.

Result: Preliminary results show the system trains a 561M-parameter transformer model across three clusters while preserving training quality. Operational emissions reduced to 5-12% of single-site baselines.

Conclusion: Curtailment-aware scheduling enables sustainable LLM training by leveraging excess renewable energy, significantly reducing carbon emissions while maintaining model quality through federated learning across distributed clusters.

Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.

</details>


### [13] [An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms](https://arxiv.org/abs/2602.22780)
*Qingyuan Zhang*

Main category: cs.DC

TL;DR: A structured temporal joint load prediction framework for microservice environments that combines service invocation graphs with multivariate load sequences to capture multi-scale load patterns and cross-service impacts.


<details>
  <summary>Details</summary>
Motivation: Cloud native environments face challenges with complex microservice invocation relations, multi-scale load fluctuations, and significant cross-service impacts that traditional load prediction methods struggle to address.

Method: Represents system as coupled time-evolving service invocation graph and multivariate load sequences; constructs neighborhood-aggregated and global summarized views; uses unified sequence encoder with lightweight structural prior in attention computation; employs multi-objective regression for service and cluster level predictions.

Result: Framework effectively captures load propagation along invocation chains while maintaining consistent modeling of local bursts and overall trends; sensitivity analyses support necessity of multi-granularity fusion and structural injection with clarified configuration ranges.

Conclusion: Provides reusable modeling paradigm for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments through structured temporal joint load prediction.

Abstract: This study targets cloud native environments where microservice invocation relations are complex, load fluctuations are multi-scale and superimposed, and cross-service impacts are significant. We propose a structured temporal joint load prediction framework oriented to microservice topology. The method represents the system as a coupled entity of a time-evolving service invocation graph and multivariate load sequences. It constructs neighborhood-aggregated and global summarized views based on service level observations. This forms layered load representations across instance, service, and cluster levels. A unified sequence encoder models multi-scale historical context. To strengthen the expression of invocation dependencies, the framework introduces a lightweight structural prior into attention computation. This enables more effective capture of load propagation and accumulation along invocation chains, while maintaining consistent modeling of local bursts and overall trends. The training objective adopts a multi-objective regression strategy that jointly optimizes service level and cluster level predictions to improve cross-granularity stability. We further conduct single-factor sensitivity analyses on key structural and training hyperparameters. We systematically examine the effects of time window length, encoding depth, and regularization strength. The results support the necessity of multi-granularity fusion and structural injection and clarify their effective configuration ranges. Overall, the framework provides a reusable modeling paradigm and implementation path for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments.

</details>


### [14] [Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks](https://arxiv.org/abs/2602.22852)
*Oliver Larsson,Thijs Metsch,Cristian Klein,Erik Elmroth*

Main category: cs.DC

TL;DR: Buoyancy is a novel abstraction that integrates application and system-level metrics to characterize workload performance in multi-tenant, heterogeneous environments, providing better bottleneck detection than traditional heuristics.


<details>
  <summary>Details</summary>
Motivation: Traditional performance metrics like CPU utilization are insufficient for capturing complex performance dynamics in modern multi-tenant, hardware-heterogeneous environments where resource contention and noisy-neighbor effects cause unexpected performance degradation across various shared resources.

Method: Introduces buoyancy as an abstraction that combines application-level metrics with system-level insights about shared resource contention to provide a holistic view of performance dynamics, explicitly capturing bottlenecks and headroom across multiple resources.

Result: Buoyancy provides 19.3% better indication of bottlenecks compared to traditional heuristics on average, and can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling decisions.

Conclusion: Buoyancy offers an intuitive, extensible, and generalizable approach for resource-aware and application-aware orchestration across heterogeneous platforms, addressing the limitations of traditional performance assessment methods in complex multi-tenant environments.

Abstract: Modern multi-tenant, hardware-heterogeneous computing environments pose significant challenges for effective workload orchestration. Simple heuristics for assessing workload performance, such as CPU utilization or application-level metrics, are often insufficient to capture the complex performance dynamics arising from resource contention and noisy-neighbor effects. In such environments, performance bottlenecks may emerge in any shared system resource, leading to unexpected and difficult-to-diagnose degradation.
  This paper introduces buoyancy, a novel abstraction for characterizing workload performance in multi-tenant systems. Unlike traditional approaches, buoyancy integrates application-level metrics with system-level insights of shared resource contention to provide a holistic view of performance dynamics. By explicitly capturing bottlenecks and headroom across multiple resources, buoyancy facilitates resource-aware and application-aware orchestration in a manner that is intuitive, extensible, and generalizable across heterogeneous platforms. We evaluate buoyancy using representative multi-tenant workloads to illustrate its ability to expose performance-limiting resource interactions. Buoyancy provides a 19.3% better indication of bottlenecks compared to traditional heuristics on average. We additionally show how buoyancy can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling and optimization decisions.

</details>


### [15] [A Simple Distributed Deterministic Planar Separator](https://arxiv.org/abs/2602.22916)
*Yaseen Abd-Elhaleem,Michal Dory,Oren Weimann*

Main category: cs.DC

TL;DR: A simple deterministic distributed algorithm for computing O(D)-size balanced separators in planar graphs with near-optimal Õ(D) round complexity.


<details>
  <summary>Details</summary>
Motivation: Previous distributed separator algorithms were either randomized or complex deterministic solutions. The need for deterministic separators to derandomize distributed algorithms for classical planar graph problems.

Method: Simple weight transfer scheme: each vertex transfers its weight to one arbitrary face it lies on, enabling a straightforward deterministic separator computation.

Result: Achieves same near-optimal Õ(D)-round complexity as previous randomized algorithm, with much simpler deterministic approach.

Conclusion: Provides a simple deterministic separator algorithm that directly derandomizes state-of-the-art distributed algorithms for planar graph problems like shortest-paths, max-flow, min-cut, and reachability.

Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.

</details>


### [16] [LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure](https://arxiv.org/abs/2602.23036)
*Jaehong Cho,Hyunmin Choi,Guseul Heo,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim 2.0 is a unified system-level simulator that models runtime hardware-software interactions in heterogeneous, disaggregated LLM serving infrastructures, enabling co-design exploration with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern LLM serving infrastructures are becoming increasingly heterogeneous (diverse accelerators, near-memory processing) and disaggregated (separated components), making performance dependent on runtime hardware-software interactions. Existing simulators cannot jointly model these complex interactions in a unified framework.

Method: LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. It uses profile-based modeling for extensible integration of emerging accelerators and memory systems while capturing dynamic serving behavior and system-level effects.

Result: The simulator reproduces key performance, memory, and power metrics with an average error of 0.97% when validated against real deployments. It maintains simulation times of around 10 minutes even for complex configurations.

Conclusion: LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures by making runtime hardware-software interactions explicit and analyzable.

Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.
  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.

</details>


### [17] [STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems](https://arxiv.org/abs/2602.23220)
*Chris Egersdoerfer,Philip Carns,Shane Snyder,Robert Ross,Dong Dai*

Main category: cs.DC

TL;DR: STELLAR is an autonomous LLM-powered tuner for parallel file systems that achieves near-optimal configurations within 5 attempts, dramatically reducing the manual effort traditionally required for I/O performance tuning in scientific computing.


<details>
  <summary>Details</summary>
Motivation: Tuning large-scale storage systems for I/O performance is complex, costly, and manpower-intensive, making it inaccessible for most domain scientists who need efficient data-intensive scientific computing.

Method: STELLAR uses LLMs for autonomous end-to-end agentic tuning with six key steps: extracting parameters from manuals, analyzing I/O traces, selecting initial strategies, rerunning applications with feedback, adjusting strategies iteratively, and summarizing experiences. It integrates RAG, tool execution, reasoning, and multiagent design to combat hallucinations.

Result: STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications, dramatically outperforming traditional methods that require hundreds of thousands of iterations.

Conclusion: STELLAR provides a promising approach to complex system optimization with large search spaces and high exploration costs, making I/O tuning accessible to domain scientists with minimal resources while offering design insights for similar optimization domains.

Abstract: I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications.
  STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrates retrieval-augmented generation (RAG), tool execution, LLM-based reasoning, and a multiagent design to stabilize reasoning and combat hallucinations.
  We evaluate the impact of each component on optimization outcomes, providing design insights for similar systems in other optimization domains. STELLAR's architecture and empirical results highlight a promising approach to complex system optimization, especially for problems with large search spaces and high exploration costs, while making I/O tuning more accessible to domain scientists with minimal added resources.

</details>


### [18] [Exploiting network topology in brain-scale simulations of spiking neural networks](https://arxiv.org/abs/2602.23274)
*Melissa Lober,Markus Diesmann,Susanne Kunkel*

Main category: cs.DC

TL;DR: The paper identifies that the real bottleneck in large-scale spiking neuronal network simulations is not interconnect speed but waiting for the slowest compute node due to variability in computation times between communication calls. It proposes a structure-aware mapping strategy inspired by brain organization to reduce synchronization frequency.


<details>
  <summary>Details</summary>
Motivation: To address the performance bottleneck in distributed large-scale spiking neuronal network simulations, which has traditionally been attributed to interconnect limitations and communication libraries, but actually stems from variability in computation times across nodes and waiting for the slowest node.

Method: Developed a statistical model to analyze simulation time based on computation time distributions between communication calls. Proposed a structure-aware mapping strategy that mimics brain organization: mapping brain areas to compute nodes with frequent local communication within areas and less frequent global communication between areas, leveraging the natural delay differences in neuronal connections.

Result: Demonstrated substantial performance gain on a real-world example by reducing synchronization frequency and computation time variability across nodes. The approach challenges the conventional belief that simulation bottlenecks are inherent in standard communication library synchronization.

Conclusion: The paper proposes a local-global hybrid communication architecture for neuronal network simulations that maps brain structure to supercomputer architecture, providing guidelines for energy-efficient simulations and raising performance standards for neuromorphic systems.

Abstract: Simulation code for conventional supercomputers serves as a reference for neuromorphic computing systems. The present bottleneck of distributed large-scale spiking neuronal network simulations is the communication between compute nodes. Communication speed seems limited by the interconnect between the nodes and the software library orchestrating the data transfer. Profiling reveals, however, that the variability of the time required by the compute nodes between communication calls is large. The bottleneck is in fact the waiting time for the slowest node. A statistical model explains total simulation time on the basis of the distribution of computation times between communication calls. A fundamental cure is to avoid communication calls because this requires fewer synchronizations and reduces the variability of computation times across compute nodes. The organization of the mammalian brain into areas lends itself to such an optimization strategy. Connections between neurons within an area have short delays, but the delays of the long-range connections across areas are an order of magnitude longer. This suggests a structure-aware mapping of areas to compute nodes allowing for a partition into more frequent communication between nodes simulating a particular area and less frequent global communication. We demonstrate a substantial performance gain on a real-world example. This work proposes a local-global hybrid communication architecture for large-scale neuronal network simulations as a first step in mapping the structure of the brain to the structure of a supercomputer. It challenges the long-standing belief that the bottleneck of simulation is synchronization inherent in the collective calls of standard communication libraries. We provide guidelines for the energy efficient simulation of neuronal networks on conventional computing systems and raise the bar for neuromorphic systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [19] [Molecule Mixture Detection and Design for MC Systems with Non-linear, Cross-reactive Receiver Arrays](https://arxiv.org/abs/2602.22799)
*Bastian Heinlein,Kaikai Zhu,Sümeyye Carkit-Yilmaz,Sebastian Lotter,Helene M. Loos,Andrea Buettner,Yansha Deng,Robert Schober,Vahid Jamali*

Main category: cs.ET

TL;DR: The paper proposes detectors and transmission schemes for air-based molecular communication systems using non-linear, cross-reactive sensors, addressing the mismatch between ideal assumptions and real sensor behavior.


<details>
  <summary>Details</summary>
Motivation: Commercial sensors for air-based molecular communication exhibit non-linear and cross-reactive behavior, contrary to the ideal linear and molecule-specific sensing assumptions in existing literature. This mismatch needs to be addressed for practical deployment.

Method: Proposed detectors based on first- and second-order moments of symbol likelihoods using Unscented Transform: 1) approximate maximum likelihood (AML) symbol-by-symbol detector for ISI-free scenarios, 2) mixture alphabet design algorithm accounting for RX characteristics, 3) ISI-adapted AML detector for high data rates, 4) sequence detector combining multiple symbol intervals, and 5) adaptive transmission scheme for low-power receivers.

Result: Validated all proposed detectors and algorithms through computer simulations using both commercially available sensor responses and artificially generated sensor data with metal-oxide semiconductor characteristics. The general system model accounts for transmitter noise, ISI, and non-linear cross-reactive receiver arrays.

Conclusion: This work enables reliable communication for a large class of molecular communication systems by addressing the practical limitations of real-world sensors, bridging the gap between theoretical assumptions and practical implementation constraints.

Abstract: Air-based molecular communication (MC) has the potential to be one of the first MC systems to be deployed in real-world applications, enabled by commercially available sensors. However, these sensors usually exhibit non-linear and cross-reactive behavior, contrary to the idealizing assumption of linear and perfectly molecule type-specific sensing often made in the MC literature. To address this mismatch, we propose several detectors and transmission schemes for a molecule mixture communication system where the receiver (RX) employs non-linear, cross-reactive sensors. All proposed schemes are based on the first- and second-order moments of the symbol likelihoods that are fed through the non-linear RX using the Unscented Transform. In particular, we propose an approximate maximum likelihood (AML) symbol-by-symbol detector for inter-symbol-interference (ISI)-free transmission scenarios and a complementary mixture alphabet design algorithm which accounts for the RX characteristics. When significant ISI is present at high data rates, the AML detector can be adapted to exploit statistical ISI knowledge. Additionally, we propose a sequence detector which combines information from multiple symbol intervals. For settings where sequence detection is not possible due to extremely limited computational power at the RX, we propose an adaptive transmission scheme which can be combined with symbol-by-symbol detection. Using computer simulations, we validate all proposed detectors and algorithms based on the responses of commercially available sensors as well as artificially generated sensor data incorporating the characteristics of metal-oxide semiconductor sensors. By employing a general system model that accounts for transmitter noise, ISI, and general non-linear, cross-reactive RX arrays, this work enables reliable communication for a large class of MC systems.

</details>


### [20] [Parallelizable Search-Space Decomposition for Large-Scale Combinatorial Optimization Problems Using Ising Machines](https://arxiv.org/abs/2602.23038)
*Eiji Kawase,Shuta Kikuchi,Hideaki Tamai,Shu Tanaka*

Main category: cs.ET

TL;DR: A search-space decomposition method for large-scale combinatorial optimization problems that uses variable interaction analysis to break problems into independent subproblems, achieving 95.32% variable reduction and 30x speedup.


<details>
  <summary>Details</summary>
Motivation: Combinatorial optimization problems are crucial in industry but many are NP-hard with exponentially growing search spaces. Conventional solvers treat problems as monolithic entities, leading to efficiency degradation as structural complexity increases.

Method: Proposes a search-space decomposition method that leverages variable structure to reduce master problem size. Formulates interaction costs between variables and individual costs as constrained maximum cut problem, converts to QUBO using penalty terms, uses Ising-model solver to decompose into independent subproblems, then solves subproblems in parallel with optimization solvers.

Result: Validated on capacitated vehicle routing problem: substantial enhancement in feasible solution rates, accelerated convergence (1 min vs 30 min for naive method), variable reduction up to 95.32%.

Conclusion: Search-space decomposition is a promising strategy for efficiently solving large-scale combinatorial optimization problems by systematically reducing problem complexity through variable interaction analysis.

Abstract: Combinatorial optimization problems are crucial in industry. However, many COPs are NP-hard, causing the search space to grow exponentially with problem size and rendering large-scale instances computationally intractable. Conventional solvers typically treat problems as monolithic entities, leading to significant efficiency degradation as structural complexity increases. To address this issue, we propose a novel search-space decomposition method that leverages the inherent structure of variables to systematically reduce the size of the master problem. We formulate interaction costs between variables and individual variable costs as a constrained maximum cut problem and convert it into a quadratic unconstrained binary optimization formulation using penalty terms. An Ising-model solver is used to rapidly decompose the problem into independent small-scale subproblems, which are subsequently solved in parallel using mathematical optimization solvers. We validated this method on the capacitated vehicle routing problem. Results demonstrate three significant benefits: a substantial enhancement in feasible solution rates, accelerated convergence, achieving in 1 min the accuracy that the naive method required 30 min to reach, and a variable reduction of up to 95.32\%. These findings suggest that search-space decomposition is a promising strategy for efficiently solving large-scale combinatorial optimization problems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [FHECore: Rethinking GPU Microarchitecture for Fully Homomorphic Encryption](https://arxiv.org/abs/2602.22229)
*Lohit Daksha,Seyda Guzelhan,Kaustubh Shivdikar,Carlos Agulló Domingo,Óscar Vera Lopez,Gilbert Jonatan,Hubert Dymarkowski,Aymane El Jerari,José Cano,José L. Abellán,John Kim,David Kaeli,Ajay Joshi*

Main category: cs.AR

TL;DR: FHECore is a specialized GPU functional unit for FHE acceleration that reduces instruction count by 2.41× for CKKS primitives and achieves 1.57× performance speedup with only 2.4% area overhead.


<details>
  <summary>Details</summary>
Motivation: FHE enables computation on encrypted data but suffers massive overheads. While ASICs have long development cycles and GPUs are specialized for ML workloads with low-precision datatypes mismatched for FHE's wide-precision modulo arithmetic, there's a need for efficient FHE acceleration on widely available platforms.

Method: FHECore integrates a specialized functional unit into GPU's Streaming Multiprocessor that natively supports wide-precision modulo-multiply-accumulate operations. The key insight is that dominant latency contributors (Number Theoretic Transform and Base Conversion) can be formulated as modulo-linear transformations and mapped onto a common hardware unit.

Result: FHECore reduces dynamic instruction count by geometric mean of 2.41× for CKKS primitives and 1.96× for end-to-end workloads, translating to performance speedups of 1.57× and 2.12× respectively, including 50% reduction in bootstrapping latency with only 2.4% area overhead.

Conclusion: FHECore demonstrates that integrating specialized FHE acceleration units into GPUs can significantly improve performance while maintaining programmability and availability, overcoming the mismatch between GPU hardware and FHE computational requirements.

Abstract: Fully Homomorphic Encryption (FHE) enables computation directly on encrypted data but incurs massive computational and memory overheads, often exceeding plaintext execution by several orders of magnitude. While custom ASIC accelerators can mitigate these costs, their long time-to-market and the rapid evolution of FHE algorithms threaten their long-term relevance. GPUs, by contrast, offer scalability, programmability, and widespread availability, making them an attractive platform for FHE. However, modern GPUs are increasingly specialized for machine learning workloads, emphasizing low-precision datatypes (e.g., INT$8$, FP$8$) that are fundamentally mismatched to the wide-precision modulo arithmetic required by FHE. Essentially, while GPUs offer ample parallelism, their functional units, like Tensor Cores, are not suited for wide-integer modulo arithmetic required by FHE schemes such as CKKS. Despite this constraint, researchers have attempted to map FHE primitives on Tensor Cores by segmenting wide integers into low-precision (INT$8$) chunks.
  To overcome these bottlenecks, we propose FHECore, a specialized functional unit integrated directly into the GPU's Streaming Multiprocessor. Our design is motivated by a key insight: the two dominant contributors to latency$-$Number Theoretic Transform and Base Conversion$-$can be formulated as modulo-linear transformations. This allows them to be mapped on a common hardware unit that natively supports wide-precision modulo-multiply-accumulate operations. Our simulations demonstrate that FHECore reduces dynamic instruction count by a geometric mean of $2.41\times$ for CKKS primitives and $1.96\times$ for end-to-end workloads. These reductions translate to performance speedups of $1.57\times$ and $2.12\times$, respectively$-$including a $50\%$ reduction in bootstrapping latency$-$all while inuring a modest $2.4\%$ area overhead.

</details>


### [22] [GRAU: Generic Reconfigurable Activation Unit Design for Neural Network Hardware Accelerators](https://arxiv.org/abs/2602.22352)
*Yuhao Liu,Salim Ullah,Akash Kumar*

Main category: cs.AR

TL;DR: GRAU: A reconfigurable activation hardware using piecewise linear fitting with power-of-two slopes, reducing LUT consumption by over 90% compared to multi-threshold activators.


<details>
  <summary>Details</summary>
Motivation: As neural network scales grow, low-precision quantization is widely used in edge accelerators. Classic multi-threshold activation hardware requires 2^n thresholds for n-bit outputs, causing rapid hardware cost increase with precision.

Method: Propose GRAU, a reconfigurable activation hardware based on piecewise linear fitting where segment slopes are approximated by powers of two. The design requires only basic comparators and 1-bit right shifters, supporting mixed-precision quantization and nonlinear functions like SiLU.

Result: Compared with multi-threshold activators, GRAU reduces LUT consumption by over 90%, achieving higher hardware efficiency, flexibility, and scalability.

Conclusion: GRAU provides an efficient, flexible, and scalable solution for activation hardware in edge accelerators, significantly reducing hardware costs while supporting various quantization precisions and nonlinear functions.

Abstract: With the continuous growth of neural network scales, low-precision quantization is widely used in edge accelerators. Classic multi-threshold activation hardware requires 2^n thresholds for n-bit outputs, causing a rapid increase in hardware cost as precision increases. We propose a reconfigurable activation hardware, GRAU, based on piecewise linear fitting, where the segment slopes are approximated by powers of two. Our design requires only basic comparators and 1-bit right shifters, supporting mixed-precision quantization and nonlinear functions such as SiLU. Compared with multi-threshold activators, GRAU reduces LUT consumption by over 90%, achieving higher hardware efficiency, flexibility, and scalability.

</details>


### [23] [Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators](https://arxiv.org/abs/2602.23334)
*Yuhao Liu,Salim Ullah,Akash Kumar*

Main category: cs.AR

TL;DR: Proposes a runtime reconfigurable multi-precision bitwise systolic array for QNN accelerators to efficiently support mixed-precision quantization with better performance than fixed-precision designs.


<details>
  <summary>Details</summary>
Motivation: Low-precision quantization reduces hardware resources but causes high accuracy loss in neural network inference, while mixed-precision quantization offers better accuracy-resource tradeoffs but requires hardware support for runtime precision reconfiguration.

Method: Designed a runtime reconfigurable multi-precision multi-channel bitwise systolic array that can dynamically adjust precision for different layers of quantized neural networks during inference.

Result: Implementation on Ultra96 FPGA shows 1.3185-3.5671× speedup for mixed-precision models, reduced critical path delay, and support for higher clock frequency (250MHz).

Conclusion: The proposed systolic array design effectively enables efficient mixed-precision QNN inference with significant performance improvements and hardware efficiency.

Abstract: Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption and accuracy. Because regular designs for multiplication on hardware cannot support the precision reconfiguration for a multi-precision Quantized Neural Network (QNN) model in runtime, we propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array design for QNN accelerators. We have implemented and evaluated our work on the Ultra96 FPGA platform. Results show that our work can achieve 1.3185 to 3.5671 times speedup in inferring mixed-precision models and has less critical path delay, supporting a higher clock frequency (250MHz).

</details>
