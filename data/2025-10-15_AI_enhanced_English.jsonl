{"id": "2510.11727", "categories": ["cs.ET", "cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11727", "abs": "https://arxiv.org/abs/2510.11727", "authors": ["Benius Dunn", "Javier Meza-Arroyo", "Armi Tiihonen", "Mark Lee", "Julia W. P. Hsu"], "title": "Multi-objective Bayesian Optimization with Human-in-the-Loop for Flexible Neuromorphic Electronics Fabrication", "comment": null, "summary": "Neuromorphic computing hardware enables edge computing and can be implemented\nin flexible electronics for novel applications. Metal oxide materials are\npromising candidates for fabricating flexible neuromorphic electronics, but\nsuffer from processing constraints due to the incompatibilities between oxides\nand polymer substrates. In this work, we use photonic curing to fabricate\nflexible metal-insulator-metal capacitors with solution-processible aluminum\noxide dielectric tailored for neuromorphic applications. Because photonic\ncuring outcomes depend on many input parameters, identifying an optimal\nprocessing condition through a traditional grid-search approach is unfeasible.\nHere, we apply multi-objective Bayesian optimization (MOBO) to determine\nphotonic curing conditions that optimize the trade-off between desired\nelectrical properties of large capacitance-frequency dispersion and low leakage\ncurrent. Furthermore, we develop a human-in-the-loop (HITL) framework for\nincorporating failed experiments into the MOBO machine learning workflow,\ndemonstrating that this framework accelerates optimization by reducing the\nnumber of experimental rounds required. Once optimization is concluded, we\nanalyze different Pareto-optimal conditions to tune the dielectrics properties\nand provide insight into the importance of different inputs through Shapley\nAdditive exPlanations analysis. The demonstrated framework of combining MOBO\nwith HITL feedback can be adapted to a wide range of multi-objective\nexperimental problems that have interconnected inputs and high experimental\nfailure rates to generate usable results for machine learning models.", "AI": {"tldr": "This paper presents a multi-objective Bayesian optimization (MOBO) approach with human-in-the-loop feedback to optimize photonic curing conditions for flexible metal-insulator-metal capacitors with aluminum oxide dielectric, accelerating the optimization process for neuromorphic electronics applications.", "motivation": "The motivation is to overcome processing constraints in fabricating flexible neuromorphic electronics using metal oxide materials, where traditional grid-search optimization is unfeasible due to the many input parameters involved in photonic curing.", "method": "The method combines multi-objective Bayesian optimization (MOBO) with a human-in-the-loop (HITL) framework to optimize photonic curing conditions, incorporating failed experiments into the machine learning workflow to accelerate optimization.", "result": "The framework successfully reduces the number of experimental rounds required for optimization and enables tuning of dielectric properties through analysis of different Pareto-optimal conditions, with insights provided by Shapley Additive exPlanations analysis.", "conclusion": "The demonstrated MOBO with HITL framework can be adapted to various multi-objective experimental problems with interconnected inputs and high failure rates, generating usable results for machine learning models in neuromorphic electronics and beyond."}}
{"id": "2510.11730", "categories": ["cs.ET", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.11730", "abs": "https://arxiv.org/abs/2510.11730", "authors": ["Connor G. McMahan", "Gavin Chang", "Raymond Nguyen", "Souren Soukiazian", "David A. Smith", "Tobias Schaedler", "David Shahan"], "title": "Wireless Sensing of Temperature, Strain and Crack Growth in 3D-Printed Metal Structures via Magnetoelastic and Thermomagnetic Inclusions", "comment": "16 pages, 9 figures", "summary": "In this study, we demonstrate the first realization of wireless strain and\ntemperature sensing within 3D-printed metallic structures using standard\nelectromagnetic inspection hardware. This establishes a path toward need-based\nparts maintenance driven by accurate damage assessments instead of relying on\nregularly scheduled maintenance teardowns, extending the service intervals of\nstructures operating in harsh environments. To this end, we encapsulate\nmagnetoelastic and thermomagnetic materials inside microtubes and embed the\nsensing elements during additive manufacturing. Mechanical and thermal stimuli\naffect the magnetic permeability of the embedded materials, which modulates the\nimpedance of a coil placed on or near the surface of the printed part. We\ndemonstrate strain sensing accurate to +/-27x10-6 over at least a 6x10-4 strain\nrange, and temperature sensing accurate to +/-0.75oC over a 70oC range, both to\na 95% confidence interval. We highlight these sensors' capabilities by\ndetecting the onset of plasticity and fatigue-driven crack growth thousands of\ncycles before critical failure. This extends non-destructive eddy-current\ndamage detection to accurate, real-time strain and temperature monitoring\nwithin metallic structures.", "AI": {"tldr": "First wireless strain and temperature sensing in 3D-printed metallic structures using electromagnetic inspection hardware, enabling need-based maintenance instead of scheduled teardowns.", "motivation": "To establish need-based parts maintenance driven by accurate damage assessments rather than regularly scheduled maintenance teardowns, extending service intervals for structures in harsh environments.", "method": "Encapsulate magnetoelastic and thermomagnetic materials inside microtubes and embed them during additive manufacturing. Mechanical/thermal stimuli affect magnetic permeability, modulating coil impedance placed near the printed part surface.", "result": "Strain sensing accurate to +/-27x10-6 over at least 6x10-4 strain range, and temperature sensing accurate to +/-0.75\u00b0C over 70\u00b0C range (95% confidence). Demonstrated detection of plasticity onset and fatigue crack growth thousands of cycles before critical failure.", "conclusion": "Extends non-destructive eddy-current damage detection to accurate, real-time strain and temperature monitoring within metallic structures, enabling predictive maintenance and extended service life."}}
{"id": "2510.12278", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12278", "abs": "https://arxiv.org/abs/2510.12278", "authors": ["Alessia Ciacco", "Francesca Guerriero", "Eneko Osaba"], "title": "Quantum Annealing for Staff Scheduling in Educational Environments", "comment": "8 pages, 3 tables, and 1 figure. Paper submitted to the International\n  Conference on Quantum Communications, Networking, and Computing (QCNC 2026)", "summary": "We address a novel staff allocation problem that arises in the organization\nof collaborators among multiple school sites and educational levels. The\nproblem emerges from a real case study in a public school in Calabria, Italy,\nwhere staff members must be distributed across kindergartens, primary, and\nsecondary schools under constraints of availability, competencies, and\nfairness. To tackle this problem, we develop an optimization model and\ninvestigate a solution approach based on quantum annealing. Our computational\nexperiments on real-world data show that quantum annealing is capable of\nproducing balanced assignments in short runtimes. These results provide\nevidence of the practical applicability of quantum optimization methods in\neducational scheduling and, more broadly, in complex resource allocation tasks.", "AI": {"tldr": "Quantum annealing approach for staff allocation across educational levels with constraints on availability, competencies, and fairness, tested on real data from Italian schools.", "motivation": "Address a real-world staff allocation problem in a public school system in Calabria, Italy, where staff must be distributed across kindergartens, primary, and secondary schools under multiple constraints.", "method": "Developed an optimization model and investigated a solution approach based on quantum annealing.", "result": "Computational experiments on real-world data show quantum annealing produces balanced assignments in short runtimes.", "conclusion": "Demonstrates practical applicability of quantum optimization methods in educational scheduling and complex resource allocation tasks."}}
{"id": "2510.11938", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11938", "abs": "https://arxiv.org/abs/2510.11938", "authors": ["Yanying Lin", "Shijie Peng", "Chengzhi Lu", "Chengzhong Xu", "Kejiang Ye"], "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters", "comment": "EuroSys 26", "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.", "AI": {"tldr": "FlexPipe dynamically reconfigures LLM serving pipelines at runtime to handle variable workloads and GPU fragmentation, achieving 8.5x better resource efficiency and 38.3% lower latency than state-of-the-art systems.", "motivation": "Current LLM serving systems use static pipeline configurations that cannot adapt to dynamic workload conditions and suffer from severe resource fragmentation in serverless clusters, leading to substantial inefficiencies.", "method": "FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity using: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation.", "result": "Evaluation on an 82-GPU cluster shows FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.", "conclusion": "FlexPipe successfully addresses the fundamental limitations of static pipeline configurations in LLM serving by enabling dynamic runtime reconfiguration, significantly improving resource utilization and performance in production environments."}}
{"id": "2510.12166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12166", "abs": "https://arxiv.org/abs/2510.12166", "authors": ["Kenneth Weiss", "Thomas M. Stitt", "Daryl Hawkins", "Olga Pearce", "Stephanie Brink", "Robert N. Rieben"], "title": "Comparing Cross-Platform Performance via Node-to-Node Scaling Studies", "comment": "16 pages; accepted to the International Journal of High Performance\n  Computing Applications (IJHPCA)", "summary": "Due to the increasing diversity of high-performance computing architectures,\nresearchers and practitioners are increasingly interested in comparing a code's\nperformance and scalability across different platforms. However, there is a\nlack of available guidance on how to actually set up and analyze such\ncross-platform studies. In this paper, we contend that the natural base unit of\ncomputing for such studies is a single compute node on each platform and offer\nguidance in setting up, running, and analyzing node-to-node scaling studies. We\npropose templates for presenting scaling results of these studies and provide\nseveral case studies highlighting the benefits of this approach.", "AI": {"tldr": "This paper provides guidance for conducting cross-platform performance studies using single compute nodes as the base unit, offering templates for presenting scaling results and case studies.", "motivation": "The increasing diversity of high-performance computing architectures has created a need for comparing code performance across different platforms, but there is a lack of available guidance on how to properly conduct such cross-platform studies.", "method": "The authors propose using a single compute node on each platform as the natural base unit for computing studies, and provide guidance for setting up, running, and analyzing node-to-node scaling studies with templates for presenting results.", "result": "The paper provides several case studies that highlight the benefits of this approach to cross-platform performance analysis.", "conclusion": "This work offers practical guidance and templates for conducting meaningful cross-platform performance comparisons using node-to-node scaling studies as a standardized methodology."}}
{"id": "2510.12277", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.12277", "abs": "https://arxiv.org/abs/2510.12277", "authors": ["Thomas Benz", "Axel Vanoni", "Michael Rogenmoser", "Luca Benini"], "title": "A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems", "comment": "6 pages, 5 figures", "summary": "With the ever-growing heterogeneity in computing systems, driven by modern\nmachine learning applications, pressure is increasing on memory systems to\nhandle arbitrary and more demanding transfers efficiently. Descriptor-based\ndirect memory access controllers (DMACs) allow such transfers to be executed by\ndecoupling memory transfers from processing units. Classical descriptor-based\nDMACs are inefficient when handling arbitrary transfers of small unit sizes.\nExcessive descriptor size and the serialized nature of processing descriptors\nemployed by the DMAC lead to large static overheads when setting up transfers.\nTo tackle this inefficiency, we propose a descriptor-based DMAC optimized to\nefficiently handle arbitrary transfers of small unit sizes. We implement a\nlightweight descriptor format in an AXI4-based DMAC. We further increase\nperformance by implementing a low-overhead speculative descriptor prefetching\nscheme without additional latency penalties in the case of a misprediction. Our\nDMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a\nKintex FPGA to evaluate its performance. Compared to an off-the-shelf\ndescriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,\nincrease bus utilization up to 2.5x in an ideal memory system with\n64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer\nflip-flops, and no block RAMs. We can extend our lead in bus utilization to\n3.6x with 64-byte-length transfers in deep memory systems. We synthesized our\nDMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44\nGHz while occupying only 49.5 kGE.", "AI": {"tldr": "Proposed a descriptor-based DMA controller optimized for small unit transfers with lightweight descriptors and speculative prefetching, achieving 1.66x lower latency and up to 3.6x higher bus utilization while using fewer hardware resources.", "motivation": "Classical descriptor-based DMACs are inefficient for arbitrary small transfers due to excessive descriptor size and serialized processing, leading to large static overheads in modern heterogeneous computing systems with demanding memory transfers.", "method": "Implemented a lightweight descriptor format in AXI4-based DMAC with low-overhead speculative descriptor prefetching scheme, integrated into 64-bit Linux-capable RISC-V SoC and emulated on Kintex FPGA.", "result": "Achieved 1.66x less latency launching transfers, up to 2.5x bus utilization in ideal memory system (3.6x in deep memory systems) with 64-byte transfers, using 11% fewer LUTs, 23% fewer FFs, no BRAMs, and synthesized at 1.44 GHz occupying 49.5 kGE.", "conclusion": "The proposed DMAC efficiently handles arbitrary small transfers with significant performance improvements and reduced hardware overhead compared to off-the-shelf solutions."}}
{"id": "2510.12280", "categories": ["cs.PF", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.12280", "abs": "https://arxiv.org/abs/2510.12280", "authors": ["Yosuke Bando", "Akinobu Mita", "Kazuhiro Hiwada", "Shintaro Sano", "Tomoya Suzuki", "Yu Nakanishi", "Kazutaka Tomida", "Hirotsugu Kajihara", "Akiyuki Kaneko", "Daisuke Taki", "Yukimasa Miyamoto", "Tomokazu Yoshida", "Tatsuo Shiozawa"], "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores", "comment": null, "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.", "AI": {"tldr": "SSD-based KV stores can effectively use microsecond-latency memory instead of expensive DRAM by leveraging software prefetching from user-level threads, achieving near-DRAM performance even with 5\u03bcs memory latency.", "motivation": "To reduce costs by offloading in-memory data structures from expensive host DRAM to cheaper microsecond-latency secondary memory while maintaining performance in SSD-based key-value stores.", "method": "Analyzed impact of microsecond memory latency on KV throughput, modeled interplay between prefetching and I/O, designed microbenchmarks, modified existing KV stores to use software prefetching from user-level threads, and tested on FPGA-based memory with adjustable latency.", "result": "KV operation throughputs matched the model predictions, and modified KV stores achieved near-DRAM throughputs even with 5\u03bcs memory latency.", "conclusion": "SSD-based KV stores can use microsecond-latency memory as a cost-effective DRAM alternative without needing new techniques, as existing prefetching methods combined with I/O provide sufficient latency tolerance."}}
{"id": "2510.12196", "categories": ["cs.DC", "8W10"], "pdf": "https://arxiv.org/pdf/2510.12196", "abs": "https://arxiv.org/abs/2510.12196", "authors": ["Petr Samoldekin", "Christian Schulz", "Henning Woydt"], "title": "GPU-Accelerated Algorithms for Process Mapping", "comment": null, "summary": "Process mapping asks to assign vertices of a task graph to processing\nelements of a supercomputer such that the computational workload is balanced\nwhile the communication cost is minimized. Motivated by the recent success of\nGPU-based graph partitioners, we propose two GPU-accelerated algorithms for\nthis optimization problem. The first algorithm employs hierarchical\nmultisection, which partitions the task graph alongside the hierarchy of the\nsupercomputer. The method utilizes GPU-based graph partitioners to accelerate\nthe mapping process. The second algorithm integrates process mapping directly\ninto the modern multilevel graph partitioning pipeline. Vital phases like\ncoarsening and refinement are accelerated by exploiting the parallelism of\nGPUs. In our experiments, both methods achieve speedups exceeding 300 when\ncompared to state-of-the-art CPU-based algorithms. The first algorithm has, on\naverage, about 10 percent greater communication costs and thus remains\ncompetitive to CPU algorithms. The second approach is much faster, with a\ngeometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower\nsolution quality. To our knowledge, these are the first GPU-based algorithms\nfor process mapping.", "AI": {"tldr": "GPU-accelerated process mapping algorithms for supercomputers that achieve significant speedups over CPU-based methods while maintaining competitive solution quality.", "motivation": "To leverage the recent success of GPU-based graph partitioners for solving process mapping problems in supercomputers, where computational workload must be balanced while minimizing communication costs.", "method": "Two GPU-accelerated approaches: 1) Hierarchical multisection that partitions task graphs alongside supercomputer hierarchy using GPU-based graph partitioners; 2) Integration of process mapping directly into multilevel graph partitioning pipeline with GPU-accelerated coarsening and refinement phases.", "result": "Both methods achieve speedups exceeding 300x compared to state-of-the-art CPU algorithms. First method has ~10% higher communication costs but remains competitive. Second method achieves geometric mean speedup of 77.6x and peak speedup of 598x at cost of lower solution quality.", "conclusion": "These are the first GPU-based algorithms for process mapping, demonstrating significant performance improvements while maintaining acceptable solution quality, with trade-offs between speed and optimization quality."}}
{"id": "2510.12436", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.12436", "abs": "https://arxiv.org/abs/2510.12436", "authors": ["Valentin Seitz", "Jordy Trilaksono", "Marta Garcia-Gasulla"], "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework", "comment": null, "summary": "Ensuring good performance is a key aspect in the development of codes that\ntarget HPC machines. As these codes are under active development, the necessity\nto detect performance degradation early in the development process becomes\napparent. In addition, having meaningful insight into application scaling\nbehavior tightly coupled to the development workflow is helpful. In this paper,\nwe introduce TALP-Pages, an easy-to-integrate framework that enables developers\nto get fast and in-repository feedback about their code performance using\nestablished fundamental performance and scaling factors. The framework relies\non TALP, which enables the on-the-fly collection of these metrics. Based on a\nfolder structure suited for CI which contains the files generated by TALP,\nTALP-Pages generates an HTML report with visualizations of the performance\nfactor regression as well as scaling-efficiency tables. We compare TALP-Pages\nto tracing-based tools in terms of overhead and post-processing requirements\nand find that TALP-Pages can produce the scaling-efficiency tables faster and\nunder tighter resource constraints. To showcase the ease of use and\neffectiveness of this approach, we extend the current CI setup of GENE-X with\nonly minimal changes required and showcase the ability to detect and explain a\nperformance improvement.", "AI": {"tldr": "TALP-Pages is a framework that provides fast, in-repository performance feedback for HPC codes using fundamental performance and scaling metrics, integrated with CI workflows.", "motivation": "To detect performance degradation early in HPC code development and provide meaningful insight into application scaling behavior during development.", "method": "Uses TALP for on-the-fly collection of performance metrics, generates HTML reports with visualizations of performance factor regression and scaling-efficiency tables based on CI-friendly folder structure.", "result": "TALP-Pages produces scaling-efficiency tables faster than tracing-based tools with lower overhead and tighter resource constraints. Successfully integrated with GENE-X CI setup with minimal changes.", "conclusion": "The framework effectively enables performance monitoring in development workflows and can detect and explain performance improvements."}}
{"id": "2510.12274", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12274", "abs": "https://arxiv.org/abs/2510.12274", "authors": ["Hao Jiang", "Meng Qin", "Ruijie Kuai", "Dandan Liang"], "title": "Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness", "comment": "16 pages, 16 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "With the rapid growth in computing power demand, cloud native networks have\nemerged as a promising solution to address the challenges of efficient resource\ncoordination, particularly in coping with the dynamic fluctuations of network\nbandwidth in clusters. We propose Metronome, a network-aware and priority-aware\nscheduling mechanism for cloud native networks. This mechanism is designed to\nsupport jobs that exhibit periodic traffic patterns and dynamic bandwidth\ndemands, particularly in the context of distributed training. Specifically,\nMetronome employs a time-division multiplexing approach that leverages job\ntraffic characteristics to construct an elastic network resource allocation\nmodel, enabling efficient bandwidth sharing across multiple jobs. In addition,\nit incorporates a multi-objective optimization strategy, jointly considering\nlatency and job priorities to achieve globally optimal as well as dynamic\nresource allocation. Finally, Metronome adapts to the dynamic environment by\nmonitoring the cluster and performing reconfiguration operations. Extensive\nexperiments with 13 common machine learning models demonstrate that Metronome\ncan enhance cluster resource utilization while guaranteeing service\nperformance. Compared with the existing Kubernetes scheduling mechanisms across\nmultiple scenarios, Metronome reduces job completion time by up to 19.50% while\nimproving average bandwidth utilization by up to 23.20%.", "AI": {"tldr": "Metronome is a network-aware, priority-aware scheduling mechanism for cloud native networks that uses time-division multiplexing and multi-objective optimization to improve bandwidth utilization and reduce job completion times.", "motivation": "Address the challenges of efficient resource coordination in cloud native networks, particularly coping with dynamic network bandwidth fluctuations in clusters, especially for jobs with periodic traffic patterns and dynamic bandwidth demands like distributed training.", "method": "Uses time-division multiplexing approach leveraging job traffic characteristics to create elastic network resource allocation model; incorporates multi-objective optimization strategy considering latency and job priorities; adapts to dynamic environment through cluster monitoring and reconfiguration operations.", "result": "Extensive experiments with 13 ML models show Metronome enhances cluster resource utilization while guaranteeing service performance. Reduces job completion time by up to 19.50% and improves average bandwidth utilization by up to 23.20% compared to existing Kubernetes scheduling mechanisms.", "conclusion": "Metronome effectively addresses dynamic bandwidth allocation challenges in cloud native networks, demonstrating significant improvements in both job completion times and bandwidth utilization across multiple scenarios."}}
{"id": "2510.12354", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12354", "abs": "https://arxiv.org/abs/2510.12354", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel", "Stefan Tai"], "title": "A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures gain traction in federated environments,\norganizations are increasingly building consumer-specific data-sharing\npipelines using modular, cloud-native transformation services. Prior work has\nshown that structuring these pipelines with reusable transformation stages\nenhances both scalability and energy efficiency. However, integrating\ntraditional cloud design patterns into such pipelines poses a challenge:\npredefining and embedding patterns can compromise modularity, reduce\nreusability, and conflict with the pipelines dynamic, consumer-driven nature.\nTo address this, we introduce a Kubernetes-based tool that enables the deferred\nand non-intrusive application of selected cloud design patterns without\nrequiring changes to service source code. The tool supports automated pattern\ninjection and collects energy consumption metrics, allowing developers to make\nenergy-aware decisions while preserving the flexible, composable structure of\nreusable data-sharing pipelines.", "AI": {"tldr": "A Kubernetes-based tool that enables deferred, non-intrusive application of cloud design patterns to data-sharing pipelines without modifying service source code, while collecting energy metrics for energy-aware decisions.", "motivation": "Traditional cloud design patterns compromise modularity and reusability when pre-defined in data-sharing pipelines, conflicting with their dynamic, consumer-driven nature.", "method": "Developed a Kubernetes-based tool that supports automated pattern injection and energy consumption metrics collection without requiring changes to service source code.", "result": "The tool enables deferred and non-intrusive application of selected cloud design patterns while preserving the flexible, composable structure of reusable data-sharing pipelines.", "conclusion": "This approach allows developers to make energy-aware decisions while maintaining the modularity and reusability benefits of data mesh architectures in federated environments."}}
{"id": "2510.12597", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12597", "abs": "https://arxiv.org/abs/2510.12597", "authors": ["Ilya Baldin", "Michael Goodrich", "Vardan Gyurjyan", "Graham Heyes", "Derek Howard", "Yatish Kumar", "David Lawrence", "Brad Sawatzky", "Stacey Sheldon", "Carl Timmer"], "title": "Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT", "comment": null, "summary": "Thomas Jefferson National Accelerator Facility (JLab) has partnered with\nEnergy Sciences Network (ESnet) to define and implement an edge to compute\ncluster computational load balancing acceleration architecture. The ESnet-JLab\nFPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to\naddress compression, fragmentation, UDP packet destination redirection (Network\nAddress Translation (NAT)) and decompression and reassembly.\n  EJFAT seamlessly integrates edge and cluster computing to support direct\nprocessing of streamed experimental data. This will directly benefit the JLab\nscience program as well as data centers of the future that require high\nthroughput and low latency for both time-critical data acquisition systems and\ndata center workflows.\n  The EJFAT project will be presented along with how it is synergistic with\nother DOE activities such as an Integrated Research Infrastructure (IRI), and\nrecent results using data sources at JLab, an EJFAT LB at ESnet, and\ncomputational cluster resources at Lawrence Berkeley National Laboratory\n(LBNL).", "AI": {"tldr": "JLab and ESnet developed EJFAT, an FPGA-accelerated architecture for computational load balancing that handles compression, fragmentation, NAT, decompression and reassembly of streamed experimental data between edge and cluster computing.", "motivation": "To address the need for high throughput and low latency processing of streamed experimental data in scientific facilities, supporting both time-critical data acquisition systems and data center workflows.", "method": "Implemented FPGA acceleration architecture focusing on compression, fragmentation, UDP packet destination redirection (NAT), decompression and reassembly to seamlessly integrate edge and cluster computing.", "result": "Successfully demonstrated integration with data sources at JLab, EJFAT load balancer at ESnet, and computational cluster resources at LBNL, showing synergy with DOE activities like Integrated Research Infrastructure.", "conclusion": "EJFAT provides an effective solution for direct processing of streamed experimental data that benefits JLab's science program and future data centers requiring high throughput and low latency capabilities."}}
