{"id": "2510.26197", "categories": ["cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26197", "abs": "https://arxiv.org/abs/2510.26197", "authors": ["Riya Samanta"], "title": "Structurally Valid Log Generation using FSM-GFlowNets", "comment": null, "summary": "Generating structurally valid and behaviorally diverse synthetic event logs\nfor interaction-aware models is a challenging yet crucial problem, particularly\nin settings with limited or privacy constrained user data. Existing methods\nsuch as heuristic simulations and LLM based generators often lack structural\ncoherence or controllability, producing synthetic data that fails to accurately\nrepresent real world system interactions. This paper presents a framework that\nintegrates Finite State Machines or FSMs with Generative Flow Networks or\nGFlowNets to generate structured, semantically valid, and diverse synthetic\nevent logs. Our FSM-constrained GFlowNet ensures syntactic validity and\nbehavioral variation through dynamic action masking and guided sampling. The\nFSM, derived from expert traces, encodes domain-specific rules, while the\nGFlowNet is trained using a flow matching objective with a hybrid reward\nbalancing FSM compliance and statistical fidelity. We instantiate the framework\nin the context of UI interaction logs using the UIC HCI dataset, but the\napproach generalizes to any symbolic sequence domain. Experimental results\nbased on distributional metrics show that our FSM GFlowNet produces realistic,\nstructurally consistent logs, achieving, for instance, under the real user logs\nbaseline, a KL divergence of 0.2769 and Chi squared distance of 0.3522,\nsignificantly outperforming GPT-4o's 2.5294/13.8020 and Gemini's\n3.7233/63.0355, alongside a leading bigram overlap of 0.1214 vs. GPT 4o's\n0.0028 and Gemini's 0.0007. A downstream use case intent classification\ndemonstrates that classifiers trained solely on our synthetic logs produced\nfrom FSM-GFlowNet achieve competitive accuracy compared to real data."}
{"id": "2510.25958", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.25958", "abs": "https://arxiv.org/abs/2510.25958", "authors": ["Lukas Pfromm", "Alish Kanani", "Harsh Sharma", "Janardhan Rao Doppa", "Partha Pratim Pande", "Umit Y. Ogras"], "title": "CHIPSIM: A Co-Simulation Framework for Deep Learning on Chiplet-Based Systems", "comment": "Accepted at IEEE Open Journal of the Solid-State Circuits Society", "summary": "Due to reduced manufacturing yields, traditional monolithic chips cannot keep\nup with the compute, memory, and communication demands of data-intensive\napplications, such as rapidly growing deep neural network (DNN) models.\nChiplet-based architectures offer a cost-effective and scalable solution by\nintegrating smaller chiplets via a network-on-interposer (NoI). Fast and\naccurate simulation approaches are critical to unlocking this potential, but\nexisting methods lack the required accuracy, speed, and flexibility. To address\nthis need, this work presents CHIPSIM, a comprehensive co-simulation framework\ndesigned for parallel DNN execution on chiplet-based systems. CHIPSIM\nconcurrently models computation and communication, accurately capturing network\ncontention and pipelining effects that conventional simulators overlook.\nFurthermore, it profiles the chiplet and NoI power consumptions at microsecond\ngranularity for precise transient thermal analysis. Extensive evaluations with\nhomogeneous/heterogeneous chiplets and different NoI architectures demonstrate\nthe framework's versatility, up to 340% accuracy improvement, and power/thermal\nanalysis capability."}
{"id": "2510.25963", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.25963", "abs": "https://arxiv.org/abs/2510.25963", "authors": ["Izzy Grosof", "Daniela Hurtado-Lange"], "title": "Outperforming Multiserver SRPT at All Loads", "comment": "36 pages. Accepted to SIGMETRICS 2026", "summary": "A well-designed scheduling policy can unlock significant performance\nimprovements with no additional resources. Multiserver SRPT (SRPT-$k$) is known\nto achieve asymptotically optimal mean response time in the heavy traffic\nlimit, as load approaches capacity. No better policy is known for the M/G/$k$\nqueue in any regime.\n  We introduce a new policy, SRPT-Except-$k+1$ & Modified SRPT (SEK-SMOD),\nwhich is the first policy to provably achieve lower mean response time than\nSRPT-$k$. SEK-SMOD outperforms SRPT-$k$ across all loads and all job size\ndistributions. The key idea behind SEK-SMOD is to prioritize large jobs over\nsmall jobs in specific scenarios to improve server utilization, and thereby\nimprove the response time of subsequent jobs in expectation. Our proof is a\nnovel application of hybrid worst-case and stochastic techniques to relative\nanalysis, where we analyze the deviations of our proposed SEK-SMOD policy away\nfrom the SRPT-$k$ baseline policy. Furthermore, we design Practical-SEK (a\nsimplified variant of SEK-SMOD) and empirically verify the improvement over\nSRPT-$k$ via simulation."}
{"id": "2510.26730", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.26730", "abs": "https://arxiv.org/abs/2510.26730", "authors": ["Zixu Shen", "Kexin Chu", "Yifan Zhang", "Dawei Xiang", "Runxin Wu", "Wei Zhang"], "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference", "comment": "12 pages, 11 figures", "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."}
{"id": "2510.26463", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.26463", "abs": "https://arxiv.org/abs/2510.26463", "authors": ["Xiaolin He", "Cenlin Duan", "Yingjie Qi", "Xiao Ma", "Jianlei Yang"], "title": "MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator", "comment": "7 pages, accepted by ASP-DAC 2026", "summary": "Computing-in-Memory (CIM) architectures have emerged as a promising solution\nfor accelerating Deep Neural Networks (DNNs) by mitigating data movement\nbottlenecks. However, realizing the potential of CIM requires specialized\ndataflow optimizations, which are challenged by an expansive design space and\nstrict architectural constraints. Existing optimization approaches often fail\nto fully exploit CIM accelerators, leading to noticeable gaps between\ntheoretical and actual system-level efficiency. To address these limitations,\nwe propose the MIREDO framework, which formulates dataflow optimization as a\nMixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical\nhardware abstraction coupled with an analytical latency model designed to\naccurately reflect the complex data transfer behaviors within CIM systems. By\njointly modeling workload characteristics, dataflow strategies, and\nCIM-specific constraints, MIREDO systematically navigates the vast design space\nto determine the optimal dataflow configurations. Evaluation results\ndemonstrate that MIREDO significantly enhances performance, achieving up to\n$3.2\\times$ improvement across various DNN models and hardware setups."}
{"id": "2510.26008", "categories": ["cs.PF", "cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26008", "abs": "https://arxiv.org/abs/2510.26008", "authors": ["Ziji Chen", "Steven Chien", "Peng Qian", "Noa Zilberman"], "title": "Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry", "comment": "12 pages, 9 figures, submitted to nsdi 26", "summary": "Modern machine learning (ML) has grown into a tightly coupled, full-stack\necosystem that combines hardware, software, network, and applications. Many\nusers rely on cloud providers for elastic, isolated, and cost-efficient\nresources. Unfortunately, these platforms as a service use virtualization,\nwhich means operators have little insight into the users' workloads. This\nhinders resource optimizations by the operator, which is essential to ensure\ncost efficiency and minimize execution time. In this paper, we argue that\nworkload knowledge is unnecessary for system-level optimization. We propose\nSystem-X, which takes a \\emph{hardware-centric} approach, relying only on\nhardware signals -- fully accessible by operators. Using low-level signals\ncollected from the system, System-X detects anomalies through an unsupervised\nlearning pipeline. The pipeline is developed by analyzing over 30 popular ML\nmodels on various hardware platforms, ensuring adaptability to emerging\nworkloads and unknown deployment patterns. Using System-X, we successfully\nidentified both network and system configuration issues, accelerating the\nDeepSeek model by 5.97%."}
{"id": "2510.26008", "categories": ["cs.PF", "cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26008", "abs": "https://arxiv.org/abs/2510.26008", "authors": ["Ziji Chen", "Steven Chien", "Peng Qian", "Noa Zilberman"], "title": "Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry", "comment": "12 pages, 9 figures, submitted to nsdi 26", "summary": "Modern machine learning (ML) has grown into a tightly coupled, full-stack\necosystem that combines hardware, software, network, and applications. Many\nusers rely on cloud providers for elastic, isolated, and cost-efficient\nresources. Unfortunately, these platforms as a service use virtualization,\nwhich means operators have little insight into the users' workloads. This\nhinders resource optimizations by the operator, which is essential to ensure\ncost efficiency and minimize execution time. In this paper, we argue that\nworkload knowledge is unnecessary for system-level optimization. We propose\nSystem-X, which takes a \\emph{hardware-centric} approach, relying only on\nhardware signals -- fully accessible by operators. Using low-level signals\ncollected from the system, System-X detects anomalies through an unsupervised\nlearning pipeline. The pipeline is developed by analyzing over 30 popular ML\nmodels on various hardware platforms, ensuring adaptability to emerging\nworkloads and unknown deployment patterns. Using System-X, we successfully\nidentified both network and system configuration issues, accelerating the\nDeepSeek model by 5.97%."}
{"id": "2510.26008", "categories": ["cs.PF", "cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26008", "abs": "https://arxiv.org/abs/2510.26008", "authors": ["Ziji Chen", "Steven Chien", "Peng Qian", "Noa Zilberman"], "title": "Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry", "comment": "12 pages, 9 figures, submitted to nsdi 26", "summary": "Modern machine learning (ML) has grown into a tightly coupled, full-stack\necosystem that combines hardware, software, network, and applications. Many\nusers rely on cloud providers for elastic, isolated, and cost-efficient\nresources. Unfortunately, these platforms as a service use virtualization,\nwhich means operators have little insight into the users' workloads. This\nhinders resource optimizations by the operator, which is essential to ensure\ncost efficiency and minimize execution time. In this paper, we argue that\nworkload knowledge is unnecessary for system-level optimization. We propose\nSystem-X, which takes a \\emph{hardware-centric} approach, relying only on\nhardware signals -- fully accessible by operators. Using low-level signals\ncollected from the system, System-X detects anomalies through an unsupervised\nlearning pipeline. The pipeline is developed by analyzing over 30 popular ML\nmodels on various hardware platforms, ensuring adaptability to emerging\nworkloads and unknown deployment patterns. Using System-X, we successfully\nidentified both network and system configuration issues, accelerating the\nDeepSeek model by 5.97%."}
{"id": "2510.26524", "categories": ["cs.PF", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26524", "abs": "https://arxiv.org/abs/2510.26524", "authors": ["Abdelhakim Ziani", "András Horváth", "Paolo Ballarini"], "title": "Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models", "comment": null, "summary": "Heavy-tailed distributions, prevalent in a lot of real-world applications\nsuch as finance, telecommunications, queuing theory, and natural language\nprocessing, are challenging to model accurately owing to their slow tail decay.\nBernstein phase-type (BPH) distributions, through their analytical tractability\nand good approximations in the non-tail region, can present a good solution,\nbut they suffer from an inability to reproduce these heavy-tailed behaviors\nexactly, thus leading to inadequate performance in important tail areas. On the\ncontrary, while highly adaptable to heavy-tailed distributions,\nhyperexponential (HE) models struggle in the body part of the distribution.\nAdditionally, they are highly sensitive to initial parameter selection,\nsignificantly affecting their precision.\n  To solve these issues, we propose a novel hybrid model of BPH and HE\ndistributions, borrowing the most desirable features from each for enhanced\napproximation quality. Specifically, we leverage an optimization to set initial\nparameters for the HE component, significantly enhancing its robustness and\nreducing the possibility that the associated procedure results in an invalid HE\nmodel. Experimental validation demonstrates that the novel hybrid approach is\nmore performant than individual application of BPH or HE models. More\nprecisely, it can capture both the body and the tail of heavy-tailed\ndistributions, with a considerable enhancement in matching parameters such as\nmean and coefficient of variation. Additional validation through experiments\nutilizing queuing theory proves the practical usefulness, accuracy, and\nprecision of our hybrid approach."}
{"id": "2510.26730", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.26730", "abs": "https://arxiv.org/abs/2510.26730", "authors": ["Zixu Shen", "Kexin Chu", "Yifan Zhang", "Dawei Xiang", "Runxin Wu", "Wei Zhang"], "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference", "comment": "12 pages, 11 figures", "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."}
