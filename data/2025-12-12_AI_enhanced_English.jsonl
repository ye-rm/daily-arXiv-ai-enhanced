{"id": "2512.09942", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09942", "abs": "https://arxiv.org/abs/2512.09942", "authors": ["Zhiming Liang", "Bin Chen", "Litao Ye", "Chen Sun", "Shuo Wang", "Zhe Peng"], "title": "A study of the spectrum resource leasing method based on ERC4907 extension", "comment": null, "summary": "The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.", "AI": {"tldr": "The paper proposes M-ERC4907, an extension to ERC4907 that enables batch configuration of multiple time slots and simultaneous authorization of multiple users, addressing limitations in decentralized multi-slot scheduling scenarios.", "motivation": "The ERC4907 standard for rentable NFTs is limited to single-user, single-time-slot authorization, which restricts its applicability and efficiency in decentralized multi-slot scheduling scenarios where multiple time slots and users need to be managed simultaneously.", "method": "The paper proposes the Multi-slot ERC4907 (M-ERC4907) extension method, which introduces novel functionalities to support batch configuration of multiple time slots and simultaneous authorization of multiple users, eliminating the rigid sequential authorization constraint of the original ERC4907 standard.", "result": "Experimental results on the Remix development platform show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency compared to the original ERC4907 standard.", "conclusion": "The M-ERC4907 extension effectively addresses the limitations of ERC4907 by enabling multi-slot and multi-user authorization, resulting in improved efficiency, reduced transaction costs, and better scalability for decentralized scheduling applications."}}
{"id": "2512.09946", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09946", "abs": "https://arxiv.org/abs/2512.09946", "authors": ["Hung-Yueh Chiang", "Bokun Wang", "Diana Marculescu"], "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs", "comment": null, "summary": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.", "AI": {"tldr": "ELANA is an open-source lightweight profiling tool for evaluating LLM latency, power consumption, and efficiency across diverse hardware platforms from edge devices to cloud GPUs.", "motivation": "The latency and power consumption of LLMs are major constraints when serving them across various hardware platforms, creating a need for benchmarking tools to optimize efficiency in model deployment and development.", "method": "Developed ELANA as a lightweight, academic-friendly profiler that analyzes model size, KV cache size, prefilling latency (TTFT), generation latency (TPOT), end-to-end latency (TTLT), and optional energy consumption logging on multi-GPU and edge GPU platforms.", "result": "ELANA supports all publicly available models on Hugging Face, offers a simple command-line interface, is fully compatible with Hugging Face APIs, and can be customized for compressed or low bit-width models.", "conclusion": "ELANA is an ideal tool for research on efficient LLMs or small-scale proof-of-concept studies, addressing the need for benchmarking to optimize LLM efficiency across diverse hardware platforms."}}
{"id": "2512.09961", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09961", "abs": "https://arxiv.org/abs/2512.09961", "authors": ["Jinyu Chen", "Long Shi", "Taotao Wang", "Jiaheng Wang", "Wei Zhang"], "title": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0", "comment": null, "summary": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.", "AI": {"tldr": "TDC-Cache framework uses two-layer architecture with DON as trusted intermediary and DRL-DC for dynamic caching optimization, reducing latency by 20% and improving cache hit rate by 18%.", "motivation": "Address efficiency concerns from redundant data replication and security vulnerabilities from data inconsistency in Web3.0 decentralized data access.", "method": "Two-layer TDC-Cache framework with DON layer as trusted intermediary, DRL-DC for dynamic caching optimization using deep reinforcement learning, and PoCL consensus for maintaining caching decision consistency.", "result": "20% reduction in average access latency, up to 18% increase in cache hit rate, and 10% improvement in average success consensus rate compared to existing approaches.", "conclusion": "First investigation into decentralized caching framework for Web3.0, providing efficient caching and enhanced system resilience against adversarial threats."}}
{"id": "2512.09963", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09963", "abs": "https://arxiv.org/abs/2512.09963", "authors": ["Phuong Tran", "Tzu-Hao Liu", "Long Tan Le", "Tung-Anh Nguyen", "Van Quan La", "Eason Yu", "Han Shu", "Choong Seon Hong", "Nguyen H. Tran"], "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "comment": "Accepted at INFOCOM 2026", "summary": "Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.", "AI": {"tldr": "GOODSPEED is a distributed inference framework that optimizes goodput and fairness in LLM speculative decoding across multiple draft servers.", "motivation": "LLMs have high computational demands for real-time inference, especially in multi-user server environments. Speculative decoding helps accelerate inference but faces challenges in ensuring both high goodput and fairness across multiple cooperating draft servers.", "method": "GOODSPEED uses a central verification server coordinating heterogeneous draft servers with small language models. It employs a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function for proportional fairness, and processes speculative outputs in parallel.", "result": "Through fluid sample path analysis, GOODSPEED converges to optimal goodput allocation in steady-state and maintains near-optimal performance with provably bounded error under dynamic workloads.", "conclusion": "GOODSPEED provides a scalable, fair, and efficient solution for multi-server distributed LLM inference systems by optimizing goodput through adaptive speculative decoding."}}
{"id": "2512.10089", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10089", "abs": "https://arxiv.org/abs/2512.10089", "authors": ["Jeongeun Kim", "Sabrina Yarzada", "Paul Chen", "Christopher Torng"], "title": "Algorithm-Driven On-Chip Integration for High Density and Low Cost", "comment": null, "summary": "Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.", "AI": {"tldr": "A new approach for scalable multi-project chip tapeouts using automated packing, inter-site communication architecture, and on-chip power domains for per-project characterization.", "motivation": "Traditional multi-project wafer services face scalability limitations as project counts increase, and existing dense integration methods lack systematic principles for arrangement, linking, and validation of many small designs within shared dies.", "method": "Three key techniques: 1) Structured formulation of design space for automated algorithm-driven packing, 2) Architecture using narrow-area regions between sites for off-chip communication, 3) Practical approach for on-chip power domains enabling per-project power characterization without low-power ASIC expertise.", "result": "Achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering scalable and cost-effective path for large-scale tapeout environments.", "conclusion": "The approach addresses scalability limitations in semiconductor workforce development platforms by providing systematic methods for dense integration of many independent hardware designs with reduced overhead and improved efficiency."}}
{"id": "2512.10428", "categories": ["cs.ET", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.10428", "abs": "https://arxiv.org/abs/2512.10428", "authors": ["Haowen Yu", "Na Fan", "Xing Liu", "Ximin Lyu"], "title": "Design and Implementation of a High-Precision Wind-Estimation UAV with Onboard Sensors", "comment": "https://www.sciencedirect.com/science/article/abs/pii/S0263224125032415?via%3Dihub", "summary": "Accurate real-time wind vector estimation is essential for enhancing the safety, navigation accuracy, and energy efficiency of unmanned aerial vehicles (UAVs). Traditional approaches rely on external sensors or simplify vehicle dynamics, which limits their applicability during agile flight or in resource-constrained platforms. This paper proposes a real-time wind estimation method based solely on onboard sensors. The approach first estimates external aerodynamic forces using a disturbance observer (DOB), and then maps these forces to wind vectors using a thin-plate spline (TPS) model. A custom-designed wind barrel mounted on the UAV enhances aerodynamic sensitivity, further improving estimation accuracy. The system is validated through comprehensive experiments in wind tunnels, indoor and outdoor flights. Experimental results demonstrate that the proposed method achieves consistently high-accuracy wind estimation across controlled and real-world conditions, with speed RMSEs as low as \\SI{0.06}{m/s} in wind tunnel tests, \\SI{0.22}{m/s} during outdoor hover, and below \\SI{0.38}{m/s} in indoor and outdoor dynamic flights, and direction RMSEs under \\ang{7.3} across all scenarios, outperforming existing baselines. Moreover, the method provides vertical wind estimates -- unavailable in baselines -- with RMSEs below \\SI{0.17}{m/s} even during fast indoor translations.", "AI": {"tldr": "Real-time wind estimation for UAVs using only onboard sensors via disturbance observer and thin-plate spline mapping, achieving high accuracy across various flight conditions.", "motivation": "Accurate wind estimation is crucial for UAV safety, navigation, and energy efficiency, but traditional methods rely on external sensors or simplified dynamics, limiting applicability during agile flight or on resource-constrained platforms.", "method": "Uses disturbance observer (DOB) to estimate external aerodynamic forces, then maps these forces to wind vectors using thin-plate spline (TPS) model. A custom wind barrel enhances aerodynamic sensitivity for improved accuracy.", "result": "Achieves high-accuracy wind estimation: speed RMSEs as low as 0.06 m/s in wind tunnels, 0.22 m/s during outdoor hover, below 0.38 m/s in dynamic flights, and direction RMSEs under 7.3\u00b0 across all scenarios. Also provides vertical wind estimates with RMSEs below 0.17 m/s.", "conclusion": "The proposed method enables accurate real-time wind estimation using only onboard sensors, outperforming existing baselines and providing vertical wind estimates unavailable in previous approaches, making it suitable for agile flight and resource-constrained UAV platforms."}}
{"id": "2512.10236", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10236", "abs": "https://arxiv.org/abs/2512.10236", "authors": ["Shagnik Pal", "Shaizeen Aga", "Suchita Pati", "Mahzabeen Islam", "Lizy K. John"], "title": "Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap", "comment": null, "summary": "As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.", "AI": {"tldr": "FiCCO proposes finer-grain compute-communication overlap for distributed ML training/inference, achieving up to 1.6x speedup through bespoke schedules and GPU DMA offloading.", "motivation": "Current shard-level overlap techniques leave up to 1.7x ideal performance on the table due to exposed data-dependent communication. Coarse-grain overlap works but limits applicability to certain network topologies and dataflow patterns.", "method": "FiCCO (Finer-grain Compute-Communication Overlap) enables one-level deeper overlap than shard-level. The approach includes: 1) characterizing inefficiency losses from operation decomposition, 2) designing a space of FiCCO schedules, 3) overlaying schedules with inefficiency signatures, 4) developing heuristics for schedule selection, and 5) offloading communication to GPU DMA engines to minimize contention.", "result": "Bespoke FiCCO schedules deliver up to 1.6x speedup in realistic ML deployments. The proposed heuristics provide accurate guidance in 81% of unseen scenarios.", "conclusion": "Finer-grain compute-communication overlap unlocks performance benefits for distributed ML by enabling wider design space, better network topology support, and finer dataflow optimization, with intelligent schedule selection balancing overlap benefits against decomposition inefficiencies."}}
{"id": "2512.10155", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10155", "abs": "https://arxiv.org/abs/2512.10155", "authors": ["Jeongeun Kim", "Christopher Torng"], "title": "A Vertically Integrated Framework for Templatized Chip Design", "comment": null, "summary": "Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.", "AI": {"tldr": "A method to generate chips from high-level object-oriented software specs, preserving mental continuity between software abstractions and chip layout for introductory learners.", "motivation": "Software developers struggle to incorporate custom hardware despite its benefits for ML/AI applications. There's a need to reduce expertise barriers for software developers to participate in chip design.", "method": "Each software object maps to a corresponding region on the die (one-to-one structural mapping). Uses modular construction with vertically composed IP blocks implementing software behavioral protocols. Employs formal type systems based on sequences to check hardware module interactions against software communication patterns. Develops hardware interconnect strategies and object-aligned layout techniques.", "result": "The approach preserves mental continuity from software to chip design for new learners and enables practical layout generation.", "conclusion": "The method reduces the expertise required for software developers to participate in chip creation by maintaining familiar software abstractions throughout the hardware design flow."}}
{"id": "2512.10622", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.10622", "abs": "https://arxiv.org/abs/2512.10622", "authors": ["Eduardo Vyhmeister", "Bastien Pietropaoli", "Alejando Martinez Molina", "Montserrat Gonzalez-Ferreiro", "Gabriel Gonzalez-Castane", "Jordi Arjona Aroca", "Andrea Visentin"], "title": "Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation - Internal Processes Perspective", "comment": null, "summary": "Data valuation and monetisation are emerging as central challenges in data-driven economies, yet no unified framework exists to measure or manage data value across organisational contexts. This paper presents a systematic literature review of metrics and key performance indicators (KPIs) relevant to data valuation and monetisation, focusing on the Internal Processes Perspective of the Balanced Scorecard (BSC). As part of a broader effort to explore all four BSC perspectives, we identify, categorise, and interrelate hundreds of metrics within a comprehensive taxonomy structured around three core clusters: Data Quality, Governance & Compliance, and Operational Efficiency. The taxonomy consolidates overlapping definitions, clarifies conceptual dependencies, and links technical, organisational, and regulatory indicators that underpin data value creation. By integrating these dimensions, it provides a foundation for the development of standardised and evidence-based valuation frameworks. Beyond its theoretical contribution, the taxonomy supports ongoing practical applications in decision-support systems and data valuation models, advancing the broader goal of establishing a coherent, dynamic approach to assessing and monetising data across industries.", "AI": {"tldr": "Systematic review creates taxonomy of data valuation metrics organized around Data Quality, Governance & Compliance, and Operational Efficiency clusters within the Balanced Scorecard framework.", "motivation": "Address the lack of unified framework for measuring and managing data value across organizations in data-driven economies, focusing on the Internal Processes Perspective of the Balanced Scorecard.", "method": "Systematic literature review identifying, categorizing, and interrelating hundreds of metrics into a comprehensive taxonomy structured around three core clusters within the BSC framework.", "result": "Developed taxonomy consolidates overlapping definitions, clarifies conceptual dependencies, and links technical, organizational, and regulatory indicators that underpin data value creation.", "conclusion": "The taxonomy provides foundation for standardized valuation frameworks and supports practical applications in decision-support systems, advancing coherent approaches to data assessment and monetization."}}
{"id": "2512.10271", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10271", "abs": "https://arxiv.org/abs/2512.10271", "authors": ["Shruti Dongare", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Nannan Zhao", "Diego Melendez Maita", "Ali R. Butt"], "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "comment": null, "summary": "Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.", "AI": {"tldr": "RLTune is an RL-based scheduling framework for DL workloads on heterogeneous GPU clusters that improves GPU utilization by 20%, reduces queueing delay by 81%, and shortens job completion time by 70% without requiring per-job profiling.", "motivation": "Modern cloud platforms face challenges in scheduling large-scale deep learning workloads due to GPU heterogeneity and limited visibility into application characteristics, while existing schedulers rely on offline profiling or application-specific assumptions.", "method": "RLTune combines reinforcement learning-driven job prioritization with mixed-integer linear programming (MILP) based job-to-node mapping to optimize system-wide objectives like job completion time, queueing delay, and resource utilization.", "result": "Trained on production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens job completion time by as much as 70%.", "conclusion": "RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management."}}
{"id": "2512.10180", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10180", "abs": "https://arxiv.org/abs/2512.10180", "authors": ["Pracheta Harlikar", "Abdel-Hameed A. Badawy", "Prasanna Date"], "title": "Neuromorphic Processor Employing FPGA Technology with Universal Interconnections", "comment": null, "summary": "Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.", "AI": {"tldr": "A low-cost FPGA-based neuromorphic processor with configurable connectivity and LIF neurons, validated on Iris and MNIST datasets, offering energy-efficient open-source platform for SNN research.", "motivation": "Limited access to flexible, open-source neuromorphic computing platforms hinders widespread adoption and experimentation in this promising field for ultra-low-power and real-time inference applications.", "method": "Implemented a neuromorphic processor on Xilinx Zynq-7000 FPGA with all-to-all configurable connectivity, leaky integrate-and-fire neuron model with customizable parameters (threshold, synaptic weights, refractory period), and UART interface for runtime reconfiguration without hardware resynthesis.", "result": "Successfully validated the architecture using benchmark datasets (Iris classification and MNIST digit recognition), with post-synthesis results demonstrating energy efficiency and scalability of the design.", "conclusion": "The FPGA-based neuromorphic processor establishes viability as an accessible, adaptable research-grade platform for real-world spiking neural network applications, with plans to release as open source."}}
{"id": "2512.10312", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10312", "abs": "https://arxiv.org/abs/2512.10312", "authors": ["Julian Rodriguez", "Piotr Lopez", "Emiliano Lerma", "Rafael Medrano", "Jacobo Hernandez"], "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments", "comment": "8 pages, 3 figures", "summary": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.", "AI": {"tldr": "Course report detailing big data workflow from Epsilon dataset processing to text analysis with RestMex, movie analysis with IMDb, and Apache Spark cluster implementation.", "motivation": "To document and share the comprehensive big data practices and methodologies implemented during a big data course, demonstrating practical applications of various data processing and analysis techniques.", "method": "Multi-stage workflow: 1) Epsilon dataset processing using group and individual strategies, 2) Text analysis and classification with RestMex, 3) Movie feature analysis with IMDb dataset, 4) Technical implementation of distributed computing cluster using Apache Spark on Linux with Scala.", "result": "A complete documentation of implemented big data practices showing successful application of various data processing techniques and distributed computing infrastructure setup.", "conclusion": "The course successfully implemented a comprehensive big data workflow covering multiple datasets and analysis techniques, culminating in a functional distributed computing cluster using modern big data technologies."}}
{"id": "2512.10231", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10231", "abs": "https://arxiv.org/abs/2512.10231", "authors": ["Zhenguo Liu", "Chengao Shi", "Chen Ding", "Jiang Xu"], "title": "SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation", "comment": "Accepted by ASP-DAC 2026 conference", "summary": "For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.\n  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.", "AI": {"tldr": "SemanticBBV replaces traditional Basic Block Vectors with a two-stage framework using semantic embeddings and set transformers to create performance-aware signatures for cross-program simulation reuse, achieving 7143x speedup.", "motivation": "Traditional Basic Block Vectors have limitations: order-dependent IDs prevent cross-program knowledge reuse, and lack semantic content predictive of hardware performance, leaving optimization potential untapped.", "method": "Two-stage framework: 1) RWKV-based semantic encoder transforms assembly basic blocks into Basic Block Embeddings (BBEs), 2) order-invariant Set Transformer aggregates BBEs weighted by execution frequency into final signature, co-trained with triplet loss for distinctiveness and CPI regression for performance sensitivity.", "result": "SemanticBBV matches traditional BBVs in single-program accuracy while enabling cross-program analysis. Simulating just 14 universal program points estimated performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving 7143x simulation speedup. Signatures adapt to new microarchitectures with minimal fine-tuning.", "conclusion": "SemanticBBV addresses fundamental limitations of traditional BBVs by creating performance-aware signatures that enable unprecedented cross-program simulation reuse and massive speedups while maintaining accuracy."}}
{"id": "2512.10425", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10425", "abs": "https://arxiv.org/abs/2512.10425", "authors": ["Fan Yu", "Guodong Li", "Si Wu", "Weijun Fang", "Sihuang Hu"], "title": "Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability", "comment": null, "summary": "Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.", "AI": {"tldr": "CP-LRCs introduce cascaded parity groups in wide stripe LRCs to enable cooperation between local and global parity blocks, reducing repair costs for single and multi-node failures while maintaining MDS-level fault tolerance.", "motivation": "Existing Locally Repairable Codes (LRCs) have structural limitations in wide stripe settings: inflated local groups increase single-node repair cost, multi-node failures trigger expensive global repair, and reliability degrades sharply. The root cause is that local and global parity blocks are designed independently, preventing cooperation during repair.", "method": "CP-LRCs embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks, creating a cascaded parity group. The authors provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform variants.", "result": "Evaluations on Alibaba Cloud show CP-LRCs reduce repair time by up to 41% for single-node failures and 26% for two-node failures compared to existing LRCs.", "conclusion": "CP-LRCs overcome limitations of traditional LRCs in wide stripe settings by enabling cooperation between local and global parity blocks through cascaded parity groups, achieving significant repair cost reductions while maintaining fault tolerance."}}
{"id": "2512.10443", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10443", "abs": "https://arxiv.org/abs/2512.10443", "authors": ["Sabtain Ahmad", "Meerzhan Kanatbekova", "Ivona Brandic", "Atakan Aral"], "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation", "comment": null, "summary": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.", "AI": {"tldr": "CFLHKD: Hierarchical clustered federated learning with knowledge distillation for improved personalization and global model performance in heterogeneous IoT environments.", "motivation": "Conventional CFL approaches suffer from fragmented learning by training independent models per cluster and fail to leverage collective cluster insights, limiting efficiency and performance.", "method": "CFLHKD uses hierarchical CFL with bi-level aggregation (cluster-specific models at edge, unified global model at cloud) and multi-teacher knowledge distillation for inter-cluster knowledge sharing while preserving cluster-specific personalization.", "result": "Extensive evaluations show CFLHKD outperforms baselines in both cluster-specific and global model accuracy, achieving 3.32-7.57% performance improvement on standard benchmark datasets.", "conclusion": "Hierarchical CFL with knowledge distillation (CFLHKD) effectively bridges local and global learning, enabling better personalization while leveraging collective cluster insights for improved federated learning performance."}}
{"id": "2512.10576", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10576", "abs": "https://arxiv.org/abs/2512.10576", "authors": ["Xinhang Chen", "Chao Zhang", "Jiahuan He", "Wei Liu", "Jianming Zhang", "Wenlong Zhou", "Xiao Li", "Pai Zeng", "Shiyong Li", "Yuanpan Qian", "Dong Li", "Zhaogeng Li"], "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp", "comment": null, "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.", "AI": {"tldr": "ESS (Extended Sparse Server) is an offload-centric system that improves DeepSeek-V3.2-Exp's decode-stage throughput by selectively offloading Latent-Cache to CPU memory, freeing GPU memory for larger batch sizes.", "motivation": "The decode-stage of PD disaggregation remains a bottleneck in DeepSeek-V3.2-Exp due to linear growth of Latent-Cache with sequence length conflicting with limited GPU memory capacity, which constrains batch-size and suppresses decode-stage throughput.", "method": "ESS selectively offloads Latent-Cache to CPU memory while keeping latency-critical components on GPU, effectively decoupling batch-size scaling from GPU memory constraints through an offload-centric system design.", "result": "ESS delivers 69.4% throughput improvement at 32K context length and up to 123% throughput improvement at 128K context length in high-fidelity simulations.", "conclusion": "ESS is a practical and scalable solution for long-context LLM serving that significantly improves decode-stage throughput and reduces deployment costs in real-world settings."}}
