{"id": "2601.18943", "categories": ["cs.ET", "cond-mat.dis-nn", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18943", "abs": "https://arxiv.org/abs/2601.18943", "authors": ["Saleh Bunaiyan", "Mohammad Alsharif", "Abdelrahman S. Abdelrahman", "Hesham ElSawy", "Suraj S. Cheema", "Suhaib A. Fahmy", "Kerem Y. Camsari", "Feras Al-Dirini"], "title": "Configurable p-Neurons Using Modular p-Bits", "comment": "Accepted for presentation at IEEE ISCAS 2026 as a lecture", "summary": "Probabilistic bits (p-bits) have recently been employed in neural networks (NNs) as stochastic neurons with sigmoidal probabilistic activation functions. Nonetheless, there remain a wealth of other probabilistic activation functions that are yet to be explored. Here we re-engineer the p-bit by decoupling its stochastic signal path from its input data path, giving rise to a modular p-bit that enables the realization of probabilistic neurons (p-neurons) with a range of configurable probabilistic activation functions, including a probabilistic version of the widely used Logistic Sigmoid, Tanh and Rectified Linear Unit (ReLU) activation functions. We present spintronic (CMOS + sMTJ) designs that show wide and tunable probabilistic ranges of operation. Finally, we experimentally implement digital-CMOS versions on an FPGA, with stochastic unit sharing, and demonstrate an order of magnitude (10x) saving in required hardware resources compared to conventional digital p-bit implementations."}
{"id": "2601.19893", "categories": ["cs.ET", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19893", "abs": "https://arxiv.org/abs/2601.19893", "authors": ["Nacereddine Sitouah", "Francesco Bruschi", "Stefano De Cillis"], "title": "Enabling SSI-Compliant Use of EUDI Wallet Credentials through Trusted Execution Environment and Zero-Knowledge Proof", "comment": null, "summary": "The passing of the eIDAS amendment marks an important milestone for EU countries and changes how they must manage digital credentials for both public services and businesses. Italy has led in adopting eIDAS, first with CIE and SPID identity schemes, and now with the Italian Wallet (IO app) aligned to eIDAS 2.0. Self-Sovereign Identity (SSI) is a decentralized model born from the success of Distributed Ledgers, giving individuals full control over their digital identity. The current eIDAS 2.0 and its implementation acts diverge from SSI principles, rendering the European Digital Identity Wallet (EUDIW) centralized and merely user-centric, prioritizing security and legal protection over true self-sovereignty.\n  This paper proposes an architecture that enables the use of IT Wallet credentials and services in an SSI-compliant environment through Trusted Execution Environments and Zero-Knowledge Proofs."}
{"id": "2601.19213", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.19213", "abs": "https://arxiv.org/abs/2601.19213", "authors": ["Weiming Hu", "Zihan Zhang", "Haoyan Zhang", "Chen Zhang", "Cong Guo", "Yu Feng", "Tianchi Hu", "Guanglin Li", "Guipeng Hu", "Junsong Wang", "Jingwen Leng"], "title": "M$^{\\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization", "comment": "17 pages, 13 figures, ASPLOS 2026", "summary": "Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\\times$ speedup and 1.75$\\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26."}
{"id": "2601.18983", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18983", "abs": "https://arxiv.org/abs/2601.18983", "authors": ["Dimitrios Tomaras", "Vana Kalogeraki", "Dimitrios Gunopulos"], "title": "Trustworthy Scheduling for Big Data Applications", "comment": "6 pages", "summary": "Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach."}
{"id": "2601.19263", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.19263", "abs": "https://arxiv.org/abs/2601.19263", "authors": ["Aybars Yunusoglu", "Talha Coskun", "Hiruna Vishwamith", "Murat Isik", "I. Can Dikmen"], "title": "A Reconfigurable Framework for AI-FPGA Agent Integration and Acceleration", "comment": "Accepted at 27th International Symposium on Quality Electronic Design (ISQED'26)", "summary": "Artificial intelligence (AI) is increasingly deployed in real-time and energy-constrained environments, driving demand for hardware platforms that can deliver high performance and power efficiency. While central processing units (CPUs) and graphics processing units (GPUs) have traditionally served as the primary inference engines, their general-purpose nature often leads to inefficiencies under strict latency or power budgets. Field-Programmable Gate Arrays (FPGAs) offer a promising alternative by enabling custom-tailored parallelism and hardware-level optimizations. However, mapping AI workloads to FPGAs remains challenging due to the complexity of hardware-software co-design and data orchestration. This paper presents AI FPGA Agent, an agent-driven framework that simplifies the integration and acceleration of deep neural network inference on FPGAs. The proposed system employs a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers with minimal developer intervention. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic. Experimental results demonstrate that the AI FPGA Agent achieves over 10x latency reduction compared to CPU baselines and 2-3x higher energy efficiency than GPU implementations, all while preserving classification accuracy within 0.2% of full-precision references. These findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment."}
{"id": "2601.19092", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.19092", "abs": "https://arxiv.org/abs/2601.19092", "authors": ["Bohan Hou", "Hongyi Jin", "Guanjie Wang", "Jinqi Chen", "Yaxing Cai", "Lijie Yang", "Zihao Ye", "Yaoyao Ding", "Ruihang Lai", "Tianqi Chen"], "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers", "comment": null, "summary": "Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends."}
{"id": "2601.19384", "categories": ["cs.AR", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.19384", "abs": "https://arxiv.org/abs/2601.19384", "authors": ["Julien Eudine", "Chu Li", "Zhuo Cheng", "Renzo Andri", "Can Firtina", "Mohammad Sadrosadati", "Nika Mansouri Ghiasi", "Konstantina Koliogeorgi", "Anirban Nag", "Arash Tavakkol", "Haiyu Mao", "Onur Mutlu", "Shai Bergman", "Ji Zhang"], "title": "GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping", "comment": null, "summary": "Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy."}
{"id": "2601.19160", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19160", "abs": "https://arxiv.org/abs/2601.19160", "authors": ["Sheng Qi", "Zhiquan Zhang", "Xuanzhe Liu", "Xin Jin"], "title": "KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing", "comment": "Accepted by NSDI'26", "summary": "FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.\n  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent."}
{"id": "2601.19747", "categories": ["cs.AR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19747", "abs": "https://arxiv.org/abs/2601.19747", "authors": ["Jiale Liu", "Taiyu Zhou", "Tianqi Jiang"], "title": "Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation", "comment": null, "summary": "In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems."}
{"id": "2601.19362", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19362", "abs": "https://arxiv.org/abs/2601.19362", "authors": ["Xinyi Wan", "Penghui Qi", "Guangxing Huang", "Chaoyi Ruan", "Min Lin", "Jialin Li"], "title": "Revisiting Parameter Server in LLM Post-Training", "comment": "Accepted in ICLR'26", "summary": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \\textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc."}
{"id": "2601.18943", "categories": ["cs.ET", "cond-mat.dis-nn", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18943", "abs": "https://arxiv.org/abs/2601.18943", "authors": ["Saleh Bunaiyan", "Mohammad Alsharif", "Abdelrahman S. Abdelrahman", "Hesham ElSawy", "Suraj S. Cheema", "Suhaib A. Fahmy", "Kerem Y. Camsari", "Feras Al-Dirini"], "title": "Configurable p-Neurons Using Modular p-Bits", "comment": "Accepted for presentation at IEEE ISCAS 2026 as a lecture", "summary": "Probabilistic bits (p-bits) have recently been employed in neural networks (NNs) as stochastic neurons with sigmoidal probabilistic activation functions. Nonetheless, there remain a wealth of other probabilistic activation functions that are yet to be explored. Here we re-engineer the p-bit by decoupling its stochastic signal path from its input data path, giving rise to a modular p-bit that enables the realization of probabilistic neurons (p-neurons) with a range of configurable probabilistic activation functions, including a probabilistic version of the widely used Logistic Sigmoid, Tanh and Rectified Linear Unit (ReLU) activation functions. We present spintronic (CMOS + sMTJ) designs that show wide and tunable probabilistic ranges of operation. Finally, we experimentally implement digital-CMOS versions on an FPGA, with stochastic unit sharing, and demonstrate an order of magnitude (10x) saving in required hardware resources compared to conventional digital p-bit implementations."}
{"id": "2601.19563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19563", "abs": "https://arxiv.org/abs/2601.19563", "authors": ["Juan Zhu", "Zixin Wang", "Shenghui Song", "Jun Zhang", "Khaled Ben Letaief"], "title": "Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization", "comment": "4 figures. Conference", "summary": "Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales."}
{"id": "2601.19893", "categories": ["cs.ET", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19893", "abs": "https://arxiv.org/abs/2601.19893", "authors": ["Nacereddine Sitouah", "Francesco Bruschi", "Stefano De Cillis"], "title": "Enabling SSI-Compliant Use of EUDI Wallet Credentials through Trusted Execution Environment and Zero-Knowledge Proof", "comment": null, "summary": "The passing of the eIDAS amendment marks an important milestone for EU countries and changes how they must manage digital credentials for both public services and businesses. Italy has led in adopting eIDAS, first with CIE and SPID identity schemes, and now with the Italian Wallet (IO app) aligned to eIDAS 2.0. Self-Sovereign Identity (SSI) is a decentralized model born from the success of Distributed Ledgers, giving individuals full control over their digital identity. The current eIDAS 2.0 and its implementation acts diverge from SSI principles, rendering the European Digital Identity Wallet (EUDIW) centralized and merely user-centric, prioritizing security and legal protection over true self-sovereignty.\n  This paper proposes an architecture that enables the use of IT Wallet credentials and services in an SSI-compliant environment through Trusted Execution Environments and Zero-Knowledge Proofs."}
