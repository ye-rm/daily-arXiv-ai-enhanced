<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation](https://arxiv.org/abs/2602.16858)
*Kathiravan Palaniappan*

Main category: cs.PF

TL;DR: CPU-based deep learning inference scalability study comparing legacy vs modern Intel Xeon processors, revealing throughput saturation limits in legacy CPUs and performance degradation from oversubscription in modern AMX-enabled systems.


<details>
  <summary>Details</summary>
Motivation: Despite specialized accelerators, many inference workloads still run on CPU-only systems in legacy data centers and cost-sensitive environments, requiring understanding of CPU-based inference scalability limits.

Method: Benchmarked ResNet models across varying batch sizes on two hardware tiers: legacy Intel Xeon E5-2403 v2 and modern Intel Xeon 6 "Granite Rapids" with AMX, and introduced GDEV-AI benchmarking framework for analyzing scalability behavior.

Result: Legacy CPUs quickly reach throughput saturation with limited scaling beyond small batch sizes, while Granite Rapids with AMX achieves substantially higher throughput but suffers performance degradation from oversubscription beyond physical core limits.

Conclusion: CPU-based inference has scalability limits that vary by architecture, with legacy systems constrained by instruction/memory bottlenecks and modern systems facing contention issues from oversubscription, requiring careful capacity planning in heterogeneous environments.

Abstract: The deployment of deep learning inference in production environments continues to grow, where throughput, latency, and hardware efficiency are critical. Although specialized accelerators are increasingly adopted, many inference workloads still run on CPU-only systems, particularly in legacy data centers and cost-sensitive environments. This study investigates the scalability limits of CPU-based inference for convolutional neural networks by benchmarking ResNet models across varying batch sizes on two hardware tiers: a legacy Intel Xeon E5-2403 v2 processor and a modern Intel Xeon 6 "Granite Rapids" platform.
  Results show that legacy CPUs quickly reach throughput saturation, with limited scaling beyond small batch sizes due to instruction-level and memory constraints. In contrast, the Granite Rapids system leverages Intel Advanced Matrix Extensions (AMX) to achieve substantially higher throughput. However, oversubscription beyond physical core limits introduces execution contention and tail-latency amplification, revealing a performance degradation regime in modern architectures.
  We introduce GDEV-AI, a reproducible benchmarking framework for analyzing scalability behavior and architectural saturation in CPU-based inference. By establishing a vendor-neutral baseline, this work provides empirical insight into performance bottlenecks and informs capacity planning in heterogeneous data center environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Low-Cost IoT-Enabled Tele-ECG Monitoring for Resource-Constrained Settings: System Design and Prototype](https://arxiv.org/abs/2602.17114)
*Seemron Neupane,Aashish Ghimire*

Main category: cs.AR

TL;DR: This paper proposes an IoT-based remote ECG monitoring system to address healthcare accessibility issues, focusing on three main components: operator, doctor, and server for data transmission.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from increasing automation leading to sedentary lifestyles and health issues, combined with limited healthcare access in many regions. Early detection of cardiovascular diseases (CDV) is crucial but requires regular monitoring and timely intervention, which is challenging due to financial and logistical burdens.

Method: The paper focuses on an IoT-based remote ECG monitoring system with three main components: 1) operator (likely patient or healthcare worker), 2) doctor for remote consultation, and 3) server infrastructure for data transmission and management.

Result: The system enables remote ECG monitoring through telemedicine, reducing travel costs and improving access to healthcare. It allows community health workers to support patients and facilitates early detection and intervention for cardiovascular diseases.

Conclusion: IoT-based remote ECG monitoring systems can effectively address healthcare accessibility challenges, particularly for chronic disease management, by leveraging telemedicine to connect patients, healthcare workers, and medical professionals through server infrastructure.

Abstract: With the availability of automation machinery and its superiority, are being slothful and inviting many diseases to invade them. The world still has so many places where people lack basic health facilities. Due to early detection and intervention, CDV can be cured to an extreme extent. It heavily reduces travel and associated costs. A remote ECG monitoring system enables community health workers to support and empower patients through telemedicine. However, there remains some financial and logistical burden. Heart disease cannot be taken lightly. These patients require regular health check-ups and the attention of health personnel in a short period if their health deteriorates suddenly and rapidly. Chronic diseases are extremely variable in their symptoms and evolution of treatment. Some, if not treated early, will end the patient's life. The trend of the INTERNET OF THINGS, IoT, is spreading massively. This paper focuses on the three main: the operator, the doctor, and the server over which the data is being sent.

</details>


### [3] [SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models](https://arxiv.org/abs/2602.17169)
*Yuhuan Xia,Tun Li,Hongji Zhou,Xianfa Zhou,Chong Chen,Ruiyu Zhang*

Main category: cs.AR

TL;DR: SimulatorCoder is an LLM-powered agent that generates and optimizes DNN accelerator simulators from natural language descriptions using structured prompting and feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: To accelerate simulator development by automating the transformation of high-level functional requirements into efficient, executable DNN accelerator simulator code, reducing manual implementation effort while maintaining accuracy.

Method: Uses LLMs with domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow to systematically generate and optimize simulator code.

Result: Generated simulators maintain cycle-level fidelity with less than 1% error compared to manual implementations and achieve lower simulation runtimes on the SCALE-Sim benchmark.

Conclusion: LLM-based methods with structured prompting and feedback mechanisms are effective for accelerating simulator development while maintaining high accuracy and performance.

Abstract: This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-aligned simulator code. Experiments based on the customized SCALE-Sim benchmark demonstrate that structured prompting and feedback mechanisms substantially improve both code generation accuracy and simulator performance. The resulting simulators not only maintain cycle-level fidelity with less than 1% error compared to manually implemented counterparts, but also consistently achieve lower simulation runtimes, highlighting the effectiveness of LLM-based methods in accelerating simulator development. Our code is available at https://github.com/xiayuhuan/SimulatorCoder.

</details>


### [4] [When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning](https://arxiv.org/abs/2602.17520)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: LLMs struggle with locally redefined semantics in formal domains like digital logic, reverting to pretrained defaults despite explicit prompt-local definitions.


<details>
  <summary>Details</summary>
Motivation: To understand LLM reliability under locally redefined semantics in formal settings like circuit specifications, where operators/components are explicitly redefined within narrow scope, requiring temporary suppression of globally learned conventions.

Method: Introduced a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluated three frontier LLMs on these tasks.

Result: Observed persistent noncompliance with local specifications, confident but incompatible assumptions (assumption injection), and dropped constraints even in elementary settings. LLMs exhibit semantic override (reverting to pretrained defaults) and assumption injection errors.

Conclusion: There's a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.

Abstract: Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Visual Insights into Agentic Optimization of Pervasive Stream Processing Services](https://arxiv.org/abs/2602.17282)
*Boris Sedlak,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: A platform for context-aware autoscaling of stream processing services on Edge devices with a learning agent that optimizes resource allocation to meet fluctuating demands while preventing resource cannibalization.


<details>
  <summary>Details</summary>
Motivation: Edge computing for smart city applications faces three key challenges: 1) fluctuating application demands and resource availability requiring dynamic scaling, 2) diverse services needing individual scaling policies, and 3) potential resource cannibalization between co-located services without proper mediation.

Method: The paper presents a platform with monitoring and adjustment interfaces for stream processing services, connected to a scaling agent that explores each service's action space to build understanding of the processing environment and optimize service execution.

Result: The demo includes a platform for context-aware autoscaling, a learning agent that optimizes service execution, and provides participants with video summaries, introductory posters, and an extensible artifact repository for building custom agents.

Conclusion: The proposed platform addresses Edge computing challenges by enabling intelligent, context-aware autoscaling of stream processing services through adaptive learning agents that prevent resource conflicts while meeting dynamic processing requirements.

Abstract: Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository.

</details>


### [6] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: The paper explores whether RMWable snapshot algorithms can be implemented using only read/write operations instead of more powerful primitives like compare&swap or load-link/store-conditional.


<details>
  <summary>Details</summary>
Motivation: To understand the limits of what can be solved using simple read/write operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, and to explore if RMWable snapshots are possible using only these basic operations.

Method: The authors present two read/write RMWable snapshot algorithms: 1) in the standard concurrent shared-memory model with finite known number of processes, and 2) in a variant with unbounded concurrency (infinitely many processes but only finitely many active at any moment).

Result: The paper demonstrates that RMWable snapshots are indeed possible using only read/write operations, providing concrete algorithms for both bounded and unbounded concurrency models.

Conclusion: RMWable snapshot algorithms can be implemented using only read/write operations, showing that the more powerful compare&swap or load-link/store-conditional operations are not necessary for this functionality, expanding our understanding of the computational power of basic read/write primitives.

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [7] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: Fed-PLoRA is a lightweight heterogeneous federated fine-tuning framework that addresses performance degradation from heterogeneous LoRA ranks in federated LLM training through parallel one-rank modules and selective folding strategies.


<details>
  <summary>Details</summary>
Motivation: Federated Learning with LoRA enables collaborative LLM fine-tuning while preserving data privacy, but heterogeneous client resources lead to different LoRA ranks, causing substantial initialization and aggregation noise that undermines performance.

Method: Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA) - replacing multi-rank LoRA with multiple parallel one-rank modules, and a Select-N-Fold strategy that folds untrained PLoRA modules into pre-trained weights before local training to accommodate heterogeneous resources.

Result: Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency, with unified analysis showing it effectively addresses initialization and aggregation noise limitations.

Conclusion: Fed-PLoRA provides an effective solution for heterogeneous federated fine-tuning of LLMs, overcoming challenges of resource heterogeneity while maintaining privacy and improving performance over state-of-the-art methods.

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [8] [Trivance: Latency-Optimal AllReduce by Shortcutting Multiport Networks](https://arxiv.org/abs/2602.17254)
*Anton Juerss,Vamsi Addanki,Stefan Schmid*

Main category: cs.DC

TL;DR: Trivance is a novel AllReduce algorithm that achieves latency-optimality in log₃n steps while reducing congestion by 3x compared to Bruck's algorithm, maintaining bandwidth-optimality for distributed computing on torus networks.


<details>
  <summary>Details</summary>
Motivation: AllReduce is a critical bottleneck in distributed computing, especially for latency-sensitive workloads. Existing approaches face trade-offs: Bruck's algorithm is latency-optimal (log₃n steps) but suffers from high congestion, while Swing reduces congestion but requires more steps (log₂n). Direct-connect topologies like torus networks exacerbate these issues due to limited bisection bandwidth.

Method: Trivance exploits both transmission ports of a bidirectional ring within each step to triple communication distance along both directions simultaneously. It performs joint reductions to improve both step count and network congestion. The algorithm naturally extends to multidimensional torus networks while retaining latency advantages.

Result: Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8MiB, in high-bandwidth settings up to 32MiB, and for 3D tori up to 128MiB. It consistently outperforms other latency-optimal algorithms throughout evaluation.

Conclusion: Trivance successfully addresses the latency-congestion trade-off in AllReduce algorithms, achieving latency-optimality while significantly reducing congestion and maintaining bandwidth-optimality, making it particularly effective for torus network topologies used in modern distributed systems.

Abstract: AllReduce is a fundamental collective operation in distributed computing and a key performance bottleneck for large-scale training and inference. Its completion time is determined by the number of communication steps, which dominates latency-sensitive workloads, and the communication distance affecting both latency- and bandwidth-bound regimes. Direct-connect topologies, such as torus networks used in Google's TPUv4, are particularly prone to large communication distances due to limited bisection bandwidth. Latency-optimal algorithms such as Bruck's complete AllReduce in $\log_3 n$ steps on a bidirectional ring, but incur large communication distances that result in substantial congestion. In contrast, recent approaches such as Swing reduce communication distance and congestion, but are inherently required to perform $\log_2 n$ steps to complete AllReduce, sacrificing latency-optimality.
  In this paper, we present Trivance, a novel AllReduce algorithm that completes within $\log_3 n$ steps, while reducing congestion compared to Bruck's algorithm by a factor of three and preserving bandwidth-optimality. Trivance exploits both transmission ports of a bidirectional ring within each step to triple the communication distance along both directions simultaneously. Furthermore, by performing joint reductions, Trivance improves both the number of steps and network congestion. We further show that Trivance extends naturally to multidimensional torus networks, retaining its latency advantage while achieving performance comparable to bandwidth-optimal algorithms for large messages.
  Our empirical evaluation shows that Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8\,MiB, in high-bandwidth settings up to 32MiB and for 3D tori up to 128MiB. Throughout the evaluation, Trivance remains the best-performing latency-optimal algorithm.

</details>


### [9] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: This paper evaluates resource elasticity in HPC job scheduling, showing that allowing jobs to dynamically adjust resource allocation (malleability) significantly improves performance metrics like turnaround time, makespan, wait times, and node utilization compared to traditional rigid scheduling.


<details>
  <summary>Details</summary>
Motivation: Traditional rigid job scheduling in HPC clusters leads to underutilized resources and increased job waiting times, creating inefficiencies in resource management and user satisfaction.

Method: The study uses real workload traces from Cori, Eagle, and Theta supercomputers to simulate varying proportions (0-100%) of malleable jobs using ElastiSim software. Five job scheduling strategies are evaluated, including a novel approach that maintains malleable jobs at their preferred allocation when possible.

Result: Malleable jobs yield significant improvements: job turnaround times decrease by 37-67%, makespan by 16-65%, wait times by 73-99%, and node utilization improves by 5-52%. Benefits remain substantial even with only 20% malleable jobs.

Conclusion: Resource malleability effectively addresses HPC inefficiencies, with even limited adoption providing substantial advantages. The findings highlight important correlations between workload characteristics, malleability proportions, and scheduling strategies, encouraging integration into HPC resource management.

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [10] [Informative Trains: A Memory-Efficient Journey to a Self-Stabilizing Leader Election Algorithm in Anonymous Graphs](https://arxiv.org/abs/2602.17541)
*Lelia Blin,Sylvain Gay,Isabella Ziccardi*

Main category: cs.DC

TL;DR: A probabilistic self-stabilizing leader election algorithm for anonymous networks using O(log log n) bits of memory per node, operating in the state model with synchronous scheduler.


<details>
  <summary>Details</summary>
Motivation: The self-stabilizing leader election problem in anonymous networks with low space memory complexity is challenging, with existing solutions requiring Ω(log n) bits for general graphs or achieving low state complexity only under restricted topologies. The goal is to design space-optimal algorithms that use O(log log n) bits per node.

Method: Probabilistic self-stabilizing algorithm operating in the state model under synchronous scheduler, assuming knowledge of global parameter N = Θ(log n). The algorithm keeps transmitting information after convergence (no silence property) and uses O(log log n) bits of memory per node.

Result: The system converges almost surely to a stable configuration with a unique leader and stabilizes within O(poly(n)) rounds with high probability, achieving the O(log log n) bits memory complexity for arbitrary anonymous networks.

Conclusion: The paper presents a space-optimal probabilistic leader election algorithm that achieves O(log log n) bits memory per node for general anonymous networks, addressing the open problem of designing space-optimal algorithms while operating without silence property and explicit termination detection.

Abstract: We study the self-stabilizing leader election problem in anonymous $n$-nodes networks. Achieving self-stabilization with low space memory complexity is particularly challenging, and designing space-optimal leader election algorithms remains an open problem for general graphs. In deterministic settings, it is known that $Ω(\log \log n)$ bits of memory per node are necessary [Blin et al., Disc. Math. \& Theor. Comput. Sci., 2023], while in probabilistic settings the same lower bound holds for some values of $n$, but only for an unfair scheduler [Beauquier et al., PODC 1999]. Several deterministic and probabilistic protocols have been proposed in models ranging from the state model to the population protocols. However, to the best of our knowledge, existing solutions either require $Ω(\log n)$ bits of memory per node for general worst case graphs, or achieve low state complexity only under restricted network topologies such as rings, trees, or bounded-degree graphs.
  In this paper, we present a probabilistic self-stabilizing leader election algorithm for arbitrary anonymous networks that uses $O(\log \log n)$ bits of memory per node. Our algorithm operates in the state model under a synchronous scheduler and assumes knowledge of a global parameter $N = Θ(\log n)$. We show that, under our protocol, the system converges almost surely to a stable configuration with a unique leader and stabilizes within $O(\mathrm{poly}(n))$ rounds with high probability. To achieve $O(\log \log n)$ bits of memory, our algorithm keeps transmitting information after convergence, i.e. it does not verify the silence property. Moreover, like most works in the field, our algorithm does not provide explicit termination detection (i.e., nodes do not detect when the algorithm has converged).

</details>


### [11] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: TopoSZp is a lightweight, topology-aware lossy compressor that preserves critical points (minima, maxima, saddle points) while maintaining high performance, achieving significantly better topology preservation and faster compression/decompression than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing error-bounded lossy compressors like SZ and ZFP fail to preserve topological structures critical for scientific analysis, while existing topology-aware compressors incur substantial computational overhead.

Method: Built on SZp compressor, integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement within a relaxed but strictly enforced error bound.

Result: Achieves 3-100× fewer non-preserved critical points, no false positives or incorrect critical point types, 100-10000× faster compression and 10-500× faster decompression than existing topology-aware compressors, while maintaining competitive compression ratios.

Conclusion: TopoSZp provides an effective solution for topology-preserving compression with high performance, addressing the limitations of both traditional error-bounded compressors and existing topology-aware methods.

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


### [12] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: This paper evaluates DAOS and Ceph object storage systems for HPC/AI applications like NWP, comparing them to traditional Lustre file systems, finding DAOS offers superior scalability and performance.


<details>
  <summary>Details</summary>
Motivation: HPC and AI applications like operational Numerical Weather Prediction require processing massive data volumes quickly. Traditional POSIX distributed file systems face performance limitations at scale, prompting exploration of alternative storage solutions.

Method: Developed software adapters to enable ECMWF's NWP to use DAOS and Ceph object storage. Conducted extensive I/O benchmarking on multiple computer systems, comparing performance against equivalent Lustre file system deployments on same hardware.

Result: Both DAOS and Ceph demonstrated excellent performance, but DAOS stood out with superior scalability and flexibility for applications to perform I/O at scale compared to both Ceph and Lustre.

Conclusion: DAOS and object storage show promising outlook for HPC centers, potentially seeing greater adoption in coming years, though not necessarily implying complete shift away from POSIX-like I/O approaches.

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>
