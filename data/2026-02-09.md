<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: D-Legion is a scalable many-core architecture using adaptive-precision systolic arrays to accelerate quantized LLM matrix multiplication, achieving significant performance gains over existing solutions.


<details>
  <summary>Details</summary>
Motivation: Large language models require substantial computational resources, and quantized LLMs offer efficiency advantages but need specialized hardware acceleration to fully realize their potential.

Method: Proposes D-Legion architecture with adaptive-precision systolic array cores organized in Legions, supporting quantized sparse/dense matrix multiplication, block structured sparsity, parallel accumulators, and optimized scheduling techniques.

Result: Achieves up to 8.2× lower latency, 3.8× higher memory savings, 3× higher psum memory savings vs state-of-the-art; 135.68 TOPS peak throughput; 2.5× lower latency, 2.3× higher throughput, 2.7× higher memory savings vs Google TPUv4i.

Conclusion: D-Legion demonstrates superior performance for accelerating quantized LLMs through its scalable many-core architecture with adaptive-precision systolic arrays and optimized memory/computation techniques.

Abstract: The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\times$ lower latency, up to 3.8$\times$ higher memory savings, and up to 3$\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\times$ lower total latency, up to 2.3$\times$ higher total throughput, and up to 2.7$\times$ higher total memory savings.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [2] [End-to-End Throughput Benchmarking of Portable Deterministic CNN-Based Signal Processing Pipelines](https://arxiv.org/abs/2602.06216)
*Christiaan Boerkamp,Akhil John Thomas*

Main category: cs.PF

TL;DR: A benchmarking methodology for evaluating deterministic CNN-based signal-processing pipelines across heterogeneous accelerators, focusing on phased-array workloads like ultrasound imaging.


<details>
  <summary>Details</summary>
Motivation: The need for portable, certifiable signal-processing implementations that avoid hardware-specific refactoring while maintaining high performance on modern AI accelerators.

Method: Developed a benchmarking methodology for end-to-end deterministic signal-processing pipelines using CNN-compatible primitives. Benchmarked a single training-free CNN-based pipeline across heterogeneous platforms (NVIDIA RTX 5090 GPU and Google TPU v5e-1) under realistic execution conditions.

Result: Demonstrated how different operator formulations (dynamic indexing, fully CNN-expressed, and sparse-matrix-based) impact performance and portability across architectures. Performance measured using sustained input throughput (MB/s), effective frame rate (FPS), and energy/memory metrics.

Conclusion: The benchmarking methodology enables evaluation of portable, certifiable signal-processing implementations across diverse accelerator platforms, revealing performance-portability tradeoffs of different operator formulations.

Abstract: This paper presents a benchmarking methodology for evaluating end-to-end performance of deterministic signal-processing pipelines expressed using CNN-compatible primitives. The benchmark targets phased-array workloads such as ultrasound imaging and evaluates complete RF-to-image pipelines under realistic execution conditions. Performance is reported using sustained input throughput (MB/s), effective frame rate (FPS), and, where available, incremental energy per run and peak memory usage. Using this methodology, we benchmark a single deterministic, training-free CNN-based signal-processing pipeline executed unmodified across heterogeneous accelerator platforms, including an NVIDIA RTX 5090 GPU and a Google TPU v5e-1. The results demonstrate how different operator formulations (dynamic indexing, fully CNN-expressed, and sparse-matrix-based) impact performance and portability across architectures. This work is motivated by the need for portable, certifiable signal-processing implementations that avoid hardware-specific refactoring while retaining high performance on modern AI accelerators.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: QEIL is a framework for efficient local LLM inference on edge devices using scaling laws and heterogeneous orchestration across CPU/GPU/NPU, achieving significant efficiency gains without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LLM inference on resource-constrained edge devices is challenging due to reliance on cloud infrastructure, creating latency issues for intelligent systems that need local processing.

Method: QEIL uses inference time scaling laws, hardware-aware routing with analytical cost models, and performance-energy trade-off metrics. A unified orchestrator combines these through progressive sample multiplexing.

Result: 7-10.5% improvement in pass@k coverage, 35.6-78.2% energy reduction, 68% average power reduction, 15.8% latency improvement, and zero accuracy loss across models from 125M to 2.6B parameters.

Conclusion: Inference time scaling laws are universal and architecture-agnostic, establishing heterogeneous edge orchestration as optimal for energy-constrained intelligent systems.

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [4] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: First end-to-end deployment of Gemma3 LLM/VLM family on AMD Ryzen AI NPU using hardware-aware techniques for efficient edge inference.


<details>
  <summary>Details</summary>
Motivation: Enable practical, low-power LLM and VLM inference at the edge by efficiently mapping transformer-based models onto tiled dataflow accelerators like AMD Ryzen AI NPU.

Method: Hardware-aware techniques including: efficient dequantization engine, optimized tiled matrix multiplication kernels, FlowQKV (chunked pipelined attention) for prefill; FusedDQP (fused dequantization-projection) and FlowKV (restructured attention) for decoding; plus compact Q4NX 4-bit quantization format.

Result: Achieved 5.2× faster prefill and 4.8× faster decoding vs iGPU; 33.5× and 2.2× vs CPU. Power efficiency improved by 67.2× vs iGPU and 222.9× vs CPU.

Conclusion: Modern NPUs can deliver practical, low-power LLM/VLM inference at the edge, providing a generalizable blueprint for mapping transformer models onto tiled dataflow accelerators.

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [5] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: iScheduler is a reinforcement learning framework for solving the Resource Investment Problem (RIP) that achieves competitive resource costs while being up to 43× faster than commercial baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional exact methods (MIP/CP) for scheduling precedence-constrained tasks under shared renewable resources become impractically slow on large instances, and dynamic updates require fast schedule revisions under tight latency constraints.

Method: iScheduler formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection using reinforcement learning. It supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes.

Result: iScheduler attains competitive resource costs while reducing time to feasibility by up to 43× against strong commercial baselines. The authors also release L-RIPLIB, an industrial-scale benchmark with 1,000 instances of 2,500-10,000 tasks.

Conclusion: The reinforcement-learning-driven iterative scheduling framework provides an effective solution for large-scale RIP instances, offering both optimization acceleration and support for dynamic reconfiguration while maintaining competitive resource costs.

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [6] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: HQP framework combines quantization and pruning for edge AI acceleration, achieving 3.12× speedup and 55% size reduction while keeping accuracy drop below 1.5%.


<details>
  <summary>Details</summary>
Motivation: Address the need for high-fidelity, real-time inference in distributed edge-cloud environments with severe latency and energy constraints, requiring aggressive model optimization.

Method: Hybrid Quantization and Pruning (HQP) framework with sensitivity-aware structural pruning using dynamic weight sensitivity metric from Fisher Information Matrix approximation, followed by conditional 8-bit post-training quantization with strict accuracy drop constraints.

Result: Achieved peak performance gain of 3.12× inference speedup and 55% model size reduction on NVIDIA Jetson edge platforms with MobileNetV3 and ResNet-18, while containing accuracy drop below 1.5% constraint.

Conclusion: HQP framework is a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures, outperforming conventional single-objective compression techniques.

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [7] [Computationally Efficient Laplacian CL-colME](https://arxiv.org/abs/2602.06070)
*Nikola Stankovic*

Main category: cs.DC

TL;DR: CL-colME: A Laplacian-based decentralized collaborative mean estimation method that improves computational efficiency over existing C-colME while maintaining convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency in existing C-colME framework which requires expensive normalization processes using doubly stochastic averaging matrices for decentralized collaborative mean estimation in heterogeneous networks.

Method: Proposes CL-colME, a novel variant that utilizes Laplacian-based consensus instead of doubly stochastic averaging matrices, avoiding the computationally expensive normalization processes while still ensuring convergence to the oracle solution.

Result: Simulation results show that CL-colME maintains the same convergence behavior and accuracy as C-colME while significantly improving computational efficiency by eliminating the normalization overhead.

Conclusion: CL-colME provides a more computationally efficient alternative to C-colME for decentralized collaborative mean estimation, achieving the same performance benefits without the expensive normalization processes, making it more suitable for practical applications in heterogeneous networks.

Abstract: Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.

</details>


### [8] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: BlockPerm-SJLT is a new sparse sketching method designed for GPU efficiency, with FlashSketch as its optimized CUDA kernel implementation, achieving 1.7x speedup over prior GPU sketches while maintaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional sparse sketches suffer from irregular memory access patterns on GPUs, degrading memory bandwidth utilization. There's a tension between random sparsity for sketching robustness and GPU efficiency.

Method: Sketch-kernel co-design approach: BlockPerm-SJLT introduces structured sparsity with a tunable parameter balancing GPU-efficiency and sketching robustness. FlashSketch is the corresponding optimized CUDA kernel implementation.

Result: FlashSketch achieves 1.7x geomean speedup over prior state-of-the-art GPU sketches across RandNLA benchmarks and GraSS (ML data attribution pipeline). Theoretical guarantees provided under oblivious subspace embedding framework.

Conclusion: The sketch-kernel co-design approach successfully addresses the GPU efficiency vs. sketching robustness trade-off, pushing the Pareto frontier of sketching quality versus speed across various regimes and tasks.

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [9] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer is a kernel-level attention framework that optimizes batched LLM inference by packing heterogeneous requests into load-balanced execution groups, reducing computation and I/O imbalance to improve throughput and latency.


<details>
  <summary>Details</summary>
Motivation: Production LLM serving requires batching requests with highly heterogeneous sequence lengths for throughput, but this creates computation and I/O imbalance that underutilizes GPU resources and causes stragglers, while existing attention optimizations like FlashAttention only work well for individual requests.

Method: PackInfer orchestrates batched requests into load-balanced execution groups that pack multiple requests into unified kernel launches. It constructs attention kernels directly over packed query-key regions to eliminate redundant computation and balance thread-block execution. It also uses I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts to reduce memory fragmentation and redundant data movement.

Result: Evaluations on real-world workloads show PackInfer reduces inference latency by 13.0-20.1% and improves throughput by 20% compared to state-of-the-art FlashAttention.

Conclusion: PackInfer effectively addresses the computation and I/O imbalance in heterogeneous batched LLM inference, significantly improving both latency and throughput by optimizing kernel-level attention execution and memory organization.

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [10] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: This paper experimentally demonstrates that simple in-memory caching with fixed TTL significantly reduces response times in small-scale web applications, providing valuable insights for educational and simple web environments.


<details>
  <summary>Details</summary>
Motivation: There's a lack of experimental work exploring the effect of simple in-memory caching in small-scale web applications, despite caching being widely used for web performance optimization. The paper aims to fill this research gap.

Method: The researchers experimentally compared two server-side web application configurations: one without caching and another with in-memory caching and fixed time-to-live. Performance evaluation was conducted using a lightweight web server framework, measuring response times through repeated HTTP requests under identical environmental conditions.

Result: The results show a significant reduction in response time for cached requests compared to non-cached requests, demonstrating the effectiveness of simple server-side caching.

Conclusion: Simple server-side caching with in-memory storage and fixed TTL is highly effective for improving web application performance, making it particularly suitable for educational environments and small-scale applications where simplicity and reproducibility are critical.

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [11] [MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments](https://arxiv.org/abs/2602.06075)
*Guangyi Liu,Pengxiang Zhao,Yaozhen Liang,Qinyi Luo,Shunye Tang,Yuxiang Chai,Weifeng Lin,Han Xiao,WenHao Wang,Siheng Chen,Zhengxi Lu,Gao Wu,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.DC

TL;DR: MemGUI-Bench is a new benchmark for evaluating memory capabilities in mobile GUI agents, addressing the lack of memory-focused assessment in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current mobile GUI agent benchmarks fail to properly assess memory capabilities, with only 5.2-11.8% memory-related tasks and no evaluation of cross-session learning, creating a gap in understanding agents' memory retention abilities.

Method: Introduced MemGUI-Bench with 128 tasks across 26 applications, MemGUI-Eval automated pipeline with Progressive Scrutiny and 7 hierarchical metrics, systematic memory taxonomy analyzing 11 agents across 5 architectures, and RQ-driven assessment.

Result: Experiments revealed significant memory deficits across all evaluated systems, identified 5 distinct failure modes, and showed that 89.8% of tasks challenge memory through cross-temporal and cross-spatial retention.

Conclusion: The benchmark provides comprehensive memory evaluation, identifies key failure patterns, synthesizes 5 actionable design implications, and will be fully open-sourced and maintained for ongoing research in mobile GUI agent memory capabilities.

Abstract: Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \textbf{\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.

</details>


### [12] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona is a unified framework that enables efficient matrix-based optimizer usage in distributed LLM training by decoupling logical optimizer assignment from physical parameter distribution, achieving 1.57x speedup on 256 GPUs.


<details>
  <summary>Details</summary>
Motivation: Matrix-based optimizers (Shampoo, Muon, SOAP) offer convergence efficiency for LLMs but conflict with tensor fragmentation in distributed frameworks like Megatron, creating a gap between optimizer requirements and distributed training architectures.

Method: Canzona decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism: alpha-Balanced Static Partitioning respects atomicity while neutralizing load imbalance. For Tensor Parallelism: Asynchronous Compute pipeline with Micro-Group Scheduling batches fragmented updates and hides reconstruction overhead.

Result: Evaluations on Qwen3 model family (up to 32B parameters) on 256 GPUs show Canzona preserves efficiency of established parallel architectures, achieving 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to baseline.

Conclusion: Canzona successfully bridges the gap between matrix-based optimizer requirements and distributed training frameworks, enabling efficient use of advanced optimizers without compromising parallel architecture efficiency.

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [13] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD is an AI-powered workflow that uses LLMs to automatically convert C++ code into optimized FPGA kernels, achieving near-expert performance while reducing the hardware expertise barrier.


<details>
  <summary>Details</summary>
Motivation: FPGA adoption in scientific and edge computing is limited by the specialized hardware expertise required. While HLS improves productivity over HDLs, competitive designs still need hardware-aware optimizations that demand expert knowledge.

Method: LAAFD uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels, automating key transformations including deep pipelining, vectorization, and dataflow partitioning. It incorporates HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time.

Result: On 15 kernels representing common HPC compute patterns, LAAFD achieves 99.9% geomean performance compared to hand-tuned baselines. For stencil workloads, it matches the performance of state-of-the-art DSL-based HLS code generators while producing more readable kernels.

Conclusion: LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency, making FPGA programming more accessible while maintaining competitive performance.

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [14] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL is a framework that simulates heterogeneous client hardware in Federated Learning experiments on a single machine, enabling realistic hardware diversity studies without requiring multiple physical devices.


<details>
  <summary>Details</summary>
Motivation: Most FL research uses simulations on central machines without considering hardware heterogeneity between parties, creating a methodological gap between research and practical deployment conditions.

Method: Programmatically emulates diverse hardware configurations through resource restriction on a single physical machine, providing configurable hardware profiles from consumer/small-lab devices and a custom hardware sampler based on real-world popularity.

Result: BouquetFL enables controlled FL experimentation under realistic hardware diversity, making it accessible to study system heterogeneity without requiring multiple physical devices.

Conclusion: The framework bridges the gap between FL research simulations and practical deployment by allowing researchers to study highly heterogeneous federations under realistic hardware conditions.

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [15] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP is a new distributed training method that uses host memory as a fast caching layer to eliminate redundant inter-node communication in bandwidth-limited clusters, achieving up to 100x higher throughput than ZeRO-3 while maintaining the same GPU memory footprint.


<details>
  <summary>Details</summary>
Motivation: Existing distributed training methods like ZeRO-3 face severe inter-node communication bottlenecks on commodity hardware clusters with limited bandwidth. Current optimizations either trade memory capacity for reduced communication (causing OOM failures) or use host memory offloading (degrading throughput due to PCIe overhead).

Method: FCDP leverages host memory as a fast caching layer rather than just overflow storage. It caches forward-pass parameters in host memory and reuses them during backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For PEFT, it selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%.

Result: On commodity cluster setups, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size and GPU memory footprint.

Conclusion: FCDP effectively addresses inter-node communication bottlenecks in bandwidth-limited clusters by intelligently using host memory as a caching layer, providing significant throughput improvements without sacrificing memory efficiency or model capacity.

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [16] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap is a dual-mapping scheduling strategy for distributed LLM serving that reconciles the trade-off between KV cache reuse (cache affinity) and load balancing by mapping each request to two candidate instances and intelligently selecting the better one based on system states.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving schedulers fail to reconcile the conflict between cache-affinity scheduling (which co-locates requests with same prompt prefixes to maximize KV cache reuse) and load-balancing scheduling (which distributes requests evenly across instances), operating within a single mapping space without a unified solution.

Method: DualMap uses a dual-mapping strategy where each request is mapped to two candidate instances via two independent hash functions based on the request prompt. It then intelligently selects the better candidate based on current system states. Three techniques enhance robustness: 1) SLO-aware request routing, 2) hotspot-aware rebalancing, and 3) lightweight dual-hash-ring scaling.

Result: Experiments on real-world workloads show DualMap improves effective request capacity by up to 2.25× under the same TTFT SLO constraints compared with state-of-the-art approaches.

Conclusion: DualMap successfully addresses the cache affinity vs. load balancing trade-off in distributed LLM serving through its dual-mapping design, achieving both objectives while maintaining robustness under dynamic and skewed workloads.

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [17] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: Framework for dynamic management of parallel processing skeletons on serverless platforms, focusing on Farm pattern with AI-driven autoscaling for better QoS and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Bring HPC-like performance and resilience to serverless/continuum environments while preserving programmability benefits of structured parallel processing skeletons.

Method: Couples reusable farm template with Gymnasium-based monitoring/control layer exposing queue, timing, and QoS metrics to reactive and learning-based controllers. Investigates AI-driven dynamic scaling for managing farm's degree of parallelism via serverless function scalability on OpenFaaS.

Result: AI-based management better accommodates platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behavior.

Conclusion: AI-driven dynamic scaling for parallel processing skeletons on serverless platforms shows promise for achieving HPC-like performance with improved QoS and resource efficiency compared to traditional reactive approaches.

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>
