<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: Torrent is a distributed DMA architecture that enables efficient point-to-multipoint data transfers in SoCs without modifying NoC hardware or protocols, using logical chains to achieve up to 7.88x speedup over unicast.


<details>
  <summary>Details</summary>
Motivation: The growing computational power vs. communication bandwidth disparity in SoCs creates bottlenecks for data-parallel workloads like AI. Native multicast support is lacking in standard interconnect protocols, and existing P2MP solutions require hardware modifications that compromise scalability and compatibility.

Method: Torrent introduces a distributed DMA architecture with Chainwrite mechanism that forms logical chains over the NoC, where data traverses through targeted destinations like a linked list. Two scheduling algorithms optimize chain order based on NoC topology for performance and energy efficiency.

Result: RTL and FPGA evaluations show up to 7.88x speedup over unicast baseline. ASIC synthesis on 16nm shows minimal area (1.2%) and power (2.3%) overhead. Chainwrite enables scalable P2MP transfers with 82CC cycle overhead and 207um2 area overhead per destination.

Conclusion: Torrent provides an efficient, scalable solution for P2MP data transfers in SoCs without requiring NoC hardware modifications, offering significant performance improvements while maintaining compatibility with existing interconnect protocols.

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>


### [2] [A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX](https://arxiv.org/abs/2512.17834)
*Darja Nonaca,Jérémy Guichemerre,Reinhard Wiesmayr,Nihat Engin Tunali,Christoph Studer*

Main category: cs.AR

TL;DR: A new short-blocklength multi-rate binary LDPC code outperforms 5G-LDPC codes for URLLC applications, with a decoder ASIC achieving 14ns latency and 9Gb/s throughput at 62pJ/b energy efficiency.


<details>
  <summary>Details</summary>
Motivation: URLLC requires low-latency codes with short blocklengths. While polar codes with SCL decoding outperform LDPC codes for short blocks, SCL decoders have high latency and poor area efficiency. There's a need for better short-blocklength LDPC codes suitable for URLLC with fully parallel message-passing decoding.

Method: Proposed a new short-blocklength multi-rate binary LDPC code designed to outperform 5G-LDPC codes for the same blocklength. Implemented a decoder ASIC in GlobalFoundries 22FDX technology supporting three rates with fully parallel message-passing decoding architecture.

Result: The decoder ASIC occupies 0.44mm² area and achieves 14ns decoding latency (lowest-in-class) with 9Gb/s information throughput at 62pJ/b energy efficiency for rate-1/2 code with 128-bit blocklength. The proposed LDPC code outperforms 5G-LDPC codes for URLLC applications.

Conclusion: The proposed LDPC code and decoder architecture provide superior performance for URLLC applications, addressing the latency limitations of polar codes with SCL decoding while maintaining high throughput and energy efficiency in a compact ASIC implementation.

Abstract: Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [3] [BEOL Ferroelectric Compute-in-Memory Ising Machine for Simulated Bifurcation](https://arxiv.org/abs/2512.17165)
*Yu Qian,Alptekin Vardar,Konrad Seidel,David Lehninger,Maximilian Lederer,Zhiguo Shi,Cheng Zhuo,Kai Ni,Thomas Kämpfe,Xunzhao Yin*

Main category: cs.ET

TL;DR: FeFET-based compute-in-memory Ising framework co-designs algorithms and hardware to efficiently solve large-scale combinatorial optimization problems, achieving 175.9x speedup over GPU with superior solution quality.


<details>
  <summary>Details</summary>
Motivation: Combinatorial optimization problems are pervasive but NP-hard, making them inefficient to solve on conventional architectures as problem size grows. Existing Ising machines suffer from poor initialization and trade-offs between algorithmic performance and hardware efficiency.

Method: Two-step algorithmic flow: 1) attention-inspired initialization exploiting global spin topology, 2) lightweight simulated bifurcation algorithm tailored for CiM implementation. Hardware uses 32x256 FeFET CiM chip with ferroelectric capacitors integrated in 180-nm CMOS platform.

Result: Achieves up to 80% reduction in required iterations through initialization, and up to 175.9x speedup over GPU-based simulated bifurcation implementation across Max-Cut instances with up to 100,000 nodes while delivering superior solution quality.

Conclusion: The FeFET-based CiM Ising framework successfully demonstrates hardware-software co-design for efficient large-scale combinatorial optimization, overcoming limitations of existing approaches through algorithmic innovation and specialized hardware acceleration.

Abstract: Computationally hard combinatorial optimization problems are pervasive in science and engineering, yet their NP-hard nature renders them increasingly inefficient to solve on conventional von Neumann architectures as problem size grows. Ising machines implemented using dynamical, digital and compute-in-memory (CiM) approaches offer a promising alternative, but often suffer from poor initialization and a fundamental trade-off between algorithmic performance and hardware efficiency. Hardware-friendly schemes such as simulated annealing converge slowly, whereas faster algorithms, including simulated bifurcation, are difficult to implement efficiently in CiM hardware, limiting both convergence speed and solution quality. To address these limitations, here we present a ferroelectric field-effect transistor (FeFET)-based CiM Ising framework that tightly co-designs algorithms and hardware to efficiently solve large-scale combinatorial optimization problems. The proposed approach employs a two-step algorithmic flow: an attention-inspired initialization that exploits global spin topology and reduces the required iterations by up to 80%, followed by a lightweight simulated bifurcation algorithm specifically tailored for CiM implementation. To natively accelerate the core vector-matrix and vector-matrix-vector operations in both steps, we fabricate a 32x256 FeFET CiM chip using ferroelectric capacitors integrated at the back end of line of a 180-nm CMOS platform. Across Max-Cut instances with up to 100,000 nodes, the proposed hardware-software co-designed solver achieves up to a 175.9x speedup over a GPU-based simulated bifurcation implementation while consistently delivering superior solution quality.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: This paper proposes a novel fixed-job-level-priority scheduler for ROS2 using the events executor to handle arbitrary DAG task graphs on uniprocessor systems, bridging real-time theory with ROS2 scheduling.


<details>
  <summary>Details</summary>
Motivation: Current ROS2 scheduling methods are limited to simple chain-based tasks and lack support for arbitrary Directed Acyclic Graphs (DAGs), creating a gap between established real-time systems theory and practical ROS2 applications.

Method: The authors use the events executor with a special implementation of the events queue and communication middleware supporting LIFO-ordered message delivery. They abstract ROS2 applications as forests of trees and map them to traditional real-time DAG task models, implementing fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems.

Result: The implementation generates the same schedules as conventional fixed-priority DAG task schedulers despite lacking access to the precedence information typically required. This demonstrates that ROS2 applications can be effectively scheduled using established real-time DAG task models.

Conclusion: The proposed approach successfully bridges the gap between established real-time systems theory and ROS2 scheduling analyses, enabling more sophisticated scheduling of arbitrary DAG task graphs in ROS2 applications on uniprocessor systems.

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [5] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: Systematic evaluation of LLMs (ChatGPT 4/5, Claude, LLaMA) for generating parallel HPC code using C++ implementations of Mandelbrot set across shared-memory, directive-based, and distributed-memory paradigms.


<details>
  <summary>Details</summary>
Motivation: Parallel programming in HPC is challenging, requiring expertise in synchronization, communication, and memory models. While LLMs show promise in code generation, their effectiveness in producing correct and efficient HPC code is not well understood.

Method: Systematically evaluate leading LLMs on generating C++ implementations of Mandelbrot set using three parallel paradigms: shared-memory, directive-based, and distributed-memory. Generated programs are compiled and executed with GCC 11.5.0 to assess correctness, robustness, and scalability.

Result: ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance in generating parallel HPC code for the Mandelbrot set across different parallel paradigms.

Conclusion: LLMs, particularly ChatGPT-4 and ChatGPT-5, show promising capabilities in generating correct and scalable parallel HPC code, though systematic evaluation reveals varying effectiveness across different models and parallel paradigms.

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [6] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve is a serving system for Diffusion Large Language Models that addresses memory footprint crisis and resource oscillation through logit-aware activation budgeting, phase-multiplexed scheduling, and head-centric sparse attention, achieving 1.6-1.8× throughput improvements.


<details>
  <summary>Details</summary>
Motivation: Existing dLLM research focuses on kernel-level optimizations but lacks a holistic serving framework. There's a critical "memory footprint crisis" caused by monolithic logit tensors and resource oscillation between compute-bound "Refresh" and bandwidth-bound "Reuse" phases in production environments.

Method: dLLM-Serve introduces three key techniques: 1) Logit-Aware Activation Budgeting to decompose transient tensor peaks, 2) Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and 3) Head-Centric Sparse Attention to decouple logical sparsity from physical storage.

Result: dLLM-Serve improves throughput by 1.61×-1.81× on RTX 4090 and 1.60×-1.74× on NVIDIA L40S, while reducing tail latency by nearly 4× under heavy contention. It establishes the first blueprint for scalable dLLM inference across heterogeneous hardware.

Conclusion: dLLM-Serve successfully converts theoretical algorithmic sparsity into tangible wall-clock acceleration, providing an efficient serving system that co-optimizes memory footprint, computational scheduling, and generation quality for diffusion LLMs.

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [7] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE is a scalable vector index for billion-scale approximate nearest neighbor search that achieves high throughput through balanced partitioning and recursive multi-level construction.


<details>
  <summary>Details</summary>
Motivation: Existing distributed vector indexes struggle to balance accuracy, latency, and throughput when scaling to billions of vectors, creating a need for better tradeoff management.

Method: SPIRE uses two key design decisions: 1) balanced partition granularity to avoid read-cost explosion, and 2) accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy.

Result: In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

Conclusion: SPIRE successfully addresses the tradeoff challenges in distributed vector indexing for billion-scale ANNS, demonstrating superior scalability and throughput performance.

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [8] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: The HEAL Data Platform is a cloud-based federated system that serves as a single point for discovering and analyzing data from NIH's HEAL Initiative, connecting multiple repositories through open-source Gen3 technology.


<details>
  <summary>Details</summary>
Motivation: HEAL Initiative studies generate diverse data types deposited across multiple NIH and third-party repositories, creating fragmentation that hinders discovery and secondary analysis. A unified platform was needed to overcome this data siloing problem.

Method: Built on open-source Gen3 platform using framework services for authentication/authorization, persistent identifiers, and metadata management. Utilizes mesh architecture with exposed APIs to interoperate with both NIH and non-NIH data repositories.

Result: Platform serves as single discovery point for over 1,000 HEAL studies with hundreds of monthly users, interoperates with 19 data repositories, provides rich metadata, and enables secure cloud-based compute environments for secondary analysis through STRIDES integration.

Conclusion: The HEAL Data Platform successfully accelerates secondary data use by providing unified discovery and analysis capabilities while ensuring FAIR principles, maximizing the value of HEAL Initiative data investments.

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [9] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec and UnifiedServe optimize MLLM serving by accelerating video decoding and eliminating inter-stage blocking, achieving up to 4.4× higher throughput.


<details>
  <summary>Details</summary>
Motivation: Current MLLM serving pipelines have bottlenecks: CPU-based video decoding dominates latency, vision encoder stage blocks LLM inference, and heterogeneous stages underutilize GPU resources.

Method: Two complementary designs: FlashCodec uses collaborative multi-GPU video decoding to reduce latency. UnifiedServe logically decouples vision-to-text and inference stages while physically sharing GPU resources to eliminate blocking and maximize utilization.

Result: The framework serves 3.0× more requests, enforces 1.5× tighter SLOs, and achieves up to 4.4× higher throughput compared to state-of-the-art systems.

Conclusion: Joint optimization of multimodal preprocessing and vision-to-text/inference stages through FlashCodec and UnifiedServe significantly improves MLLM serving efficiency and performance.

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>
