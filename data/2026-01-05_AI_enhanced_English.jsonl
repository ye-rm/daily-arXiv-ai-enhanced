{"id": "2601.00317", "categories": ["cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00317", "abs": "https://arxiv.org/abs/2601.00317", "authors": ["Estefan\u00eda Recayte"], "title": "On the Error Floor Evaluation of NOMA-Irregular Repetition Slotted ALOHA", "comment": null, "summary": "In this work, we provide a simple yet tight analytical approximation of the packet loss rate in the error floor region for a non-orthogonal multiple access (NOMA)-based irregular repetition slotted ALOHA (IRSA) scheme. Considering an Internet of Things (IoT) scenario, users randomly select both the number of replicas based on a designed degree distribution and the transmission power from predetermined levels, while successive interference cancellation (SIC) is performed at the receiver. Our derived packet loss rate expression in the finite length regime is promptly evaluated. Its accuracy is validated through Monte-Carlo simulations, demonstrating a strong match across channel loads, including those beyond the low load regime", "AI": {"tldr": "Analytical approximation of packet loss rate in error floor region for NOMA-based IRSA scheme in IoT scenarios with finite-length regime validation.", "motivation": "Need for simple yet tight analytical approximation of packet loss rate in error floor region for NOMA-based IRSA schemes in IoT scenarios to enable performance evaluation without extensive Monte-Carlo simulations.", "method": "Derived analytical expression for packet loss rate in finite length regime for NOMA-based IRSA where users randomly select replica numbers from designed degree distribution and transmission power from predetermined levels, with SIC performed at receiver.", "result": "Derived packet loss rate expression is promptly evaluated and validated through Monte-Carlo simulations, showing strong match across channel loads including beyond low load regime.", "conclusion": "Successfully developed accurate analytical approximation for packet loss rate in error floor region for NOMA-based IRSA that matches simulation results across various channel loads."}}
{"id": "2601.00343", "categories": ["cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.00343", "abs": "https://arxiv.org/abs/2601.00343", "authors": ["Estefan\u00eda Recayte", "Leonardo Badia", "Andrea Munari"], "title": "Two-Step Interference Cancellation for Energy Saving in Irregular Repetition Slotted ALOHA", "comment": null, "summary": "We evaluate a modification of irregular repetition slotted ALOHA (IRSA) involving intermediate decoding and early transmission termination by some nodes, upon their decoding success. This is meant to avoid unnecessary transmissions, thereby reducing energy consumption. We expect this to be particularly useful at low loads, where most transmissions can be avoided as they do not often result in a collision and are therefore redundant. To validate this proposal, we observe that most of the literature related to IRSA considers an asymptotic heavily loaded regime; thus, we also present a model of energy consumption and success probability for frames of limited length and low offered loads. Thanks to our analysis, also confirmed by simulation, we are able to show that the proposed technique is able to reduce IRSA energy consumption by minimizing transmissions, while preserving performance gains over standard ALOHA. For example, we are able to get a 33% energy saving at offered loads around 10% without affecting throughput.", "AI": {"tldr": "Modified IRSA with early transmission termination reduces energy consumption by avoiding unnecessary transmissions at low loads while maintaining performance gains over standard ALOHA.", "motivation": "To reduce energy consumption in IRSA by avoiding unnecessary transmissions, especially at low loads where collisions are infrequent and many transmissions are redundant.", "method": "Modified IRSA with intermediate decoding and early transmission termination upon decoding success; developed energy consumption and success probability model for limited frame length and low offered loads.", "result": "Proposed technique reduces IRSA energy consumption by minimizing transmissions while preserving performance gains over standard ALOHA, achieving 33% energy saving at 10% offered load without affecting throughput.", "conclusion": "Early transmission termination in IRSA effectively reduces energy consumption at low loads while maintaining system performance, validated by both analysis and simulation."}}
{"id": "2601.00380", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00380", "abs": "https://arxiv.org/abs/2601.00380", "authors": ["Hanzhe Li", "Bingchen Lin", "Mengyuan Xu"], "title": "Word Frequency Counting Based on Serverless MapReduce", "comment": "6 pages, 4 figures, International Conference on Engineering Management, Information Technology and Intelligence (EMITI 2024)", "summary": "With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.", "AI": {"tldr": "This paper combines serverless computing (Function as a Service) with MapReduce to optimize word frequency counting tasks by finding the optimal number of Map and Reduce functions for improved efficiency.", "motivation": "The motivation is to leverage the benefits of serverless computing (high concurrency, robustness) and MapReduce (popular big data processing model) to reduce execution time and increase efficiency for data processing tasks like word frequency counting.", "method": "The paper uses a MapReduce programming model implemented on a serverless computing platform to experimentally determine the optimal number of Map and Reduce functions for specific workloads.", "result": "Experiments show that as the number of Map and Reduce functions increases, both execution time reduces and overall program efficiency improves at different rates for the same workload.", "conclusion": "The discovery of optimal Map and Reduce function configurations can help organizations and programmers find optimized solutions for big data processing tasks using serverless MapReduce frameworks."}}
{"id": "2601.00450", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00450", "abs": "https://arxiv.org/abs/2601.00450", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.", "AI": {"tldr": "REAP-cache eliminates read disturbance accumulation in STT-MRAM caches without performance penalty, improving MTTF by 171x with minimal area/energy overhead.", "motivation": "STT-MRAM caches suffer from high read disturbance error rates. Conventional parallel cache access (reading all blocks in a set for tag comparison) causes accumulation of read disturbances in unrequested blocks, degrading reliability despite ECC protection.", "method": "Proposes REAP-cache (Read Error Accumulation Preventer) - a scheme that eliminates read disturbance accumulation without compromising cache performance. The method addresses the fundamental issue of parallel cache block reads during tag comparison operations.", "result": "REAP-cache extends cache Mean Time To Failure (MTTF) by 171x, while increasing cache area by less than 1% and energy consumption by only 2.7%.", "conclusion": "The proposed REAP-cache effectively solves the read disturbance accumulation problem in STT-MRAM caches, significantly improving reliability with negligible overhead, making STT-MRAM more viable for on-chip cache applications."}}
{"id": "2601.00397", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00397", "abs": "https://arxiv.org/abs/2601.00397", "authors": ["Amey Agrawal", "Mayank Yadav", "Sukrit Kumar", "Anirudha Agrawal", "Garv Ghai", "Souradeep Bera", "Elton Pinto", "Sirish Gambhira", "Mohammad Adain", "Kasra Sohrab", "Chus Antonanzas", "Alexey Tumanov"], "title": "Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving", "comment": null, "summary": "Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.\n  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.", "AI": {"tldr": "Revati is a time-warp emulator that enables fast performance modeling of LLM serving systems by executing real serving code without physical GPUs, using time jumps instead of actual GPU kernel execution.", "motivation": "Testing LLM serving configurations on GPU clusters is slow and expensive (hours and thousands of dollars), while existing simulators require re-implementing serving system control logic which becomes burdensome as frameworks evolve.", "method": "Revati intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps by fast-forwarding virtual time by predicted kernel durations. It uses a coordination protocol to synchronize these jumps across distributed processes while preserving causality.", "result": "On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.", "conclusion": "Revati provides an efficient solution for performance modeling of LLM serving systems by directly executing real serving code at simulation-like speeds, eliminating the need for physical GPUs or re-implementation of control logic."}}
{"id": "2601.00456", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00456", "abs": "https://arxiv.org/abs/2601.00456", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.", "AI": {"tldr": "ROBIN is an efficient ECC configuration for STT-MRAM caches that addresses data-dependent error patterns, reducing cache error rate by 28.6x compared to conventional ECCs.", "motivation": "STT-MRAM is promising for on-chip cache memories but suffers from high error rates. Conventional ECCs lose efficiency due to data-dependent error patterns in STT-MRAM.", "method": "First conducted comprehensive analysis of conventional ECC inefficiency, then proposed ROBIN - an efficient ECC configuration specifically designed to handle data-dependent error patterns in STT-MRAM.", "result": "Conventional ECCs increase cache error rate by average 151.7%, while ROBIN reduces this value by more than 28.6x, significantly improving correction capability.", "conclusion": "ROBIN effectively addresses the data-dependent error pattern problem in STT-MRAM caches, making it a superior ECC solution compared to conventional approaches for on-chip cache applications."}}
{"id": "2601.00530", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00530", "abs": "https://arxiv.org/abs/2601.00530", "authors": ["Ravi Teja Pagidoju"], "title": "Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure", "comment": "Accepted at 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025)", "summary": "Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.", "AI": {"tldr": "This paper presents a systematic comparison of POS workload deployments on Google Cloud Platform and Microsoft Azure using free-tier resources, benchmarking performance metrics and cost efficiency.", "motivation": "There is little empirical research on platform-specific performance for retail workloads despite the digital transformation accelerating cloud-based POS adoption. The study aims to provide transparent methodology for small retailers and researchers to evaluate POS workloads on cloud platforms.", "method": "Used systematic, repeatable comparison of POS workload deployments on GCP and Azure with real-time API endpoints and open-source benchmarking code. Employed free-tier cloud resources to measure performance metrics (response latency, throughput, scalability) and estimate operational costs based on actual resource usage and current public cloud pricing. All tables and figures generated directly from code outputs to ensure consistency.", "result": "GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. The study identifies architectural components leading to these differences.", "conclusion": "Establishes a strong, open benchmarking methodology for retail cloud applications and provides the first comprehensive, code-driven comparison of POS-specific workloads across leading cloud platforms, offering a helpful framework for merchants considering cloud POS implementation."}}
{"id": "2601.00644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.00644", "abs": "https://arxiv.org/abs/2601.00644", "authors": ["Yuchen Li", "Rui Kong", "Zhonghao Lyu", "Qiyang Li", "Xinran Chen", "Hengyi Cai", "Lingyong Yan", "Shuaiqiang Wang", "Jiashu Zhao", "Guangxu Zhu", "Linghe Kong", "Guihai Chen", "Haoyi Xiong", "Dawei Yin"], "title": "FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding", "comment": null, "summary": "Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.", "AI": {"tldr": "FlexSpec is a communication-efficient edge-cloud collaborative inference framework that uses a shared-backbone architecture to enable a single static edge-side draft model to work with evolving cloud-side target models, eliminating repeated model synchronization and reducing communication overhead.", "motivation": "Deploying LLMs in mobile/edge environments faces constraints from limited resources, scarce bandwidth, and frequent model evolution. Existing edge-cloud collaborative inference with speculative decoding requires tight coupling between edge and cloud models, leading to excessive communication overhead from repeated model synchronization, increased latency, and limited scalability.", "method": "FlexSpec uses a shared-backbone architecture that allows a single static edge-side draft model to remain compatible with evolving cloud-side target models. It also implements a channel-aware adaptive speculation mechanism that dynamically adjusts speculative draft length based on real-time channel state information and device energy budgets.", "result": "Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional speculative decoding approaches in terms of inference efficiency.", "conclusion": "FlexSpec addresses the limitations of existing edge-cloud collaborative inference by decoupling edge deployment from cloud-side model updates, eliminating the need for edge-side retraining or repeated model downloads, and substantially reducing communication and maintenance costs while maintaining inference efficiency."}}
