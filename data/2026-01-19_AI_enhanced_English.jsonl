{"id": "2601.10998", "categories": ["cs.DC", "cs.MM", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.10998", "abs": "https://arxiv.org/abs/2601.10998", "authors": ["Shinsuk Kang", "Youngjae Kim"], "title": "AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning", "comment": "16 pages, 7 figures, 5 tables. Submitted for publication", "summary": "Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.", "AI": {"tldr": "AFLL is an adaptive load stabilization system for MMO game servers that learns causal relationships between server messages and client requests to enable predictive throttling, reducing CPU time by 48.3% and thread contention by 64.4% while maintaining sub-100ms response times.", "motivation": "Traditional MMO server load management approaches either uniformly throttle all messages (damaging gameplay) or use fixed heuristic rules that fail to adapt to dynamic workloads, creating a need for intelligent, adaptive load stabilization.", "method": "AFLL uses real-time feedback loop learning with backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery.", "result": "With 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations.", "conclusion": "AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems, achieving remarkable reproducibility (CV < 2% across all metrics) and identifying a three-stage causal chain linking message blocking to load reduction."}}
{"id": "2601.11156", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11156", "abs": "https://arxiv.org/abs/2601.11156", "authors": ["Niklas Kowallik", "Trever Schirmer", "David Bermbach"], "title": "Konflux: Optimized Function Fusion for Serverless Applications", "comment": null, "summary": "Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.\n  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.", "AI": {"tldr": "A system for analyzing all possible function fusion configurations in FaaS applications through local emulation, revealing optimal trade-offs between cost and latency influenced by pricing models.", "motivation": "Optimizing FaaS deployments is challenging due to the vast number of possible function fusion configurations, making brute-force benchmarking in production too costly and time-consuming.", "method": "Developed a system that analyzes every possible fusion setup by emulating the FaaS platform locally, eliminating the need for live platform reconfiguration and reducing experimentation costs.", "result": "Evaluation across multiple FaaS applications and resource limits showed that only a limited set of fusion configurations represent optimal solutions, with these optima strongly influenced by the specific pricing model in use.", "conclusion": "The proposed system enables comprehensive analysis of function fusion configurations locally, revealing that optimal cost-latency trade-offs depend on pricing models and can be efficiently identified without expensive production testing."}}
{"id": "2601.11487", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11487", "abs": "https://arxiv.org/abs/2601.11487", "authors": ["Paulo S\u00e9rgio Almeida"], "title": "Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering", "comment": "16 pages, 5 figures", "summary": "Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.", "AI": {"tldr": "The paper introduces a novel hybrid causal delivery algorithm combining sender-buffering (SPS) and receiver-buffering (FIFO) to achieve constant metadata overhead and computational efficiency.", "motivation": "Existing causal delivery approaches have significant limitations: receiver-buffering methods incur prohibitive metadata overhead for large-scale systems, while sender-buffering approaches have throughput scalability and liveness issues. There's a need for a topology-agnostic algorithm with constant metadata overhead and optimal processing efficiency.", "method": "The paper introduces a hybrid SPS+FIFO algorithm that combines sender-buffering to enforce Sender Permission to Send (SPS) and receiver-buffering to enforce FIFO ordering. This approach uses careful data structure design to achieve amortized constant processing overhead while maintaining effectively constant metadata size per message.", "result": "The proposed algorithm overcomes limitations of sender-only buffering approaches, achieves effectively constant metadata size per message, and is computationally optimal with amortized effectively constant processing overhead. It appears to be the first topology-agnostic causal delivery algorithm with these properties.", "conclusion": "The novel hybrid SPS+FIFO algorithm provides an efficient, scalable solution for causal delivery in distributed systems, addressing the metadata overhead and computational efficiency limitations of existing approaches while being generally applicable across different communication topologies."}}
{"id": "2601.10953", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.10953", "abs": "https://arxiv.org/abs/2601.10953", "authors": ["Junming Zhang", "Qinyan Zhang", "Huajun Sun", "Feiyang Gao", "Sheng Hu", "Rui Nie", "Xiangshui Miao"], "title": "SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding", "comment": null, "summary": "Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.", "AI": {"tldr": "SwiftKV Attention is a novel single-pass attention algorithm for edge accelerators that processes KV cache once per token without score materialization, achieving 7.16\u00d7 speedup over native attention. The SwiftKV-MHA accelerator enables multi-head parallel decoding with 13.48\u00d7 latency reduction.", "motivation": "Edge acceleration for large language models is challenging due to resource constraints. Current approaches struggle with fast attention inference and efficient decoding on edge accelerators, particularly with multi-head LLM decoding support limitations.", "method": "SwiftKV Attention: per-token pipelined, low-latency single-pass attention algorithm that processes each (kt, vt) in KV cache exactly once without score materialization, blockwise softmax, or second pass. SwiftKV-MHA accelerator: enables high precision attention and low precision GEMV on same processor array for multi-head parallel decoding.", "result": "SwiftKV Attention achieves 7.16\u00d7 speedup over native attention on edge accelerators. SwiftKV-MHA reduces attention latency by 13.48\u00d7, improves generation speed by 17.4%, and increases token efficiency by 1.98\u00d7 compared to state-of-the-art works.", "conclusion": "The proposed SwiftKV Attention algorithm and SwiftKV-MHA accelerator effectively address edge acceleration challenges for LLMs, enabling fast attention inference and efficient multi-head parallel decoding on resource-constrained edge devices."}}
{"id": "2601.11057", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.11057", "abs": "https://arxiv.org/abs/2601.11057", "authors": ["Hongshi Tan", "Yao Chen", "Xinyu Chen", "Qizhen Zhang", "Cheng Chen", "Weng-Fai Wong", "Bingsheng He"], "title": "RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs", "comment": "Accepted by HPCA 2026", "summary": "Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.", "AI": {"tldr": "RidgeWalker is a high-performance FPGA accelerator for Graph Random Walks that achieves 7-8x speedup over state-of-the-art solutions through asynchronous pipeline architecture and adaptive load balancing.", "motivation": "Graph Random Walks are important for approximating graph properties but are difficult to accelerate due to data dependencies, irregular memory access, and imbalanced execution. Existing FPGA solutions underutilize hardware potential.", "method": "Leverages the Markov property to decompose GRWs into stateless fine-grained tasks that can execute out-of-order. Uses an asynchronous pipeline architecture with feedback-driven scheduler based on queuing theory for perfect pipelining and adaptive load balancing.", "result": "Achieves average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups up to 71.0x and 22.9x respectively across various GRW algorithms and real-world graph datasets.", "conclusion": "RidgeWalker demonstrates that efficient FPGA acceleration of Graph Random Walks is achievable through proper architectural design that addresses the fundamental challenges of data dependencies and load imbalance."}}
{"id": "2601.10874", "categories": ["cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.10874", "abs": "https://arxiv.org/abs/2601.10874", "authors": ["Amer Diwan", "Prabhakar Raghavan", "Eli Upfal"], "title": "Balanced allocation: considerations from large scale service environments", "comment": null, "summary": "We study d-way balanced allocation, which assigns each incoming job to the lightest loaded among d randomly chosen servers. While prior work has extensively studied the performance of the basic scheme, there has been less published work on adapting this technique to many aspects of large-scale systems. Based on our experience in building and running planet-scale cloud applications, we extend the understanding of d-way balanced allocation along the following dimensions:\n  (i) Bursts: Events such as breaking news can produce bursts of requests that may temporarily exceed the servicing capacity of the system. Thus, we explore what happens during a burst and how long it takes for the system to recover from such bursts. (ii) Priorities: Production systems need to handle jobs with a mix of priorities (e.g., user facing requests may be high priority while other requests may be low priority). We extend d-way balanced allocation to handle multiple priorities. (iii) Noise: Production systems are often typically distributed and thus d-way balanced allocation must work with stale or incorrect information. Thus we explore the impact of noisy information and their interactions with bursts and priorities.\n  We explore the above using both extensive simulations and analytical arguments. Specifically we show, (i) using simulations, that d-way balanced allocation quickly recovers from bursts and can gracefully handle priorities and noise; and (ii) that analysis of the underlying generative models complements our simulations and provides insight into our simulation results.", "AI": {"tldr": "The paper extends d-way balanced allocation (power of d choices) to handle real-world challenges in large-scale systems: bursts, priorities, and noisy information, showing both simulation and analytical results.", "motivation": "Prior work extensively studied basic d-way balanced allocation but didn't address practical challenges in planet-scale cloud applications. The authors aim to extend understanding to handle bursts (temporary overload), multiple job priorities, and noisy/stale information in distributed systems.", "method": "Extends d-way balanced allocation algorithm to handle bursts, priorities, and noisy information. Uses both extensive simulations and analytical arguments to study these extensions. Analyzes underlying generative models to complement simulation results.", "result": "Simulations show d-way balanced allocation quickly recovers from bursts and gracefully handles priorities and noise. Analytical results complement simulations and provide insight into simulation findings.", "conclusion": "The paper successfully extends d-way balanced allocation to address practical challenges in large-scale cloud systems, demonstrating its robustness to bursts, ability to handle multiple priorities, and resilience to noisy information through combined simulation and analytical approaches."}}
{"id": "2601.11292", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.11292", "abs": "https://arxiv.org/abs/2601.11292", "authors": ["Yiqi Zhou", "JunHao Ma", "Xingyang Li", "Yule Sheng", "Yue Yuan", "Yikai Wang", "Bochang Wang", "Yiheng Wu", "Shan Shen", "Wei Xing", "Daying Sun", "Li Li", "Zhiqiang Xiao"], "title": "OpenACM: An Open-Source SRAM-Based Approximate CiM Compiler", "comment": "Accepted by DATE 2026", "summary": "The rise of data-intensive AI workloads has exacerbated the ``memory wall'' bottleneck. Digital Compute-in-Memory (DCiM) using SRAM offers a scalable solution, but its vast design space makes manual design impractical, creating a need for automated compilers. A key opportunity lies in approximate computing, which leverages the error tolerance of AI applications for significant energy savings. However, existing DCiM compilers focus on exact arithmetic, failing to exploit this optimization. This paper introduces OpenACM, the first open-source, accuracy-aware compiler for SRAM-based approximate DCiM architectures. OpenACM bridges the gap between application error tolerance and hardware automation. Its key contribution is an integrated library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic), enabling designers to make fine-grained accuracy-energy trade-offs. The compiler automates the generation of the DCiM architecture, integrating a transistor-level customizable SRAM macro with variation-aware characterization into a complete, open-source physical design flow based on OpenROAD and the FreePDK45 library. This ensures full reproducibility and accessibility, removing dependencies on proprietary tools. Experimental results on representative convolutional neural networks (CNNs) demonstrate that OpenACM achieves energy savings of up to 64\\% with negligible loss in application accuracy. The framework is available on \\href{https://github.com/ShenShan123/OpenACM}{OpenACM:URL}", "AI": {"tldr": "OpenACM is the first open-source, accuracy-aware compiler for SRAM-based approximate digital compute-in-memory architectures that enables automated design with fine-grained accuracy-energy trade-offs.", "motivation": "Data-intensive AI workloads face memory wall bottlenecks, and while digital compute-in-memory using SRAM offers a scalable solution, its vast design space requires automated compilers. Existing compilers focus on exact arithmetic and miss the opportunity to exploit approximate computing for energy savings in error-tolerant AI applications.", "method": "OpenACM integrates a library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic) and automates DCiM architecture generation. It combines transistor-level customizable SRAM macros with variation-aware characterization into a complete open-source physical design flow using OpenROAD and FreePDK45 library.", "result": "Experimental results on representative convolutional neural networks show OpenACM achieves up to 64% energy savings with negligible loss in application accuracy. The framework is fully reproducible and accessible as open-source.", "conclusion": "OpenACM successfully bridges the gap between application error tolerance and hardware automation for approximate DCiM architectures, providing a practical solution for energy-efficient AI acceleration through automated, accuracy-aware compilation."}}
