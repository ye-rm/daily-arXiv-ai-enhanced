{"id": "2510.27583", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.27583", "abs": "https://arxiv.org/abs/2510.27583", "authors": ["Chandrish Ambati", "Trung Diep"], "title": "AMD MI300X GPU Performance Analysis", "comment": null, "summary": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication."}
{"id": "2510.26944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.26944", "abs": "https://arxiv.org/abs/2510.26944", "authors": ["Hoa Nguyen", "Pongstorn Maidee", "Jason Lowe-Power", "Alireza Kaviani"], "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies", "comment": null, "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."}
{"id": "2510.26985", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26985", "abs": "https://arxiv.org/abs/2510.26985", "authors": ["Mostafa Darvishi"], "title": "Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies", "comment": "5 figures, 3 tables", "summary": "This paper presents an in-depth analysis of timing closure challenges and\nconstraints in Field Programmable Gate Arrays (FPGAs) and Application Specific\nIntegrated Circuits (ASICs). We examine core timing principles, architectural\ndistinctions, and design methodologies influencing timing behavior in both\ntechnologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA\n(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance\ntrade-offs. Experimental results show ASICs achieve superior timing of 45ps\nsetup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and\n120ps hold times, validating their suitability for high-performance designs."}
{"id": "2510.26913", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26913", "abs": "https://arxiv.org/abs/2510.26913", "authors": ["Junyi Shen", "Noppanat Wadlom", "Lingfeng Zhou", "Dequan Wang", "Xu Miao", "Lei Fang", "Yao Lu"], "title": "FlowMesh: A Service Fabric for Composable LLM Workflows", "comment": null, "summary": "AI deployment increasingly resembles a pipeline of data transformation,\nfine-tuning, and agent interactions rather than a monolithic LLM job; recent\nexamples include RLHF/RLAIF training and agentic workflows. To cope with this\nshift, we propose FlowMesh, a multi-tenant service fabric that executes and\noptimizes these workloads as one shared service instead of isolated pipelines.\nIt decomposes workflows into fine-grained operators with recorded lineage,\nenabling de-duplication of work across users and batching requests on the same\nhardware while preserving per-workflow provenance. A global control plane\nmaintains a cluster-wide pool of ready operators and uses a single utility\nfunction to pick both the batch and the worker, balancing throughput, cost, and\ndata locality on heterogeneous GPUs. The data plane is an elastic fleet of\nstateless workers backed by a content-addressable store, enabling rapid,\nautomatic scale-out, safe retry after preemption, and portability across\nmanaged clusters such as Kubernetes and geo-distributed GPU marketplaces such\nas Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost\nreduction and 2.0x lower energy usage, provides a similar or better latency\nprofile, and remains efficient under dynamic and failure-prone conditions."}
{"id": "2510.27095", "categories": ["cs.ET", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.27095", "abs": "https://arxiv.org/abs/2510.27095", "authors": ["Shubham Jadhav", "Kaustav Roy", "Luis Amaro", "Thejas Basavarajappa", "Madhav Ramesh", "Debdeep Jena", "Huili", "Xing", "Amit Lal"], "title": "Lorentzian Switching Dynamics in HZO-based FeMEMS Synapses for Neuromorphic Weight Storage", "comment": "19 pages, 5 figures", "summary": "Neuromorphic computing demands synaptic elements that can store and update\nweights with high precision while being read non-destructively. Conventional\nferroelectric synapses store weights in remnant polarization states and might\nrequire destructive electrical readout, limiting endurance and reliability. We\ndemonstrate a ferroelectric MEMS (FeMEMS) based synapse in which analog weights\nare stored in the piezoelectric coefficient $d_{31,eff}$ of a released\nHf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) MEMS unimorph. Partial switching of\nferroelectric domains modulates $d_{31,eff}$, and a low-amplitude mechanical\ndrive reads out the weight without read-disturb in the device yielding more\nthan 7-bit of programming levels. The mechanical switching distribution\nfunction follows a Lorentzian distribution as a logarithmic function of partial\npoling voltage ($V_p$) consistent with nucleation-limited switching (NLS), and\nthe median threshold extracted from electromechanical data obeys a Merz-type\nfield-time law with a dimensionless exponent $\\alpha = 3.62$. These\nrelationships establish a quantitative link between mechanical weights and\nelectrical switching kinetics. This mechanically read synapse avoids\ndepolarization and charge-injection effects, provides bipolar weights (well\nsuited for excitatory and inhibitory synapses), directly reveals partial domain\npopulations, and offers a robust, energy-efficient route toward high-bit\nneuromorphic hardware."}
{"id": "2510.27070", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27070", "abs": "https://arxiv.org/abs/2510.27070", "authors": ["Dong Tong"], "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review", "comment": null, "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."}
{"id": "2510.27039", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27039", "abs": "https://arxiv.org/abs/2510.27039", "authors": ["Zhuo Zheng", "Lingran Meng", "Ziyu Lin"], "title": "A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration", "comment": null, "summary": "Accurate traffic flow forecasting is essential for the development of\nintelligent transportation systems (ITS), supporting tasks such as traffic\nsignal optimization, congestion management, and route planning. Traditional\nmodels often fail to effectively capture complex spatial-temporal dependencies\nin large-scale road networks, especially under the influence of external\nfactors such as weather, holidays, and traffic accidents. To address this\nchallenge, this paper proposes a cloud-based hybrid model that integrates\nSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture\nfor traffic flow prediction. The model leverages the strengths of GNNs in\nmodeling spatial correlations across road networks and the Transformers'\nability to capture long-term temporal dependencies. External contextual\nfeatures are incorporated via feature fusion to enhance predictive accuracy.\nThe proposed model is deployed on a cloud computing platform to achieve\nscalability and real-time adaptability. Experimental evaluation of the dataset\nshows that our model outperforms baseline methods (LSTM, TCN, GCN, pure\nTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings\nsuggest that the hybrid GNN-Transformer approach provides an effective and\nscalable solution for cloud-based ITS applications, offering methodological\nadvancements for traffic flow forecasting and practical implications for\ncongestion mitigation."}
{"id": "2510.27107", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.27107", "abs": "https://arxiv.org/abs/2510.27107", "authors": ["Zhipeng Liao", "Kunming Shao", "Jiangnan Yu", "Liang Zhao", "Tim Kwang-Ting Cheng", "Chi-Ying Tsui", "Jie Yang", "Mohamad Sawan"], "title": "A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable Medical LLMs-Agents", "comment": "Accepted by BioCAS2025", "summary": "With powerful and integrative large language models (LLMs), medical AI agents\nhave demonstrated unique advantages in providing personalized medical\nconsultations, continuous health monitoring, and precise treatment plans.\nRetrieval-Augmented Generation (RAG) integrates personal medical documents into\nLLMs by an external retrievable database to address the costly retraining or\nfine-tuning issues in deploying customized agents. While deploying medical\nagents in edge devices ensures privacy protection, RAG implementations impose\nsubstantial memory access and energy consumption during the retrieval stage.\nThis paper presents a hierarchical retrieval architecture for edge RAG,\nleveraging a two-stage retrieval scheme that combines approximate retrieval for\ncandidate set generation, followed by high-precision retrieval on pre-selected\ndocument embeddings. The proposed architecture significantly reduces energy\nconsumption and external memory access while maintaining retrieval accuracy.\nSimulation results show that, under TSMC 28nm technology, the proposed\nhierarchical retrieval architecture has reduced the overall memory access by\nnearly 50% and the computation by 75% compared to pure INT8 retrieval, and the\ntotal energy consumption for 1 MB data retrieval is 177.76 {\\mu}J/query."}
{"id": "2510.27257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27257", "abs": "https://arxiv.org/abs/2510.27257", "authors": ["Mengshi Qi", "Jiaxuan Peng", "Jie Zhang", "Juan Zhu", "Yong Li", "Huadong Ma"], "title": "Synergistic Tensor and Pipeline Parallelism", "comment": null, "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP."}
{"id": "2510.27289", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27289", "abs": "https://arxiv.org/abs/2510.27289", "authors": ["Zhengchang Hua", "Panagiotis Oikonomou", "Karim Djemame", "Nikos Tziritas", "Georgios Theodoropoulos"], "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination", "comment": "16 pages, 8 figures. Accepted by the 25th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP'25)", "summary": "The coordination of large-scale, decentralised systems, such as a fleet of\nElectric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a\nsignificant challenge for modern control systems. While collaborative Digital\nTwins have been proposed as a solution to manage such systems without\ncompromising the privacy of individual agents, deriving globally optimal\ncontrol policies from the high-level information they share remains an open\nproblem. This paper introduces Digital Twin Assisted Multi-Agent Deep\nDeterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid\narchitecture that integrates a multi-agent reinforcement learning framework\nwith a collaborative DT network. Our core contribution is a simulation-assisted\nlearning algorithm where the centralised critic is enhanced by a predictive\nglobal model that is collaboratively built from the privacy-preserving data\nshared by individual DTs. This approach removes the need for collecting\nsensitive raw data at a centralised entity, a requirement of traditional\nmulti-agent learning algorithms. Experimental results in a simulated V2G\nenvironment demonstrate that DT-MADDPG can achieve coordination performance\ncomparable to the standard MADDPG algorithm while offering significant\nadvantages in terms of data privacy and architectural decentralisation. This\nwork presents a practical and robust framework for deploying intelligent,\nlearning-based coordination in complex, real-world cyber-physical systems."}
{"id": "2510.27317", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27317", "abs": "https://arxiv.org/abs/2510.27317", "authors": ["Shuyi Chen", "Panagiotis Oikonomou", "Zhengchang Hua", "Nikos Tziritas", "Karim Djemame", "Nan Zhang", "Georgios Theodoropoulos"], "title": "Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing", "comment": "Accepted by the 21st IEEE International Conference on Green Computing\n  and Communications (GreenCom 2025)", "summary": "Multi-access Edge Computing (MEC) delivers low-latency services by hosting\napplications near end-users. To promote sustainability, these systems are\nincreasingly integrated with renewable Energy Harvesting (EH) technologies,\nenabling operation where grid electricity is unavailable. However, balancing\nthe intermittent nature of harvested energy with dynamic user demand presents a\nsignificant resource allocation challenge. This work proposes an online\nstrategy for an MEC system powered exclusively by EH to address this trade-off.\nOur strategy dynamically schedules computational tasks with dependencies and\ngoverns energy consumption through real-time decisions on server frequency\nscaling and service module migration. Experiments using real-world datasets\ndemonstrate our algorithm's effectiveness in efficiently utilizing harvested\nenergy while maintaining low service latency."}
{"id": "2510.27351", "categories": ["cs.DC", "65Y05, 65Y10, 90C59, 68T20"], "pdf": "https://arxiv.org/pdf/2510.27351", "abs": "https://arxiv.org/abs/2510.27351", "authors": ["Milena Veneva"], "title": "ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method", "comment": "10 pages, 6 figures, 4 tables, DLCP conference 2025, Moscow, Russia", "summary": "This paper presents a machine learning (ML)-based heuristic for finding the\noptimum sub-system size for the CUDA implementation of the parallel partition\nalgorithm. Computational experiments for different system of linear algebraic\nequation (SLAE) sizes are conducted, and the optimum sub-system size for each\nof them is found empirically. To estimate a model for the sub-system size, we\nperform the k-nearest neighbors (kNN) classification method. Statistical\nanalysis of the results is done. By comparing the predicted values with the\nactual data, the algorithm is deemed to be acceptably good. Next, the heuristic\nis expanded to work for the recursive parallel partition algorithm as well. An\nalgorithm for determining the optimum sub-system size for each recursive step\nis formulated. A kNN model for predicting the optimum number of recursive steps\nfor a particular SLAE size is built."}
{"id": "2510.27656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27656", "abs": "https://arxiv.org/abs/2510.27656", "authors": ["Nandor Licker", "Kevin Hu", "Vladimir Zaytsev", "Lequn Chen"], "title": "RDMA Point-to-Point Communication for LLM Systems", "comment": null, "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in."}
