<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: The paper introduces an open-source microbenchmark suite to systematically evaluate NVIDIA's Blackwell (B200) GPU architecture, comparing it to H200 generation across memory subsystem, tensor cores, and floating-point precisions, showing significant performance and efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: GPU architectures are rapidly evolving for exascale computing and machine learning, but performance implications of architectural innovations like Blackwell's tensor cores, tensor memory, decompression engine, and dual chips remain poorly understood. There's a lack of systematic methodologies to quantify these improvements before hardware development cycles complete.

Method: Developed an open-source microbenchmark suite to analyze Blackwell GPUs, comparing them to H200 generation. Systematically evaluated memory subsystem, tensor core pipeline, and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Benchmarked dense/sparse GEMM, transformer inference, and training workloads.

Result: B200's tensor core enhancements achieve 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Memory analysis shows 58% reduction in memory access latency for cache-misses, fundamentally changing optimal algorithm design strategies.

Conclusion: The microbenchmark suite provides practical insights for optimizing workloads to fully utilize modern GPU architectures, enabling application developers to make informed architectural decisions and guiding future GPU design directions based on systematic evaluation of architectural innovations.

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [2] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: A near-memory architecture (NM-TOS) is proposed to accelerate TOS-based corner detection for event-based cameras on resource-constrained edge devices, achieving significant latency and energy reductions through hardware-software co-optimization.


<details>
  <summary>Details</summary>
Motivation: Event-based cameras offer high speed and low power for surveillance and autonomous driving, but corner detection algorithms like TOS face significant latency on edge devices, undermining the advantages of event-based vision systems.

Method: Proposes NM-TOS architecture with read-write decoupled 8T SRAM cell, pipelined patch updates, hardware-software co-optimized peripheral circuits, and dynamic voltage/frequency scaling (DVFS) for power-latency tradeoffs.

Result: Achieves 24.7x latency reduction and 1.2x energy reduction at 1.2V, or 1.93x latency and 6.6x energy reduction at 0.6V. Monte Carlo simulations show robust operation with zero bit error rate above 0.62V. Corner detection AUC shows minor reductions of 0.027-0.015 at 0.6V.

Conclusion: The NM-TOS architecture successfully addresses latency bottlenecks in event-based corner detection, enabling efficient implementation on edge devices while maintaining detection accuracy with minimal performance degradation.

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [3] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: SAT-based mapping approach for CGRAs outperforms state-of-the-art modulo scheduling techniques, achieving better results in 47.72% of benchmarks.


<details>
  <summary>Details</summary>
Motivation: The performance of Coarse-Grain Reconfigurable Arrays (CGRAs) heavily depends on mapping quality, and current state-of-the-art compilation techniques using modulo scheduling and graph algorithms like Max-Clique Enumeration have limitations in exploring the solution space effectively.

Method: Proposes SAT-MapIt: a SAT formulation approach using a novel kernel mobility schedule (KMS) that combines data-flow graph and architectural information to create boolean constraints. Uses iterative SAT solving with increasing II (Iteration Interval) until a valid mapping is found.

Result: SAT-MapIt outperforms state-of-the-art alternatives in 47.72% of benchmarks, sometimes finding lower II values and other times finding valid mappings when previous methods failed.

Conclusion: SAT-based formulation provides more effective exploration of the mapping solution space for CGRAs compared to traditional graph algorithm approaches, demonstrating superior performance in nearly half of tested benchmarks.

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [4] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: A SAT-based compilation method using Kernel Mobility Schedule to find optimal mappings for CGRA accelerators, reducing compilation time and improving mapping quality compared to state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: CGRA accelerators need efficient compilation to maximize speedup; current techniques may not find optimal mappings with minimal iteration interval (II), limiting performance potential.

Method: Proposes a SAT formulation using Kernel Mobility Schedule to encode all possible mappings for a given DFG and II, generating architectural constraints to find valid mappings with minimal II.

Result: Experimental results show reduced compilation time on average and higher quality mappings compared to existing state-of-the-art techniques.

Conclusion: The SAT-based approach with Kernel Mobility Schedule effectively improves CGRA compilation by finding optimal mappings with minimal II, enhancing both compilation efficiency and mapping quality.

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async](https://arxiv.org/abs/2512.02278)
*Yi Liu,Chen Qian*

Main category: cs.DC

TL;DR: Fantasy is a GPU cluster system that pipelines vector search and data transfer using GPUDirect Async to improve throughput for large-scale similarity search beyond single GPU memory limits.


<details>
  <summary>Details</summary>
Motivation: Vector similarity search is critical for AI applications like LLMs, but as vector datasets grow, graph sizes exceed single GPU memory capacity. Existing CPU-GPU architectures suffer from data loading stalls that hinder GPU computation.

Method: Fantasy uses a GPU cluster with GPUDirect Async to pipeline vector search and data transfer, overlapping computation and network communication to eliminate loading stalls.

Result: The system significantly improves search throughput for large graphs and supports large query batch sizes by efficiently utilizing GPU cluster resources.

Conclusion: Fantasy provides an efficient solution for large-scale vector similarity search by overcoming single GPU memory limitations through pipelined computation and communication in GPU clusters.

Abstract: Vector similarity search has become a critical component in AI-driven applications such as large language models (LLMs). To achieve high recall and low latency, GPUs are utilized to exploit massive parallelism for faster query processing. However, as the number of vectors continues to grow, the graph size quickly exceeds the memory capacity of a single GPU, making it infeasible to store and process the entire index on a single GPU. Recent work uses CPU-GPU architectures to keep vectors in CPU memory or SSDs, but the loading step stalls GPU computation. We present Fantasy, an efficient system that pipelines vector search and data transfer in a GPU cluster with GPUDirect Async. Fantasy overlaps computation and network communication to significantly improve search throughput for large graphs and deliver large query batch sizes.

</details>


### [6] [DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications](https://arxiv.org/abs/2512.02300)
*Haoyu Zheng,Shouwei Gao,Jie Ren,Wenqian Dong*

Main category: cs.DC

TL;DR: DOLMA is a framework for HPC memory disaggregation that intelligently offloads data objects to remote memory with minimal performance impact (≤16% degradation) while reducing local memory usage by up to 63%.


<details>
  <summary>Details</summary>
Motivation: Memory disaggregation can scale memory capacity and improve utilization in HPC systems, but remote memory access overhead poses significant challenges for compute-intensive applications sensitive to data locality.

Method: DOLMA identifies and offloads data objects to remote memory, provides quantitative analysis for local memory sizing, leverages predictable HPC access patterns for prefetching via dual-buffer design, and balances local/remote usage while maintaining multi-thread concurrency.

Result: Evaluation with eight HPC workloads shows DOLMA limits performance degradation to less than 16% while reducing local memory usage by up to 63% on average.

Conclusion: DOLMA provides a flexible and efficient solution for leveraging disaggregated memory in HPC domains with minimal performance compromise.

Abstract: Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant chal- lenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while pro- viding quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging dis- aggregated memory in HPC domains while minimally compromis- ing application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degrada- tion to less than 16% while reducing local memory usage by up to 63%, on average.

</details>


### [7] [Solutions for Distributed Memory Access Mechanism on HPC Clusters](https://arxiv.org/abs/2512.02546)
*Jan Meizner,Maciej Malawski*

Main category: cs.DC

TL;DR: This paper evaluates remote memory access mechanisms in distributed HPC systems, comparing shared storage and MPI-based approaches to local memory access, finding that MPI-backed remote access performs similarly to local access.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop efficient remote memory access mechanisms for distributed HPC systems, particularly for medical use-cases that require high-performance data access across clusters.

Method: The paper evaluates various remote memory access mechanisms using two distinct HPC clusters, comparing solutions based on shared storage and MPI (over Infiniband and Slingshot) against local memory access benchmarks.

Result: The results show that remote access performance, especially when backed by MPI, is similar to local memory access, indicating that efficient distributed memory access is achievable.

Conclusion: The paper concludes that MPI-based remote memory access provides performance comparable to local access, making it a viable solution for distributed HPC systems with potential benefits for medical applications.

Abstract: Paper presents and evaluates various mechanisms for remote access to memory in distributed systems based on two distinct HPC clusters. We are comparing solutions based on the shared storage and MPI (over Infiniband and Slingshot) to the local memory access. This paper also mentions medical use-cases that would mostly benefit from the described solution. We have found out that results for remote access esp. backed by MPI are similar to local memory access.

</details>


### [8] [Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems](https://arxiv.org/abs/2512.02646)
*Alex Barceló,Sebastián A. Cajas Ordoñez,Jaydeep Samanta,Andrés L. Suárez-Cetrulo,Romila Ghosh,Ricardo Simón Carbajo,Anna Queralt*

Main category: cs.DC

TL;DR: Active storage systems integrated into computing continuum optimize AI workload distribution by embedding computation in storage, reducing data transfer overhead and improving resource utilization.


<details>
  <summary>Details</summary>
Motivation: Traditional cloud architectures struggle with AI workload volume and velocity, causing storage, computation, and data movement inefficiencies. Existing frameworks lack flexibility for heterogeneous devices and rapidly changing AI algorithms in computing continuum environments.

Method: Proposes software architecture for seamless AI workload distribution across computing continuum using Python libraries and dataClay active storage platform. Embeds computation directly into storage architectures to reduce data transfer overhead.

Result: Active storage offloading significantly improves memory efficiency and training speeds while maintaining accuracy. Benefits include reduced memory consumption, optimized storage requirements, faster training times, and improved execution efficiency across devices.

Conclusion: Active storage has potential to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with low entry barrier for domain experts and developers.

Abstract: The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.
  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.
  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.

</details>


### [9] [Distributed and Autonomic Minimum Spanning Trees](https://arxiv.org/abs/2512.02683)
*Luiz A. Rodrigues,Elias P. Duarte,Luciana Arantes*

Main category: cs.DC

TL;DR: An autonomic algorithm for building and maintaining a scalable spanning tree in distributed systems using VCube topology, enabling efficient broadcast with bounded degree and depth.


<details>
  <summary>Details</summary>
Motivation: Traditional one-to-all broadcast communication is not scalable as it places heavy load on the sender. There's a need for a scalable approach to enable efficient message broadcasting in distributed systems.

Method: Proposes an autonomic algorithm that builds and maintains a spanning tree connecting all processes using the VCube virtual topology. The algorithm ensures each vertex has in-degree and tree depth at most log₂n, and dynamically reconstructs the tree as processes fail or recover.

Result: The algorithm creates scalable spanning trees with bounded degree and depth, supports up to n-1 process failures while maintaining connectivity, and enables two broadcast algorithms (best-effort and reliable). Simulation results show comparisons with alternatives.

Conclusion: The autonomic spanning tree algorithm provides a scalable solution for broadcast communication in distributed systems, overcoming the limitations of traditional one-to-all approaches through bounded-degree trees and fault tolerance.

Abstract: The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.

</details>


### [10] [Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science](https://arxiv.org/abs/2512.02818)
*Sean R. Wilkinson,Patrick Widener,Sarp Oral,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: The paper proposes that HPC centers should actively foster FAIR ecosystems to enable better sharing and reuse of computational components across disciplines, using a component-based approach rather than focusing on entire workflows.


<details>
  <summary>Details</summary>
Motivation: HPC users develop customized digital artifacts tightly coupled to specific centers, leading to duplication of effort. Domain-specific FAIR initiatives create silos that limit cross-disciplinary collaboration.

Method: Builds on the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory architecture to propose a model tailored to HPC needs, emphasizing FAIR individual workflow components rather than entire workflows.

Result: Proposes a component-based FAIR approach that better supports diverse and evolving HPC user needs while maximizing long-term value of computational work.

Conclusion: HPC centers should play an active role in creating FAIR ecosystems that enable researchers to effectively discover, share, and reuse computational components across multiple scientific disciplines.

Abstract: High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.

</details>
