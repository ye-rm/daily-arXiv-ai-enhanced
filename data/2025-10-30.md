<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Cryogenic Characterization of Ferroelectric Non-volatile Capacitors](https://arxiv.org/abs/2510.25040)
*Madhav Vadlamani,Dyutimoy Chakraborty,Jianwei Jia,Halid Mulaosmanovic,Stefan Duenkel,Sven Beyer,Suman Datta,Shimeng Yu*

Main category: cs.ET

TL;DR: This paper demonstrates that operating ferroelectric capacitive crossbar arrays at cryogenic temperatures (down to 77K) significantly reduces thermal noise, enabling higher effective number of bits (~5 bits) for 128x128 MAC operations in in-memory computing systems.


<details>
  <summary>Details</summary>
Motivation: Ferroelectric capacitive crossbar arrays offer energy-efficient in-memory computing but suffer from thermal noise limiting the effective number of bits for weighted sum operations. Lowering temperature directly reduces thermal noise since it's proportional to temperature.

Method: Characterized non-volatile capacitors on a foundry 28 nm platform at cryogenic temperatures to evaluate memory window and ON state retention down to 77K, then used calibrated device models to simulate capacitive crossbar arrays in SPICE at lower temperatures.

Result: Demonstrated higher effective number of bits (~5 bits) for 128x128 multiple-and-accumulate operations when operating at cryogenic temperatures compared to room temperature operation.

Conclusion: Cryogenic operation of ferroelectric capacitive crossbar arrays is an effective approach to mitigate thermal noise limitations and achieve higher computational precision for in-memory computing applications.

Abstract: Ferroelectric-based capacitive crossbar arrays have been proposed for
energy-efficient in-memory computing in the charge domain. They combat the
challenges like sneak paths and high static power faced by resistive crossbar
arrays but are susceptible to thermal noise limiting the effective number of
bits (ENOB) for the weighted sum. A direct way to reduce this thermal noise is
by lowering the temperature as thermal noise is proportional to temperature. In
this work, we first characterize the non-volatile capacitors (nvCaps) on a
foundry 28 nm platform at cryogenic temperatures to evaluate the memory window,
ON state retention as a function of temperature down to 77K, and then use the
calibrated device models to simulate the capacitive crossbar arrays in SPICE at
lower temperatures to demonstrate higher ENOB (~5 bits) for 128x128
multiple-and-accumulate (MAC) operations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree is the first dataset-level framework that transforms operational weather radar archives into FAIR-compliant, cloud-optimized datasets using hierarchical structures and Zarr serialization, enabling scalable analysis with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Weather radar data are scientifically valuable but underutilized due to fragmented archives, vendor-specific formats, and poor alignment with FAIR principles, which hinders large-scale research, reproducibility, and cloud-native computation.

Method: Built on FM-301/CfRadial 2.1 standard and implemented using xarray DataTree, Radar DataTree organizes radar volume scans as hierarchical, metadata-rich structures and serializes them to Zarr. It uses Icechunk for ACID-compliant storage and versioning to enable efficient parallel computation across thousands of scans.

Result: The framework demonstrates significant performance gains in case studies including Quasi-Vertical Profile (QVP) and precipitation accumulation workflows. All tools and datasets are released openly via the Raw2Zarr repository.

Conclusion: This work provides a reproducible and extensible foundation for radar data stewardship, high-performance geoscience, and AI-ready weather infrastructure, addressing current limitations in radar data utilization.

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [3] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: Proposes Multi-Resolution Model Fusion (MRMF) method to reduce neural network training time by combining models trained on reduced-resolution data and refining with original resolution data, achieving up to 47% training time reduction without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Neural network training is time-consuming, especially with large high-dimensional data. Need efficient methodologies to reduce computational costs while maintaining model performance.

Method: Multi-Resolution Model Fusion (MRMF) that trains models on reduced-resolution data first, then fuses and refines them with original resolution data. Uses progressive convergence strategy across resolution stages.

Result: Reduces training time by up to 47% for CosmoFlow and 44% for Neuron Inverter applications compared to original resolution training, while maintaining same model accuracy.

Conclusion: The multi-resolution model fusion approach effectively reduces neural network training time significantly without compromising model accuracy, making it valuable for scientific applications with large datasets.

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [4] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: This paper proposes ER-Mapping and NI-Balancer to optimize Mixture-of-Experts (MoE) models on wafer-scale chips by balancing communication pressure and hiding expert migration overhead, achieving significant performance improvements over traditional GPU clusters.


<details>
  <summary>Details</summary>
Motivation: MoE models face communication bottlenecks in GPU clusters due to expensive cross-node all-to-all communication. Wafer-scale chips (WSCs) offer high-performance networking but suffer from imbalanced communication pressure in mesh topology and high-overhead expert migration due to lack of on-wafer disk.

Method: Proposes Entwined Ring Mapping (ER-Mapping) to co-design attention and MoE layer mappings for balanced communication pressure, and Non-invasive Balancer (NI-Balancer) to split expert migration into multiple steps using cold links of both layers to hide migration overhead.

Result: ER-Mapping achieves up to 62% communication reduction. NI-Balancer delivers 54% improvement in MoE computation and 22% in communication. WSC platform provides 39% higher per-device MoE performance compared to SOTA NVL72 supernode.

Conclusion: The proposed techniques effectively address communication bottlenecks in MoE models on wafer-scale chips, demonstrating significant performance advantages over traditional GPU clusters through balanced communication mapping and optimized migration strategies.

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [5] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: A multi-stage approach for secure AI training in healthcare that uses simulated clinical knowledge graphs and federated learning to enable model development without accessing sensitive patient data, validated in a student challenge.


<details>
  <summary>Details</summary>
Motivation: To address GDPR restrictions that limit clinical data use for personalized medicine, especially for rare diseases with small cohorts, while maintaining data privacy and enabling predictive AI development.

Method: Four-stage approach: (1) Model design on simulated clinical knowledge graphs, (2) Integration into FeatureCloud federated learning framework in protected environment, (3) Training on real data within hospital environment under hospital supervision, (4) Execution of verified evaluation scripts returning only aggregated performance metrics.

Result: Successfully validated during TUM.ai Makeathon 2024 where 50 students developed patient classification and diagnosis models without access to real data, demonstrating practical implementation of privacy-preserving AI in healthcare.

Conclusion: Deploying secure algorithms via federated frameworks like FeatureCloud provides a practical solution for achieving privacy-preserving AI in healthcare while complying with GDPR regulations.

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [6] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: This chapter provides a classification of data-intensive workloads and surveys scheduling approaches for large-scale distributed systems, addressing challenges like parallelism, data locality, QoS requirements, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The explosive growth of big data has led to more complex and computationally demanding workloads that require effective scheduling techniques to handle challenges like varying parallelism, data locality, QoS requirements, and energy efficiency in large-scale distributed systems.

Method: The authors propose a classification of data-intensive workloads and provide an overview of commonly used scheduling approaches, presenting novel strategies from the literature.

Result: The chapter offers a comprehensive classification framework and survey of scheduling techniques for data-intensive applications in distributed systems.

Conclusion: The work sheds light on open challenges and future directions in scheduling for data-intensive workloads in large-scale distributed computing environments.

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [7] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: This paper studies the gathering problem for mobile agents with possibly non-unique labels, characterizing which teams are gatherable and providing efficient algorithms with minimal common knowledge requirements.


<details>
  <summary>Details</summary>
Motivation: To determine whether all agent labels must be pairwise distinct for deterministic gathering, and to understand the minimal common knowledge needed for efficient gathering algorithms.

Method: Characterizes gatherable teams, designs poly(n,logλ)-time gathering algorithm requiring only O(log log log μ) bits of common knowledge, and proves near-optimality of this dependency.

Result: Full characterization of gatherable teams, efficient gathering algorithm with minimal common knowledge, and first deterministic poly(n,logλ)-time algorithm for distinct labels without common knowledge.

Conclusion: Not all labels need to be distinct for deterministic gathering, and minimal common knowledge (O(log log log μ) bits) suffices for efficient gathering, with near-optimal dependency.

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [8] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming introduces a novel approach using Windowed CRDTs for scalable global aggregations in stream processing, achieving 5x lower latency and 2x higher throughput than existing systems.


<details>
  <summary>Details</summary>
Motivation: Current stream processing systems face scalability bottlenecks with global aggregations due to single-task computation or static aggregation trees, leading to latency issues and poor failure recovery.

Method: Uses deterministic programming model with Windowed Conflict-Free Replicated Data Types (Windowed CRDTs) for shared replicated state, enabling decentralized coordination and efficient failure recovery.

Result: Achieved 5x lower latency and 2x higher throughput compared to existing systems, with 11x latency reduction under failure scenarios.

Conclusion: Demonstrates effectiveness of decentralized coordination with determinism and utility of Windowed CRDTs for scalable global aggregations in stream processing.

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [The influence of the random numbers quality on the results in stochastic simulations and machine learning](https://arxiv.org/abs/2510.25269)
*Benjamin A. Antunes*

Main category: cs.PF

TL;DR: Study investigates how PRNG statistical quality affects computational results in stochastic applications, finding that very poor quality PRNGs cause significant deviations while mid-to-good quality PRNGs perform comparably to top-tier ones once a statistical robustness threshold is met.


<details>
  <summary>Details</summary>
Motivation: To explore the underinvestigated impact of PRNG statistical quality on computational results in stochastic simulations and machine learning applications, where PRNGs are widely used but their quality effects remain unclear.

Method: Evaluated 7 PRNGs ranging from low-quality LCGs to high-quality generators (Mersenne Twister, PCG, Philox) across four tasks: epidemiological ABM, two MNIST classification implementations, and RL CartPole environment. Each experiment repeated 30 times per generator with fixed seeds.

Result: Very poor quality PRNGs (bad LCG failing 125 TestU01 Crush tests) produced significant deviations in epidemic dynamics, reduced MNIST accuracy, and severely degraded RL performance. Mid-to-good quality PRNGs performed comparably to top-tier ones in most tasks, except RL where performance scaled with statistical quality.

Conclusion: Once a generator meets sufficient statistical robustness threshold, its family/design has negligible impact on outcomes for most workloads, allowing selection based on performance and implementation. However, low-quality PRNGs in sensitive stochastic computations can introduce substantial systematic errors.

Abstract: Pseudorandom number generators (PRNGs) are ubiquitous in stochastic
simulations and machine learning (ML), where they drive sampling, parameter
initialization, regularization, and data shuffling. While widely used, the
potential impact of PRNG statistical quality on computational results remains
underexplored. In this study, we investigate whether differences in PRNG
quality, as measured by standard statistical test suites, can influence
outcomes in representative stochastic applications. Seven PRNGs were evaluated,
ranging from low-quality linear congruential generators (LCGs) with known
statistical deficiencies to high-quality generators such as Mersenne Twister,
PCG, and Philox. We applied these PRNGs to four distinct tasks: an
epidemiological agent-based model (ABM), two independent from-scratch MNIST
classification implementations (Python/NumPy and C++), and a reinforcement
learning (RL) CartPole environment. Each experiment was repeated 30 times per
generator using fixed seeds to ensure reproducibility, and outputs were
compared using appropriate statistical analyses. Results show that very poor
statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests,
produces significant deviations in ABM epidemic dynamics, reduces MNIST
classification accuracy, and severely degrades RL performance. In contrast,
mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush
tests-performed comparably to top-tier PRNGs in most tasks, with the RL
experiment being the primary exception where performance scaled with
statistical quality. Our findings indicate that, once a generator meets a
sufficient statistical robustness threshold, its family or design has
negligible impact on outcomes for most workloads, allowing selection to be
guided by performance and implementation considerations. However, the use of
low-quality PRNGs in sensitive stochastic computations can introduce
substantial and systematic errors.

</details>
