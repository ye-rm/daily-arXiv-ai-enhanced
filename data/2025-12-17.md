<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement](https://arxiv.org/abs/2512.14151)
*Songze Liu,Hongkun Du,Shaowen Wang*

Main category: cs.AR

TL;DR: ACPC is an adaptive cache pollution control mechanism using Temporal CNN for access prediction and priority-aware replacement to optimize LLM inference memory efficiency.


<details>
  <summary>Details</summary>
Motivation: LLM inference workloads generate highly irregular and bursty memory access patterns that cause traditional prefetching and replacement policies to mispredict, leading to severe cache pollution and degraded system performance.

Method: Proposes Adaptive Cache Pollution Control (ACPC) integrating Temporal Convolutional Network (TCN)-based access prediction with priority-aware replacement strategy. TCN learns temporal dependencies in token access sequences to identify high-reuse cache lines, while replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy.

Result: ACPC reduces cache pollution by 41.7%, improves cache hit rate by 8.9%, achieves 60.0% reduction in L2 miss penalty, increases token generation throughput by 15.9%, and achieves lowest final loss of 0.21 compared to state-of-the-art ML-based baselines.

Conclusion: ACPC effectively recognizes useful cache lines and mitigates redundant prefetches under dynamic LLM access behaviors, providing a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

Abstract: Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

</details>


### [2] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: ReadyPower is a new analytical power modeling framework that improves upon classical analytical models like McPAT by addressing implementation discrepancies, achieving better accuracy than ML-based models while being more reliable, interpretable, and easier to use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing power modeling approaches: classical analytical models (like McPAT) suffer from significant inaccuracies, while ML-based models have issues with unreliability, limited interpretability, and difficulty in usage, preventing their widespread industry adoption despite better accuracy in research settings.

Method: ReadyPower introduces a new analytical framework that bridges discrepancies between real processor implementations and analytical models by incorporating architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model. Parameters at these three different levels are determined using different methods to improve accuracy.

Result: ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with ML-based baselines, averaged across different training scenarios, on both BOOM and XiangShan CPU architectures.

Conclusion: ReadyPower provides a ready-for-use power modeling framework that is reliable, interpretable, and handy, overcoming the limitations of both classical analytical models and ML-based approaches while achieving superior accuracy.

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [3] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: TEMP framework enables efficient LLM training on wafer-scale chips by optimizing tensor parallelism to leverage high D2D bandwidth while overcoming memory constraints and 2D mesh topology limitations.


<details>
  <summary>Details</summary>
Motivation: Wafer-scale chips offer high computation power and bandwidth for LLM training, but face memory-compute trade-offs due to limited area. Existing tensor parallelism approaches fail to address the unique challenges of WSC's 2D mesh topology and memory constraints.

Method: Proposes TEMP framework with three key components: 1) topology-aware tensor-stream partition (TSPP) to leverage bandwidth for memory relief, 2) traffic-conscious mapping to reduce contention, and 3) dual-level wafer solving to optimize design search.

Result: TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models, demonstrating effective optimization of memory efficiency and computational throughput on wafer-scale chips.

Conclusion: The TEMP framework successfully addresses wafer-scale chip limitations for LLM training by integrating topology-aware partitioning, traffic optimization, and efficient design search, unlocking the full potential of WSC hardware for large language model training.

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [4] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: PADE is a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration that eliminates the need for separate sparsity predictors, achieving significant speedups and energy efficiency improvements over existing solutions.


<details>
  <summary>Details</summary>
Motivation: While sparse attention methods reduce the quadratic cost of self-attention, current approaches suffer from impracticality due to the heavy overhead of added sparsity predictors, which severely impacts hardware efficiency.

Method: PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) to accurately identify trivial tokens; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce I/O and computational complexity, combined with custom accelerator designs.

Result: PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU, and 5.1x, 4.3x, and 3.4x energy savings compared to SOTA accelerators Sanger, DOTA, and SOFA respectively on 22 benchmarks.

Conclusion: PADE enables practical sparsity acceleration without relying on added sparsity predictors, advancing the state-of-the-art in efficient attention mechanisms through algorithm-hardware co-design.

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing](https://arxiv.org/abs/2512.14002)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: Proposed SARound algorithm improves approximation ratio for deadline-constrained task offloading in Vehicular Edge Computing from 1/6 to 1/4, with online framework and comprehensive simulator validation.


<details>
  <summary>Details</summary>
Motivation: Vehicular Edge Computing faces challenges in efficient task offloading for time-critical applications due to constrained bandwidth/computational resources, stringent deadlines, and rapidly changing network conditions.

Method: Formulated Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), proposed SARound approximation algorithm using Linear Program rounding and local-ratio techniques, designed online service subscription and offloading control framework, and developed VecSim simulator using OMNeT++ and Simu5G.

Result: SARound improves best-known approximation ratio for DOAP from 1/6 to 1/4, consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency, validated with object detection applications and real-world taxi trace data.

Conclusion: The proposed SARound algorithm and online framework effectively address deadline-constrained task offloading challenges in VEC, providing improved performance guarantees and practical applicability for real-time vehicular applications.

Abstract: Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\mathbf{P}$, we propose $\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\frac{1}{6}$ to $\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.

</details>


### [6] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: A hybrid ML-based auto-scaling algorithm for edge computing that combines proactive demand prediction with reactive SLA-aware adjustments, reducing SLA violations from 23% to 6% in Kubernetes environments.


<details>
  <summary>Details</summary>
Motivation: Edge computing enables low-latency IoT applications but requires SLA compliance for performance, reliability, and availability. Existing auto-scaling solutions suffer from performance issues and configuration complexity, leading to high SLA violation rates up to 23%.

Method: Proposes a hybrid auto-scaling algorithm combining: 1) ML-based proactive component predicting incoming resource requests, and 2) reactive autoscaler considering current resource utilization and SLA constraints. Implemented as a Kubernetes extension and evaluated with real applications in edge environments.

Result: The proposed solution reduces SLA violation rate from 23% (existing solutions) to only 6%, outperforming baseline approaches and ensuring stable SLA compliance across various applications.

Conclusion: The hybrid ML-based auto-scaling algorithm effectively addresses SLA compliance challenges in edge computing by combining proactive prediction with reactive adjustments, demonstrating significant improvement over existing solutions in Kubernetes-managed edge environments.

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [7] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: Analysis of stability and performance penalties in parallel computing systems with synchronization barriers, focusing on barrier systems in Apache Spark and developing models to quantify overhead from barrier-induced idle periods.


<details>
  <summary>Details</summary>
Motivation: Barriers in parallel systems (like Apache Spark's Barrier Execution Mode) create idle periods on workers, reducing system stability and performance compared to barrier-free workloads. Need to understand and quantify these penalties.

Method: 1) Analyze stability of (s,k,l) barrier systems where jobs depart after l out of k tasks complete. 2) Derive performance bounds for hybrid barrier systems with mixed workloads (with/without barriers, varying parallelism). 3) Compare bounds and simulation results to real Spark system benchmarks. 4) Study real system overhead distribution and attribute it to dual event/polling-driven scheduling mechanism. 5) Develop overhead model and validate through simulation.

Result: The paper provides analytical stability analysis for barrier systems, performance bounds for hybrid systems, and identifies that barrier overhead in real Spark systems stems from the dual event/polling scheduling mechanism. The developed model is validated against real system data through simulation.

Conclusion: Barriers in parallel systems create significant stability and performance penalties due to worker idle periods. The analysis provides tools to quantify these effects, with the overhead model explaining real system behavior and offering insights for optimizing barrier-based parallel computing systems.

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [8] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Bj√∂rkqvist*

Main category: cs.DC

TL;DR: PruneX is a distributed training system that reduces inter-node communication by co-designing pruning algorithms with cluster hierarchy, achieving 60% communication reduction and 6.75x speedup on 64 GPUs.


<details>
  <summary>Details</summary>
Motivation: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters, and conventional pruning-aware systems fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by dense collective primitives.

Method: PruneX introduces Hierarchical Structured ADMM (H-SADMM) algorithm that enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction. The system uses a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects.

Result: Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer.

Conclusion: PruneX successfully co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage in distributed training, demonstrating significant communication reduction and performance improvements over existing approaches.

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>
