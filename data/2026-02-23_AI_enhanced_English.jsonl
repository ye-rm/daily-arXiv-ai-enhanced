{"id": "2602.18072", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18072", "abs": "https://arxiv.org/abs/2602.18072", "authors": ["Gwenevere Frank", "Gopabandhu Hota", "Keli Wang", "Christopher Deng", "Krish Arora", "Diana Vins", "Abhinav Uppal", "Omowuyi Olajide", "Kenneth Yoshimoto", "Qingbo Wang", "Mari Yamaoka", "Johannes Leugering", "Stephen Deiss", "Leif Gibb", "Gert Cauwenberghs"], "title": "HiAER-Spike Software-Hardware Reconfigurable Platform for Event-Driven Neuromorphic Computing at Scale", "comment": "Leif Gibb, Gert Cauwenberghs are equal authors. arXiv admin note: substantial text overlap with arXiv:2504.03671", "summary": "In this work, we present HiAER-Spike, a modular, reconfigurable, event-driven neuromorphic computing platform designed to execute large spiking neural networks with up to 160 million neurons and 40 billion synapses - roughly twice the neurons of a mouse brain at faster than real time. This system, assembled at the UC San Diego Supercomputer Center, comprises a co-designed hard- and software stack that is optimized for run-time massively parallel processing and hierarchical address-event routing (HiAER) of spikes while promoting memory-efficient network storage and execution. The architecture efficiently handles both sparse connectivity and sparse activity for robust and low-latency event-driven inference for both edge and cloud computing. A Python programming interface to HiAER-Spike, agnostic to hardware-level detail, shields the user from complexity in the configuration and execution of general spiking neural networks with minimal constraints in topology. The system is made easily available over a web portal for use by the wider community. In the following, we provide an overview of the hard- and software stack, explain the underlying design principles, demonstrate some of the system's capabilities and solicit feedback from the broader neuromorphic community. Examples are shown demonstrating HiAER-Spike's capabilities for event-driven vision on benchmark CIFAR-10, DVS event-based gesture, MNIST, and Pong tasks.", "AI": {"tldr": "HiAER-Spike is a neuromorphic computing platform capable of running large spiking neural networks with 160M neurons and 40B synapses at faster than real-time speeds, featuring a modular hardware-software co-design optimized for sparse connectivity and activity.", "motivation": "To create a scalable neuromorphic computing platform that can execute large-scale spiking neural networks comparable to biological brains (mouse brain scale) with efficient event-driven processing for both edge and cloud applications, while providing accessible tools for the research community.", "method": "Developed a modular, reconfigurable hardware-software co-designed system with hierarchical address-event routing (HiAER) for spike communication, optimized for massively parallel processing and memory-efficient network storage. Includes a Python programming interface that abstracts hardware complexity and a web portal for community access.", "result": "Successfully demonstrated capabilities on benchmark tasks including CIFAR-10, DVS event-based gesture recognition, MNIST, and Pong, showing robust low-latency event-driven inference. The system achieves processing speeds faster than real-time for networks with up to 160 million neurons and 40 billion synapses.", "conclusion": "HiAER-Spike represents a significant advancement in neuromorphic computing, providing a scalable platform for large-scale spiking neural networks with efficient event-driven processing, accessible programming interface, and community availability through a web portal."}}
{"id": "2602.18158", "categories": ["cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.18158", "abs": "https://arxiv.org/abs/2602.18158", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2026.108414", "summary": "A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.", "AI": {"tldr": "An exact multi-objective task allocation framework for edge-hub-cloud workflow applications that jointly optimizes reliability and latency using binary integer linear programming with time redundancy techniques.", "motivation": "Critical workflow applications use streamlined edge-hub-cloud architecture, but task allocation is challenging due to device limitations and diverse operating conditions. Existing methods don't holistically address reliability-latency trade-offs for these critical applications.", "method": "Proposes an exact multi-objective task allocation framework using comprehensive binary integer linear programming formulation that considers relative importance of objectives, incorporates time redundancy techniques, and accounts for crucial constraints often overlooked.", "result": "In real-world application: 84.19% average improvement in reliability and 49.81% in latency over baseline strategies. Scalable across diverse workflows with runtimes averaging 0.03 to 50.94 seconds.", "conclusion": "The framework effectively optimizes reliability and latency for edge-hub-cloud workflow applications, demonstrating practicality and scalability across diverse workflow types and system conditions."}}
{"id": "2602.18140", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.18140", "abs": "https://arxiv.org/abs/2602.18140", "authors": ["Mohammad Farahani", "Mohammad Rasoul Roshanshah", "Saeed Safari"], "title": "Flexi-NeurA: A Configurable Neuromorphic Accelerator with Adaptive Bit-Precision Exploration for Edge SNNs", "comment": null, "summary": "Neuromorphic accelerators promise unparalleled energy efficiency and computational density for spiking neural networks (SNNs), especially in edge intelligence applications. However, most existing platforms exhibit rigid architectures with limited configurability, restricting their adaptability to heterogeneous workloads and diverse design objectives. To address these limitations, we present Flexi-NeurA -- a parameterizable neuromorphic accelerator (core) that unifies configurability, flexibility, and efficiency. Flexi-NeurA allows users to customize neuron models, network structures, and precision settings at design time. By pairing these design-time configurability and flexibility features with a time-multiplexed and event-driven processing approach, Flexi-NeurA substantially reduces the required hardware resources and total power while preserving high efficiency and low inference latency. Complementing this, we introduce Flex-plorer, a heuristic-guided design-space exploration (DSE) tool that determines cost-effective fixed-point precisions for critical parameters -- such as decay factors, synaptic weights, and membrane potentials -- based on user-defined trade-offs between accuracy and resource usage. Based on the configuration selected through the Flex-plorer process, RTL code is configured to match the specified design. Comprehensive evaluations across MNIST, SHD, and DVS benchmarks demonstrate that the Flexi-NeurA and Flex-plorer co-framework achieves substantial improvements in accuracy, latency, and energy efficiency. A three-layer 256--128--10 fully connected network with LIF neurons mapped onto two processing cores achieves 97.23% accuracy on MNIST with 1.1~ms inference latency, utilizing only 1,623 logic cells, 7 BRAMs, and 111~mW of total power -- establishing Flexi-NeurA as a scalable, edge-ready neuromorphic platform.", "AI": {"tldr": "Flexi-NeurA is a configurable neuromorphic accelerator core with Flex-plorer DSE tool that enables customizable SNN designs with optimized fixed-point precision, achieving high efficiency for edge applications.", "motivation": "Existing neuromorphic accelerators have rigid architectures with limited configurability, restricting adaptability to heterogeneous workloads and diverse design objectives for edge intelligence applications.", "method": "Flexi-NeurA provides parameterizable neuron models, network structures, and precision settings at design time with time-multiplexed event-driven processing. Flex-plorer is a heuristic-guided DSE tool that determines cost-effective fixed-point precisions for critical parameters based on accuracy-resource trade-offs.", "result": "A three-layer 256-128-10 fully connected LIF network achieves 97.23% accuracy on MNIST with 1.1ms latency, using only 1,623 logic cells, 7 BRAMs, and 111mW total power across MNIST, SHD, and DVS benchmarks.", "conclusion": "Flexi-NeurA with Flex-plorer co-framework establishes a scalable, edge-ready neuromorphic platform with substantial improvements in accuracy, latency, and energy efficiency through unified configurability, flexibility, and efficiency."}}
{"id": "2602.17678", "categories": ["cs.DC", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17678", "abs": "https://arxiv.org/abs/2602.17678", "authors": ["Oreofe Solarin"], "title": "It's Not Just Timestamps: A Study on Docker Reproducibility", "comment": null, "summary": "Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.", "AI": {"tldr": "Only 2.7% of Docker builds are bitwise reproducible without configuration changes; after adjustments, 78.7% remain non-reproducible due to developer choices like uncleaned caches and floating versions.", "motivation": "To assess the real-world reproducibility of container builds by testing the promise that rebuilding from Dockerfiles should produce identical hashes for software supply chain integrity.", "method": "Built a Docker measurement pipeline and applied it to a stratified sample of 2,000 GitHub repositories containing Dockerfiles, analyzing build success rates and bitwise reproducibility with and without infrastructure configuration changes.", "result": "Only 56% of Dockerfiles produced buildable images; just 2.7% were bitwise reproducible without configuration changes. After modifying infrastructure configurations, reproducibility improved by 18.6%, but 78.7% of buildable Dockerfiles remained non-reproducible due to timestamps, metadata, uncleaned caches, logs, documentation, and floating versions.", "conclusion": "Developer-controlled factors are the dominant causes of non-reproducibility, and concrete Dockerfile guidelines derived from these patterns can inform future linters and CI checks for reproducible containers."}}
{"id": "2602.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17774", "abs": "https://arxiv.org/abs/2602.17774", "authors": ["Wael Al-Manasrah", "Zuhair AlSader", "Tim Brecht", "Ahmed Alquraan", "Samer Al-Kiswany"], "title": "Message-Oriented Middleware Systems: Technology Overview", "comment": null, "summary": "We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.\n  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.", "AI": {"tldr": "Comprehensive analysis of 10 open-source message-oriented middleware systems examining 42 features with 134 options, finding they provide frameworks for modern cloud apps with high flexibility and identifying opportunity for community consolidation.", "motivation": "To systematically characterize and compare open-source message-oriented middleware (MOM) systems to understand their features, evolution, and suitability for modern cloud applications, helping practitioners make informed choices.", "method": "Rigorous methodology to select and study ten popular and diverse MOM systems, examining 42 features with 134 different options for each system, creating an annotated dataset for verification and comparison.", "result": "MOM systems have evolved to provide frameworks for modern cloud applications with high flexibility and configurability, offering core building blocks including transaction support, active messaging, resource management, flow control, and native multi-tenancy support.", "conclusion": "There is an opportunity for the community to consolidate efforts on fewer open-source projects, and the publicly available annotated dataset helps practitioners understand and compare different systems' features."}}
{"id": "2602.17808", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.17808", "abs": "https://arxiv.org/abs/2602.17808", "authors": ["Nathan Ng", "Walid A. Hanafy", "Prashanthi Kadambi", "Balachandra Sunil", "Ayush Gupta", "David Irwin", "Yogesh Simmhan", "Prashant Shenoy"], "title": "Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs", "comment": null, "summary": "IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.\n  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.", "AI": {"tldr": "SwapLess is a system for adaptive TPU-CPU collaborative inference that reduces latency by dynamically adjusting model partitioning and CPU allocation based on workload conditions.", "motivation": "IoT applications use on-device AI accelerators for performance, but limited on-chip memory forces model segment swapping, increasing latency. Naive CPU-accelerator partitioning can worsen latency by shifting too much computation to CPU or insufficiently reducing swapping, especially in multi-tenant dynamic environments.", "method": "SwapLess uses an analytic queueing model that captures partition-dependent CPU/TPU service times and swapping overheads across different workload mixes and request rates. It continuously adjusts both partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead.", "result": "Implementation on Edge TPU-equipped platforms shows SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads compared to default Edge TPU compiler.", "conclusion": "SwapLess effectively addresses memory-constrained Edge TPU inference by adaptively optimizing CPU-TPU collaboration, significantly reducing latency for both single- and multi-tenant workloads through intelligent online partitioning and resource allocation."}}
{"id": "2602.17811", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.17811", "abs": "https://arxiv.org/abs/2602.17811", "authors": ["Guy Blelloch", "Andrew Brady", "Laxman Dhulipala", "Jeremy Fineman", "Kishen Gowda", "Chase Hutton"], "title": "Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation", "comment": "57 pages", "summary": "A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.\n  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.\n  Our second result is a $O(c \\log n)$ orientation algorithm with expected worst-case $O(\\sqrt{\\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \\log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.\n  Our final result is a $O(c + \\log n)$-orientation algorithm with $O(\\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\\log^9 n)$ worst-case work per edge whp.", "AI": {"tldr": "Faster parallel batch-dynamic algorithms for maintaining low out-degree orientations in undirected graphs with polylogarithmic depth and improved work bounds.", "motivation": "Existing parallel batch-dynamic algorithms for maintaining low out-degree orientations have suboptimal work bounds. The paper aims to develop faster algorithms that minimize work while maintaining polylogarithmic depth for processing batches of edge insertions/deletions in parallel.", "method": "Develops three parallel batch-dynamic algorithms: 1) First algorithm with asymptotically optimal orientation and optimal expected work bounds (amortized), 2) O(c log n)-orientation algorithm with O(\u221alog n) expected worst-case work per edge, 3) O(c + log n)-orientation algorithm with O(log\u00b2 n) expected worst-case work per edge.", "result": "1) Logarithmic factor improvement over prior best work bounds (Liu et al.), 2) Matches best-known sequential worst-case O(c log n) orientation algorithm (in expectation), 3) Significant improvement over recent result (Ghaffari and Koo) reducing work from O(log\u2079 n) to O(log\u00b2 n) while maintaining similar orientation quality.", "conclusion": "The paper presents substantial improvements in parallel batch-dynamic algorithms for low out-degree orientations, achieving better work bounds while maintaining polylogarithmic depth, with applications in dynamic graph processing and parallel computing."}}
{"id": "2602.17817", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17817", "abs": "https://arxiv.org/abs/2602.17817", "authors": ["Ehsan Yousefzadeh-Asl-Miandoab", "Reza Karimzadeh", "Danyal Yorulmaz", "Bulat Ibragimov", "P\u0131nar T\u00f6z\u00fcn"], "title": "GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations", "comment": null, "summary": "Collocating deep learning training tasks improves GPU utilization but causes drastic slowdowns due to resource contention and risks Out-of-Memory (OOM) failures. Accurate memory estimation is essential for robust collocation, while GPU utilization -- a key proxy for resource contention -- enables interference-aware scheduling to reduce slowdowns and improve throughput. Existing GPU memory estimators span three paradigms -- analytical models, CPU-side libraries, and ML-based estimators -- each with distinct limitations: dependence on detailed model specifications, intrusive integration, poor generalization, and varying latency overhead. GPU heterogeneity further complicates estimation, as identical tasks can exhibit markedly different memory footprints across hardware generations. GPU utilization remains comparatively understudied, further complicated by the non-additive nature of utilization metrics and hardware sensitivity. We conduct a systematic analysis of representative estimators from each paradigm -- Horus, PyTorch FakeTensor, and our lightweight ML-based estimator -- evaluating accuracy, generalizability, and practical overhead. We construct a synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and train MLP- and Transformer-based estimators for memory prediction. We further experiment with utilization estimation on the same dataset. Our evaluation reveals key tradeoffs and validates estimators against real-world unseen models. Significant challenges remain: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. We release all datasets, tools, and artifacts to support further research.", "AI": {"tldr": "Systematic analysis of GPU memory estimation methods for deep learning collocation, comparing three paradigms and evaluating their accuracy, generalizability, and practical overhead.", "motivation": "Collocating deep learning training tasks improves GPU utilization but causes slowdowns due to resource contention and OOM failures. Accurate memory estimation is essential for robust collocation, and GPU utilization metrics enable interference-aware scheduling to reduce slowdowns and improve throughput.", "method": "Conducted systematic analysis of three GPU memory estimation paradigms: analytical models (Horus), CPU-side libraries (PyTorch FakeTensor), and lightweight ML-based estimators. Constructed synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and trained MLP- and Transformer-based estimators for memory prediction. Also experimented with utilization estimation on the same dataset.", "result": "Evaluation revealed key tradeoffs: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. Validated estimators against real-world unseen models and identified significant challenges in current approaches.", "conclusion": "Accurate GPU memory estimation remains challenging due to hardware heterogeneity and architectural variations. The study provides datasets, tools, and artifacts to support further research in this critical area for efficient deep learning task collocation."}}
{"id": "2602.17834", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17834", "abs": "https://arxiv.org/abs/2602.17834", "authors": ["Duncan Adamson", "Will Rosenbaum", "Paul G. Spirakis"], "title": "Distributed Triangle Enumeration in Hypergraphs", "comment": null, "summary": "In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.", "AI": {"tldr": "Systematic study of distributed sub-hypergraph enumeration in hypergraphs, introducing computational models, algorithms for triangle enumeration, sparse hypergraph classes, and general techniques.", "motivation": "Subgraph detection and enumeration have become central in distributed graph algorithms due to theoretical challenges and practical applications. The paper aims to extend this to hypergraphs by initiating systematic study of distributed sub-hypergraph enumeration.", "method": "1) Introduce computational models for hypergraphs generalizing CONGEST model; 2) Devise algorithms for distributed triangle enumeration and prove optimality in two models; 3) Introduce classes of sparse and \"everywhere sparse\" hypergraphs with efficient algorithms; 4) Describe general techniques for designing efficient algorithms in hypergraph models.", "result": "The paper establishes foundational computational models for distributed hypergraph algorithms, provides optimal triangle enumeration algorithms in certain models, and develops efficient algorithms for sparse hypergraph classes.", "conclusion": "This work initiates systematic study of distributed sub-hypergraph enumeration, providing computational models, algorithms, and techniques that lay groundwork for future research in distributed hypergraph algorithms."}}
{"id": "2602.18007", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18007", "abs": "https://arxiv.org/abs/2602.18007", "authors": ["Jon Hu", "Thomas Jia", "Jing Zhu", "Zhendong Yu"], "title": "Joint Training on AMD and NVIDIA GPUs", "comment": null, "summary": "As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.", "AI": {"tldr": "A system enabling efficient training of large language models across heterogeneous AMD-NVIDIA GPU clusters, achieving near-native NVIDIA performance through direct cross-vendor GPU communication.", "motivation": "As LLMs scale, training demands outgrow single-vendor homogeneous clusters, requiring solutions for heterogeneous environments to leverage diverse hardware resources efficiently.", "method": "Two approaches: 1) Compatibility-oriented CPU-Forwarding Communication with differentiated back-end selection and multi-NIC parallel transfer; 2) Higher-performance Device-Direct Communication using CPU-offloading P2P mechanism for direct cross-vendor GPU data transfer without host-memory staging.", "result": "Device-Direct Communication achieves up to 98% of the throughput of NVIDIA homogeneous systems when training LLaMA-8B and Qwen2-7B models, while maintaining training stability and correctness.", "conclusion": "The proposed heterogeneous training solution enables efficient LLM training across AMD-NVIDIA environments with near-native performance, addressing the limitations of single-vendor homogeneous clusters for large-scale model training."}}
{"id": "2602.18188", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18188", "abs": "https://arxiv.org/abs/2602.18188", "authors": ["Antonio Cruciani", "Avinandan Das", "Alesya Raevskaya", "Jukka Suomela"], "title": "It does not matter how you define locally checkable labelings", "comment": "40 pages", "summary": "Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.\n  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the \"node-edge checkable\" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\\log^* n)$ rounds in the LOCAL model).", "AI": {"tldr": "The paper shows that locally checkable labeling (LCL) problems are robust to variations, specifically demonstrating that the \"node-edge checkable\" formalism (restricted to regular unlabeled graphs) can be translated to and from standard LCL problems with minimal overhead.", "motivation": "To demonstrate the robustness and flexibility of the LCL framework by showing that even highly restricted variants (node-edge checkable problems on regular unlabeled graphs) are essentially equivalent to standard LCL problems in terms of distributed complexity.", "method": "The authors present local reductions in both directions between standard LCL problems and the restricted node-edge checkable formalism, using symmetry-breaking oracles. The reductions incur only an additive O(log* n) rounds overhead in the LOCAL model.", "result": "The paper proves that the family of LCL problems is extremely robust to variations, showing that one can translate between the two formalisms with minimal overhead (at most additive O(log* n) rounds).", "conclusion": "The LCL framework is highly robust and flexible, maintaining its theoretical properties even when restricted to simpler formalisms like node-edge checkable problems on regular unlabeled graphs, which cannot directly reference complex graph properties like short cycles."}}
{"id": "2602.18287", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18287", "abs": "https://arxiv.org/abs/2602.18287", "authors": ["Andrea D'Iapico", "Monica Vitali"], "title": "Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum", "comment": null, "summary": "The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.", "AI": {"tldr": "An approach for generating energy-efficient deployment plans for cloud-native applications across cloud-edge continuum using automatically learned green constraints based on energy consumption patterns, communication, and infrastructure environmental characteristics.", "motivation": "Addressing the environmental sustainability of IT by reducing energy consumption and greenhouse gas emissions in cloud-native applications deployed across cloud-edge continuum, where identifying energy-efficient deployment strategies is challenging due to dynamic application behavior and varying carbon intensity of infrastructure nodes.", "method": "Proposes a methodology and architecture for generating green-aware constraints derived from continuous analysis of energy consumption patterns, inter-component communication, and environmental characteristics of infrastructure nodes. These constraints inform schedulers to produce environmentally friendly deployment plans, with automatic learning and updating over time using monitoring data.", "result": "Validated through realistic deployment scenarios of a cloud-native application, demonstrating effectiveness in reducing energy usage and associated emissions through adaptive, energy-aware orchestration.", "conclusion": "The approach enables automatic generation of green deployment plans that adapt to dynamic conditions, contributing to environmental sustainability of IT infrastructure while maintaining application performance requirements."}}
