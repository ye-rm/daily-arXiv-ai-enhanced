<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [NecoFuzz: Effective Fuzzing of Nested Virtualization via Fuzz-Harness Virtual Machines](https://arxiv.org/abs/2512.08858)
*Reima Ishii,Takaaki Fukai,Takahiro Shinagawa*

Main category: cs.OS

TL;DR: NecoFuzz is the first fuzzing framework targeting nested virtualization-specific logic in hypervisors, generating boundary-state VMs guided by hardware virtualization specs to find vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Nested virtualization is widely used in cloud platforms but increases hypervisor complexity and attack surface. Existing hypervisor fuzzing approaches don't address nested virtualization due to challenges in generating effective VM instances with vast state spaces.

Method: NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by approximate models of hardware-assisted virtualization specifications (Intel VT-x and AMD-V). It extends AFL++ to support these fuzz-harness VMs.

Result: Achieved 84.7% code coverage for nested virtualization-specific code on Intel VT-x and 74.2% on AMD-V. Discovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.

Conclusion: NecoFuzz successfully addresses the challenge of fuzzing nested virtualization by generating boundary-state VMs guided by hardware specifications, significantly improving coverage of security-critical code and uncovering important vulnerabilities.

Abstract: Nested virtualization is now widely supported by major cloud vendors, allowing users to leverage virtualization-based technologies in the cloud. However, supporting nested virtualization significantly increases host hypervisor complexity and introduces a new attack surface in cloud platforms. While many prior studies have explored hypervisor fuzzing, none has explicitly addressed nested virtualization due to the challenge of generating effective virtual machine (VM) instances with a vast state space as fuzzing inputs.
  We present NecoFuzz, the first fuzzing framework that systematically targets nested virtualization-specific logic in hypervisors. NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by an approximate model of hardware-assisted virtualization specifications. Since vulnerabilities in nested virtualization often stem from incorrect handling of unexpected VM states, this specification-guided, boundary-oriented generation significantly improves coverage of security-critical code across different hypervisors.
  We implemented NecoFuzz on Intel VT-x and AMD-V by extending AFL++ to support fuzz-harness VMs. NecoFuzz achieved 84.7% and 74.2% code coverage for nested virtualization-specific code on Intel VT-x and AMD-V, respectively, and uncovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: A toolchain and performance model for predicting MPI communication benefits from CXL.mem shared memory technology, enabling targeted optimizations for HPC applications.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous memory technologies like CXL.mem enable shared memory pooling across multiple nodes, offering potential for more efficient inter-node communication in HPC systems compared to traditional MPI messaging.

Method: Extended performance evaluation toolchain combining memory trace sampling (using extended Mitos tool) with an extended performance model that analyzes MPI data access patterns at per-call granularity, examining both on-node MPI buffer accesses and cross-node traffic.

Result: Validated the models on two applications (2D heat transfer miniapp and HPCG benchmark), demonstrating support for targeted optimizations by identifying specific MPI calls with highest potential for speedup using CXL.mem.

Conclusion: The approach enables prediction of performance benefits from CXL.mem for MPI data exchange, allowing identification and optimization of specific MPI calls that would benefit most from direct CXL.mem implementations.

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [3] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: Chopper is a profiling framework that provides multi-granularity analysis of LLM training on AMD MI300X GPUs, revealing frequency overhead as the largest performance bottleneck in FSDP training.


<details>
  <summary>Details</summary>
Motivation: There's a lack of understanding about how modern GPU systems behave under real-world distributed LLM training workloads, particularly the complex interactions between communication, computation, memory behavior, and power management in multi-GPU setups.

Method: Developed Chopper framework to collect, align, and visualize GPU kernel traces and hardware performance counters across multiple granularities (kernels, operations, layers, phases, iterations, GPUs). Applied it to characterize Llama 3 8B training under FSDP on 8-GPU AMD MI300X nodes.

Result: Revealed memory determinism enables higher, more stable GPU and memory frequencies. Identified frequency overhead (DVFS effects) as the single largest contributor to performance gap, exceeding MFMA utilization loss, communication/computation overlap, and kernel launch overheads.

Conclusion: Chopper provides the first holistic characterization of LLM training on AMD MI300X GPUs, offering actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [4] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) is a POSIX-compliant filesystem with multi-credential functionality using DataCapsule storage, designed for edge computing with three-layer architecture and demonstrated functional correctness.


<details>
  <summary>Details</summary>
Motivation: To create a filesystem that integrates multi-credential functionality within a POSIX-compliant framework for edge computing environments, addressing the need for secure, credential-aware data access in distributed systems.

Method: Three-component architecture: 1) DataCapsule server for storage and replication on edge nodes, 2) middleware in Trusted Execution Environment for permission management, and 3) POSIX-compliant client filesystem adaptable to various architectures.

Result: CFS successfully provides multi-credential Common Access API with high functional correctness, though read/write performance is modest, making it viable for real-world software development applications.

Conclusion: CFS demonstrates feasibility as a multi-credential POSIX filesystem for edge computing with promising functional correctness, though performance improvements are needed for broader practical adoption.

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [5] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES is a framework for hyper-distributed applications across cloud-edge continuum that addresses scalability, interoperability, and trust limitations through resource monetization, orchestration, and semantic interoperability.


<details>
  <summary>Details</summary>
Motivation: Industry demands for hyper-distributed applications spanning cloud to edge in domains like smart manufacturing, transportation, and agriculture are growing, but current solutions struggle with scalability, interoperability, and trust limitations.

Method: HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) establishes an open, seamless, and secure environment where cloud-to-edge resources are intelligently orchestrated, data/services are monetized in a distributed marketplace, and knowledge is shared through semantic interoperability.

Result: HERMES bridges key facets of resource orchestration, monetization, and semantic interoperability to create a foundation for next-generation distributed applications.

Conclusion: HERMES lays the foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous across the computing continuum.

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [6] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: High-performance emulation of single/double-precision complex matrix multiplication on INT8 hardware using Ozaki-II scheme, achieving 4-6.5x speedups over native cuBLAS routines.


<details>
  <summary>Details</summary>
Motivation: Modern architectures have low-precision matrix units with higher throughput than high-precision ones, creating interest in emulating high-precision operations on low-precision hardware for better performance and power efficiency.

Method: Extends Ozaki-II scheme framework to develop high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines.

Result: On NVIDIA B200 GPU, achieves 4.0x-5.6x speedups for single-precision and 4.4x-6.5x for double-precision complex matrix multiplication over native cuBLAS routines for large problem sizes. Methods can trade accuracy for speed or vice versa.

Conclusion: Proposed approach has potential to serve as default algorithm across wide range of applications due to significant speedups and flexible accuracy-performance trade-offs.

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [7] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: Magneton is a differential energy debugging tool that identifies software energy inefficiencies in ML systems by comparing energy consumption between similar implementations at the operator level.


<details>
  <summary>Details</summary>
Motivation: ML model training and deployment are highly energy-intensive, with existing optimization focusing mainly on hardware efficiency while overlooking significant software energy waste from poor design choices, redundant operations, and inefficient implementations in ML frameworks.

Method: Differential energy debugging approach that compares energy consumption between similar ML systems at the operator level. Magneton automatically pinpoints code regions and configuration choices responsible for excessive energy use by analyzing competing implementations of similar functionality.

Result: Applied to 9 popular ML systems across LLM inference, general ML frameworks, and image generation. Detected 16 known cases of software energy inefficiency and discovered 8 previously unknown cases, with 7 confirmed by developers.

Conclusion: Software energy waste is a significant but overlooked problem in ML systems, and differential energy debugging with Magneton provides an effective approach to detect and diagnose these inefficiencies, helping developers optimize energy consumption.

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [8] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: The paper analyzes mutex/lock implementations for lightweight threads (coroutines), showing traditional OS thread locks cause deadlocks, and proposes modified TTAS and MCS locks plus a cohort lock that balances both approaches.


<details>
  <summary>Details</summary>
Motivation: Lightweight threads (coroutines) have lower overhead than OS threads but require manual context switches, making traditional mutex implementations ineffective and potentially causing deadlocks. Different lightweight thread libraries also need specialized lock implementations.

Method: The authors modify TTAS (Test-Test-And-Set) and MCS (Mellor-Crummey and Scott) locks for lightweight threads, focusing on two context switch mechanisms: yielding and sleeping. They analyze performance differences and propose a cohort lock that combines MCS queues with a common TTAS lock.

Result: Traditional OS thread locks cause deadlocks with lightweight threads. Modified TTAS and MCS locks work but perform differently based on settings. The cohort lock provides a balanced solution that works well across different lightweight thread libraries.

Conclusion: Lightweight threads require specialized mutex implementations. The cohort lock, combining MCS queues with TTAS, offers the best general-purpose solution for different lightweight thread libraries by balancing performance characteristics.

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [9] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: Model-based testing approach for distributed actor systems that generates exhaustive test suites from finite-state automaton models without modifying code or interfering with execution environment.


<details>
  <summary>Details</summary>
Motivation: There's a gap between formally verified specifications and actual implementations of distributed systems - model checking verifies specifications but doesn't guarantee bug-free implementations.

Method: Model-based testing using finite-state automaton models to generate exhaustive test suites covering all states and transitions, specifically for distributed systems in the actor model without code modifications or runtime interference.

Result: Successfully verified an implementation of a replication algorithm based on Viewstamped Replication used in a real-world system.

Conclusion: The approach effectively bridges the gap between formal specifications and implementations by providing exhaustive testing coverage for distributed actor systems without intrusive modifications.

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [10] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: Spatial and temporal workload shifting in cloud computing can reduce carbon, water, and land-use footprints by 20-85%, with spatial shifting being most effective and temporal shifting providing additional incremental benefits.


<details>
  <summary>Details</summary>
Motivation: To investigate how spatial and temporal shifting of cloud workloads can reduce environmental footprints (carbon, water, land use) in cloud computing infrastructure.

Method: Simulation study using real-world data from AWS and Azure cloud providers, analyzing workload traces for big data analytics and FaaS applications, with sensitivity analysis for prediction errors and seasonal variations.

Result: Spatial shifting reduces footprints by 20-85% depending on scenario; temporal shifting provides smaller reductions; combined strategies yield greatest overall reduction, mainly driven by spatial shifting with temporal adjustments adding incremental benefits.

Conclusion: Workload shifting strategies, particularly spatial shifting combined with temporal adjustments, offer substantial environmental footprint reductions in cloud computing that are robust to prediction errors and seasonal variations.

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: NysX is an FPGA accelerator for Nyström-based Hyperdimensional Computing graph classification that achieves significant speedup and energy efficiency gains while improving accuracy through optimized landmark selection and hardware design.


<details>
  <summary>Details</summary>
Motivation: Real-time, energy-efficient graph classification on edge devices is needed for various applications. While Hyperdimensional Computing (HDC) is suitable for edge platforms, existing Nyström-based HDC methods face challenges including redundancy in landmark sampling, memory constraints, expensive codebook lookups, and load imbalance in sparse matrix operations.

Method: NysX integrates four key optimizations: 1) hybrid landmark selection combining uniform sampling with determinantal point processes to reduce redundancy, 2) streaming architecture for Nyström projection matrix maximizing memory bandwidth, 3) minimal-perfect-hash lookup engine for O(1) key-to-index mapping, and 4) sparsity-aware SpMV engines with static load balancing.

Result: Implemented on AMD Zynq UltraScale+ FPGA, NysX achieves 6.85× speedup and 169× energy efficiency over CPU baselines, and 4.32× speedup with 314× energy efficiency over GPU baselines, while improving classification accuracy by 3.4% on average across TUDataset benchmarks.

Conclusion: NysX demonstrates that optimized hardware acceleration can enable real-time, energy-efficient graph classification on resource-constrained edge platforms while simultaneously improving accuracy through better landmark selection and efficient hardware design.

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Multi-domain performance analysis with scores tailored to user preferences](https://arxiv.org/abs/2512.08715)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: The paper shows that averaging performance across domains requires careful consideration of weights and user preferences, revealing four domain types (easiest, most difficult, preponderant, bottleneck) and provides visualization tools for binary classification.


<details>
  <summary>Details</summary>
Motivation: Algorithm performance depends heavily on domain-specific distributions, making it crucial to properly analyze what happens when averaging performance across multiple domains to understand domain characteristics and their impact on overall evaluation.

Method: Adopts a probabilistic framework treating performance as probability measures, analyzes weighted means as summarizations, identifies scores where summarized performance equals weighted arithmetic mean, and defines four domain types based on user preferences.

Result: Shows that only specific scores (including ranking scores parameterized by user preferences) allow summarized performance to equal weighted arithmetic mean, with weights depending on user preferences, leading to rigorous definitions of four domain types.

Conclusion: Provides theoretical framework for analyzing performance averaging across domains, introduces new domain classifications based on user preferences, and develops visual tools specifically for two-class classification tasks.

Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [13] [Resonant and Stochastic Vibration in Neurorehabilitation](https://arxiv.org/abs/2512.08009)
*Ava Hays,Nolan Kosnic,Ryan Miller,Kunal Siddhawar*

Main category: cs.ET

TL;DR: Survey of vibration-based interventions (stochastic and resonant) for neurorehabilitation, examining mechanisms, clinical applications in aging, stroke, Parkinson's, and discussing challenges in optimization, safety, and translation to clinical practice.


<details>
  <summary>Details</summary>
Motivation: Neurological injuries and age-related decline impair sensory processing and motor coordination; vibration interventions offer potential to stimulate sensory pathways and motor circuits for functional recovery through neuroplasticity mechanisms.

Method: Survey review methodology synthesizing evidence on two main vibration modalities: whole-body vibration (for balance, mobility, fine motor function) and focused muscle vibration/wearable stochastic resonance devices (for upper-limb rehabilitation).

Result: Identifies therapeutic applications in aging adults, stroke survivors, and Parkinson's patients; highlights challenges in parameter optimization, generalizability, safety, scalability, ecological validity, and standardization across modalities.

Conclusion: Outlines key research needs for translating vibration-based interventions into reliable clinical tools, emphasizing protocol refinement, usability improvement, and integration into broader neurorehabilitation frameworks.

Abstract: Neurological injuries and age-related decline can impair sensory processing and disrupt motor coordination, gait, and balance. As mechanisms of neuroplasticity have become better understood, vibration-based interventions have gained attention as potential tools to stimulate sensory pathways and motor circuits to support functional recovery. This survey reviews stochastic and resonant vibration modalities, describing their mechanisms, therapeutic rationales, and clinical applications. We synthesize evidence on whole-body vibration for improving balance, mobility, and fine motor function in aging adults, stroke survivors, and individuals with Parkinson's disease, with attention to challenges in parameter optimization, generalizability, and safety. We also assess recent developments in focused muscle vibration and wearable stochastic resonance devices for upper-limb rehabilitation, evaluating their clinical promise along with limitations in scalability, ecological validity, and standardization. Across these modalities, we identify key variables that shape therapeutic outcomes and highlight ongoing efforts to refine protocols, improve usability, and integrate vibration techniques into broader neurorehabilitation frameworks. We conclude by outlining the most important research needs for translating vibration-based interventions into reliable and deployable clinical tools.

</details>
