{"id": "2512.20073", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20073", "abs": "https://arxiv.org/abs/2512.20073", "authors": ["Hongyang Shang", "Shuai Dong", "Ye Ke", "Arindam Basu"], "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras", "comment": null, "summary": "This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.", "AI": {"tldr": "3D Stack In-Sensor-Computing architecture for event-based vision processing using DRAM leakage for timestamp normalization, achieving significant power/area reduction while maintaining accuracy in CV tasks.", "motivation": "To overcome the memory wall problem in event-based vision processing by integrating sensing, memory, and computation in a 3D stacked architecture for real-time, resource-efficient processing.", "method": "Proposes 3DS-ISC architecture with real-time normalization using exponential decay function for time-surface construction. Uses DRAM leakage characterization for timestamp normalization, custom MOMCAP for charge storage, and low leakage switch to extend storage time.", "result": "Achieves 69x power reduction, 2.2x latency reduction, and 1.9x area reduction compared to 2D counterpart. Three orders of magnitude power reduction vs 16-bit SRAM timestamp storage. Comparable accuracy in CV tasks: 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, 97% on DVS128 Gesture, and highest SSIM (0.62) for image reconstruction.", "conclusion": "The 3D-ISC architecture enables efficient event-based vision processing with significant resource savings while maintaining competitive accuracy, establishing foundation for real-time applications and future integration of advanced computational circuits."}}
{"id": "2512.20198", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.20198", "abs": "https://arxiv.org/abs/2512.20198", "authors": ["Huizheng Wang", "Taiquan Wei", "Hongbin Wang", "Zichuan Wang", "Xinru Tang", "Zhiheng Yue", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling", "comment": "Accepted for publication in IEEE Transactions on Computers", "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.", "AI": {"tldr": "STAR is a cross-stage algorithm-hardware co-design for efficient Transformer inference under large-scale token parallelism, achieving significant speedup and energy efficiency improvements over existing accelerators.", "motivation": "Existing dynamic sparsity accelerators fail under large-scale token parallelism scenarios due to stage-isolated optimizations, missing opportunities for cross-stage coordination to reduce redundant computation and memory access.", "method": "STAR introduces: 1) leading-zero-based sparsity prediction using log-domain add-only operations, 2) distributed sorting and sorted updating FlashAttention mechanism, 3) coordinated tiling strategy for fine-grained stage interaction, and 4) dedicated accelerator architecture with multi-core spatial architecture for ultra-long sequences.", "result": "Achieves up to 9.2\u00d7 speedup and 71.2\u00d7 energy efficiency over A100, surpasses SOTA accelerators by up to 16.1\u00d7 energy and 27.1\u00d7 area efficiency gains. Spatial-STAR achieves 20.1\u00d7 throughput improvement over baseline.", "conclusion": "Cross-stage coordination in sparsity acceleration flow is crucial for efficient Transformer inference under large-scale token parallelism, and STAR's algorithm-hardware co-design effectively addresses this challenge with substantial performance gains."}}
{"id": "2512.20495", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20495", "abs": "https://arxiv.org/abs/2512.20495", "authors": ["He Zhu", "Zheng Liu", "Xingyang Li", "Anbang Wu", "Jieru Zhao", "Fangxin Liu", "Yiming Gan", "Jingwen Leng", "Yu Feng"], "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization", "comment": null, "summary": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.\n  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.", "AI": {"tldr": "Nebula is a framework for accelerating large-scale 3D Gaussian splatting collaborative rendering that reduces cloud-client bandwidth by 1925% and achieves 2.7\u00d7 motion-to-photon speedup through temporal-aware LoD search and stereo rasterization.", "motivation": "Current architectural designs for 3D Gaussian splatting (3DGS) overlook scalability and are fragile for extremely large-scale 3DGS. Additionally, VR bandwidth requirements make it impossible to deliver high-fidelity, smooth VR content from the cloud via traditional video streaming.", "method": "Nebula streams intermediate results after LoD search instead of videos, reducing data communication. It introduces temporal-aware LoD search in the cloud to tame irregular memory access and reduce redundant data access by exploiting temporal coherence. On the client side, it proposes novel stereo rasterization that enables two eyes to share most computations during stereo rendering with bit-accurate quality.", "result": "Nebula achieves 2.7\u00d7 motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming with minimal hardware augmentations.", "conclusion": "Nebula provides a coherent acceleration framework for large-scale 3DGS collaborative rendering that significantly improves performance and reduces bandwidth requirements for cloud-based VR content delivery."}}
{"id": "2512.20571", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20571", "abs": "https://arxiv.org/abs/2512.20571", "authors": ["Brennan Romero", "D. G. Perera"], "title": "Composing Mini Oscilloscope on Embedded Systems", "comment": "22 pages, 11 figures", "summary": "In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.", "AI": {"tldr": "A custom oscilloscope system built using Nuvoton NUC-140 embedded platform achieves 90% of standard oscilloscope features including triggering modes, scaling, and calibration.", "motivation": "To create a cost-effective, portable oscilloscope alternative using embedded systems for debugging purposes, replicating core oscilloscope functionality.", "method": "Used Nuvoton NUC-140 embedded platform as front-end with custom daughter board connecting BNC probes, keypad, and calibration signal; LCD display for waveforms.", "result": "System successfully implements 90% of typical oscilloscope features including automatic, edge-triggered, single modes, vertical/horizontal scaling, and probe calibration.", "conclusion": "The proposed system proves to be a competent debugging tool that replicates most essential oscilloscope functionalities using embedded hardware."}}
{"id": "2512.19842", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19842", "abs": "https://arxiv.org/abs/2512.19842", "authors": ["Andrea Sordello", "Marco Mellia", "Idilio Drago", "Rodolfo Valentim", "Francesco Musumeci", "Massimo Tornatore", "Federico Cerutti", "Martino Trevisan", "Alessio Botta", "Willen Borges Coelho"], "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform", "comment": null, "summary": "The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.", "AI": {"tldr": "Holoscope is a cloud-native platform for deploying distributed telescope and honeypot sensors to monitor Internet attack traffic across multiple networks.", "motivation": "The increasing complexity and scale of Internet attacks require distributed, cooperative observatories that can monitor malicious traffic across diverse networks, but existing solutions lack simplicity in deployment and management.", "method": "Built on K3s and WireGuard, Holoscope uses a modular design with Infrastructure-as-Code principles to provide secure connectivity, automated node onboarding, resilient operation, dynamic sensor orchestration, automated recovery, and processing capabilities.", "result": "Holoscope was successfully deployed and operated across multiple institutions and cloud networks in Europe and Brazil, providing unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.", "conclusion": "Holoscope demonstrates that lightweight, cloud-native platforms can effectively simplify the deployment and management of distributed security sensors, enabling cooperative monitoring of Internet attacks across diverse networks while addressing operational challenges in resource-constrained environments."}}
{"id": "2512.20178", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.20178", "abs": "https://arxiv.org/abs/2512.20178", "authors": ["Chen Zhuang", "Lingqi Zhang", "Benjamin Brock", "Du Wu", "Peng Chen", "Toshio Endo", "Satoshi Matsuoka", "Mohamed Wahib"], "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication", "comment": "Under Review", "summary": "Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \\method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\\times$, 56.0$\\times$, 23.4$\\times$, and 8.8$\\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.", "AI": {"tldr": "A distributed SpMM framework with sparsity-aware and hierarchical communication strategies achieves strong scalability up to 128 GPUs with significant speedups over state-of-the-art baselines.", "motivation": "Distributed SpMM suffers from substantial communication overhead that limits performance and scalability in high-performance computing and deep learning applications.", "method": "Proposes: (1) fine-grained sparsity-aware communication strategy exploiting sparse matrix patterns, and (2) hierarchical communication strategy integrating sparsity-aware approach with two-tier GPU network architectures to reduce redundant communication.", "result": "Achieves strong scalability up to 128 GPUs with geometric mean speedups of 221.5\u00d7, 56.0\u00d7, 23.4\u00d7, and 8.8\u00d7 over CAGNET, SPA, BCL, and CoLa baselines respectively.", "conclusion": "The proposed communication optimizations effectively address inefficiencies in distributed SpMM, demonstrating significant performance improvements and scalability."}}
{"id": "2512.20103", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.20103", "abs": "https://arxiv.org/abs/2512.20103", "authors": ["Md Mahfuzur Rahman", "Nishith Tripathi", "Jeffrey H. Reed", "Lingjia Liu"], "title": "Developing an NTN Architecture for End-to-End Performance Evaluation", "comment": null, "summary": "Non-Terrestrial Networks (NTN) are emerging as critical enablers of global connectivity, particularly in remote, unserved, underserved, or maritime regions lacking traditional infrastructure. While much of the existing work on NTN focuses on theoretical or simulated evaluations, practical implementations remain limited. In this paper, we present SpaceNET, a transparent NTN testbed that leverages the Starlink Low Earth Orbit (LEO) satellite constellation in conjunction with Mininet-based emulation to perform end-to-end performance assessments across real-world maritime and terrestrial endpoints that can also be applied to 5th generation (5G). Specifically, we establish a bidirectional link between a ground terminal located in Blacksburg, Virginia, and a maritime terminal aboard a cruise ship near Key West, Florida. We report detailed transmission control protocol (TCP) throughput, user datagram protocol (UDP) throughput, and latency measurements using two different user terminals - a) Smartphone, and b) very small aperture terminal (VSAT), emphasizing the transparent nature of the NTN payload, where the satellite acts solely as a relay node. Our results provide new insights into the performance limits and reliability of commercial LEO-based NTN applications. The SpaceNET testbed offers a reproducible and extensible platform for future research in NTN routing, mobility support, and cross-layer optimization.", "AI": {"tldr": "SpaceNET is a transparent NTN testbed using Starlink LEO satellites with Mininet emulation for end-to-end performance testing between ground and maritime terminals, providing TCP/UDP throughput and latency measurements for smartphone and VSAT terminals.", "motivation": "NTNs are critical for global connectivity in remote/maritime areas, but practical implementations are limited compared to theoretical/simulated evaluations. There's a need for real-world NTN testbeds to assess performance and reliability.", "method": "Created SpaceNET testbed using Starlink LEO constellation with Mininet-based emulation. Established bidirectional link between ground terminal (Blacksburg, VA) and maritime terminal (cruise ship near Key West, FL). Measured TCP throughput, UDP throughput, and latency using smartphone and VSAT terminals in transparent satellite relay configuration.", "result": "Provided detailed performance measurements revealing insights into performance limits and reliability of commercial LEO-based NTN applications. Demonstrated the testbed's capability for real-world NTN assessment.", "conclusion": "SpaceNET offers a reproducible, extensible platform for future NTN research in routing, mobility support, and cross-layer optimization, bridging the gap between theoretical work and practical NTN implementations."}}
{"id": "2512.19849", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.19849", "abs": "https://arxiv.org/abs/2512.19849", "authors": ["Ziming Mao", "Yihan Zhang", "Chihan Cui", "Kaichao You", "Zhongjie Chen", "Zhiying Xu", "Scott Shenker", "Costin Raiciu", "Yang Zhou", "Ion Stoica"], "title": "UCCL-EP: Portable Expert-Parallel Communication", "comment": null, "summary": "Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.", "AI": {"tldr": "UCCL-EP is a portable expert parallelism communication system that achieves DeepEP-level performance across heterogeneous GPU and NIC hardware by replacing GPU-initiated RDMA with a GPU-CPU control channel and CPU proxies.", "motivation": "Current EP communication systems like DeepEP show strong performance but poor portability across heterogeneous GPU and NIC platforms due to architecture limitations requiring tight GPU-NIC integration.", "method": "UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel where compact token-routing commands are transferred to multithreaded CPU proxies that issue GPUDirect RDMA operations. It also emulates ordering semantics using RDMA immediate data for correctness on NICs lacking such ordering.", "result": "On AWS EFA, UCCL-EP outperforms best existing EP solution by up to 2.1\u00d7 for dispatch/combine throughput. On NVIDIA-only platforms, it matches DeepEP performance. It improves token throughput on SGLang by up to 40% on NVIDIA+EFA and improves DeepSeek-V3 training throughput by up to 45% on 16-node AMD+Broadcom platform.", "conclusion": "UCCL-EP successfully addresses portability limitations of existing EP systems while maintaining high performance across diverse hardware platforms, enabling efficient MoE workloads on heterogeneous GPU and NIC infrastructures."}}
{"id": "2512.19851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19851", "abs": "https://arxiv.org/abs/2512.19851", "authors": ["Aditya Bhosale", "Laxmikant Kale"], "title": "An Adaptive Distributed Stencil Abstraction for GPUs", "comment": null, "summary": "The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.", "AI": {"tldr": "A distributed, adaptive abstraction for stencil computations on multi-node GPUs that bridges Python prototyping with HPC execution using NumPy-like syntax.", "motivation": "The Python scientific computing ecosystem is limited to single-node parallelism, creating a gap between prototyping in NumPy and high-performance execution on modern supercomputers. Traditional HPC abstractions are rigid while hardware accelerators and energy efficiency needs require resource adaptivity.", "method": "Built using CharmTyles, a framework based on the adaptive Charm++ runtime, featuring a familiar NumPy-like syntax to minimize porting effort from prototype to production code. Provides adaptive, distributed abstraction for stencil computations on multi-node GPUs.", "result": "Demonstrates resource elasticity by dynamically rescaling running applications across different numbers of nodes. Achieves significant performance improvements over both specialized high-performance stencil DSLs and generalized NumPy replacements.", "conclusion": "The proposed abstraction successfully bridges the gap between Python prototyping and HPC execution, providing adaptive resource management while maintaining familiar syntax and achieving superior performance compared to existing alternatives."}}
{"id": "2512.19972", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19972", "abs": "https://arxiv.org/abs/2512.19972", "authors": ["Pengchao Han", "Xi Huang", "Yi Fang", "Guojun Han"], "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions", "comment": "Published in IEEE TNSE", "summary": "Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.", "AI": {"tldr": "This paper provides a comprehensive review of knowledge distillation in collaborative learning, focusing on memory and knowledge mechanisms across various distributed learning patterns.", "motivation": "The paper aims to bridge the gap in understanding how knowledge distillation leverages memory and knowledge across distributed agents in collaborative learning systems, as these mechanisms remain underexplored despite being central to efficient knowledge transfer.", "method": "The authors conduct a systematic review by defining and categorizing memory and knowledge within KD processes, examining their interrelationships, and analyzing various collaborative learning patterns including distributed, hierarchical, and decentralized structures. They particularly focus on task heterogeneity in distributed learning patterns covering federated learning, multi-agent domain adaptation, federated multi-modal learning, federated continual learning, federated multi-task learning, and federated graph knowledge embedding.", "result": "The analysis provides a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings, categorizes existing work based on memory and knowledge handling approaches, and examines how memory and knowledge dynamics shape KD effectiveness across different collaborative learning patterns.", "conclusion": "The paper concludes by discussing existing challenges (including model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns) and proposing future directions for advancing knowledge distillation techniques in collaborative learning contexts."}}
{"id": "2512.20017", "categories": ["cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.20017", "abs": "https://arxiv.org/abs/2512.20017", "authors": ["Hexu Zhao", "Xiaoteng Liu", "Xiwen Min", "Jianhao Huang", "Youming Deng", "Yanfei Li", "Ang Li", "Jinyang Li", "Aurojit Panda"], "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction", "comment": "13 pages main text, plus appendix", "summary": "Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.", "AI": {"tldr": "Gaian is a distributed training system for Point-based Differentiable Rendering that reduces communication overhead and improves training throughput through locality optimization.", "motivation": "Existing distributed training systems for PBDR are tightly coupled to specific methods and suffer from severe communication overhead due to poor data locality, limiting scalability to high-resolution and large scenes.", "method": "Gaian provides a unified API expressive enough to support existing PBDR methods while exposing rich data-access information, which it leverages to optimize locality and reduce communication.", "result": "Across six datasets and up to 128 GPUs, Gaian reduces communication by up to 91% and improves training throughput by 1.50x-3.71x compared to existing systems.", "conclusion": "Gaian enables efficient distributed training for PBDR methods by addressing communication bottlenecks through locality-aware optimizations, making high-resolution and large-scale 3D scene reconstruction more practical."}}
{"id": "2512.20064", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20064", "abs": "https://arxiv.org/abs/2512.20064", "authors": ["Yaojian Chen", "Si-Qiu Gong", "Lin Gan", "Yanfei Liu", "An Yang", "Yinuo Wang", "Chao-yang Lu", "Guangwen Yang"], "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling", "comment": "12 pages, 13 figures", "summary": "Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.", "AI": {"tldr": "Fast-MPS is a multi-level parallel framework for scalable Matrix Product State sampling that combines data and tensor parallelism, achieving 10x speedup and scaling to thousands of processes for large-scale simulations.", "motivation": "Traditional MPS sampling approaches face limitations: data parallelism is constrained by memory and I/O bottlenecks for large-scale MPS, while model parallelism has rigid process bindings and poor scalability. There's a need for a scalable solution that can handle rapidly increasing MPS scales in complex problems.", "method": "Fast-MPS combines data parallelism across samples with tensor parallelism along bond dimensions. It eliminates memory and I/O pressure through compression and overlapping techniques, reviving data parallelism for large-scale MPS sampling.", "result": "Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming state-of-the-art approaches.", "conclusion": "Fast-MPS demonstrates great potential for high-performance tensor network applications by providing a scalable multi-level parallel framework that overcomes the limitations of traditional MPS sampling approaches."}}
{"id": "2512.20163", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20163", "abs": "https://arxiv.org/abs/2512.20163", "authors": ["Leszek G\u0105sieniec", "Tytus Grodzicki", "Tomasz Jurdzi\u0144ski", "Jakub Kowalski", "Grzegorz Stachowiak"], "title": "Population Protocols Revisited: Parity and Beyond", "comment": null, "summary": "For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.\n  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\\log^3 n)$ states and achieve silent stabilisation in $O(\\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.", "AI": {"tldr": "First efficient population protocols for parity and congruence predicates using O(log\u00b3 n) states and O(log\u00b3 n) time, introducing a new computing paradigm with population weights and robust clocking.", "motivation": "Address the long-standing gap in population protocols where no protocols achieved both time- and space-efficiency for congruency predicates like parity computation, which are complementary to majority computation in Presburger Arithmetic.", "method": "Introduces a novel computing paradigm integrating population weights, robust clocking mechanism, efficient anomaly detection with switching mechanism. This allows universal design of efficient multistage stable population protocols, relaxing strict optimization for universality and robustness.", "result": "First efficient parity and congruence protocols using O(log\u00b3 n) states that achieve silent stabilization in O(log\u00b3 n) time. The weight system enables implicit conversion between unary and binary representations.", "conclusion": "The new paradigm successfully addresses the efficiency gap for congruency predicates in population protocols, with applications extending to computation and representation of (sub-)population sizes and other problems."}}
{"id": "2512.20184", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20184", "abs": "https://arxiv.org/abs/2512.20184", "authors": ["Chaoyi Ruan", "Yiliang Wang", "Ziji Shi", "Jialin Li"], "title": "Reaching Agreement Among Reasoning LLM Agents", "comment": null, "summary": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.", "AI": {"tldr": "Aegean is a consensus protocol for multi-agent reasoning that provides formal guarantees and reduces latency by 1.2-20x compared to existing heuristic approaches while maintaining answer quality.", "motivation": "Current multi-agent systems use static heuristic workflows (fixed loops, barrier sync) that waste resources, cause high latency due to stragglers, and risk finalizing transient agreements. There's a need for formal foundations similar to distributed consensus problems.", "method": "Proposes a formal model of multi-agent refinement with correctness guarantees and formal semantics. Introduces Aegean consensus protocol for stochastic reasoning agents, implemented in Aegean-Serve engine with incremental quorum detection for early termination.", "result": "Evaluation on four mathematical reasoning benchmarks shows Aegean reduces latency by 1.2-20x vs state-of-the-art baselines while maintaining answer quality within 2.5%. Provides provable safety and liveness guarantees across both local GPU deployments and commercial APIs.", "conclusion": "Consensus-based orchestration eliminates straggler delays without sacrificing correctness, offering formal reliability guarantees for multi-agent reasoning systems."}}
{"id": "2512.20210", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20210", "abs": "https://arxiv.org/abs/2512.20210", "authors": ["Yinan Ni", "Xiao Yang", "Yuqi Tang", "Zhimin Qiu", "Chen Wang", "Tingzhou Yuan"], "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs", "comment": null, "summary": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.", "AI": {"tldr": "P-LoRA is a serverless inference system for LoRA-based LLMs that uses predictive prefetching and memory management to reduce cold starts and fragmentation, achieving 1.52x higher throughput and 35% lower TTFT.", "motivation": "Serverless computing offers benefits for LLM inference but faces challenges with LoRA adapters: reactive loading causes cold start latency and frequent swapping leads to GPU memory fragmentation.", "method": "P-LoRA introduces: 1) LSTM-based traffic predictor to forecast adapter demand and proactively prefetch hot adapters, and 2) page-based adapter memory management inspired by OS virtual memory to maintain high GPU utilization.", "result": "Reduces cold start latency by up to 68%, keeps GPU memory utilization above 87% even under heterogeneous adapter ranks, achieves 1.52x higher throughput than S-LoRA, and reduces average TTFT by 35% under high concurrency.", "conclusion": "P-LoRA effectively addresses the challenges of serving multiple fine-tuned LLMs via LoRA in serverless environments through proactive prediction and fragmentation-aware memory management."}}
{"id": "2512.20394", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20394", "abs": "https://arxiv.org/abs/2512.20394", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults", "comment": null, "summary": "As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies", "AI": {"tldr": "Proposes a fault-aware Reinforcement Learning routing scheme using PPO for Gaussian Interconnected Networks, outperforming greedy adaptive routing under high fault densities.", "motivation": "As Network-on-Chip and Wireless Sensor Networks scale, network topology becomes critical. Gaussian Interconnected Networks offer good theoretical properties but adaptive routing is vulnerable to node/link faults, especially Gaussian-distributed failures like thermal hotspots or physical damage clusters, which challenge traditional deterministic routing.", "method": "Proposes a fault-aware Reinforcement Learning routing scheme using PPO (Proximal Policy Optimization) agent with specific reward structure designed to penalize fault proximity. The system dynamically learns to bypass faulty regions in Gaussian Interconnected Networks.", "result": "RL agent significantly outperforms greedy adaptive shortest-path routing: sustains Packet Delivery Ratio (PDR) of 0.95 at 40% fault density vs. 0.66 for greedy. Under low network load of 20%, RL achieves 0.57 PDR vs. 0.43 for greedy, showing better congestion management.", "conclusion": "The RL-based routing approach effectively manages fault-prone topologies, demonstrating superior performance over traditional adaptive routing in stochastic, fault-prone Gaussian Interconnected Networks."}}
{"id": "2512.20485", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20485", "abs": "https://arxiv.org/abs/2512.20485", "authors": ["Tanisha Fonseca", "Gengrui Zhang"], "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient", "comment": null, "summary": "Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.", "AI": {"tldr": "WOC is a dual-path consensus protocol that dynamically routes operations based on access patterns to optimize for both node heterogeneity and workload independence.", "motivation": "Existing consensus protocols fail to optimize for both node heterogeneity and workload independence simultaneously. Cabinet handles node heterogeneity but serializes operations through a global leader, limiting parallelism. EPaxos enables parallel execution but treats nodes uniformly, ignoring performance differences.", "method": "WOC uses a dual-path approach: independent operations execute through a fast path using object-specific weighted quorums (one network round-trip), while conflicting/shared objects route through a leader-coordinated slow path with node-weighted consensus.", "result": "WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.", "conclusion": "WOC successfully addresses the limitations of existing consensus protocols by dynamically routing operations based on access patterns, optimizing for both node heterogeneity and workload independence."}}
