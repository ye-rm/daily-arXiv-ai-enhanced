<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: Swizzled Head-first Mapping addresses NUMA bottlenecks in disaggregated AI GPUs by spatially aligning attention heads with GPU NUMA domains, achieving 50% higher performance and 80-97% L2 cache hit rates on AMD MI300X.


<details>
  <summary>Details</summary>
Motivation: Disaggregated AI GPUs with multi-chiplet designs suffer from non-uniform memory access (NUMA) effects that undermine traditional GPU kernel scheduling strategies, creating critical bottlenecks in large-scale attention workloads.

Method: Swizzled Head-first Mapping - a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse.

Result: On AMD's MI300X architecture, achieves up to 50% higher performance over state-of-the-art attention algorithms and sustains consistently high L2 cache hit rates of 80-97%.

Conclusion: NUMA-aware scheduling is fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [2] [BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction](https://arxiv.org/abs/2511.02196)
*Liwei Ni,Jiaxi Zhang,Shenggen Zheng,Junfeng Liu,Xingyu Meng,Biwei Xie,Xingquan Li,Huawei Li*

Main category: cs.AR

TL;DR: BoolSkeleton is a Boolean network skeletonization method that improves consistency and reliability in design-specific evaluations by transforming networks into Boolean dependency graphs and applying pattern-based reduction while preserving critical functionality.


<details>
  <summary>Details</summary>
Motivation: Boolean equivalence allows networks with identical functionality to have different graph structures, creating challenges for consistency between Boolean networks in logic optimization tasks.

Method: Two-step approach: preprocessing transforms Boolean networks into Boolean dependency graphs with functionality-related node status, then pattern reduction applies homogeneous and heterogeneous patterns with parameter K controlling fanin size for granular graph reduction.

Result: Validated through four analysis tasks (compression analysis, classification, critical path analysis, timing prediction), showing robustness across scenarios and achieving over 55% improvement in average accuracy for timing prediction compared to original Boolean networks.

Conclusion: BoolSkeleton demonstrates potential to enhance design consistency in logic synthesis by providing reliable skeletonization that maintains critical functionality while improving evaluation consistency.

Abstract: Boolean equivalence allows Boolean networks with identical functionality to
exhibit diverse graph structures. This gives more room for exploration in logic
optimization, while also posing a challenge for tasks involving consistency
between Boolean networks. To tackle this challenge, we introduce BoolSkeleton,
a novel Boolean network skeletonization method that improves the consistency
and reliability of design-specific evaluations. BoolSkeleton comprises two key
steps: preprocessing and reduction. In preprocessing, the Boolean network is
transformed into a defined Boolean dependency graph, where nodes are assigned
the functionality-related status. Next, the homogeneous and heterogeneous
patterns are defined for the node-level pattern reduction step. Heterogeneous
patterns are preserved to maintain critical functionality-related dependencies,
while homogeneous patterns can be reduced. Parameter K of the pattern further
constrains the fanin size of these patterns, enabling fine-tuned control over
the granularity of graph reduction. To validate BoolSkeleton's effectiveness,
we conducted four analysis/downstream tasks around the Boolean network:
compression analysis, classification, critical path analysis, and timing
prediction, demonstrating its robustness across diverse scenarios. Furthermore,
it improves above 55% in the average accuracy compared to the original Boolean
network for the timing prediction task. These experiments underscore the
potential of BoolSkeleton to enhance design consistency in logic synthesis.

</details>


### [3] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: This paper implements Whisper's core computational kernel on the IMAX CGLA accelerator, demonstrating superior energy efficiency compared to CPUs and GPUs for ASR tasks.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between energy efficiency and programmability in generative AI for Automatic Speech Recognition, where ASICs are efficient but inflexible while general-purpose processors are programmable but less efficient.

Method: Hardware/software co-design approach implementing Whisper's core kernel on the IMAX CGLA accelerator, evaluated via FPGA prototype and projected for 28 nm ASIC implementation.

Result: The projected ASIC achieves 1.90x higher energy efficiency than NVIDIA Jetson AGX Orin and 9.83x higher than NVIDIA RTX 4090 for the Q8_0 model.

Conclusion: CGLA platforms are promising for sustainable ASR on power-constrained edge devices, offering both energy efficiency and programmability.

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [4] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus is a three-stage framework that improves Verilog code generation by focusing LLM reasoning on critical decision points, using density-guided filtering, simulation-based ranking, and inconsistency mining for refinement.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for LLM-based Verilog generation miss opportunities to focus reasoning on the most informative parts of the design, leading to challenges in ensuring functional correctness.

Method: Three-stage framework: 1) Pre-ranking with density-guided filtering to retain candidates in the "reasoning sweet spot", 2) Ranking using simulation and self-consistency clustering, 3) Post-ranking refinement with inconsistency mining and reasoning-augmented prompts.

Result: Significant improvement in pass@1 correctness on the VerilogEval-Human benchmark across multiple reasoning LLMs.

Conclusion: VFocus effectively enhances Verilog generation for complex hardware design tasks by sharpening LLM reasoning focus on critical decision points.

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [5] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: Implementation of a facial expression recognition system on SoC FPGA using DPU with multi-threading, achieving 25 FPS throughput and 2.4x power efficiency.


<details>
  <summary>Details</summary>
Motivation: Previous work used Haar Cascade detector on CPU which was less accurate for profile and variable illumination conditions, and required separate accelerators for different DNN inferences.

Method: Used DenseBox for face detection and CNN for facial expression recognition on the same DPU (systolic array CNN accelerator), with multi-threading to improve throughput and DPU utilization.

Result: Achieved overall system throughput of 25 FPS and throughput per power consumption of 2.4 times.

Conclusion: The proposed approach enables efficient use of FPGA resources while maintaining small circuit size and improves both performance and power efficiency.

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [6] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: First implementation of radix-4 digit-recurrence algorithm for posit division, achieving >80% energy reduction with small area overhead and fewer iterations compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Posit arithmetic offers better accuracy and dynamic range than IEEE 754 floating-point, but division operations remain challenging due to hardware complexity. Existing posit division methods need improvement in efficiency.

Method: Used digit-recurrence algorithm with radix-4 implementation, incorporating hardware optimizations like redundant arithmetic, on-the-fly quotient conversion, and operand scaling to reduce latency, area, and power.

Result: Significant performance improvements across multiple posit configurations: >80% energy reduction with small area overhead, substantial decrease in iteration count compared to existing methods.

Conclusion: The adapted digit-recurrence algorithm demonstrates strong potential to enhance efficiency of posit-based arithmetic units, making posit division more practical and energy-efficient.

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [7] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: First implementation and evaluation of stable-diffusion.cpp kernels on IMAX3 CGRA accelerator, showing promising performance and power efficiency for AI image generation workloads.


<details>
  <summary>Details</summary>
Motivation: To assess the capabilities of the general-purpose IMAX3 CGRA accelerator by executing demanding image generation workloads and establish foundations for future AI-specialized accelerators.

Method: Implemented primary computational kernels from stable-diffusion.cpp on IMAX3 CGRA, evaluated performance on FPGA prototype, and projected potential for future ASIC implementation.

Result: IMAX3 achieved promising performance and power efficiency despite its general-purpose architecture, with particularly strong results projected for ASIC implementation.

Conclusion: Provides guidelines for future IMAX architectural designs and contributes to developing energy-efficient, on-device, multi-modal AI platforms through refined versatile accelerator platforms.

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: This review presents a taxonomy of workload schedulers based on architecture and design, focusing on key factors affecting throughput and scalability, with special attention to Google's Borg as an advanced example.


<details>
  <summary>Details</summary>
Motivation: To analyze deployed workload schedulers and create a hierarchical taxonomy based on their architecture and design, focusing on factors that impact throughput and scalability.

Method: The review analyzes actively used workload schedulers, categorizes them into hierarchical groups based on architecture and design, and examines incremental improvements that enhanced system architecture.

Result: A taxonomy of workload schedulers is presented, with Google's Borg identified as one of the most advanced published systems in this category.

Conclusion: The review provides a structured classification of workload schedulers that highlights key design factors affecting performance and scalability, with Borg serving as a benchmark for advanced systems.

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [9] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: This CDR outlines FAIR's computing infrastructure plans from 2028 onward, focusing on federated, scalable systems to support diverse research needs with open data policies.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive computing infrastructure that meets the varied requirements of FAIR's research groups while ensuring scalability and flexibility for future data challenges.

Method: Proposes a federated and centrally-orchestrated computing model with open data, software, and services policies, designed to serve multiple research lines from initial science phase to full modular implementation.

Result: A detailed conceptual design for computing infrastructure that addresses current research requirements while being adaptable to future data demands at FAIR.

Conclusion: The plan aims to create a robust, scalable computing ecosystem that can effectively support FAIR's diverse research programs through a federated architecture with central coordination.

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [10] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: Cloud computing innovation has shifted from scientific computing to AI/ML dominance, creating cost models that don't suit scientific needs, potentially forcing science to run on unintended platforms.


<details>
  <summary>Details</summary>
Motivation: To examine how cloud computing's focus on AI/ML has created resource models that are poorly suited for scientific computing, potentially limiting scientific discovery.

Method: Analysis of past, current, and potential future cloud cost models through discussion of computing evolution from scientific to AI/ML-driven innovation.

Result: Current cloud cost models designed for AI/ML use cases create resource contention issues for scientific users, potentially forcing scientific workloads to run on unintended platforms.

Conclusion: There is a need to develop cloud cost models that better support scientific discovery alongside AI/ML workloads to ensure continued scientific progress.

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [11] [SPHERE: Spherical partitioning for large-scale routing optimization](https://arxiv.org/abs/2511.01863)
*Robert Fabian Lindermann,Paul-Niklas Ken Kandora,Simon Caspar Zeller,Adrian Asmund Fessler,Steffen Rebennack*

Main category: cs.DC

TL;DR: SPHERE is a heuristic routing algorithm that identifies vertices close to both source and target, then recursively partitions the path-finding problem into smaller subproblems without requiring boundary repair, achieving faster runtimes than existing methods on large graphs.


<details>
  <summary>Details</summary>
Motivation: Traditional shortest-path routing algorithms face time and memory challenges when expanding search frontiers in large weighted undirected graphs, necessitating more efficient approaches.

Method: SPHERE identifies an s-t overlap (vertices close to both source and target in hop count), selects an anchor in this overlap to partition the problem into subproblems s→a and a→t, and recursively applies the same procedure on large subproblems using their induced subgraphs.

Result: SPHERE achieves faster runtimes and smaller optimality gaps than Louvain-based routing and METIS-based pipeline on large networks with over a million nodes and edges, while also outperforming Dijkstra in runtime.

Conclusion: SPHERE provides an effective heuristic for shortest-path routing in large graphs by leveraging source-target awareness and recursive partitioning, offering improved performance over existing methods without requiring boundary repair.

Abstract: We study shortest-path routing in large weighted, undirected graphs, where
expanding search frontiers raise time and memory costs for exact solvers. We
propose \emph{SPHERE}, a source-target-aware heuristic that identifies an
$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.
Selecting an anchor $a$ in this overlap partitions the task into two
subproblems with unchanged problem-topology, $s\to a$ and $a\to t$; if either
remains large, the procedure recurses on its induced subgraph. Because the cut
lies inside the overlap, concatenating the resulting subpaths yields a valid
$s\to t$ route without boundary repair. SPHERE is independent of the downstream
solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large
networks, it achieves faster runtimes and smaller optimality gaps than
Louvain-based routing and a METIS-based pipeline, even on graphs with more than
a million nodes and edges, while also outperforming Dijkstra in runtime.

</details>


### [12] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: The paper analyzes geospatial decentralization in major PoS blockchains and proposes GPoS to improve it with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: Current Proof of Stake blockchains show limited geospatial decentralization, with consensus voting power concentrated in few geographic regions, which threatens regulatory resilience, robustness, and fairness.

Method: Proposed Geospatially aware Proof of Stake (GPoS) that integrates geospatial diversity with stake-based voting power, evaluated experimentally on BFT protocols like HotStuff and CometBFT.

Result: Experimental evaluation shows 45% average improvement in geospatial decentralization (measured by Gini coefficient of Eigenvector centrality) with minimal performance overhead in consensus protocols.

Conclusion: GPoS effectively improves geospatial decentralization in PoS blockchains while maintaining minimal impact on consensus performance.

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [13] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning is a comprehensive study that characterizes the deployment of reasoning LLMs on edge GPUs, systematically quantifying latency-accuracy tradeoffs and providing guidance for optimal edge deployment.


<details>
  <summary>Details</summary>
Motivation: Edge intelligence is increasingly demanded by autonomous systems like robotics, offering privacy, resilience, and cost advantages over cloud solutions. However, deploying LLMs for reasoning tasks on edge GPUs faces challenges from strict latency constraints and limited computational resources, with scarce guidance on optimal design combinations.

Method: The study systematically quantifies latency-accuracy tradeoffs across various LLM architectures and model sizes, evaluates prompt-based and model-tuning techniques for reducing reasoning token length while maintaining performance, and profiles test-time scaling methods with varying parallelism to maximize accuracy under strict latency budgets.

Result: EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs by analyzing various design factors including reasoning vs non-reasoning architectures, model sizes, token budgets, and test-time scaling strategies.

Conclusion: The work provides comprehensive characterization of reasoning LLM deployment on edge GPUs, offering systematic guidance to help developers balance multiple design factors and navigate the constraints of latency and computational resources to achieve optimal performance in edge intelligence applications.

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [14] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: This paper develops models for analyzing multi-core processors with variable structure and multi-functional cores, focusing on reliability, fault tolerance, viability, and flexibility using logical probabilistic methods.


<details>
  <summary>Details</summary>
Motivation: To analyze and evaluate efficiency indicators (reliability, fault tolerance, viability, flexibility) of multi-core processors with variable structure and multi-functional cores, addressing the need for comprehensive performance assessment in modern processor architectures.

Method: Using logical probabilistic methods to develop models for: reliability and fault tolerance evaluation of processor cores as multi-functional elements; logical probabilistic models of shortest paths, flexibility, and performance conditions; and models for estimating reliability, fault tolerance, and lifetime considering all possible performance states.

Result: The paper presents results of structural analysis for two-core and four-core processors, along with trends showing increasing efficiency indicators for multi-core processors.

Conclusion: The developed models provide comprehensive tools for evaluating the efficiency and performance characteristics of multi-core processors with variable structure and multi-functional cores, demonstrating improved efficiency indicators through structural analysis.

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [15] [Learned Cost Model for Placement on Reconfigurable Dataflow Hardware](https://arxiv.org/abs/2511.01872)
*Etash Guha,Tianxiao Jiang,Andrew Deng,Jian Zhang,Muthu Annamalai*

Main category: cs.DC

TL;DR: A learned approach for predicting throughput of ML model mappings on reconfigurable systems, achieving 31%-52% higher accuracy than hand-designed analytical models and resulting in 5.6% faster compiled graphs.


<details>
  <summary>Details</summary>
Motivation: Mapping ML model dataflow graphs to reconfigurable systems is challenging due to varying throughput and resource constraints. Existing hand-designed analytical models rely on proxy features or intuition, introducing significant error in throughput prediction.

Method: Developed a learned approach that predicts throughput of different mappings without requiring expensive complete measurements. The approach maintains accuracy even after removing performance annotations.

Result: The learned approach predicts throughput 31%-52% more accurately across various graphs compared to traditional hand-designed analytical models. It shows no accuracy degradation when performance annotations are removed.

Conclusion: Using this learned throughput prediction approach leads to 5.6% faster compiled graphs, demonstrating its practical effectiveness for optimizing ML model mappings on reconfigurable systems.

Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is
difficult, as different mappings have different throughputs and consume
resource constraints differently. To solve this, a model to evaluate the
throughput of mappings is necessary as measuring throughput completely is
expensive. Many use a hand-designed analytical model, relying on proxy features
or intuition, introducing error. We provide a Learned Approach that predicts
throughput 31%-52% more accurately over a variety of graphs. In addition, our
approach shows no accuracy degradation after removing performance annotations.
We show that using this approach results in 5.6% faster compiled graphs.

</details>


### [16] [HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing](https://arxiv.org/abs/2511.01881)
*Zhengxin Fang,Hui Ma,Gang Chen,Rajkumar Buyya*

Main category: cs.DC

TL;DR: HGraphScale is a novel autoscaling approach for microservice applications in container-based clouds that uses hierarchical graph neural networks to capture microservice dependencies and deployment schemes, achieving significant improvements in response time under budget constraints.


<details>
  <summary>Details</summary>
Motivation: Microservice architecture provides lightweight, flexible, and resilient application development, but the intricate dependencies between microservices and container deployment schemes create challenges for effective resource scaling in cloud environments.

Method: Proposes HGraphScale approach that uses a hierarchical graph neural network to model microservice dependencies and deployment schemes, enabling effective scaling decisions for rapidly changing user request workloads.

Result: Extensive experiments with real-world traces show HGraphScale reduces average response time by up to 80.16% compared to state-of-the-art autoscaling approaches under the same VM rental budget constraints.

Conclusion: HGraphScale effectively addresses the challenges of autoscaling in microservice-based container clouds by leveraging hierarchical graph neural networks to capture complex dependencies and deployment schemes, delivering superior performance in response time reduction.

Abstract: Microservice architecture has become a dominant paradigm in application
development due to its advantages of being lightweight, flexible, and
resilient. Deploying microservice applications in the container-based cloud
enables fine-grained elastic resource allocation. Autoscaling is an effective
approach to dynamically adjust the resource provisioned to containers. However,
the intricate microservice dependencies and the deployment scheme of the
container-based cloud bring extra challenges of resource scaling. This article
proposes a novel autoscaling approach named HGraphScale. In particular,
HGraphScale captures microservice dependencies and the deployment scheme by a
newly designed hierarchical graph neural network, and makes effective scaling
actions for rapidly changing user requests workloads. Extensive experiments
based on real-world traces of user requests are conducted to evaluate the
effectiveness of HGraphScale. The experiment results show that the HGraphScale
outperforms existing state-of-the-art autoscaling approaches by reducing at
most 80.16\% of the average response time under a certain VM rental budget of
application providers.

</details>


### [17] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner is a sidecar shim that enables near-zero copy and serialization-free data transfer between WebAssembly-based serverless functions, significantly reducing latency and improving throughput.


<details>
  <summary>Details</summary>
Motivation: Serverless functions are stateless and rely on external services for data storage/exchange, which involves costly serialization/deserialization operations with multiple data copies and context switching, leading to increased latency and resource consumption.

Method: Roadrunner reduces data copies by mapping function memory and moving data along a dedicated virtual data hose, bypassing serialization/deserialization processes to achieve near-native latency performance.

Result: Roadrunner improves inter-function communication latency by 44-89%, reduces serialization overhead in 97% of data transfers, and increases throughput by 69 times compared to state-of-the-art WebAssembly-based serverless functions.

Conclusion: Roadrunner effectively addresses data transfer overhead in serverless computing by enabling near-zero copy communication, significantly improving performance metrics for WebAssembly-based serverless functions.

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [18] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mL-R introduces memoization and variable offloading to optimize ADMM-FFT laminography reconstruction, achieving 52.8% average performance improvement while enabling scaling to 2Kx2Kx2K problems.


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT provides high reconstruction accuracy for laminography but suffers from excessive computation time and large memory consumption, limiting its scalability.

Method: mL-R employs memoization to replace time-consuming FFT operations by leveraging the observation that similar FFT operations appear in ADMM-FFT iterations. It uses techniques to make memoization performance-beneficial and scalable, plus variable offloading to save CPU memory and scale across GPUs.

Result: mL-R enables scaling ADMM-FFT to 2Kx2Kx2K problems (largest ever for laminography reconstruction with ADMM-FFT) and achieves 52.8% average performance improvement (up to 65.4%) compared to original ADMM-FFT.

Conclusion: mL-R successfully optimizes ADMM-FFT for laminography reconstruction through memoization and memory management techniques, significantly improving performance and enabling larger-scale reconstructions.

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [19] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: The paper proposes moving beyond the bulk synchronous parallel (BSP) model to address inefficiencies in distributed GPU execution for large language models, introducing the 'Three Taxes' framework and novel fine-grained programming patterns that eliminate these taxes through direct tile-level pipelines and fine-grained dataflow synchronization.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale, distributed execution across multiple GPUs using conventional BSP model introduces significant performance inefficiencies that need to be addressed.

Method: The authors exploit libraries like Iris for Triton to access in-kernel communication primitives, enabling design of novel fine-grained programming patterns that create direct tile-level producer-consumer pipelines and replace global barriers with fine-grained dataflow synchronization.

Result: Applying this methodology to critical kernels (All-Gather + GEMM operation and Flash Decode algorithm) achieves 10-20% speedup in end-to-end latency over BSP-based approaches.

Conclusion: The approach establishes a more programmable and efficient paradigm for distributed LLM workloads by systematically eliminating the three identified taxes.

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [20] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: Proposes operator-level autoscaling for large generative models to improve efficiency while maintaining SLOs, achieving 40% GPU reduction and 35% energy savings.


<details>
  <summary>Details</summary>
Motivation: Existing static provisioning and model-level autoscaling treat models as monoliths, leading to degraded performance and resource underutilization due to poor adaptability to dynamic inference traffic.

Method: Operator-level autoscaling framework that allocates resources at finer granularity, optimizing scaling, batching, and placement based on individual operator profiles.

Result: Preserves SLOs with up to 40% fewer GPUs and 35% less energy, or achieves 1.6x higher throughput with 5% less energy under fixed resources.

Conclusion: The operator, rather than the entire model, is fundamentally a more effective unit for scaling large generative workloads.

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [21] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: The paper presents two novel scheduling algorithms for optimizing tensor contractions in Lattice QCD simulations, focusing on improving temporal locality and reducing peak memory usage through better tensor reuse.


<details>
  <summary>Details</summary>
Motivation: LQCD simulations require computing correlation functions involving many large tensor contractions that consume significant GPU memory. Current approaches face challenges in scheduling these contractions efficiently to optimize tensor reuse and minimize data movement.

Method: Proposed two fast scheduling algorithms that reorder binary batch tensor contractions to increase temporal locality via input/intermediate tensor reuse. The schedulers leverage application-specific features like binary contractions and locality within contraction trees to minimize peak memory usage.

Result: The schedulers achieved up to 2.1x improvement in peak memory, up to 4.2x reduction in evictions, up to 1.8x reduction in data traffic, and up to 1.9x faster correlation function computation time when integrated into the Redstar LQCD analysis software.

Conclusion: The proposed scheduling algorithms effectively optimize tensor contractions in LQCD simulations by improving temporal locality and reducing memory requirements, leading to significant performance improvements in correlation function computation.

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [22] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: This paper proposes using Split Computing to reduce processing time and power consumption for 3D object detection in autonomous driving by distributing neural network inference between edge devices and cloud servers.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art 3D object detection models using LiDAR point clouds are computationally intensive, causing long processing times and high power consumption on edge devices in autonomous driving systems.

Method: The study leverages Split Computing, a distributed machine learning inference approach that splits deep neural network execution between edge devices and cloud servers, transmitting only intermediate data to reduce computational burden on edge devices.

Result: Experimental results show significant improvements: splitting after voxelization reduces inference time by 70.8% and edge device execution time by 90.0%; splitting within the network reduces inference time by up to 57.1% and edge device execution time by up to 69.5%.

Conclusion: Split Computing effectively addresses computational challenges in 3D object detection for autonomous driving by reducing processing time and power consumption on edge devices while maintaining data security through transmission of only intermediate network data.

Abstract: The field of autonomous driving technology is rapidly advancing, with deep
learning being a key component. Particularly in the field of sensing, 3D point
cloud data collected by LiDAR is utilized to run deep neural network models for
3D object detection. However, these state-of-the-art models are complex,
leading to longer processing times and increased power consumption on edge
devices. The objective of this study is to address these issues by leveraging
Split Computing, a distributed machine learning inference method. Split
Computing aims to lessen the computational burden on edge devices, thereby
reducing processing time and power consumption. Furthermore, it minimizes the
risk of data breaches by only transmitting intermediate data from the deep
neural network model. Experimental results show that splitting after
voxelization reduces the inference time by 70.8% and the edge device execution
time by 90.0%. When splitting within the network, the inference time is reduced
by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.

</details>


### [23] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: FedAttn is a federated attention framework that integrates federated learning into self-attention mechanisms for distributed LLM inference, achieving privacy protection, communication efficiency, and computational efficiency by allowing local self-attention with periodic KV matrix exchange.


<details>
  <summary>Details</summary>
Motivation: Address privacy vulnerabilities, communication overhead, and computational bottlenecks in deploying LLMs at the edge for collaborative scenarios.

Method: Integrate federated paradigm into self-attention mechanism, enabling participants to perform local self-attention over their own tokens while periodically exchanging and aggregating KV matrices across Transformer blocks.

Result: Theoretical analysis reveals error propagation dynamics shaped by local self-attention and heterogeneous token relevance, and characterizes trade-off between response quality and communication/computation efficiency. Experimental results validate analysis and show optimization opportunities through sparse attention and adaptive KV aggregation.

Conclusion: FedAttn provides a principled foundation for porting federated optimization techniques to collaborative LLM inference, demonstrating potential for scalability and efficiency in real-world edge deployments.

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [24] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: This paper compares four performance portability frameworks (Kokkos, OpenMP, RAJA, OCCA) for scientific computing applications on GPU hardware, finding significant performance variability and highlighting optimization needs.


<details>
  <summary>Details</summary>
Motivation: The rise of heterogeneous computing architectures creates demand for vendor-agnostic performance portability frameworks to enable efficient execution of scientific computing applications across different hardware platforms with minimal code changes.

Method: The study evaluated two scientific applications (N-body simulation and structured grid simulation) using distributed memory approach and hardware acceleration through four frameworks on a Polaris supercomputer node with four NVIDIA A100 GPUs.

Result: OCCA showed faster execution for small-scale problems due to JIT compilation but lacks optimized reduction algorithms. OpenMP performed poorly in structured grid simulations due to inefficient data synchronization. Significant performance variability was observed among frameworks.

Conclusion: Further optimization is needed to maximize each framework's capabilities, particularly for reduction algorithms, data communication, and memory management. Future work will focus on scalability studies and comprehensive statistical analysis.

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [25] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos* is a simpler and correct variant of Egalitarian Paxos that addresses complexity and correctness issues in the original leaderless replication protocol, while generalizing it to optimal process counts.


<details>
  <summary>Details</summary>
Motivation: Egalitarian Paxos introduced leaderless state-machine replication to avoid single points of failure and reduce latency, but suffered from complexity, ambiguous specification, and nontrivial bugs that needed addressing.

Method: The paper presents EPaxos* with a simpler failure-recovery algorithm that has been rigorously proved correct, and generalizes the protocol to cover optimal failure thresholds where n ≥ max{2e+f-1, 2f+1}.

Result: EPaxos* maintains the benefits of leaderless replication (non-zero throughput with up to f crashes, fast 2-message-delay execution under certain conditions) while being simpler and provably correct.

Conclusion: EPaxos* successfully addresses the complexity and correctness issues of Egalitarian Paxos while preserving its advantages and achieving optimal process counts for the generalized failure thresholds.

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [26] [Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live](https://arxiv.org/abs/2511.02230)
*Hanchen Li,Qiuyang Mang,Runyuan He,Qizheng Zhang,Huanzhi Mao,Xiaokun Chen,Alvin Cheung,Joseph Gonzalez,Ion Stoica*

Main category: cs.OS

TL;DR: Continuum is a serving system that optimizes job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling to address pauses caused by tool calls.


<details>
  <summary>Details</summary>
Motivation: Agentic LLM applications face challenges with workflow continuity due to pauses from tool calls, causing KV cache eviction and increased waiting times in multi-turn scenarios, which existing solutions fail to address effectively.

Method: Continuum predicts tool call durations and selectively pins KV cache in GPU memory with time-to-live values based on turn number, combined with program-level first-come-first-serve scheduling to prevent scheduling bubbles.

Result: Evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows Continuum significantly improves average job completion times and performs well across different hardware setups and DRAM offloading schemes.

Conclusion: Continuum effectively addresses the continuity challenges in multi-turn agent workloads by modeling tool call variability and program continuity, outperforming state-of-the-art baselines.

Abstract: Agentic LLM applications interleave LLM generation requests with tool calls.
These tool calls break the continuity of the workflow by creating pauses
between LLM requests, bringing many challenges for the serving system,
especially under multi-turn scenarios. Each pause potentially causes KV cache
eviction and extra waiting time before entering the continuous batch for the
following LLM request. Since these pauses happen for each call, this problem
becomes increasingly severe as turn number grow for agentic programs. Previous
works either fail to incorporate information from the tool call, evicting KV
cache that leads to repetitive prefill or loading, or ignore the continuity of
a multi-turn program, creating waiting time between turns that increases
per-request latency.
  We present Continuum, a serving system to optimize job completion time for
multi-turn agent workloads by combining tool-aware KV cache timeout with
program-level scheduling. By predicting tool call durations in agentic
workflows, Continuum selectively pins the KV cache in GPU memory with a
time-to-live value based on total turn number. When combined with program-level
first-come-first-serve, Continuum prevents scheduling bubbles, preserves
multi-turn continuity, and optimizes for throughput for complex agentic
workflows. By modeling the variability of tool call and agent program
continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on
real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models
shows that Continuum significantly improves the average job completion times,
and remains performant across different hardware setups and DRAM offloading
schemes. Preview code is available at:
https://github.com/Hanchenli/vllm-continuum

</details>
