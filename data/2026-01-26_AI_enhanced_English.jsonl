{"id": "2601.16294", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16294", "abs": "https://arxiv.org/abs/2601.16294", "authors": ["Evangelos Georganas", "Alexander Heinecke", "Pradeep Dubey"], "title": "Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple", "comment": null, "summary": "General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance \"glass jaws\". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.", "AI": {"tldr": "The paper proposes using space filling curves (SFC) and generalized Hilbert curves to create platform-oblivious and shape-oblivious matrix multiplication schemes that achieve high data locality and outperform vendor libraries by up to 2x.", "motivation": "Modern matrix multiplication accelerators have high FLOP/Byte ratios, making optimal GEMM implementation challenging. Vendor libraries require extensive tuning for specific platforms and matrix shapes, leading to performance \"glass jaws\" and making exhaustive tuning infeasible.", "method": "The authors use space filling curves (SFC) to partition the matrix multiplication computation space, specifically employing recent advancements in generalized SFC (Generalized Hilbert Curves). This creates platform-oblivious and shape-oblivious schemes with inherent data locality. They extend this to implement Communication-Avoiding (CA) algorithms that replicate input tensors and minimize communication/data-movement.", "result": "The approach achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x (geometric-mean speedup) for a range of GEMM shapes. The implementation is compact (~30 lines of code).", "conclusion": "Space filling curves provide an effective solution to the cumbersome tuning problem in matrix multiplication, enabling platform-oblivious and shape-oblivious schemes with high performance and minimal code complexity."}}
{"id": "2601.16460", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16460", "abs": "https://arxiv.org/abs/2601.16460", "authors": ["Ivan Klianev"], "title": "Consensus In Asynchrony", "comment": null, "summary": "We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.", "AI": {"tldr": "The paper presents an event-based synchronization algorithm for deterministic fault-tolerant consensus in asynchronous systems, achieving vector agreement with safety, liveness, and tolerance to one crash, while reconciling with FLP impossibility through analysis of agreement types and implicit assumptions.", "motivation": "To address the fundamental challenge of achieving deterministic fault-tolerant consensus in asynchronous systems despite the well-known FLP impossibility result, by examining the underlying assumptions and agreement types that may allow circumventing this impossibility.", "method": "Developed an event-based synchronization algorithm for deterministic consensus that achieves vector agreement. Analyzed FLP impossibility by identifying two types of agreements (data-independent and data-dependent) and three implicit assumptions underlying the theorem.", "result": "The algorithm successfully terminates with valid vector agreement, providing safety, liveness, and tolerance to one crash. Experimental results show that the third implicit assumption in FLP theorem lacks supporting evidence, suggesting the impossibility may not hold for all agreement types.", "conclusion": "Event-based synchronization is sufficient for deterministic fault-tolerant consensus in asynchrony. The FLP impossibility result depends on implicit assumptions that may not always hold, particularly for data-independent agreements, opening possibilities for practical consensus solutions."}}
{"id": "2601.16536", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16536", "abs": "https://arxiv.org/abs/2601.16536", "authors": ["Yuanhong He", "Peiyu Niu", "Jun Chen", "Chenchen Zhang", "Chao Yang"], "title": "W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs", "comment": null, "summary": "As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.", "AI": {"tldr": "First practical W4A16 matrix multiplication kernel for Huawei Ascend 910 NPU that achieves 1.01-1.74x speedup over data-parallel approaches, with maximum 1.48x speedup over native FP16, revealing memory transfer as the primary bottleneck rather than dequantization computation.", "motivation": "As LLMs scale, weight-only quantization (W4A16) is critical for reducing memory footprint, but efficient deployment on Huawei's Ascend 910 NPU is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture.", "method": "Design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Tailored specifically for Ascend 910 NPU architecture.", "result": "Performance evaluations show method outperforms data-parallel approaches when K >> N (typical in LLM decoding), achieving speedups ranging from 1.01x to 1.74x. Maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch.", "conclusion": "The primary bottleneck is not dequantization computation but extra global memory transfer for weights. The method lays foundation for efficient deployment of quantized LLMs on domain-specific accelerators and provides insights for future optimizations."}}
{"id": "2601.16635", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16635", "abs": "https://arxiv.org/abs/2601.16635", "authors": ["Julian Legler"], "title": "Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices", "comment": "Accepted Artifact Paper at ICSOC2025", "summary": "Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We present GOXN (Green Observability eXperiment eNginE), an energy experimentation engine for Kubernetes-based microservices that quantifies compute, network, and storage energy at the service level. Using GOXN, we evaluated the OpenTelemetry Demo under varying configurations (monitoring, tracing, service mesh) and steady synthetic load, collecting metrics from Kepler and cAdvisor. Our additive energy model derives service-level energy from container-level data. Results show that excluding network and storage can underestimate auxiliary-service energy by up to 63%, and that high tracing loads shift energy dominance toward network and storage.", "AI": {"tldr": "GOXN is an energy experimentation engine for Kubernetes microservices that measures compute, network, and storage energy at service level, revealing that excluding network/storage can underestimate auxiliary-service energy by up to 63%.", "motivation": "Service-level energy measurement for microservice-based applications remains underexplored, and existing approaches often miss network and storage energy, leading to underestimation of consumption in distributed setups.", "method": "Developed GOXN (Green Observability eXperiment eNginE) for Kubernetes-based microservices, using Kepler and cAdvisor metrics with an additive energy model to derive service-level energy from container-level data.", "result": "Excluding network and storage energy can underestimate auxiliary-service energy by up to 63%, and high tracing loads shift energy dominance toward network and storage components.", "conclusion": "Comprehensive service-level energy measurement must include compute, network, and storage components to accurately assess microservice energy consumption, especially under varying configurations and loads."}}
{"id": "2601.16935", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.16935", "abs": "https://arxiv.org/abs/2601.16935", "authors": ["Wei Wei", "Jingye Xu", "Sahidul Islam", "Dakai Zhu", "Chen Pan", "Mimi Xie"], "title": "AERO: Adaptive and Efficient Runtime-Aware OTA Updates for Energy-Harvesting IoT", "comment": "Accepted at DATE 2026", "summary": "Energy-harvesting (EH) Internet of Things (IoT) devices operate under intermittent energy availability, which disrupts task execution and makes energy-intensive over-the-air (OTA) updates particularly challenging. Conventional OTA update mechanisms rely on reboots and incur significant overhead, rendering them unsuitable for intermittently powered systems. Recent live OTA update techniques reduce reboot overhead but still lack mechanisms to ensure consistency when updates interact with runtime execution. This paper presents AERO, an Adaptive and Efficient Runtime-Aware OTA update mechanism that integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. By identifying update-affected execution regions and dynamically adjusting dependencies, AERO ensures consistent up date integration while adapting to intermittent energy availability. Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches.", "AI": {"tldr": "AERO is a runtime-aware OTA update mechanism for energy-harvesting IoT devices that integrates updates into task DAGs and schedules them under energy constraints to ensure consistency without reboots.", "motivation": "Energy-harvesting IoT devices face intermittent power, making conventional OTA update mechanisms (which rely on reboots) unsuitable due to high overhead. Existing live update techniques lack consistency guarantees when updates interact with runtime execution.", "method": "AERO integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. It identifies update-affected execution regions and dynamically adjusts dependencies to ensure consistent update integration.", "result": "Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches for energy-harvesting IoT devices.", "conclusion": "AERO provides an adaptive and efficient runtime-aware OTA update mechanism that ensures consistency while adapting to intermittent energy availability in EH IoT devices, overcoming limitations of conventional and existing live update approaches."}}
{"id": "2601.16637", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16637", "abs": "https://arxiv.org/abs/2601.16637", "authors": ["Jun Doi", "Tomonori Shirakawa", "Yukio Kawashima", "Seiji Yunoki", "Hiroshi Horii"], "title": "GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms", "comment": null, "summary": "Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\\sim$40$\\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.", "AI": {"tldr": "GPU-accelerated Selected Basis Diagonalization (SBD) using Thrust library achieves ~40\u00d7 speedup over CPU execution for quantum-classical workflows.", "motivation": "SBD is computationally intensive in Sample-based Quantum Diagonalization (SQD), forming the dominant classical workload that needs acceleration for efficient quantum-classical workflows.", "method": "GPU-accelerated implementation using Thrust library with restructured components (configuration processing, excitation generation, matrix-vector operations) using fine-grained data-parallel primitives and flattened GPU-friendly data layouts.", "result": "Achieves up to ~40\u00d7 speedup over CPU execution and substantially reduces total runtime of SQD iterations.", "conclusion": "GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows."}}
{"id": "2601.16956", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.16956", "abs": "https://arxiv.org/abs/2601.16956", "authors": ["Avinash Maurya", "M. Mustafa Rafique", "Franck Cappello", "Bogdan Nicolae"], "title": "DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers", "comment": null, "summary": "The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.", "AI": {"tldr": "DataStates-LLM is a novel checkpointing architecture for large-scale LLM training that addresses 3D heterogeneity in distributed model states, achieving 4\u00d7 higher throughput and 2.2\u00d7 faster training compared to state-of-the-art solutions.", "motivation": "Current checkpointing solutions treat massive distributed LLM states as opaque binary blobs, ignoring the 3D heterogeneity (memory location, sharding patterns, data types, serialization requirements) which causes significant runtime overheads from blocking transfers, data-oblivious serialization, and storage I/O contention.", "method": "Introduces DataStates-LLM with State Providers to decouple state abstraction from data movement, exploiting parameter immutability during forward/backward passes to perform lazy, non-blocking asynchronous snapshots, and efficiently coalescing fragmented heterogeneous shards while overlapping metadata serialization with bulk tensor I/O.", "result": "Evaluated on models up to 70B parameters on 256 A100-40GB GPUs, DataStates-LLM achieves up to 4\u00d7 higher checkpointing throughput and reduces end-to-end training time by up to 2.2\u00d7 compared to state-of-the-art solutions.", "conclusion": "DataStates-LLM effectively mitigates serialization and heterogeneity bottlenecks in extreme-scale LLM training through its novel checkpointing architecture that addresses the 3D heterogeneity of distributed model states."}}
