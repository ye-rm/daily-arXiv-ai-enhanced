<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: This paper introduces D-com, a novel accelerator architecture that enables efficient input decomposition for large language models, overcoming previous limitations by using progressive decomposition algorithms and compute replication to achieve significant speedup with minimal model quality degradation.


<details>
  <summary>Details</summary>
Motivation: Large language models have reached over 1T parameters, creating computational and memory challenges. Previous model decomposition approaches focused on weight decomposition but incurred high runtime latency that often exceeded the benefits.

Method: The authors adopt progressive decomposition using Lanczos algorithm, design a co-accelerator architecture, introduce compute replication to address memory-bound operations, develop output shape-preserving computation, and implement multi-track decomposition to handle outlier channels separately.

Result: The proposed D-com accelerator achieves 6.2x speedup in decomposition operations and provides 22% end-to-end latency improvements compared to A100 GPU, with only small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

Conclusion: Input decomposition can be significantly beneficial with proper algorithm selection and hardware support, overcoming previous limitations and providing substantial performance improvements for large language models.

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [2] [Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks](https://arxiv.org/abs/2510.13362)
*Angelos Athanasiadis,Nikolaos Tampouratzis,Ioannis Papaefstathiou*

Main category: cs.AR

TL;DR: A framework for implementing full-precision CNNs on FPGAs using Darknet, achieving similar performance and energy efficiency as quantized FPGA solutions without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Real-time AI applications need efficient computational solutions as conventional processors struggle with performance, power, and latency balance in embedded/edge systems.

Method: Uses Darknet-based framework to implement CNNs on heterogeneous CPU-FPGA systems while maintaining full precision in all neural network parameters.

Result: Achieves similar performance and energy efficiency compared to FPGA frameworks that use quantization, but without any degradation in neural network accuracy.

Conclusion: FPGAs provide a promising alternative for CNN implementation, combining high performance with energy efficiency and reconfigurability while preserving full precision.

Abstract: The growing demand for real-time processing in artificial intelligence
applications, particularly those involving Convolutional Neural Networks
(CNNs), has highlighted the need for efficient computational solutions.
Conventional processors, very often, fall short in balancing performance, power
consumption, and latency, especially in embedded systems and edge computing
platforms. Field-Programmable Gate Arrays (FPGAs) offer a promising
alternative, combining high performance with energy efficiency and
reconfigurability. The presented framework addresses the complex and demanding
computations of CNNs on FPGAs maintaining full precision in all neural network
parameters. Specifically, our framework is based on Darknet which is very
widely used for the design of CNNs and allows the designer, by using a similar
input to that given to Darknet, to efficiently implement a CNN in a
heterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA
frameworks that support quantization, our solution aims to offer similar
performance and/or energy efficiency without any degradation on the NN
accuracy.

</details>


### [3] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,Jos√© Cano*

Main category: cs.AR

TL;DR: The paper proposes F-BFQ, a flexible accelerator for BFP-quantized LLMs that dynamically switches between BFP variants without reconfiguration, achieving 1.4x speedup over CPU execution on edge devices.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly deployed on edge devices using quantization techniques like BFP to reduce memory and computational requirements. However, mixed BFP quantization across model layers requires specialized accelerators that can handle different BFP variants without reconfiguration overhead.

Method: Proposed a Flexible Block Floating-Point Quantization (F-BFQ) accelerator that can dynamically switch between two BFP quantization variants and perform matrix multiplication operations. The design was deployed on AMD Kria board.

Result: The F-BFQ accelerator reduced inference time by 1.4x on average over Arm NEON-based CPU execution across three BFP quantized LLMs, achieving 5.2 tokens per second (~3.9 words per second).

Conclusion: The F-BFQ accelerator successfully addresses the need for flexible hardware support for mixed BFP quantization in LLMs, enabling efficient deployment on resource-constrained edge devices with significant performance improvements over CPU-based execution.

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor is an efficient decentralized scheduler for data centers that uses cached server information and batch updates to reduce communication overhead by 55-66%, while improving throughput, latency, and makespan through a novel load scoring mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the communication overhead in decentralized schedulers that rely on real-time probing, and to better handle dynamic, multidimensional resource requirements in heterogeneous clusters.

Method: Leverages weighted balls-into-bins model with b-batched setting, uses cached server information updated in batches, and employs a novel load score that captures anti-affinity between servers and tasks instead of just counting pending tasks.

Result: Reduces scheduling messages by 55-66%, increases throughput by up to 33.2% and 21.5%, reduces mean makespan latency by 12.1% and 7.2%, and improves tail latency by 21.9% and 24.6% across Azure VM placements and serverless function workloads.

Conclusion: Dodoor demonstrates significant improvements in communication efficiency and scheduling performance for heterogeneous data center environments through its batch-based approach and novel load scoring mechanism.

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [5] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: A new FDIRS framework for integrated distributed systems that improves performance, efficiency, and reliability using heterogeneous distributed database techniques.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing integrated systems frameworks (ERPSD and ERPDRT) and improve system performance, speed, and reliability.

Method: Proposed FDIRS framework using heterogeneous distributed database technique, analyzed existing frameworks, and conducted simulations for comparison.

Result: FDIRS framework showed improved performance, speed in user response, efficiency, and reliability while eliminating problems from previous frameworks.

Conclusion: The FDIRS framework successfully enhances integrated distributed systems by overcoming previous framework limitations and delivering better performance metrics.

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [6] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe is a dynamic orchestration framework for disaggregated LLM serving that addresses resource allocation inefficiencies, load imbalance between prefill and decode stages, and cache-induced hotspots through layer-level weight migration, attention-level KV Cache migration, and global cache sharing.


<details>
  <summary>Details</summary>
Motivation: Current disaggregated LLM serving systems face three key limitations: static resource allocation cannot adapt to dynamic workloads, inherent load imbalance between compute-bound prefill and memory-bound decode stages, and prefix cache aware routing that skews load distribution and causes hotspots.

Method: BanaServe introduces layer level weight migration, attention level Key Value Cache (KV Cache) migration, and Global KV Cache Store sharing with layer wise overlapped transmission, enabling both coarse-grained (layer level) and fine-grained (attention level) load redistribution with minimal latency overhead.

Result: Compared to vLLM, BanaServe achieves 1.2x-3.9x higher throughput with 3.9%-78.4% lower total processing time, and outperforms DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

Conclusion: BanaServe effectively addresses the key limitations of current disaggregated LLM serving systems by enabling dynamic resource rebalancing and eliminating cache-induced hotspots, resulting in significantly improved throughput and reduced latency.

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [7] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Gro√ümann,Mattthias Schimek*

Main category: cs.DC

TL;DR: This paper presents the first distributed-memory parallel reduction algorithms for finding maximum-weight independent sets in large graphs, achieving significant speedups over sequential approaches while maintaining good solution quality.


<details>
  <summary>Details</summary>
Motivation: Finding maximum-weight independent sets is an important NP-hard optimization problem, but existing approaches are sequential and cannot handle graphs at massive scales. There's a need for distributed algorithms that can process billion-scale graphs efficiently.

Method: The authors developed distributed-memory parallel reduction algorithms, including distributed reduce-and-greedy and reduce-and-peel algorithms. These use data-reduction rules to create equivalent smaller instances while maintaining the ability to reconstruct optimal solutions.

Result: Experiments on up to 1024 processors showed good scalability and reduction impact. The asynchronous reduce-and-peel approach achieved 33√ó average speedup over sequential state-of-the-art methods with similar solution quality. Reduce-and-greedy achieved up to 50√ó speedup at lower quality. The approach handled graphs with over 1 billion vertices and 17 billion edges.

Conclusion: The proposed distributed algorithms enable efficient processing of massive graphs for maximum-weight independent set problems, offering significant speed improvements while maintaining competitive solution quality compared to sequential approaches.

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [8] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: This paper introduces a service-level energy model for microservices that captures energy consumption across containers, including network and storage components, addressing gaps in existing approaches that overlook cross-container service interactions.


<details>
  <summary>Details</summary>
Motivation: Microservice architectures increase cloud resource demand and energy consumption, but existing energy measurement approaches focus on CPU/memory at container level or system-wide assessments, missing the energy impact of cross-container interactions and auxiliary services like observability and monitoring.

Method: The authors developed a service-level energy model that captures distributed microservice execution across containers, supported by an experimentation tool that measures energy consumption in CPU, memory, network, and storage components.

Result: Experimental validation with diverse configurations of auxiliary services showed that omitting network and storage energy measurements can lead to underestimation of auxiliary service energy use by up to 63%.

Conclusion: Comprehensive energy assessments including network and storage components are essential for designing energy-efficient microservice architectures, as traditional approaches significantly underestimate the energy impact of auxiliary services.

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [9] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: ARES is an adaptive decoding rescheduling system that uses LLM-native length prediction to address workload imbalance in LLM inference, improving performance and reducing failures.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference systems suffer from workload imbalance due to output length variations, causing SLO violations and OOM failures in decode phases, especially for long-output reasoning tasks.

Method: Proposes a lightweight LLM-native prediction method using hidden states to model remaining generation length, and a dynamic balancing mechanism that integrates current and predicted workloads for rescheduling.

Result: Reduces MAE by 49.42%, cuts predictor parameters by 93.28%, reduces P99 TPOT by 74.77%, and achieves up to 2.24 times higher goodput.

Conclusion: ARES effectively addresses workload imbalance in LLM inference through adaptive rescheduling based on accurate length prediction, significantly improving system performance and reliability.

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [10] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit C√¥t√©,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST is a framework that enables Inference-as-a-Service across distributed HPC clusters, providing cloud-like access to AI models like LLMs through an OpenAI-compliant API on private, secure environments.


<details>
  <summary>Details</summary>
Motivation: To address the growing demand for private, secure, and scalable AI inference in scientific workflows, allowing researchers to generate billions of tokens daily on-premises without relying on commercial cloud infrastructure.

Method: Leverages Globus Auth and Globus Compute to run parallel inference workloads via an OpenAI-compliant API on private HPC clusters. Supports multiple inference backends (e.g., vLLM), auto-scales resources, maintains 'hot' nodes for low-latency execution, and offers both high-throughput batch and interactive modes.

Result: Enables researchers to run parallel inference workloads across federated clusters, targeting numerous hosted models in a cluster-agnostic manner.

Conclusion: FIRST provides a scalable solution for private AI inference on existing HPC infrastructure, eliminating dependency on commercial cloud services while maintaining security and performance.

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [11] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timoth√© Albouy,Antonio Fern√°ndez Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: This paper provides complete characterization of tight conditions on n and t for solving all binary output tasks in distributed systems, unifying problems like consensus and symmetry breaking.


<details>
  <summary>Details</summary>
Motivation: To explore necessary and sufficient system conditions for solving distributed tasks with binary outputs, focusing on output sets rather than validity or value multiplicity.

Method: Using an output-set approach that disregards validity and value multiplicity, analyzing conditions under which every class of binary output tasks is solvable in both synchronous and asynchronous systems.

Result: Complete characterization of tight conditions on n (number of processes) and t (number of crash failures) for solvability of all binary output task classes.

Conclusion: The output-set approach yields highly general results that unify multiple distributed computing problems and produces impossibility proofs that hold for stronger task formulations.

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>
