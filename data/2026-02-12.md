<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs](https://arxiv.org/abs/2602.10218)
*Chenhui Deng,Zhongzhi Yu,Guan-Ting Liu,Nathaniel Pinckney,Haoxing Ren*

Main category: cs.AR

TL;DR: ACE-RTL unifies domain-adapted RTL models with agentic systems using frontier LLMs to improve hardware design automation through iterative refinement with parallel scaling.


<details>
  <summary>Details</summary>
Motivation: Current approaches to LLM-based hardware design automation follow two separate paths: domain-adapted RTL models that internalize hardware semantics, and agentic systems using frontier LLMs with simulation feedback. These approaches have complementary strengths and weaknesses that need to be unified for better performance.

Method: ACE-RTL integrates an RTL-specialized LLM (trained on 1.7 million RTL samples) with a frontier reasoning LLM through three components: generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness using Agentic Context Evolution (ACE). A parallel scaling strategy reduces iteration requirements.

Result: On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.

Conclusion: ACE-RTL successfully unifies domain-adapted RTL models with agentic systems using frontier LLMs, demonstrating superior performance in hardware design automation through synergistic integration and efficient parallel scaling.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.

</details>


### [2] [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](https://arxiv.org/abs/2602.10254)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.AR

TL;DR: This paper proposes an area-efficient in-memory computing architecture for Mixture-of-Experts transformers that uses crossbar-level multiplexing and expert grouping to reduce peripheral circuit overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Process-in-memory (PIM) architectures are promising for MoE deployment due to energy efficiency benefits, but PIM chips suffer from large area overhead in peripheral circuits. The authors aim to create a more area-efficient MoE architecture for PIM.

Method: 1) Crossbar-level multiplexing strategy that exploits MoE sparsity by deploying experts on crossbars and sharing peripheral circuits; 2) Expert grouping and group-wise scheduling to handle load imbalance and contention; 3) Gate-output (GO) cache to store router results and bypass expensive computations during generation.

Result: The architecture improves area efficiency of the MoE part by up to 2.2x compared to state-of-the-art. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x respectively when generating 8 tokens. Total performance density reaches 15.6 GOPS/W/mm².

Conclusion: The proposed area-efficient in-memory computing architecture successfully addresses PIM area overhead challenges for MoE transformers through circuit sharing, expert grouping, and caching strategies, achieving significant improvements in area efficiency, performance, and energy efficiency.

Abstract: Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

</details>


### [3] [Fault Tolerant Design of IGZO-based Binary Search ADCs](https://arxiv.org/abs/2602.10790)
*Paula Carolina Lozano Duarte,Sule Ozev,Mehdi Tahoori*

Main category: cs.AR

TL;DR: Hierarchical fault injection framework for analyzing defect sensitivity in Binary Search ADCs implemented in unipolar thin-film technologies like IGZO, enabling selective redundancy strategies with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Thin-film technologies like IGZO enable flexible electronics for wearable sensing and health monitoring, but their high defect densities and process variations compared to CMOS make ADC sensor interfaces vulnerable to manufacturing defects, which remains poorly understood.

Method: Hierarchical fault injection framework combining transistor-level defect characterization with system-level fault propagation analysis to efficiently explore single and multiple fault scenarios across the conversion hierarchy in Binary Search ADCs implemented in n-type only technologies.

Result: The framework identifies critical fault-sensitive components and enables selective redundancy strategies, improving fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, with only 4.2% area overhead and 6% power increase.

Conclusion: The methodology provides an effective approach for defect-tolerant design in emerging unipolar thin-film technologies, validated on IGZO-TFTs but applicable to all similar technologies, addressing critical reliability challenges in flexible electronics.

Abstract: Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.

</details>


### [4] [DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets](https://arxiv.org/abs/2602.10654)
*Derek Christ,Thomas Zimmermann,Philippe Barbie,Dmitri Saberi,Yao Yin,Matthias Jung*

Main category: cs.AR

TL;DR: This paper presents a timed Petri net and Python modeling approach for JEDEC DRAM standards to address the complexity of evolving DRAM protocols and enable verification of memory systems.


<details>
  <summary>Details</summary>
Motivation: JEDEC DRAM standards have increasingly complex protocol specifications that are difficult to understand due to evolving features and complex device hierarchies. Existing simplified state machines fail to capture parallel bank operations, making protocol comprehension challenging.

Method: The authors developed an evolved modeling approach using timed Petri nets combined with Python. This model provides an accurate representation of DRAM protocols that is both understandable and directly executable.

Result: The model enables evaluation of interesting metrics and verification of controller RTL models, DRAM logic, and memory simulators by providing a more accurate representation of DRAM protocols than existing simplified state machines.

Conclusion: The timed Petri net and Python modeling approach offers a more effective way to understand and verify complex JEDEC DRAM protocols, addressing limitations of current simplified state machine representations.

Abstract: The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.
  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.

</details>


### [5] [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016)
*Jinxin Yu,Yudong Pan,Mengdi Wang,Huawei Li,Yinhe Han,Xiaowei Li,Ying Wang*

Main category: cs.AR

TL;DR: 3D-Flow is a 3D-stacked spatial accelerator that uses vertical TSVs for register-to-register communication to overcome SRAM access bottlenecks in Transformer models, achieving significant energy and speed improvements.


<details>
  <summary>Details</summary>
Motivation: Transformer models face memory bottlenecks due to quadratic attention complexity. While existing solutions reduce off-chip traffic, on-chip SRAM accesses now consume over 60% of energy in long-sequence workloads, making cache access the new primary bottleneck.

Method: Proposes 3D-Flow: a hybrid-bonded, 3D-stacked spatial accelerator enabling register-to-register communication across vertically partitioned PE tiers using sub-10 um vertical TSVs. Also designs 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers for bubble-free vertical dataflow without SRAM roundtrips.

Result: Evaluations on Transformer workloads (OPT and QWEN models) show 46-93% energy reduction and 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.

Conclusion: 3D-Flow effectively addresses the SRAM access bottleneck in Transformer accelerators through vertical 3D integration and specialized scheduling, delivering substantial energy and performance improvements over existing architectures.

Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis](https://arxiv.org/abs/2602.10246)
*Mayur Akewar,Sandeep Madireddy,Dongsheng Luo,Janki Bhimani*

Main category: cs.DC

TL;DR: KORAL is a knowledge-driven reasoning framework that combines Large Language Models with structured Knowledge Graphs to diagnose SSD performance and reliability issues, providing expert-level analysis without requiring large datasets or extensive expert input.


<details>
  <summary>Details</summary>
Motivation: Diagnosing SSD performance and reliability is difficult due to fragmented, time-disjoint data, shifting workloads, evolving architectures, and environmental factors. Existing methods require large datasets and expert input but offer limited insights.

Method: KORAL integrates LLMs with structured Knowledge Graphs: creates Data KG from fragmented telemetry and integrates Literature KG from literature, reports, and traces. Both graphs guide the LLM to deliver evidence-based, explainable analysis aligned with domain vocabulary and constraints.

Result: Evaluation using real production traces shows KORAL delivers expert-level diagnosis and recommendations with grounded explanations, improving reasoning transparency, guiding operator decisions, reducing manual effort, and providing actionable insights to improve service quality.

Conclusion: KORAL is the first end-to-end system combining LLMs and KGs for full-spectrum SSD reasoning (Descriptive, Predictive, Prescriptive, and What-if analysis). The authors release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis.

Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL

</details>


### [7] [Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A](https://arxiv.org/abs/2602.10262)
*Aaron Jarmusch,Connor Vitz,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: Characterization of AMD MI300A APU's advanced features (FP8 matrix cores, ACE concurrency, structured sparsity) using microbenchmarks to guide scheduling and optimization for HPC/HPC-AI workloads.


<details>
  <summary>Details</summary>
Motivation: Modern HPC and HPC-AI workloads increasingly rely on AMD MI300A's advanced features (FP8 matrix cores, asynchronous compute engines, structured sparsity), but their execution characteristics and system-level implications remain insufficiently understood.

Method: Execution-centric characterization using targeted microbenchmarks to analyze FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A APU, evaluating occupancy thresholds, fairness, throughput trade-offs, and context-dependent sparsity benefits.

Result: Quantified occupancy thresholds, fairness metrics, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. Evaluated representative case studies (transformer-style, concurrent, and mixed-precision kernels) showing how these effects translate to application-level performance and predictability.

Conclusion: Provides practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes, helping optimize HPC and HPC-AI workloads.

Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.

</details>


### [8] [Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores](https://arxiv.org/abs/2602.10378)
*Elliot L. Epstein,Rajat Vadiraj Dwaraknath,John Winnicki*

Main category: cs.DC

TL;DR: Flash-SD-KDE accelerates score-debiased kernel density estimation using Tensor Cores and matrix multiplication optimization, achieving up to 47× speedup over GPU baselines.


<details>
  <summary>Details</summary>
Motivation: Score-debiased KDE (SD-KDE) offers better asymptotic convergence than classical KDE but is significantly slower in practice due to empirical score computation, limiting its practical use at scale.

Method: Re-order SD-KDE computation to expose matrix-multiplication structure, enabling efficient use of Tensor Cores on GPUs for acceleration.

Result: 47× faster than strong SD-KDE GPU baseline, 3,300× faster than scikit-learn's KDE on 32k-sample 16D problem; completes 1M-sample 16D task with 131k queries in 2.3s on single GPU.

Conclusion: Flash-SD-KDE makes score-debiased density estimation practical at previously infeasible scales through GPU acceleration and Tensor Core optimization.

Abstract: Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.

</details>


### [9] [Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems](https://arxiv.org/abs/2602.10486)
*Vijay K. Garg,Rohan Garg*

Main category: cs.DC

TL;DR: Parallel and distributed methods for computing least fixed points of multiple monotone inflationary functions using coordinate-wise overwriting instead of join operations or contraction assumptions.


<details>
  <summary>Details</summary>
Motivation: Modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads, which classic sequential methods like Knaster-Tarski theorem cannot handle.

Method: Three convergence theorems under progressively relaxed synchronization: (1) interleaving semantics with fair scheduling, (2) parallel execution with update-only-on-change semantics, and (3) distributed execution with bounded staleness and i-locality constraints.

Result: First exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

Conclusion: The approach enables parallel and distributed algorithms for problems like transitive closure, stable marriage, shortest paths, and fair division with subsidy, overcoming limitations of prior work.

Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

</details>


### [10] [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)
*Youhe Jiang,Fangcheng Fu,Eiko Yoneki*

Main category: cs.DC

TL;DR: BOute is a quality-aware scheduling system that jointly optimizes heterogeneous query routing and model deployment across diverse GPUs for cost-efficient LLM serving using multi-objective Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLM deployments requires cost-efficient serving systems. Current approaches separately address algorithmic query routing or system-level GPU heterogeneity, but lack integrated co-design for optimal cost-performance trade-offs.

Method: BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize query routing strategies (directing queries to appropriate models) and model deployment configurations across heterogeneous GPUs, ensuring quality requirements while maximizing cost-efficiency.

Result: BOute outperforms state-of-the-art LLM serving systems by up to 157% (59% average) under identical cost budgets and quality requirements, or reduces serving costs by 15%-61% (38% average) while maintaining performance targets.

Conclusion: BOute effectively addresses the algorithm-system co-design challenge for cost-efficient LLM serving by jointly optimizing heterogeneous query routing and model deployment through Bayesian optimization, achieving significant cost savings while guaranteeing response quality.

Abstract: The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.

</details>


### [11] [Fine-Tuning GPT-5 for GPU Kernel Generation](https://arxiv.org/abs/2602.11000)
*Ali Tehrani,Yahya Emara,Essam Wissam,Wojciech Paluch,Waleed Atallah,Łukasz Dudziak,Mohamed S. Abdelfattah*

Main category: cs.DC

TL;DR: Reinforcement learning fine-tuning of GPT-5 for GPU kernel generation significantly improves correctness and performance over baseline models and traditional compilers.


<details>
  <summary>Details</summary>
Motivation: GPU kernel development is complex and requires specialized expertise, but LLMs struggle with GPU code generation due to limited training data, compiler biases, and poor hardware generalization, making supervised fine-tuning ineffective.

Method: Developed Makora's environment and tools for reinforcement learning fine-tuning of frontier models, specifically fine-tuning GPT-5 for Triton code generation using RL instead of supervised learning.

Result: Fine-tuned model improved kernel correctness from 43.7% to 77.0% (+33.3 percentage points), increased problems outperforming TorchInductor from 14.8% to 21.8%, and achieved state-of-the-art on KernelBench. In full coding agent setup, solved 97.4% of expanded KernelBench problems with 2.12x geometric mean speedup.

Conclusion: Targeted RL post-training can unlock LLM capabilities in specialized technical domains where supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.

Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.

</details>


### [12] [Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots](https://arxiv.org/abs/2602.11125)
*Animesh Maiti,Abhinav Chakraborty,Bibhuti Das,Subhash Bhagat,Krishnendu Mukhopadhyaya*

Main category: cs.DC

TL;DR: Distributed algorithms for min-sum uniform coverage of oblivious robots on line segments and circles, achieving optimal movement cost with solvability characterization.


<details>
  <summary>Details</summary>
Motivation: Study coordination of autonomous, anonymous, identical robots under Look-Compute-Move model to achieve uniform spacing while minimizing total movement distance, addressing fundamental swarm robotics problems.

Method: Deterministic distributed algorithms for line segments and circles under asynchronous scheduler with non-rigid motion; characterization of unsolvable configurations for circles.

Result: Optimal min-sum uniform coverage achieved for line segments; for circles, identified solvable/unsolvable configurations and provided optimal algorithm for solvable cases.

Conclusion: Complete characterization of deterministic solvability for min-sum coverage with oblivious robots, achieving optimal cost whenever possible under the considered robot model.

Abstract: We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [13] [Fungal systems for security and resilience](https://arxiv.org/abs/2602.10543)
*Andrew Adamatzky*

Main category: cs.ET

TL;DR: Fungi and mycelial networks proposed as biohybrid systems for security and resilience in extreme environments where conventional digital technologies fail.


<details>
  <summary>Details</summary>
Motivation: Modern security and critical systems face challenges in disrupted, uncertain environments with physical damage and degraded communications. Conventional digital technologies (centralized sensors, software-defined control, energy-intensive monitoring) struggle under such extreme conditions.

Method: Proposes fungi, particularly living mycelial networks, as a novel class of biohybrid systems. Maps fungal properties (decentralized control, embodied memory, autonomous repair) to security applications. Discusses how fungi can function as distributed sensing substrates, self-healing materials, and low-observability anomaly-detection layers.

Result: The paper presents a conceptual framework for using fungal networks as biohybrid systems for security, resilience, and protection in extreme environments. It maps fungal properties to specific applications in infrastructure protection, environmental monitoring, tamper evidence, and long-duration resilience.

Conclusion: Fungi offer promising alternatives to conventional digital technologies for security and resilience applications in extreme environments due to their decentralized nature, self-healing capabilities, and low observability, representing a novel approach to biohybrid systems for protection.

Abstract: Modern security, infrastructure, and safety-critical systems increasingly operate in environments characterised by disruption, uncertainty, physical damage, and degraded communications. Conventional digital technologies -- centralised sensors, software-defined control, and energy-intensive monitoring -- often struggle under such conditions. We propose fungi, and in particular living mycelial networks, as a novel class of biohybride systems for security, resilience, and protection in extreme environments. We discuss how fungi can function as distributed sensing substrates, self-healing materials, and low-observability anomaly-detection layers. We map fungal properties -- such as decentralised control, embodied memory, and autonomous repair -- to applications in infrastructure protection, environmental monitoring, tamper evidence, and long-duration resilience.

</details>
