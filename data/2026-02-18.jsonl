{"id": "2602.15166", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15166", "abs": "https://arxiv.org/abs/2602.15166", "authors": ["Tanner Andrulis", "Michael Gilbert", "Vivienne Sze", "Joel S. Emer"], "title": "Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation", "comment": null, "summary": "The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.\n  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers."}
{"id": "2602.15172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15172", "abs": "https://arxiv.org/abs/2602.15172", "authors": ["Michael Gilbert", "Tanner Andrulis", "Vivienne Sze", "Joel S. Emer"], "title": "The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation", "comment": null, "summary": "The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.\n  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.\n  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\\%$ higher than optimal) even when given $1000\\times$ the runtime ($>10$ hours)."}
{"id": "2602.15336", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15336", "abs": "https://arxiv.org/abs/2602.15336", "authors": ["Yogeswar Reddy Thota", "Setareh Rafatirad", "Homayoun Houman", "Tooraj Nikoubin"], "title": "Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction."}
{"id": "2602.15388", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15388", "abs": "https://arxiv.org/abs/2602.15388", "authors": ["Yonghao Wang", "Jiaxin Zhou", "Yang Yin", "Hongqin Lyu", "Zhiteng Chao", "Wenchao Ding", "Jing Ye", "Tiancheng Wang", "Huawei Li"], "title": "Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification", "comment": "6 pages, 6 figures", "summary": "While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage."}
{"id": "2602.15204", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15204", "abs": "https://arxiv.org/abs/2602.15204", "authors": ["Kevin Garner", "Polykarpos Thomadakis", "Nikos Chrisochoides"], "title": "Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation", "comment": "52 pages, 19 figures, 13 tables", "summary": "This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software."}
{"id": "2602.15032", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15032", "abs": "https://arxiv.org/abs/2602.15032", "authors": ["Nasir Kenarangui", "Laszlo B. Kish", "Arthur Powalka"], "title": "Pairwise XOR and XNOR Gates in Squeezed Instantaneous Noise Based Logic", "comment": "18 pages", "summary": "Instantaneous noise-based logic (INBL) is a novel computing approach that encodes binary information using stochastic processes. It uses 2M orthogonal stochastic reference noises for M noise-bits to construct an exponentially large Hilbert space (hyperspace) of dimension 2^M. INBL offers a classical alternative to quantum-style parallelism for specific problems with exponential speedup compared to classical algorithms. Building on recent work that introduced pairwise XOR and XNOR operations defined for a symmetric INBL scheme, this paper implements these gates for a squeezed INBL scheme. Hyperspace vectors are product strings corresponding to M-bit long binary numbers. The proposed operations can apply pairwise on hyperspace vectors and their superpositions, while remaining compatible with the squeezed reference system. We validate that the squeezed-scheme XOR/XNOR gate operations have correct Boolean behavior over both bitwise and targeted M-bit strings and demonstrate that the operations preserve instantaneous evaluation. The results show that the XOR/XNOR toolkit, previously developed for symmetric INBL, can be tailored for the squeezed scheme. This development is a key part of the gate set needed for more complex INBL algorithms in the squeezed INBL scheme and advances the objective of gate universality in INBL. It further strengthens the case for INBL as a flexible, classical computing framework that can emulate some structural advantages of quantum computation."}
{"id": "2602.15356", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15356", "abs": "https://arxiv.org/abs/2602.15356", "authors": ["Patrick G. Bridges", "Derek Schafer", "Jack Lange", "James B. White", "Anthony Skjellum", "Evan Suggs", "Thomas Hines", "Purushotham Bangalore", "Matthew G. F. Dosanjh", "Whit Schonbein"], "title": "Co-Design and Evaluation of a CPU-Free MPI GPU Communication Abstraction and Implementation", "comment": null, "summary": "Removing the CPU from the communication fast path is essential to efficient GPU-based ML and HPC application performance. However, existing GPU communication APIs either continue to rely on the CPU for communication or rely on APIs that place significant synchronization burdens on programmers. In this paper we describe the design, implementation, and evaluation of an MPI-based GPU communication API enabling easy-to-use, high-performance, CPU-free communication. This API builds on previously proposed MPI extensions and leverages HPE Slingshot 11 network card capabilities. We demonstrate the utility and performance of the API by showing how the API naturally enables CPU-free gather/scatter halo exchange communication primitives in the Cabana/Kokkos performance portability framework, and through a performance comparison with Cray MPICH on the Frontier and Tuolumne supercomputers. Results from this evaluation show up to a 50% reduction in medium message latency in simple GPU ping-pong exchanges and a 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs of the Frontier supercomputer."}
{"id": "2602.15033", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15033", "abs": "https://arxiv.org/abs/2602.15033", "authors": ["Naoya Onizawa", "Takahiro Hanyu"], "title": "High Convergence Rates of CMOS Invertible Logic Circuits Based on Many-Body Hamiltonians", "comment": "5 pages", "summary": "This paper introduces CMOS invertible-logic (CIL) circuits based on many-body Hamiltonians. CIL can realize probabilistic forward and backward operations of a function by annealing a corresponding Hamiltonian using stochastic computing. We have created a Hamiltonian that includes three-body interaction of spins (probabilistic nodes). It provides some degrees of freedom to design a simpler landscape of Hamiltonian (energy) than that of the conventional two-body Hamiltonian. The simpler landscape makes it easier to reach the global minimum energy. The proposed three-body CIL circuits are designed and evaluated with the conventional two-body CIL circuits, resulting in few-times higher convergence rates with negligible area overhead on FPGA."}
{"id": "2602.15379", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15379", "abs": "https://arxiv.org/abs/2602.15379", "authors": ["Zhihao Shu", "Md Musfiqur Rahman Sanim", "Hangyu Zheng", "Kunxiong Zhu", "Miao Yin", "Gagan Agrawal", "Wei Niu"], "title": "FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations", "comment": null, "summary": "The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs."}
{"id": "2602.15048", "categories": ["cs.ET", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.15048", "abs": "https://arxiv.org/abs/2602.15048", "authors": ["Akash Deep", "Andrea Samore", "Alistair McEwan", "Andrew McBride", "Shanmugam Kumar"], "title": "Full-Field Damage Monitoring in Architected Lattices Using In situ Electrical Impedance Tomography", "comment": null, "summary": "Electrical impedance tomography (EIT) enables non-invasive, spatially continuous reconstruction of internal conductivity distributions, providing full field sensing beyond conventional point measurements. Here, we report the first in situ implementation of EIT within a tunable architected lattice materials framework, enabling systematic exploration across a broad lattice design space while achieving real time monitoring of damage evolution, including early stage, prefracture events, in 3D printed multifunctional lattice composites. Lattices are designed via Voronoi based branch trunk branch motifs inspired by 2D wallpaper symmetries and fabricated using CNT infused photocurable resins, with nanoscale filler dispersion confirmed by field emission scanning electron microscopy. Sixteen electrodes distributed along the lattice periphery enable EIT measurements during quasi static tensile loading. Conductivity maps reconstructed using adjacent and across current injection schemes resolve sequential ligament fracture with high temporal resolution, with localised conductivity loss quantitatively coinciding with fracture sites, including regions remote from electrodes. Architectural tunability allows systematic control of EIT imaging sensitivity to early stage damage, while pronounced resistance discontinuities at failure further corroborate spatial localisation; global end to end resistance measurements complement macroscopic stress strain responses. Collectively, these results establish in situ EIT as a scalable, full field sensing modality for architected multifunctional materials, providing an experimentally validated pathway toward autonomous, intelligent materials and data rich material states that can inform digital twin frameworks for structural, biomedical, and energy related applications."}
{"id": "2602.15794", "categories": ["cs.DC", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15794", "abs": "https://arxiv.org/abs/2602.15794", "authors": ["Boris Sedlak", "Víctor Casamayor Pujol", "Ildefons Magrans de Abril", "Praveen Kumar Donta", "Adel N. Toosi", "Schahram Dustdar"], "title": "Service Orchestration in the Computing Continuum: Structural Challenges and Vision", "comment": null, "summary": "The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC."}
{"id": "2602.15049", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15049", "abs": "https://arxiv.org/abs/2602.15049", "authors": ["Mohamed Khalil Brik", "Ahmed Shokry", "Moustafa Youssef"], "title": "Quantum Optimization for Access Point Selection Under Budget Constraint", "comment": "9 pages, 13 figures. Accepted as a short paper at the 2026 IEEE International Conference on Quantum Communications, Networking, and Computing (QCNC 2026), Kobe, Japan, April 6-8, 2026", "summary": "Optimal Access Point (AP) selection is crucial for accurate indoor localization, yet it is constrained by budget, creating a trade-off between localization accuracy and deployment cost. Classical approaches to AP selection are often computationally expensive, hindering their application in large-scale 3D indoor environments.\n  In this paper, we introduce a quantum APs selection algorithm under a budget constraint. The proposed algorithm leverages quantum annealing to identify the most effective subset of APs allowed within a given budget. We formulate the APs selection problem as a quadratic unconstrained binary optimization (QUBO) problem, making it suitable for quantum annealing solvers. The proposed technique can drastically reduce infrastructure requirements with a negligible impact on performance.\n  We implement the proposed quantum algorithm and deploy it in a realistic 3D testbed. Our results show that the proposed approach can reduce the number of required APs by 96.1% while maintaining a comparable 3D localization accuracy. Furthermore, the proposed quantum approach outperforms classical AP selection algorithms in both accuracy and computational speed. Specifically, our technique achieves a time of 0.20 seconds, representing a speedup of 61 times over its classical counterpart, while reducing the mean localization error by 10% compared to the classical counterpart. For floor localization, the quantum approach achieves 73% floor accuracy, outperforming both the classical AP selection (58.6%) and even using the complete set of APs (70.4%). This highlights the promise of the proposed quantum APs selection algorithm for large-scale 3D localization."}
{"id": "2602.15477", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15477", "abs": "https://arxiv.org/abs/2602.15477", "authors": ["Asma Taheri Monfared", "Andrea Bombarda", "Angelo Gargantini", "Majid Haghparast"], "title": "Quantum Computing for Healthcare Digital Twin Systems", "comment": "13 pages, 3 figures", "summary": "The growing complexity of healthcare systems requires advanced computational models for real-time monitoring, secure data exchange, and intelligent decision-making. Digital Twins (DTs) provide virtual representations of physical healthcare entities, enabling continuous patient monitoring and personalized care. However, classical DT frameworks face limitations in scalability, computational efficiency, and security. Recent studies have introduced Quantum Digital Twins (QDTs) to enhance performance through quantum computing, addressing challenges such as quantum-resistant security and efficient task offloading in healthcare environments. Despite these advances, most existing QDT models remain constrained by fundamental challenges related to quantum hardware limitations, hybrid classical-quantum system integration, cloud-based quantum access, scalability, and clinical trust. This paper provides a comprehensive review of QDTs for healthcare, with a particular focus on identifying and analyzing the key challenges that currently hinder their real-world adoption. Furthermore, it outlines critical research directions and enabling strategies aimed at advancing the development of secure, reliable, and clinically viable quantum digital twin systems for next-generation healthcare applications."}
{"id": "2602.15794", "categories": ["cs.DC", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15794", "abs": "https://arxiv.org/abs/2602.15794", "authors": ["Boris Sedlak", "Víctor Casamayor Pujol", "Ildefons Magrans de Abril", "Praveen Kumar Donta", "Adel N. Toosi", "Schahram Dustdar"], "title": "Service Orchestration in the Computing Continuum: Structural Challenges and Vision", "comment": null, "summary": "The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC."}
