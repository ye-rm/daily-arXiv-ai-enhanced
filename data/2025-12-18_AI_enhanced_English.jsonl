{"id": "2512.15515", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15515", "abs": "https://arxiv.org/abs/2512.15515", "authors": ["Zhihan Xu", "Rajgopal Kannan", "Viktor K. Prasanna"], "title": "FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption", "comment": null, "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.\n  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.", "AI": {"tldr": "FAME is an FPGA accelerator for homomorphic encrypted matrix multiplication that achieves 221x speedup over CPU implementations through bandwidth-efficient design and novel datapath for fine-grained data reuse.", "motivation": "Homomorphic encryption enables secure computation on encrypted data but suffers from high computational costs, especially for matrix multiplication, which limits practical deployment in privacy-preserving applications like machine learning.", "method": "Developed a cost model for on-chip memory requirements, designed a novel datapath for Homomorphic Linear Transformation (HLT) that reduces off-chip memory traffic and on-chip memory demand through fine-grained data reuse, and implemented FAME FPGA accelerator supporting arbitrary matrix shapes and configurable HE parameters.", "result": "FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating scalability and practicality for large-scale consecutive HE MM and real-world workloads.", "conclusion": "The FPGA-based FAME accelerator effectively addresses the computational bottleneck of homomorphic encrypted matrix multiplication through bandwidth-efficient design, making HE more practical for real-world privacy-preserving applications."}}
{"id": "2512.15028", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15028", "abs": "https://arxiv.org/abs/2512.15028", "authors": ["Chin Fang", "Timothy Stitt", "Michael J. McManus", "Toshio Moriya"], "title": "Reexamining Paradigms of End-to-End Data Movement", "comment": "19 pages and 13 figures", "summary": "The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.", "AI": {"tldr": "The paper argues that focusing solely on network bandwidth is insufficient for high-performance data transfer, and identifies six common paradigms beyond network constraints that impact data movement, showing that bottlenecks often reside outside the network core.", "motivation": "The motivation is to challenge the network-centric view of high-performance data transfer that equates provisioned link speeds with practical data movement capabilities, recognizing that this view is incomplete for real-world edge-to-core data movement workflows.", "method": "The researchers investigated six common paradigms affecting data transfer, including network latency, TCP congestion control, CPU performance, and virtualization. They validated findings using a latency-emulation-capable testbed for WAN performance prediction and extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link between Switzerland and California.", "result": "Results show that principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design approach ensures consistent performance across different bandwidths (1 Gbps to 100 Gbps+), effectively closing the gap between benchmark results and complex production environments.", "conclusion": "The paper concludes that a comprehensive approach considering both network and host-side factors is essential for achieving sustainable high-performance data transfer, moving beyond the narrow focus on raw network bandwidth to address the full spectrum of edge-to-core data movement challenges."}}
{"id": "2512.15306", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15306", "abs": "https://arxiv.org/abs/2512.15306", "authors": ["Erik Schultheis", "Dan Alistarh"], "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs", "comment": null, "summary": "We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.", "AI": {"tldr": "LLMQ is a CUDA/C++ implementation for training medium-sized language models (3B-32B parameters) on affordable commodity GPUs, achieving efficient training on consumer-grade hardware with optimizations for memory and communication bottlenecks.", "motivation": "To enable training of medium-sized language models on affordable, commodity GPUs which have limited memory and slower communication compared to datacenter-grade hardware, making LLM training more accessible.", "method": "End-to-end CUDA/C++ implementation with optimizations including activation checkpointing, offloading, and copy-engine based collectives. Uses standard 8-bit training pipeline without additional algorithmic approximations.", "result": "Can train/fine-tune 7B model on single 16GB mid-range gaming card, or 32B model on workstation with 4 RTX 4090s. Achieves ~50% FLOP utilization and efficiency rivals production-scale systems on more expensive cloud-grade GPUs.", "conclusion": "LLMQ demonstrates that efficient medium-sized LLM training is possible on affordable commodity GPUs through targeted optimizations, making LLM development more accessible without sacrificing efficiency."}}
{"id": "2512.15595", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15595", "abs": "https://arxiv.org/abs/2512.15595", "authors": ["Daniel J\u00fcnger", "Kevin Kristensen", "Yunsong Wang", "Xiangyao Yu", "Bertil Schmidt"], "title": "Optimizing Bloom Filters for Modern GPU Architectures", "comment": "13 pages, 12 figures", "summary": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.", "AI": {"tldr": "GPU-optimized Bloom filters achieve massive throughput (billions of ops/sec) while maintaining high accuracy, overcoming traditional speed-precision trade-offs through vectorization, thread cooperation, and compute latency optimizations.", "motivation": "GPUs offer massive parallelism and high bandwidth ideal for accelerating Bloom filters, but GPU designs remain underexplored compared to CPU-optimized implementations, creating a research gap.", "method": "Explores GPU Bloom filter design space along three dimensions: vectorization, thread cooperation, and compute latency, with a focus on cache-aware optimizations when filters fit within GPU cache domain.", "result": "Optimized design achieves 11.35\u00d7 speedup for bulk filter lookup and 15.4\u00d7 for construction compared to state-of-the-art, reaching above 92% of practical speed-of-light on B200 GPU while maintaining high accuracy.", "conclusion": "GPU-optimized Bloom filters can overcome traditional speed-precision trade-offs, delivering both high throughput and accuracy, with modular CUDA/C++ implementation to be made openly available."}}
{"id": "2512.15659", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15659", "abs": "https://arxiv.org/abs/2512.15659", "authors": ["A. Jesse Jiryu Davis", "Murat Demirbas", "Lingzhi Deng"], "title": "LeaseGuard: Raft Leases Done Right", "comment": null, "summary": "Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.", "AI": {"tldr": "LeaseGuard is a novel lease algorithm for Raft that provides consistent reads with zero network overhead, improves write throughput 10x, and allows 99% of reads to succeed instantly on new leaders.", "motivation": "Raft-based systems need consistent reads but current approaches have high communication overhead (safety checks) or use vaguely specified lease protocols that hurt availability, leading to incorrect implementations.", "method": "LeaseGuard leverages Raft election guarantees, is formally specified in TLA+, and includes two novel optimizations: one for quick write throughput restoration and another for improved read availability during leader failover.", "result": "LeaseGuard reduces consistent read overhead from 1 to 0 network roundtrips, improves write throughput from ~1000 to ~10,000 writes/sec, and allows 99% of reads to succeed instantly on new leaders (vs. traditional leases that ban all reads).", "conclusion": "LeaseGuard provides a simple, rigorously specified lease algorithm that maximizes availability while ensuring read consistency in Raft-based distributed databases, solving key limitations of existing approaches."}}
{"id": "2512.15705", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15705", "abs": "https://arxiv.org/abs/2512.15705", "authors": ["Xuting Liu", "Daniel Alexander", "Siva Kesava Reddy Kakarla", "Behnaz Arzani", "Vincent Liu"], "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX", "comment": null, "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.", "AI": {"tldr": "DREX is an early-exit LLM inference system that uses dynamic rebatching to accelerate inference while preserving output quality by eliminating involuntary exits.", "motivation": "Traditional batching frameworks are inefficient for early-exit LLMs because they can't handle requests exiting at different times, forcing either uniform decisions that miss EE opportunities or premature exits that degrade quality.", "method": "Dynamic Rebatching reorganizes batches at each early-exit point: requests meeting exit criteria are processed immediately, while continuing requests are buffered and regrouped into new batches for deeper layers. DREX implements this with copy-free rebatching buffers and an EE/SLA-aware scheduler.", "result": "DREX improves throughput by 2-12% compared to baselines while maintaining output quality, and completely eliminates involuntary exits that degrade quality.", "conclusion": "Dynamic Rebatching with DREX provides an effective solution for accelerating early-exit LLM inference while preserving the quality guarantees intended by EE models."}}
