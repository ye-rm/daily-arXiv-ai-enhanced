{"id": "2602.02892", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02892", "abs": "https://arxiv.org/abs/2602.02892", "authors": ["Zhuolun Xiang", "Andrei Tonkikh", "Alexander Spiegelman"], "title": "Prefix Consensus For Censorship Resistant BFT", "comment": null, "summary": "Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.\n  We address this by introducing a new abstraction and protocol stack. First, we introduce \\emph{Prefix Consensus}, where parties input vectors and output $(v^{\\sf low},v^{\\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\\sf low}\\preceq v_j^{\\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.\n  We then define \\emph{Strong Prefix Consensus}, requiring agreement on the \\emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.\n  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \\emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.\n  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$."}
{"id": "2602.02987", "categories": ["cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.02987", "abs": "https://arxiv.org/abs/2602.02987", "authors": ["Ruihan Lin", "Zezhen Ding", "Zean Han", "Jiheng Zhang"], "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control", "comment": null, "summary": "Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \\emph{prefill} phase that processes user input, followed by a memory-bound \\emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics."}
{"id": "2602.03081", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03081", "abs": "https://arxiv.org/abs/2602.03081", "authors": ["Mohammadali Khodabandehlou", "Jared Coleman", "Niranjan Suri", "Bhaskar Krishnamachari"], "title": "Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling", "comment": null, "summary": "Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead."}
{"id": "2602.03246", "categories": ["cs.DC", "cs.NI", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.03246", "abs": "https://arxiv.org/abs/2602.03246", "authors": ["Tamoghna Sarkar", "Bhaskar Krishnamachari"], "title": "Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization", "comment": "10pages, 7 figures, submitted a version conference", "summary": "This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion."}
{"id": "2602.02579", "categories": ["cs.OS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02579", "abs": "https://arxiv.org/abs/2602.02579", "authors": ["Shihao Wang", "Jiahao Chen", "Yanqi Pan", "Hao Huang", "Yichen Hao", "Xiangyu Zou", "Wen Xia", "Wentao Zhang", "Haitao Wang", "Junhong Li", "Chongyang Qiu", "Pengfei Wang"], "title": "ProphetKV: User-Query-Driven Selective Recomputation for Efficient KV Cache Reuse in Retrieval-Augmented Generation", "comment": null, "summary": "The prefill stage of long-context Retrieval-Augmented Generation (RAG) is severely bottlenecked by computational overhead. To mitigate this, recent methods assemble pre-calculated KV caches of retrieved RAG documents (by a user query) and reprocess selected tokens to recover cross-attention between these pre-calculated KV caches. However, we identify a fundamental \"crowding-out effect\" in current token selection criteria: globally salient but user-query-irrelevant tokens saturate the limited recomputation budget, displacing the tokens truly essential for answering the user query and degrading inference accuracy.\n  We propose ProphetKV, a user-query-driven KV Cache reuse method for RAG scenarios. ProphetKV dynamically prioritizes tokens based on their semantic relevance to the user query and employs a dual-stage recomputation pipeline to fuse layer-wise attention metrics into a high-utility set. By ensuring the recomputation budget is dedicated to bridging the informational gap between retrieved context and the user query, ProphetKV achieves high-fidelity attention recovery with minimal overhead. Our extensive evaluation results show that ProphetKV retains 96%-101% of full-prefill accuracy with only a 20% recomputation ratio, while achieving accuracy improvements of 8.8%-24.9% on RULER and 18.6%-50.9% on LongBench over the state-of-the-art approaches (e.g., CacheBlend, EPIC, and KVShare)."}
{"id": "2602.02574", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2602.02574", "abs": "https://arxiv.org/abs/2602.02574", "authors": ["Edgard El Cham"], "title": "WritePolicyBench: Benchmarking Memory Write Policies under Byte Budgets", "comment": "10 pages, 4 figures", "summary": "We introduce WritePolicyBench, a benchmark for evaluating memory write policies: decision rules that choose what to store, merge, and evict under a strict byte budget while processing a stream with document/API drift. The benchmark provides (i) task generators with controlled non-stationarity, (ii) an explicit action interface for external memory, (iii) a byte-accurate cost model, and (iv) standardized metrics that measure both task success and budget efficiency."}
{"id": "2602.03444", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.03444", "abs": "https://arxiv.org/abs/2602.03444", "authors": ["Arivarasan Karmegam", "Lucianna Kiffer", "Antonio Fern√°ndez Anta"], "title": "Exploiting Multi-Core Parallelism in Blockchain Validation and Construction", "comment": null, "summary": "Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \\(p\\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \\(B\\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime."}
{"id": "2602.03474", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03474", "abs": "https://arxiv.org/abs/2602.03474", "authors": ["Shachar Meir", "David Peleg"], "title": "Recursive Energy Efficient Agreement", "comment": null, "summary": "Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \\emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system."}
{"id": "2602.03495", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03495", "abs": "https://arxiv.org/abs/2602.03495", "authors": ["Zeyu Zhu", "Gang Li", "Peisong Wang", "Zitao Mo", "Minnan Pei", "Zhuoran Song", "Xiaoyao Liang", "Jian Cheng"], "title": "DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs", "comment": null, "summary": "Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks."}
{"id": "2602.03802", "categories": ["cs.DC", "cs.AI", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.03802", "abs": "https://arxiv.org/abs/2602.03802", "authors": ["Grigory Begunov", "Alexander Tyurin"], "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods", "comment": null, "summary": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios."}
