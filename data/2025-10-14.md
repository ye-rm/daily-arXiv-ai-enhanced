<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [Man-Made Heuristics Are Dead. Long Live Code Generators!](https://arxiv.org/abs/2510.08803)
*Rohit Dwivedula,Divyanshu Saxena,Aditya Akella,Swarat Chaudhuri,Daehyeok Kim*

Main category: cs.OS

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Policy design for various systems controllers has conventionally been a
manual process, with domain experts carefully tailoring heuristics for the
specific instance in which the policy will be deployed. In this paper, we
re-imagine policy design via a novel automated search technique fueled by
recent advances in generative models, specifically Large Language Model
(LLM)-driven code generation. We outline the design and implementation of
PolicySmith, a framework that applies LLMs to synthesize instance-optimal
heuristics. We apply PolicySmith to two long-standing systems policies - web
caching and congestion control, highlighting the opportunities unraveled by
this LLM-driven heuristic search. For caching, PolicySmith discovers heuristics
that outperform established baselines on standard open-source traces. For
congestion control, we show that PolicySmith can generate safe policies that
integrate directly into the Linux kernel.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [2] [When to Reason: Semantic Router for vLLM](https://arxiv.org/abs/2510.08731)
*Chen Wang,Xunzhuo Liu,Yuhan Liu,Yue Zhu,Xiangxi Mo,Junchen Jiang,Huamin Chen*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) demonstrate substantial accuracy gains when
augmented with reasoning modes such as chain-of-thought and inference-time
scaling. However, reasoning also incurs significant costs in inference latency
and token usage, with environmental and financial impacts, which are
unnecessary for many simple prompts. We present a semantic router that
classifies queries based on their reasoning requirements and selectively
applies reasoning only when beneficial. Our approach achieves a 10.2 percentage
point improvement in accuracy on the MMLU-Pro benchmark while reducing response
latency by 47.1% and token consumption by 48.5% compared to direct inference
with vLLM. These results demonstrate that semantic routing offers an effective
mechanism for striking a balance between accuracy and efficiency in open-source
LLM serving systems

</details>


### [3] [Designing and Evaluating an AI-driven Immersive Multidisciplinary Simulation (AIMS) for Interprofessional Education](https://arxiv.org/abs/2510.08891)
*Ruijie Wang,Jie Lu,Bo Pei,Evonne Jones,Jamey Brinson,Timothy Brown*

Main category: cs.ET

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Interprofessional education has long relied on case studies and the use of
standardized patients to support teamwork, communication, and related
collaborative competencies among healthcare professionals. However, traditional
approaches are often limited by cost, scalability, and inability to mimic the
dynamic complexity of real-world clinical scenarios. To address these
challenges, we designed and developed AIMS (AI-Enhanced Immersive
Multidisciplinary Simulations), a virtual simulation that integrates a large
language model (Gemini-2.5-Flash), a Unity-based virtual environment engine,
and a character creation pipeline to support synchronized, multimodal
interactions between the user and the virtual patient. AIMS was designed to
enhance collaborative clinical reasoning and health promotion competencies
among students from pharmacy, medicine, nursing, and social work. A formal
usability testing session was conducted which participants assumed professional
roles on a healthcare team and engaged in a mix of scripted and unscripted
conversations. Participants explored the patient's symptoms, social context,
and care needs. Usability issues were identified (e.g., audio routing, response
latency) and used to guide subsequent refinements. Findings in general suggest
that AIMS supports realistic, profession-specific and contextually appropriate
conversations. We discussed both technical and pedagogical innovations of AIMS
and concluded with future directions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits](https://arxiv.org/abs/2510.08873)
*Haoran Jin,Jirong Yang,Yunpeng Liu,Barry Lyu,Kangqi Zhang,Nathaniel Bleier*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern AI acceleration faces a fundamental challenge: conventional
assumptions about memory requirements, batching effectiveness, and
latency-throughput tradeoffs are systemwide generalizations that ignore the
heterogeneous computational patterns of individual neural network operators.
However, going towards network-level customization and operator-level
heterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While
chiplet-based approaches have been proposed to amortize NRE costs, reuse
opportunities remain limited without carefully identifying which chiplets are
truly necessary. This paper introduces Mozart, a chiplet ecosystem and
accelerator codesign framework that systematically constructs low cost bespoke
application-specific integrated circuits (BASICs). BASICs leverage
operator-level disaggregation to explore chiplet and memory heterogeneity,
tensor fusion, and tensor parallelism, with place-and-route validation ensuring
physical implementability. The framework also enables constraint-aware
system-level optimization across deployment contexts ranging from datacenter
inference serving to edge computing in autonomous vehicles. The evaluation
confirms that with just 8 strategically selected chiplets, Mozart-generated
composite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy,
energy-cost product, energy-delay product (EDP), and energy-delay-cost product
compared to traditional homogeneous accelerators. For datacenter LLM serving,
Mozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In
speculative decoding, Mozart delivers throughput improvements of 24.6-58.6%
while reducing energy consumption by 38.6-45.6%. For autonomous vehicle
perception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under
real-time constraints.

</details>


### [5] [A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing](https://arxiv.org/abs/2510.08940)
*Abel Beyene,Zhongpan Wu,Yunus Dawji,Karim Hammad,Ebrahim Ghafar-Zadeh,Sebastian Magierowski*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing
importance in several life sciences fields as their small footprints enable a
broader range of use cases than their larger, stationary counterparts. However,
as currently designed, they lack sufficient embedded computing to process the
large volume of measurements generated by their internal sensory system. As a
consequence, they rely on external devices for additional processing
capability. This dependence on external processing places a significant
communication burden on the sequencer's embedded electronics. Moreover, it also
prevents a truly mobile solution for sequencing in real-time. Anticipating
next-generation machines that include suitably advanced processing, we present
a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide
semiconductor (CMOS). Our design, based on a general-purpose reduced
instruction set computing (RISC-V) core, also includes accelerators for DNA
detection that allow our system to demonstrate a 13X performance improvement
over commercial embedded multicore processors combined with a near 3000X boost
in energy efficiency.

</details>


### [6] [HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization](https://arxiv.org/abs/2510.09010)
*Yipu Zhang,Chaofang Ma,Jinming Ge,Lin Jiang,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

</details>


### [7] [Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge](https://arxiv.org/abs/2510.09339)
*Sebastian Magierowski,Zhongpan Wu,Abel Beyene,Karim Hammad*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts,
driving demand for efficient machine learning at the edge. This domain
leverages deep learning techniques familiar from speech and time-series
analysis for both low-level signal processing and high-level genomic
interpretation. Unlike audio, however, nanopore sequencing presents raw data
rates over 100X higher, requiring more aggressive compute and memory handling.
In this paper, we present a CMOS system-on-chip (SoC) designed for mobile
genetic analysis. Our approach combines a multi-core RISC-V processor with
tightly coupled accelerators for deep learning and bioinformatics. A
hardware/software co-design strategy enables energy-efficient operation across
a heterogeneous compute fabric, targeting real-time, on-device genome analysis.
This work exemplifies the integration of deep learning, edge computing, and
domain-specific hardware to advance next-generation mobile genomics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [9] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [10] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>
