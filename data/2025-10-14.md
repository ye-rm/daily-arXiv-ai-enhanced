<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 14]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: The paper proposes THEAS algorithm for dynamic resource adaptation in heterogeneous systems to improve energy efficiency while maintaining computational performance, especially for fluctuating workloads and non-pinning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance energy efficiency in heterogeneous systems where workloads fluctuate significantly and are not uniformly distributed, requiring dynamic adaptation of resource levels.

Method: Deployed THEAS algorithm that balances performance and power consumption through dynamic resource adaptation, with comparative analysis against CFS, EAS, HeteroSched, and Utility-Based Scheduling.

Result: THEAS algorithm ensures balance between performance and power consumption, making it suitable for real-time applications. Comparative analysis shows its advantages in adaptability, core selection, performance scaling, cache awareness, overhead, and real-time suitability.

Conclusion: THEAS algorithm provides an effective solution for dynamic resource adaptation in heterogeneous systems, achieving energy efficiency while maintaining necessary computational resources for fluctuating workloads.

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [2] [QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters](https://arxiv.org/abs/2510.09851)
*Haci Ismail Aslan,Syed Muhammad Mahmudul Haque,Joel Witzke,Odej Kao*

Main category: cs.DC

TL;DR: QONNECT is a vendor-agnostic orchestration framework that enables QoS-driven application deployment across heterogeneous Kubernetes clusters in cloud-fog-edge environments.


<details>
  <summary>Details</summary>
Motivation: Standard Kubernetes schedulers lack support for user-defined objectives like energy efficiency, cost optimization, and global performance, forcing manual cluster-by-cluster placement decisions.

Method: Distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster, using YAML-based interface for QoS specification.

Result: Demonstrated dynamic policy-driven microservice placement, automated failover, QoS-compliant rescheduling, and leader re-election on federated testbed with up to nine clusters using Istio Bookinfo application.

Conclusion: QONNECT bridges declarative deployment models with operational QoS goals, transforming the cloud-edge continuum into a unified, self-optimizing platform.

Abstract: Modern applications increasingly span across cloud, fog, and edge
environments, demanding orchestration systems that can adapt to diverse
deployment contexts while meeting Quality-of-Service (QoS) requirements.
Standard Kubernetes schedulers do not account for user-defined objectives such
as energy efficiency, cost optimization, and global performance, often leaving
operators to make manual, cluster-by-cluster placement decisions. To address
this need, we present QONNECT, a vendor-agnostic orchestration framework that
enables declarative, QoS-driven application deployment across heterogeneous
Kubernetes and K3s clusters. QONNECT introduces a distributed architecture
composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and
lightweight Resource Agents in each cluster. Through a minimal YAML-based
interface, users specify high-level QoS goals, which the system translates into
concrete placement and migration actions. Our implementation is evaluated on a
federated testbed of up to nine cloud-fog-edge clusters using the Istio
Bookinfo microservice application. The system demonstrates dynamic,
policy-driven microservice placement, automated failover, QoS-compliant
rescheduling, and leader re-election after node failure, all without manual
intervention. By bridging the gap between declarative deployment models and
operational QoS goals, QONNECT transforms the cloud-edge continuum into a
unified, self-optimizing platform.

</details>


### [3] [FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments](https://arxiv.org/abs/2510.10126)
*Sehar Zehra,Hassan Jamil Syed,Ummay Faseeha*

Main category: cs.DC

TL;DR: FedMon is a federated eBPF framework that combines kernel-level telemetry with federated learning for scalable, privacy-preserving anomaly detection across multiple Kubernetes clusters, achieving 94% precision with 60% bandwidth reduction.


<details>
  <summary>Details</summary>
Motivation: Kubernetes multi-cluster deployments need scalable and privacy-preserving anomaly detection. Existing eBPF monitors are limited to single clusters, while centralized approaches face bandwidth, privacy, and heterogeneity challenges.

Method: Lightweight eBPF agents capture syscalls and network events, extract local features, and share only model updates with a global server. A hybrid detection engine combines Variational Autoencoders with Isolation Forests for temporal pattern modeling and outlier detection.

Result: Deployed across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall, and F1-score of 0.92, while reducing bandwidth usage by 60% compared to centralized baselines.

Conclusion: FedMon enhances accuracy, scalability, and privacy, providing an effective defense for large-scale, multi-tenant cloud-native environments.

Abstract: Kubernetes multi-cluster deployments demand scalable and privacy-preserving
anomaly detection. Existing eBPF-based monitors provide low-overhead system and
network visibility but are limited to single clusters, while centralized
approaches incur bandwidth, privacy, and heterogeneity challenges. We propose
FedMon, a federated eBPF framework that unifies kernel-level telemetry with
federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF
agents capture syscalls and network events, extract local statistical and
sequence features, and share only model updates with a global server. A hybrid
detection engine combining Variational Autoencoders (VAEs) with Isolation
Forests enables both temporal pattern modeling and outlier detection. Deployed
across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,
and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to
centralized baselines. Results demonstrate that FedMon enhances accuracy,
scalability, and privacy, providing an effective defense for large-scale,
multi-tenant cloud-native environments.

</details>


### [4] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: This chapter overviews edge computing architecture, SLAs, and auto-scaling algorithms for hybrid cloud-edge environments, highlighting performance and configuration challenges in meeting service level agreements.


<details>
  <summary>Details</summary>
Motivation: Edge computing's decentralization through microservices requires low latencies to meet strict SLAs, but existing auto-scaling algorithms face performance issues and complexity in hybrid cloud-edge setups.

Method: Provides an overview of edge computing architecture, SLAs, and analyzes existing research on auto-scaling algorithms for edge environments, examining their benefits and drawbacks.

Result: Identifies that while cloud computing handles peak demands, hybrid cloud-edge environments are necessary for SLA compliance, but current scaling solutions have limitations.

Conclusion: The chapter synthesizes edge computing challenges and existing algorithmic approaches for SLA compliance, highlighting the need for improved scaling solutions in hybrid environments.

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [5] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: The paper challenges the conventional wisdom of using CPU limits for cloud-native applications, arguing that CPU limits harm performance and increase costs for latency-sensitive applications, and calls for rethinking autoscaling and billing paradigms.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from practical experiences showing CPU limits negatively impact application performance and costs, contradicting academic research and industry best practices that assume CPU limits are essential for operational safety.

Method: The authors provide empirical evidence against CPU limits and analyze the erroneous beliefs driving their indiscriminate adoption, while also identifying specific scenarios where CPU limits can be beneficial.

Result: The research demonstrates that CPU limits are detrimental for latency-sensitive applications and should be completely avoided in such cases, prompting a need to reconsider fundamental cloud resource management approaches.

Conclusion: CPU limits should be eschewed for latency-sensitive applications, requiring a fundamental rethinking of auto-scaling and billing paradigms, though they can still be beneficial in specific scenarios like background jobs when used judiciously.

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [6] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE is a speculative decoding-aware expert-offloading framework that accelerates Mixture-of-Experts (MoE) inference through speculative expert prefetching, cutoff-layer policies, and pipelined runtime, achieving 1.07-3.5× speedup over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Combining MoE with speculative decoding (SD) inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification, but existing MoE offloading systems are SD-agnostic and don't address this bottleneck.

Method: SP-MoE introduces: (1) speculative expert prefetching using structural correspondence between draft and target models, (2) cutoff-layer policy based on empirical profiles and analytical latency model, and (3) pipelined runtime with asynchronous prefetch threads and batched I/O.

Result: Extensive experiments show SP-MoE achieves 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.

Conclusion: SP-MoE successfully addresses the memory and bandwidth bottlenecks in SD-MoE inference through SD-aware expert-offloading and compute-communication pipelining.

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [7] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: FLAMMABLE is a multi-model federated learning framework that optimizes training by adapting client batch sizes and assigning multiple models based on system capabilities, achieving 1.1-10× faster time-to-accuracy and 1.3-5.4% higher accuracy than baselines.


<details>
  <summary>Details</summary>
Motivation: Multi-model FL faces amplified challenges from data, system, and model heterogeneity that existing single-model FL solutions cannot efficiently address, requiring a specialized framework.

Method: FLAMMABLE intelligently adapts client batch sizes and engages clients to train multiple carefully chosen models per round based on their system capabilities.

Result: Extensive evaluations show FLAMMABLE boosts time-to-accuracy performance by 1.1-10× and improves final model accuracy by 1.3-5.4% compared to baselines.

Conclusion: FLAMMABLE effectively addresses multi-model FL challenges and the authors developed the first benchmark platform for reproducible MMFL research.

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [8] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: DCP is a dynamic context parallel training framework that uses fine-grained blockwise partitioning to adapt to varying sequence lengths and attention patterns, reducing communication overhead and improving computation balance compared to static parallelization methods.


<details>
  <summary>Details</summary>
Motivation: Existing context parallel methods use static configurations that don't account for the dynamic nature of training data, particularly variations in sequence lengths and attention patterns, leading to unnecessary communication overhead and imbalanced computation.

Method: DCP introduces fine-grained blockwise partitioning of both data and computation, enabling flexible mapping of data and computation blocks to devices to adapt to varying sequence characteristics.

Result: DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns, with end-to-end training speed-ups of 0.94x~1.16x for causal masks and 1.00x~1.46x for sparse masks.

Conclusion: DCP effectively addresses the limitations of static context parallel methods by dynamically adapting to data characteristics, significantly reducing communication overhead and improving training efficiency for long-context generative AI models.

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [9] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [10] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [11] [A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems](https://arxiv.org/abs/2510.11189)
*Yangyang Wen,Paul Townend,Per-Olov Östberg,Abel Souza,Clément Courageux-Sudan*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As microservice-based systems scale across the cloud-edge continuum,
traditional centralized scheduling mechanisms increasingly struggle with
latency, coordination overhead, and fault tolerance. This paper presents a new
architectural direction: leveraging service mesh sidecar proxies as
decentralized, in-situ schedulers to enable scalable, low-latency coordination
in large-scale, cloud-native environments. We propose embedding lightweight,
autonomous scheduling logic into each sidecar, allowing scheduling decisions to
be made locally without centralized control. This approach leverages the
growing maturity of service mesh infrastructures, which support programmable
distributed traffic management. We describe the design of such an architecture
and present initial results demonstrating its scalability potential in terms of
response time and latency under varying request rates. Rather than delivering a
finalized scheduling algorithm, this paper presents a system-level
architectural direction and preliminary evidence to support its scalability
potential.

</details>


### [12] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [13] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>


### [14] [A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem](https://arxiv.org/abs/2510.11697)
*Matteo Mordacchini,Emanuele Carlini,Patrizio Dazzi*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in
a decentralized network. MWVC, a classical NP-hard problem, is foundational in
applications such as network monitoring and resource placement. We propose a
fully decentralized protocol where each node makes decisions using only local
knowledge and communicates with its neighbors. The method is adaptive,
communication-efficient, and avoids centralized coordination. We evaluate the
protocol on real-world and synthetic graphs, comparing it to both centralized
and decentralized baselines. Our results demonstrate competitive solution
quality with reduced communication overhead, highlighting the feasibility of
MWVC computation in decentralized environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor](https://arxiv.org/abs/2510.10484)
*Buqing Xu,Jianfeng Zhu,Yichi Zhang,Qinyi Cai,Guanhua Li,Shaojun Wei,Leibo Liu*

Main category: cs.PF

TL;DR: CAPSim is a novel ML-based CPU simulator that uses attention-based neural networks and instruction trace sampling to predict execution time of benchmarks much faster than traditional cycle-accurate simulators like GEM5.


<details>
  <summary>Details</summary>
Motivation: Traditional CPU simulators like GEM5 are slow for simulating modern multi-core CPUs, and existing ML approaches only estimate cycles for basic blocks rather than complete programs.

Method: Uses attention-based neural network performance predictor and instruction trace sampling with context annotation. The attention mechanism captures long-range influences in instruction traces and focuses on critical context information.

Result: CAPSim achieves 2.2-8.3x speedup compared to GEM5's O3 simulator while maintaining accuracy, and outperforms state-of-the-art deep learning approaches.

Conclusion: CAPSim demonstrates that ML-based approaches can effectively accelerate CPU simulation while maintaining prediction accuracy for complete benchmark programs.

Abstract: CPU simulators are vital for computer architecture research, primarily for
estimating performance under different programs. This poses challenges for fast
and accurate simulation of modern CPUs, especially in multi-core systems.
Modern CPU peformance simulators such as GEM5 adopt the cycle-accurate and
event-driven approach, which is timeconsuming to simulate the extensive
microarchitectural behavior of a real benchmark running on out-of-order CPUs.
Recently, machine leaning based approach has been proposed to improve
simulation speed, but they are currently limited to estimating the cycles of
basic blocks rather than the complete benchmark program. This paper introduces
a novel ML-based CPU simulator named CAPSim, which uses an attention-based
neural network performance predictor and instruction trace sampling method
annotated with context. The attention mechanism effectively captures long-range
influence within the instruction trace, emphasizing critical context
information. This allows the model to improve performance prediction accuracy
by focusing on important code instruction. CAPSim can predict the execution
time of unseen benchmarks at a significantly fast speed compared with an
accurate O3 simulator built with gem5. Our evaluation on a commercial Intel
Xeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to
using gem5 built simulator, which is superior to the cutting-edge deep learning
approach

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: ISAAC is an LLM-aided CPU verification framework that uses multi-agent stimulus generation with micro-architectural knowledge and FPGA-parallel simulation to achieve massive speed-ups and detect previously unknown bugs.


<details>
  <summary>Details</summary>
Motivation: CPU verification is a critical bottleneck in IC development, with current differential testing methods facing issues in stimulus generation (low-quality, redundant tests) and simulation infrastructure (slow feedback, limited visibility).

Method: ISAAC uses a multi-agent stimulus engine infused with micro-architectural knowledge and historical bug patterns for targeted test generation, plus a lightweight forward-snapshot mechanism and decoupled co-simulation architecture enabling single ISS to drive multiple DUTs in parallel on FPGAs.

Result: Achieved up to 17,536x speed-up over software RTL simulation and detected several previously unknown bugs, with two specific bugs reported in the paper.

Conclusion: ISAAC successfully addresses CPU verification bottlenecks through LLM-aided stimulus generation and FPGA-parallel simulation, demonstrating significant performance improvements and bug detection capabilities on mature CPU designs.

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [17] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: ADiP is a novel adaptive-precision systolic array architecture for efficient matrix multiplication acceleration in transformers, supporting multiple precision modes and achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Transformers require efficient acceleration due to substantial memory and computational requirements, and quantization plays a vital role in reducing memory usage while enabling dynamic precision adjustment for enhanced matrix multiplication.

Method: The architecture consists of NxN adaptive-precision processing elements and shared accumulators, supporting multiple computation modes including symmetric single-matrix multiplication and asymmetric multi-matrix multiplication with shared input matrix. It adapts to different precisions (8bitx8bit, 8bitx4bit, 8bitx2bit) and uses analytical models for latency and throughput analysis.

Result: ADiP achieves up to 4x higher computational throughput, latency improvement up to 53.6%, and energy improvement up to 24.4% for BitNet-1.58B MHA workloads. At 64x64 size with 4096 PEs, it achieves peak throughput of 8.192 TOPS (8bitx8bit), 16.384 TOPS (8bitx4bit), and 32.768 TOPS (8bitx2bit).

Conclusion: ADiP provides an efficient adaptive-precision systolic array architecture that significantly enhances matrix multiplication performance for transformer workloads through dynamic precision adjustment and improved data reuse.

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [18] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Bhasha-Rupantarika is a lightweight multilingual translation system using algorithm-hardware codesign for resource-limited settings, achieving 4.1x model size reduction and 4.2x inference speedup through ultra-low precision quantization on FPGA accelerators.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient multilingual translation system suitable for deployment in resource-limited IoT devices, addressing the need for real-time translation in low-resource linguistic contexts.

Method: Uses algorithm-hardware codesign with sub-octet precision quantization (FP8, INT8, INT4, FP4) and FPGA accelerators for deployment, focusing on bidirectional translation between Indian and international languages.

Result: Achieved 4.1x model size reduction (FP4), 4.2x inference speedup, 66 tokens/s throughput (4.8x improvement), 1.96x reduction in LUTs, 1.65x decrease in FFs, and 2.2x throughput enhancement compared to OPU.

Conclusion: The system provides a viable solution for deployable multilingual AI systems through quantization-aware translation and hardware efficiency, with publicly available code and datasets for reproducibility.

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [19] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: This paper presents an in-storage computing architecture using 3D Ferroelectric NAND (FeNAND) combined with hyperdimensional computing to enable efficient mass spectrometry library search, achieving 43x speedup and 21x energy efficiency improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of mass spectrometry data (exceeding hundreds of terabytes) creates challenges for efficient large-scale library search in drug discovery, as traditional processors struggle with this data volume, making in-storage computing a promising alternative.

Method: The authors propose an ISP architecture using 3D FeNAND structure with higher density and speed, integrated with hyperdimensional computing (HDC) for parallel processing and dual-bound approximate matching (D-BAM) distance metric tailored to FeNAND structure to parallelize vector computations.

Result: The proposed method achieves 43x speedup and 21x higher energy efficiency compared to state-of-the-art 3D NAND methods while maintaining comparable accuracy for mass spectrometry spectral library search.

Conclusion: The combination of FeNAND-based in-storage computing with hyperdimensional computing and specialized distance metrics enables efficient processing of large-scale mass spectrometry data, overcoming throughput limitations of traditional NAND structures for computational applications.

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [20] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: An automated framework for accelerating sparse LLM inference on compute-in-memory accelerators using novel mapping and scheduling strategies that exploit block-diagonal sparsity.


<details>
  <summary>Details</summary>
Motivation: Structured sparsity reduces LLM size but conventional architectures struggle with memory-bound decode stages. CIM architectures help but naive sparse matrix mapping leads to poor array utilization and computational inefficiency.

Method: Developed an automated framework with novel mapping and scheduling strategies that specifically exploit block-diagonal sparsity patterns in sparse LLMs to optimize CIM array utilization.

Result: Achieved over 50% improvement in CIM array utilization, with more than 4x reduction in both memory footprint and number of required floating-point operations.

Conclusion: The proposed framework effectively bridges the gap between sparse LLMs and CIM accelerators, significantly improving computational efficiency and enabling more practical deployment of large language models on resource-constrained systems.

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>
