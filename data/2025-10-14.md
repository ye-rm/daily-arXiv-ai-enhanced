<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 14]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: The paper proposes THEAS algorithm for dynamic resource adaptation in heterogeneous systems to balance performance and power consumption, especially for fluctuating workloads and non-pinning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance energy efficiency while maintaining computational resources in systems with significantly fluctuating workloads and non-uniform workload distribution in heterogeneous systems.

Method: Deployed THEAS algorithm that dynamically adapts resource levels and ensures balance between performance and power consumption. Compared with CFS, EAS, HeteroSched, and Utility-Based Scheduling.

Result: Comparative analysis shows THEAS's effectiveness across adaptability, core selection, performance scaling, cache awareness, overhead, and real-time suitability metrics.

Conclusion: THEAS algorithm is suitable for wide range of real-time applications by effectively balancing performance and power consumption in dynamic workload scenarios.

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [2] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: The paper challenges the conventional wisdom of using CPU limits for cloud-native applications, arguing that CPU limits often harm performance and increase costs for latency-sensitive applications, and calls for rethinking autoscaling and billing paradigms.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from practical experiences showing that CPU limits negatively impact application performance and costs, contradicting academic research and industry best practices that assume CPU limits are essential for operational safety.

Method: The authors provide empirical evidence against the indiscriminate use of CPU limits and analyze scenarios where CPU limits can be beneficial when used judiciously.

Result: The research demonstrates that CPU limits are detrimental for latency-sensitive applications and should be completely avoided for such workloads, while being selectively useful only for specific cases like background jobs.

Conclusion: The paper concludes that the widespread adoption of CPU limits is based on erroneous beliefs, calls for fundamental rethinking of autoscaling and billing approaches, and opens new research directions for resource management without CPU limits.

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [3] [QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters](https://arxiv.org/abs/2510.09851)
*Haci Ismail Aslan,Syed Muhammad Mahmudul Haque,Joel Witzke,Odej Kao*

Main category: cs.DC

TL;DR: QONNECT is a vendor-agnostic orchestration framework that enables QoS-driven application deployment across heterogeneous Kubernetes clusters in cloud-fog-edge environments, addressing limitations of standard Kubernetes schedulers.


<details>
  <summary>Details</summary>
Motivation: Standard Kubernetes schedulers lack support for user-defined objectives like energy efficiency, cost optimization, and global performance, forcing operators to make manual placement decisions across multiple clusters.

Method: QONNECT uses a distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster. Users specify QoS goals via YAML interface, which the system translates into placement and migration actions.

Result: Evaluation on a federated testbed with up to nine cloud-fog-edge clusters using Istio Bookinfo microservice application demonstrated dynamic policy-driven placement, automated failover, QoS-compliant rescheduling, and leader re-election without manual intervention.

Conclusion: QONNECT bridges the gap between declarative deployment models and operational QoS goals, transforming the cloud-edge continuum into a unified, self-optimizing platform.

Abstract: Modern applications increasingly span across cloud, fog, and edge
environments, demanding orchestration systems that can adapt to diverse
deployment contexts while meeting Quality-of-Service (QoS) requirements.
Standard Kubernetes schedulers do not account for user-defined objectives such
as energy efficiency, cost optimization, and global performance, often leaving
operators to make manual, cluster-by-cluster placement decisions. To address
this need, we present QONNECT, a vendor-agnostic orchestration framework that
enables declarative, QoS-driven application deployment across heterogeneous
Kubernetes and K3s clusters. QONNECT introduces a distributed architecture
composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and
lightweight Resource Agents in each cluster. Through a minimal YAML-based
interface, users specify high-level QoS goals, which the system translates into
concrete placement and migration actions. Our implementation is evaluated on a
federated testbed of up to nine cloud-fog-edge clusters using the Istio
Bookinfo microservice application. The system demonstrates dynamic,
policy-driven microservice placement, automated failover, QoS-compliant
rescheduling, and leader re-election after node failure, all without manual
intervention. By bridging the gap between declarative deployment models and
operational QoS goals, QONNECT transforms the cloud-edge continuum into a
unified, self-optimizing platform.

</details>


### [4] [FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments](https://arxiv.org/abs/2510.10126)
*Sehar Zehra,Hassan Jamil Syed,Ummay Faseeha*

Main category: cs.DC

TL;DR: FedMon is a federated eBPF framework that combines kernel-level telemetry with federated learning for scalable, privacy-preserving anomaly detection across multiple Kubernetes clusters.


<details>
  <summary>Details</summary>
Motivation: Kubernetes multi-cluster deployments need scalable and privacy-preserving anomaly detection, but existing eBPF monitors are limited to single clusters while centralized approaches face bandwidth, privacy, and heterogeneity challenges.

Method: Uses lightweight eBPF agents to capture syscalls and network events, extracts local statistical and sequence features, and shares only model updates via federated learning. Combines Variational Autoencoders with Isolation Forests for hybrid temporal pattern modeling and outlier detection.

Result: Achieved 94% precision, 91% recall, and 0.92 F1-score across three Kubernetes clusters, while reducing bandwidth usage by 60% compared to centralized approaches.

Conclusion: FedMon enhances accuracy, scalability, and privacy, providing effective defense for large-scale, multi-tenant cloud-native environments.

Abstract: Kubernetes multi-cluster deployments demand scalable and privacy-preserving
anomaly detection. Existing eBPF-based monitors provide low-overhead system and
network visibility but are limited to single clusters, while centralized
approaches incur bandwidth, privacy, and heterogeneity challenges. We propose
FedMon, a federated eBPF framework that unifies kernel-level telemetry with
federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF
agents capture syscalls and network events, extract local statistical and
sequence features, and share only model updates with a global server. A hybrid
detection engine combining Variational Autoencoders (VAEs) with Isolation
Forests enables both temporal pattern modeling and outlier detection. Deployed
across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,
and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to
centralized baselines. Results demonstrate that FedMon enhances accuracy,
scalability, and privacy, providing an effective defense for large-scale,
multi-tenant cloud-native environments.

</details>


### [5] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: This chapter overviews edge computing architecture, its uses, benefits, and challenges for resource scaling, focusing on meeting Service Level Agreements (SLAs) through auto-scaling algorithms in hybrid cloud-edge environments.


<details>
  <summary>Details</summary>
Motivation: Edge computing enables decentralization via microservices requiring low latencies to meet SLAs for performance, reliability, and availability. Hybrid cloud-edge environments are needed to handle peak demands while ensuring SLA compliance, but existing auto-scaling algorithms face performance issues and configuration complexity.

Method: The chapter provides a brief overview of edge computing architecture, its uses, benefits, and scaling challenges. It introduces Service Level Agreements and reviews existing research on algorithms used in edge computing to meet these agreements.

Result: The analysis covers the benefits and drawbacks of various auto-scaling algorithms proposed for edge computing environments to achieve SLA compliance.

Conclusion: The chapter synthesizes current research on edge computing resource scaling and SLA compliance algorithms, highlighting both their advantages and limitations in hybrid cloud-edge environments.

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [6] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE is a speculative decoding-aware expert-offloading framework that accelerates Mixture-of-Experts (MoE) inference through speculative expert prefetching, cutoff-layer policy, and pipelined runtime.


<details>
  <summary>Details</summary>
Motivation: Combining MoE with speculative decoding (SD) inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification, which existing MoE offloading systems don't address.

Method: SP-MoE introduces speculative expert prefetching using structural correspondence between draft and target models, a cutoff-layer policy to bound prefetch depth, and a pipelined runtime with asynchronous prefetch threads and batched I/O.

Result: SP-MoE achieves 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.

Conclusion: SP-MoE is the first SD-aware expert-offloading framework that effectively addresses the memory and bandwidth bottlenecks in MoE inference with speculative decoding.

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [7] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: FLAMMABLE is a comprehensive Multi-Model Federated Learning framework that optimizes training by intelligently adapting client batch sizes and assigning multiple models based on client capabilities, achieving significant performance improvements over existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing FL solutions and naive extensions don't efficiently address the amplified challenges in MMFL settings, which include data heterogeneity, system heterogeneity, and additional heterogeneity across multiple models being trained in parallel.

Method: FLAMMABLE optimizes model training by intelligently adapting client batch sizes while engaging clients to train multiple carefully chosen models depending on their system capabilities in each training round.

Result: Extensive evaluations show FLAMMABLE boosts MMFL time-to-accuracy performance by 1.1-10.0× while improving final model accuracy by 1.3-5.4% compared to several known baselines.

Conclusion: FLAMMABLE effectively addresses the challenges of Multi-Model Federated Learning and the authors also developed the first benchmark platform for MMFL to enable future reproducible research in this area.

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [8] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: DCP is a dynamic context parallel training framework that uses fine-grained blockwise partitioning to adapt to varying sequence lengths and attention patterns, reducing communication overhead and improving computation balance compared to static methods.


<details>
  <summary>Details</summary>
Motivation: Existing context parallel methods use static configurations that don't account for the dynamic nature of training data, leading to unnecessary communication overhead and imbalanced computation due to variability in sequence lengths and token relationships.

Method: Introduces fine-grained blockwise partitioning of both data and computation, enabling flexible mapping of data and computation blocks to devices to adapt to varying sequence characteristics.

Result: DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns, with end-to-end training speed-up of 0.94x~1.16x for causal masks and 1.00x~1.46x for sparse masks.

Conclusion: DCP effectively addresses the limitations of static context parallel methods by dynamically adapting to data characteristics, significantly reducing communication overhead and improving training efficiency.

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [9] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: A spin-free, kernel-lock-free mutex that cooperates with user-mode schedulers, formally proven FIFO-fair and linearizable using CSP/FDR, designed for ProcessJ language to manage shared channel access.


<details>
  <summary>Details</summary>
Motivation: To create a fair mutex that works with user-mode schedulers without kernel involvement, providing formal guarantees for process-oriented languages managing shared communication channels.

Method: Uses a lock-free queue to park waiting processes, ensuring FIFO ordering for access to shared resources. Protocol designed for ProcessJ language with formal CSP modeling and FDR verification.

Result: Successfully demonstrated that the protocol behaves as a locking mutex with FIFO fairness and linearizability, with reusable proof methods for coroutine runtime designs.

Conclusion: The presented mutex provides formally verified fairness and correctness for shared resource access in process-oriented programming, with reusable verification techniques for similar systems.

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [10] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: A new framework called FIDRS for integrated distributed and reliable systems that improves performance, speed, and reliability using heterogeneous distributed databases and RMSD algorithm.


<details>
  <summary>Details</summary>
Motivation: To address limitations in previous integrated systems frameworks by improving satisfaction, performance, and reliability while removing existing problems.

Method: Uses three parts: analysis of previous frameworks, new framework design with heterogeneous distributed database technique, and RMSD algorithm for faster response times in big databases.

Result: Simulation results show improved efficiency, performance, and reliability compared to previous frameworks, with reduced response times.

Conclusion: The FIDRS framework successfully increases efficiency, performance, and reliability of integrated systems while solving problems from previous frameworks.

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [11] [A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems](https://arxiv.org/abs/2510.11189)
*Yangyang Wen,Paul Townend,Per-Olov Östberg,Abel Souza,Clément Courageux-Sudan*

Main category: cs.DC

TL;DR: A decentralized scheduling approach using service mesh sidecar proxies as in-situ schedulers for scalable cloud-edge microservices.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized scheduling struggles with latency, coordination overhead, and fault tolerance as microservices scale across cloud-edge continuum.

Method: Embed lightweight autonomous scheduling logic into service mesh sidecar proxies, enabling local scheduling decisions without centralized control.

Result: Initial results demonstrate scalability potential with improved response time and latency under varying request rates.

Conclusion: This paper presents a system-level architectural direction using service mesh infrastructure for decentralized scheduling rather than a finalized algorithm.

Abstract: As microservice-based systems scale across the cloud-edge continuum,
traditional centralized scheduling mechanisms increasingly struggle with
latency, coordination overhead, and fault tolerance. This paper presents a new
architectural direction: leveraging service mesh sidecar proxies as
decentralized, in-situ schedulers to enable scalable, low-latency coordination
in large-scale, cloud-native environments. We propose embedding lightweight,
autonomous scheduling logic into each sidecar, allowing scheduling decisions to
be made locally without centralized control. This approach leverages the
growing maturity of service mesh infrastructures, which support programmable
distributed traffic management. We describe the design of such an architecture
and present initial results demonstrating its scalability potential in terms of
response time and latency under varying request rates. Rather than delivering a
finalized scheduling algorithm, this paper presents a system-level
architectural direction and preliminary evidence to support its scalability
potential.

</details>


### [12] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: This paper explores distributed computing techniques for large language models (LLMs) from two perspectives: democratizing LLMs to run on consumer-grade computers with metaheuristics-based modifications, and comparing three state-of-the-art LLM serving techniques.


<details>
  <summary>Details</summary>
Motivation: Current LLMs with billions of parameters are too large for single computing nodes to handle training, fine-tuning, or inference, necessitating distributed computing approaches to properly utilize these models.

Method: The research examines distributed computing techniques from two angles: 1) Techniques to run large models on consumer-grade computers with novel metaheuristics-based modifications to existing systems, and 2) A comparative study of three state-of-the-art LLM serving techniques.

Result: The paper presents both a novel metaheuristics-based modification for running LLMs on consumer hardware and a comparative analysis of three advanced LLM serving techniques, though specific performance metrics are not detailed in the abstract.

Conclusion: Distributed computing techniques are essential for effectively utilizing large language models, and the research demonstrates approaches for both democratizing access to LLMs on consumer hardware and evaluating modern serving systems.

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [13] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: The paper analyzes performance bottlenecks in S_N transport solvers on modern many-core architectures and proposes an Asynchronous Many-Task (AMT) algorithm that improves computational performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Discrete ordinates S_N transport solvers on unstructured meshes face scaling challenges due to complex data dependencies, memory access patterns, and high-dimensional domains, particularly on modern many-core architectures with high core counts.

Method: The authors review performance bottlenecks in existing transport solvers, survey performance across various compute hardware, and develop a new Asynchronous Many-Task (AMT) algorithm for shared memory parallelism.

Result: The proposed AMT algorithm demonstrates increased computational performance compared to the existing method, with analysis explaining why performance improvements are achieved.

Conclusion: The new Asynchronous Many-Task approach effectively addresses scaling limitations in S_N transport solvers and provides better performance on modern many-core architectures.

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>


### [14] [A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem](https://arxiv.org/abs/2510.11697)
*Matteo Mordacchini,Emanuele Carlini,Patrizio Dazzi*

Main category: cs.DC

TL;DR: A decentralized protocol for computing Minimum Weighted Vertex Cover (MWVC) using only local knowledge and neighbor communication, achieving competitive solution quality with low communication overhead.


<details>
  <summary>Details</summary>
Motivation: MWVC is a fundamental NP-hard problem with applications in network monitoring and resource placement, but existing approaches often require centralized coordination which is impractical in decentralized networks.

Method: Proposed a fully decentralized protocol where each node makes decisions based only on local information and communicates with immediate neighbors, avoiding centralized coordination while being adaptive and communication-efficient.

Result: Evaluation on real-world and synthetic graphs shows competitive solution quality compared to both centralized and decentralized baselines, with significantly reduced communication overhead.

Conclusion: The study demonstrates the feasibility of computing MWVC in decentralized environments through local decision-making and neighbor communication, offering practical solutions for distributed network applications.

Abstract: We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in
a decentralized network. MWVC, a classical NP-hard problem, is foundational in
applications such as network monitoring and resource placement. We propose a
fully decentralized protocol where each node makes decisions using only local
knowledge and communicates with its neighbors. The method is adaptive,
communication-efficient, and avoids centralized coordination. We evaluate the
protocol on real-world and synthetic graphs, comparing it to both centralized
and decentralized baselines. Our results demonstrate competitive solution
quality with reduced communication overhead, highlighting the feasibility of
MWVC computation in decentralized environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor](https://arxiv.org/abs/2510.10484)
*Buqing Xu,Jianfeng Zhu,Yichi Zhang,Qinyi Cai,Guanhua Li,Shaojun Wei,Leibo Liu*

Main category: cs.PF

TL;DR: CAPSim is a novel ML-based CPU simulator that uses attention-based neural networks to predict execution time of benchmarks, achieving 2.2-8.3x speedup over gem5 simulators while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional CPU simulators like gem5 are time-consuming for modern multi-core systems, and existing ML approaches only estimate basic block cycles rather than complete program execution time.

Method: Uses attention-based neural network performance predictor with instruction trace sampling annotated with context, capturing long-range influence within instruction traces and emphasizing critical context information.

Result: CAPSim achieves 2.2-8.3x speedup compared to gem5 built simulator while maintaining prediction accuracy for unseen benchmarks, superior to state-of-the-art deep learning approaches.

Conclusion: The attention mechanism effectively improves performance prediction accuracy by focusing on important code instructions, enabling fast and accurate simulation of complete benchmark programs.

Abstract: CPU simulators are vital for computer architecture research, primarily for
estimating performance under different programs. This poses challenges for fast
and accurate simulation of modern CPUs, especially in multi-core systems.
Modern CPU peformance simulators such as GEM5 adopt the cycle-accurate and
event-driven approach, which is timeconsuming to simulate the extensive
microarchitectural behavior of a real benchmark running on out-of-order CPUs.
Recently, machine leaning based approach has been proposed to improve
simulation speed, but they are currently limited to estimating the cycles of
basic blocks rather than the complete benchmark program. This paper introduces
a novel ML-based CPU simulator named CAPSim, which uses an attention-based
neural network performance predictor and instruction trace sampling method
annotated with context. The attention mechanism effectively captures long-range
influence within the instruction trace, emphasizing critical context
information. This allows the model to improve performance prediction accuracy
by focusing on important code instruction. CAPSim can predict the execution
time of unseen benchmarks at a significantly fast speed compared with an
accurate O3 simulator built with gem5. Our evaluation on a commercial Intel
Xeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to
using gem5 built simulator, which is superior to the cutting-edge deep learning
approach

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: ISAAC is an LLM-aided CPU verification framework that uses multi-agent stimulus generation with micro-architectural awareness and FPGA-parallel simulation to dramatically improve verification efficiency and bug detection.


<details>
  <summary>Details</summary>
Motivation: CPU verification is a major bottleneck in IC development, with current industrial practices suffering from inefficient stimulus generation and slow simulation infrastructure that delays debugging cycles.

Method: ISAAC employs a multi-agent stimulus engine with micro-architectural knowledge, plus a forward-snapshot mechanism and decoupled co-simulation architecture enabling single ISS to drive multiple DUTs in parallel on FPGAs.

Result: Achieved up to 17,536x speed-up over software RTL simulation and detected several previously unknown bugs in a mature CPU that had undergone multiple successful tape-outs.

Conclusion: ISAAC effectively addresses CPU verification bottlenecks through LLM-aided stimulus generation and FPGA parallelism, significantly accelerating verification while improving bug detection capabilities.

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [17] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: ADiP is an adaptive-precision systolic array architecture that enhances matrix multiplication efficiency for transformers by supporting multiple precision modes (8x8, 8x4, 8x2) and computation modes, achieving up to 4x higher throughput and significant latency/energy improvements.


<details>
  <summary>Details</summary>
Motivation: Transformers require efficient acceleration due to substantial memory and computational demands. Quantization reduces memory usage, and adaptive precision architectures can further optimize matrix multiplication by dynamically adjusting precision.

Method: Proposes ADiP architecture with NxN adaptive-precision PEs and shared accumulators, supporting symmetric single-matrix and asymmetric multi-matrix multiplication with shared input matrix. Includes analytical models for latency/throughput and hardware design space exploration using 22nm technology.

Result: Achieves up to 4x higher computational throughput, 53.6% latency improvement, and 24.4% energy improvement for BitNet-1.58B. Peak throughput of 8.192 TOPS (8x8), 16.384 TOPS (8x4), and 32.768 TOPS (8x2) with 4096 PEs.

Conclusion: ADiP effectively addresses transformer acceleration needs through adaptive precision and flexible computation modes, demonstrating significant performance and efficiency gains across various transformer workloads.

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [18] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Bhasha-Rupantarika is a lightweight multilingual translation system using algorithm-hardware codesign for resource-limited settings, achieving 4.1x model size reduction and 4.2x inference speedup through ultra-low precision quantization on FPGA accelerators.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient multilingual translation system suitable for deployment in resource-constrained environments like IoT devices, addressing the need for real-time performance with limited computational resources.

Method: Algorithm-hardware codesign approach with sub-octet precision quantization (FP8, INT8, INT4, FP4) deployed on FPGA accelerators, focusing on bidirectional translation between Indian and international languages.

Result: Achieved 4.1x model size reduction (FP4), 4.2x inference speedup, 66 tokens/s throughput (4.8x improvement), 1.96x LUT reduction, 1.65x FF decrease, and 2.2x throughput enhancement compared to OPU.

Conclusion: The system provides a viable solution for deployable multilingual AI systems through quantization-aware translation and hardware efficiency, with publicly available code and datasets for reproducibility and further development.

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [19] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: This paper presents an in-storage computing architecture using 3D Ferroelectric NAND (FeNAND) with hyperdimensional computing to accelerate mass spectrometry library searches, achieving 43x speedup and 21x energy efficiency improvement.


<details>
  <summary>Details</summary>
Motivation: The massive growth of mass spectrometry data (hundreds of terabytes) creates computational bottlenecks for drug discovery applications, and traditional processors struggle with this data volume efficiently.

Method: Combines 3D FeNAND structure with hyperdimensional computing (HDC) and a dual-bound approximate matching (D-BAM) distance metric to parallelize vector computations for spectral library search.

Result: Achieved 43x speedup and 21x higher energy efficiency compared to state-of-the-art 3D NAND methods while maintaining comparable accuracy.

Conclusion: The proposed ISP architecture with FeNAND and HDC successfully addresses throughput limitations of traditional NAND structures and enables efficient large-scale mass spectrometry data processing.

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [20] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: An automated framework that improves CIM accelerator efficiency for sparse LLM inference through novel mapping and scheduling strategies, achieving over 50% better array utilization and 4x reduction in memory footprint and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Structured sparsity reduces LLM size but conventional architectures struggle with memory-bound decode stages. CIM architectures help but naive sparse matrix mapping leads to poor array utilization and computational inefficiency.

Method: Developed an automated framework with novel mapping and scheduling strategies that exploit block-diagonal sparsity patterns to optimize CIM array usage.

Result: Achieved over 50% improvement in CIM array utilization, more than 4x reduction in memory footprint, and more than 4x reduction in floating-point operations required.

Conclusion: The proposed framework effectively accelerates sparse LLM inference on CIM accelerators by optimizing sparse matrix mapping, significantly improving computational efficiency and reducing resource requirements.

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>
