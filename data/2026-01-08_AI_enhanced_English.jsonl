{"id": "2601.03976", "categories": ["cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.03976", "abs": "https://arxiv.org/abs/2601.03976", "authors": ["Gorka Nieto", "Idoia de la Iglesia", "Cristina Perfecto", "Unai Lopez-Novoa"], "title": "On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process", "comment": "Submitted to IEEE Transactions on Cognitive Communications and Networking", "summary": "Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption.", "AI": {"tldr": "A decentralized Deep Reinforcement Learning agent for task offloading decisions in edge computing, evaluated in a real testbed with 5G-connected devices, MEC server, and cloud server.", "motivation": "Task offloading enables less capable devices to run demanding applications, but deciding where to execute tasks is complex. Existing approaches need real-world evaluation in edge computing environments.", "method": "Proposed a decentralized DRL agent for computing location selection, implemented in a real testbed with various edge devices connected to MEC and cloud servers via 5G. Evaluated both agent performance and local execution trade-offs.", "result": "The paper evaluates the agent's performance in meeting task requirements and assesses the implications of running DRL agents locally, including trade-offs between local vs. remote training in terms of latency and energy consumption.", "conclusion": "The work provides real-world evaluation of decentralized DRL for edge computing task offloading, highlighting practical considerations for deploying such agents in edge environments with 5G connectivity."}}
{"id": "2601.03390", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03390", "abs": "https://arxiv.org/abs/2601.03390", "authors": ["Daniel Qian", "Xiyu Hao", "Jinkun Geng", "Yuncheng Yao", "Aurojit Panda", "Jinyang Li", "Anirudh Sivaraman"], "title": "Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication", "comment": null, "summary": "As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2\u0394$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.\n  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2\u0394+ \\varepsilon$, where $\\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.", "AI": {"tldr": "Aspen is a leaderless Byzantine Fault Tolerant protocol that achieves near-optimal latency (2\u0394+\u03b5) by removing the no-contention requirement through a best-effort sequencing layer with loosely synchronized clocks and network delay estimates.", "motivation": "As BFT protocols are increasingly used in permissioned blockchains for user-facing applications like payments, low latency becomes crucial. Existing leaderless BFT protocols with optimistic fast paths only work without contention, causing costly recovery when concurrent requests cause replicas to diverge.", "method": "Aspen uses a leaderless BFT protocol with a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. It requires n = 3f + 2p + 1 replicas to tolerate up to f Byzantine nodes, with 2p extra nodes allowing the fast path to proceed even if up to p replicas diverge due to unpredictable network delays. When optimistic conditions fail, it falls back to PBFT-style protocol.", "result": "In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, achieving 1.2 to 3.3\u00d7 improvement compared to previous protocols while supporting 19,000 requests per second.", "conclusion": "Aspen successfully removes the no-contention limitation of previous leaderless BFT protocols, achieving near-optimal latency while maintaining safety and liveness under partial synchrony through a hybrid approach combining optimistic fast path with fallback to traditional BFT."}}
{"id": "2601.03862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03862", "abs": "https://arxiv.org/abs/2601.03862", "authors": ["Francesco D'Amato", "Roberto Saltini", "Thanh-Hai Tran", "Yann Vonlanthen", "Luca Zanolini"], "title": "Majorum: Ebb-and-Flow Consensus with Dynamic Quorums", "comment": null, "summary": "Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.\n  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one.", "AI": {"tldr": "Majorum is an ebb-and-flow consensus protocol that combines dynamic availability with partial synchrony to achieve both liveness during participant churn and strong safety during network partitions.", "motivation": "To address the limitation of dynamically available protocols which cannot provide strong safety guarantees during network partitions or extended asynchrony, while maintaining the ability to remain live despite honest participants going offline and rejoining.", "method": "Combines a dynamically available protocol (based on quorum-based TOB-SVD) with a partially synchronous finality protocol that irrevocably finalizes a prefix, creating an ebb-and-flow construction.", "result": "Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot, with each slot finalizing the next block extending the previously finalized one when conditions remain favorable.", "conclusion": "Majorum successfully addresses the safety limitations of purely dynamically available protocols while maintaining their liveness advantages through an ebb-and-flow architecture."}}
{"id": "2601.03992", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03992", "abs": "https://arxiv.org/abs/2601.03992", "authors": ["Qi Wu", "Chao Fang", "Jiayuan Chen", "Ye Lin", "Yueqi Zhang", "Yichuan Bai", "Yuan Du", "Li Du"], "title": "A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments.", "AI": {"tldr": "An efficient inference framework for MoE models on GPU-NDP edge systems that addresses load imbalance, GPU underutilization, and data pre-profiling challenges through tensor parallelism, load-balancing scheduling, and dataset-free pre-fetching.", "motivation": "MoE models enable edge deployment by separating model capacity from active computation, but their large memory footprint requires GPU-NDP systems. However, three critical challenges exist: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation, and 3) extensive data pre-profiling needed for unpredictable expert activation patterns.", "method": "Three key optimizations: 1) Exploiting underexplored tensor parallelism to partition and compute large expert parameters across multiple NDP units for edge low-batch scenarios. 2) A load-balancing-aware scheduling algorithm that distributes expert computations across NDP units and GPU to maximize resource utilization. 3) A dataset-free pre-fetching strategy that proactively loads frequently accessed experts to minimize activation delays.", "result": "Experimental results show the framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches.", "conclusion": "The proposed framework significantly enhances MoE inference efficiency in resource-constrained edge environments by addressing key deployment challenges through innovative parallelism exploitation, intelligent scheduling, and proactive data management."}}
{"id": "2601.04123", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04123", "abs": "https://arxiv.org/abs/2601.04123", "authors": ["Francisco Ponce", "Simone Gazza", "Andrea D'Iapico", "Roberto Amadini", "Antonio Brogi", "Stefano Forti", "Saverio Giallorenzo", "Pierluigi Plebani", "Davide Usai", "Monica Vitali", "Gianluigi Zavattaro", "Jacopo Soldani"], "title": "Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum", "comment": "Submitted to Cluster Computing", "summary": "Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.\n  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact.", "AI": {"tldr": "FREEDA toolchain automates failure-resilient and carbon-efficient deployment of microservice-based applications across Cloud-Edge Continuum, adapting to changing conditions while balancing resilience, performance, and sustainability.", "motivation": "Deploying microservice-based applications on heterogeneous Cloud-Edge infrastructures requires balancing conflicting objectives like failure resilience, performance, and environmental sustainability, which is challenging due to dynamic operational conditions.", "method": "Introduces FREEDA toolchain that continuously adapts deployment configurations to changing conditions, resource availability, and sustainability constraints. Uses experimental suite with diverse simulated and emulated scenarios to validate effectiveness against real-world challenges.", "result": "FREEDA demonstrates capability to autonomously reconfigure deployments through service migration, flavor selection adjustments, and workload rebalancing, successfully achieving optimal balance among resilience, efficiency, and environmental impact.", "conclusion": "FREEDA toolchain effectively automates and optimizes microservice deployment across Cloud-Edge Continuum, addressing the complex trade-offs between failure resilience, performance, and carbon emissions in dynamic environments."}}
