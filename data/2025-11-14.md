<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.OS](#cs.OS) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: CacheX enables accurate cache abstraction probing in cloud VMs without hardware/hypervisor support, improving cache utilization through contention-aware scheduling and virtual color-aware page management.


<details>
  <summary>Details</summary>
Motivation: Cache-based optimizations are ineffective in cloud VMs due to limited visibility and control over provisioned caches, as VMs are unaware of cache partitioning/sharing details and cannot influence memory-to-cache mappings.

Method: Proposes CacheX, which probes accurate cache abstraction using eviction sets, and implements two techniques: LLC contention-aware task scheduling and virtual color-aware page cache management in x86 Linux kernel.

Result: Evaluation demonstrates CacheX effectively improves cache utilization for various workloads in public cloud VMs.

Conclusion: CacheX provides a practical solution to overcome cache visibility limitations in cloud environments, enabling better cache utilization without requiring hardware or hypervisor modifications.

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [2] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: Ksurf, a variance-minimizing estimator, improves resource orchestration in cloud environments by reducing latency variance (41% at p95, 47% at p99), lowering CPU usage by 4%, and saving 7% in costs on Kubernetes benchmarks.


<details>
  <summary>Details</summary>
Motivation: Cloud environments face challenges with large configuration spaces and uncertainties, exacerbated by varying numbers of virtual machines that increase workload variability and noise, reducing orchestration accuracy.

Method: Ksurf is employed as a contextual multi-armed bandit objective function model within the Drone orchestrator to estimate resources under high cloud variability, leveraging its variance-minimizing properties.

Result: Ksurf achieves significant improvements: 41% lower latency variance at p95, 47% at p99, 4% reduction in CPU usage, 7 MB reduction in master node memory usage, and 7% cost savings in average worker pod count on VarBench Kubernetes benchmark.

Conclusion: Ksurf effectively enhances resource orchestration in highly variable cloud scenarios, demonstrating substantial performance gains and cost efficiency when integrated with contextual bandit techniques.

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [3] [A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond](https://arxiv.org/abs/2511.09776)
*Ramesh Adhikari,Costas Busch,Pavan Poudel*

Main category: cs.DC

TL;DR: This paper presents efficient transaction scheduling algorithms for fog-cloud computing networks with moving transactions and shared objects, achieving approximation guarantees for minimizing total scheduling costs in networks with constant doubling dimension.


<details>
  <summary>Details</summary>
Motivation: Transaction scheduling is crucial for efficient resource allocation in distributed systems, particularly in fog-cloud computing where both transactions and shared objects can move within the network. The challenge is to minimize total scheduling costs while handling dynamic movement of transactions and objects.

Method: The authors develop scheduling algorithms for networks with constant doubling dimension. They first address the single shared object case with an algorithm providing O(log n · log D) approximation, then extend to multiple shared objects (up to k per transaction) with O(k · log n · log D) approximation. They also provide fully distributed versions without requiring global transaction knowledge.

Result: The proposed algorithms achieve provable approximation guarantees: O(log n · log D) for single shared object scheduling and O(k · log n · log D) for multiple shared objects, where n is the number of nodes and D is the network diameter. The distributed versions maintain these guarantees without global knowledge.

Conclusion: The paper presents efficient transaction scheduling algorithms for fog-cloud computing networks that handle moving transactions and objects, providing strong approximation guarantees and distributed implementations suitable for practical deployment in dynamic network environments.

Abstract: Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\log n \cdot \log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \cdot \log n \cdot \log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.

</details>


### [4] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa is a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance mechanisms to address challenges in large-scale LLM pretraining across distributed clusters.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in LLM scales requires distributed pretraining across large clusters, but existing performance modeling approaches fail to comprehensively account for optimization features and ignore the substantial overhead from essential fault tolerance mechanisms like checkpoint recovery.

Method: MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. A MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks.

Result: Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. Comprehensive tuning experiments systematically reveal the key factors influencing pretraining performance under different configurations.

Conclusion: MoFa provides solid a priori guidance for LLM pretraining system design and deployment by accurately modeling performance and identifying optimization opportunities in large-scale distributed training environments.

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [5] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: The paper identifies 'Lit Silicon' - a thermal imbalance issue in multi-GPU systems that causes performance variation during LLM training, and proposes detection/mitigation techniques that achieve up to 6% performance and 4% power improvements.


<details>
  <summary>Details</summary>
Motivation: GPU systems power modern datacenters but suffer from performance variation that impacts HPC and AI workloads like LLM training. Thermal imbalance coupled with concurrent computation-communication (C3) creates node-level straggler GPUs that slow down the entire system.

Method: Analyzed single-node multi-GPU LLM training performance, identified thermal-induced straggling coupled with C3 (Lit Silicon effect), developed analytical performance/power models, designed detection/mitigation techniques, and evaluated three power management solutions including GPU power optimization and CPU power sloshing.

Result: Experiments on AMD MI300X GPU systems with LLM training frameworks showed up to 6% performance improvement and 4% power reduction, potentially saving hundreds of millions of dollars in datacenters.

Conclusion: The proposed solution effectively addresses the Lit Silicon problem with minimal overhead, providing a new node-level power management layer that can be easily adopted in datacenters for significant cost savings.

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [6] [Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach](https://arxiv.org/abs/2511.10146)
*Jaime Sebastian Burbano,Arnova Abdullah,Eldiyar Zhantileuov,Mohan Liyanage,Rolf Schuster*

Main category: cs.DC

TL;DR: MO-HAN is a lightweight edge server selection method that combines latency prediction with reliability and hysteresis-based handover to reduce latency and minimize unnecessary server switches in dynamic network conditions.


<details>
  <summary>Details</summary>
Motivation: Latency-sensitive embedded applications need effective edge server selection methods to handle dynamic network congestion in multi-server architectures, but existing approaches often lack proper reliability considerations and cause excessive handovers.

Method: Uses passive measurements (arrival rate, utilization, payload size) with an exponentially modulated rational delay model to compute a score balancing predicted latency and reliability, incorporating hysteresis-based handover to ensure meaningful gains before switching servers.

Result: MO-HAN consistently outperforms static and fair-distribution baselines by reducing both mean and tail latencies, while cutting handovers by nearly 50% compared to pure opportunistic selection.

Conclusion: The method achieves significant performance gains without requiring intrusive instrumentation or heavy learning infrastructure, making it practical for deployment on resource-constrained embedded devices in edge computing environments.

Abstract: Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.

</details>


### [7] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: A supervised learning model for selecting sparse matrix reordering algorithms that reduces solution time by 55.37% compared to using only AMD algorithm.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for sparse matrix ordering rely on brute-force search or empirical knowledge, lacking adaptability to diverse matrix structures, necessitating an automated and intelligent selection approach.

Method: Supervised learning-based model that learns the correlation between matrix characteristics and commonly used reordering algorithms to enable automated algorithm selection.

Result: Experiments on Florida sparse matrix dataset show the model accurately predicts optimal reordering algorithms, achieving 55.37% reduction in solution time and average speedup ratio of 1.45 compared to using only AMD algorithm.

Conclusion: The proposed supervised learning approach enables intelligent and automated selection of sparse matrix reordering algorithms, significantly improving computational efficiency for large-scale sparse matrices.

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [8] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: This paper presents a novel categorization of modern workload schedulers into three classes: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers, and Big Data Schedulers, tracing their evolution and comparing their characteristics.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic classification and understanding of modern workload schedulers by examining their evolution, algorithms, and features across different computing environments.

Method: The authors categorize schedulers into three classes and analyze their historical development from early implementations to modern versions, considering both algorithmic approaches and feature sets.

Result: The paper identifies and describes three distinct classes of schedulers, highlights their evolutionary paths, and discusses key differences between them while noting their chronological development patterns.

Conclusion: Despite operating at different scales (local vs. distributed systems), all scheduler classes share similar design focus in their scheduling strategies, revealing common principles across computing environments.

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [9] [FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing](https://arxiv.org/abs/2511.10442)
*Aarush Agarwal,Raymond He,Jan Kieseler,Matteo Cremonesi,Shah Rukh Qasim*

Main category: cs.DC

TL;DR: FastGraph is a GPU-optimized k-nearest neighbor algorithm that accelerates graph construction in low-dimensional spaces (2-10 dimensions), achieving 20-40x speedup over existing libraries with minimal memory overhead.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottlenecks in graph construction for graph neural networks, particularly in low-dimensional spaces where existing methods are inefficient.

Method: Uses a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning to optimize computational and memory efficiency.

Result: Achieves 20-40x speedup over state-of-the-art libraries (FAISS, ANNOY, SCANN) in dimensions <10 with virtually no memory overhead, benefiting GNN workflows in applications like particle clustering and visual object tracking.

Conclusion: FastGraph provides significant performance improvements for graph construction in low-dimensional spaces, enabling more efficient GNN-based workflows across various computationally intensive applications.

Abstract: We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.

</details>


### [10] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE is a framework that synthesizes high-fidelity execution traces to model LLM workloads, enabling scalable exploration of parallelization strategies and system configurations without requiring access to large-scale infrastructure.


<details>
  <summary>Details</summary>
Motivation: Current approaches for modeling LLM workload execution rely on collecting traces from real systems, which is limited to major cloud providers and cannot easily adapt to future larger-scale configurations.

Method: STAGE uses symbolic tensor graph generation to synthesize execution traces that support comprehensive parallelization strategies and systematically explore LLM architectures and system configurations.

Result: STAGE successfully synthesizes high-fidelity LLM traces spanning over 32K GPUs while maintaining tensor-level accuracy in compute, memory, and communication operations.

Conclusion: STAGE provides a scalable solution for pre-deployment system optimization and design-space exploration of distributed LLM workloads, and will be made publicly available to advance research in distributed machine learning systems.

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [11] [The Configuration Wall: Characterization and Elimination of Accelerator Configuration Overhead](https://arxiv.org/abs/2511.10397)
*Josse Van Delm,Anton Lydike,Joren Dumoulin,Jonas Crols,Xiaoling Yi,Ryan Antonio,Jackson Woodruff,Tobias Grosser,Marian Verhelst*

Main category: cs.PF

TL;DR: The paper addresses the 'configuration wall' problem where CPU setup time for hardware accelerators limits performance. It introduces a roofline model variant to quantify configuration-bound performance and proposes compiler optimizations to reduce configuration overhead, achieving 2x geomean performance improvement.


<details>
  <summary>Details</summary>
Motivation: Modern systems offload compute to accelerators for efficiency, but CPU setup/synchronization time grows with accelerator complexity, creating a 'configuration wall' that limits performance, especially for faster accelerators.

Method: Developed a roofline model variant to identify configuration-bound performance, and created domain-specific compiler abstractions with optimization passes in MLIR to eliminate redundant configuration cycles and hide remaining ones.

Result: Implementation on open-source OpenGeMM system demonstrated 2x geomean performance boost by reducing configuration overhead.

Conclusion: The work provides insights into how accelerator setup mechanisms affect performance and enables automatic code generation to overcome the configuration wall.

Abstract: Contemporary compute platforms increasingly offload compute kernels from CPU to integrated hardware accelerators to reach maximum performance per Watt. Unfortunately, the time the CPU spends on setup control and synchronization has increased with growing accelerator complexity. For systems with complex accelerators, this means that performance can be configuration-bound. Faster accelerators are more severely impacted by this overlooked performance drop, which we call the configuration wall. Prior work evidences this wall and proposes ad-hoc solutions to reduce configuration overhead. However, these solutions are not universally applicable, nor do they offer comprehensive insights into the underlying causes of performance degradation. In this work, we first introduce a widely-applicable variant of the well-known roofline model to quantify when system performance is configuration-bound. To move systems out of the performance-bound region, we subsequently propose a domain-specific compiler abstraction and associated optimization passes. We implement the abstraction and passes in the MLIR compiler framework to run optimized binaries on open-source architectures to prove its effectiveness and generality. Experiments demonstrate a geomean performance boost of 2x on the open-source OpenGeMM system, by eliminating redundant configuration cycles and by automatically hiding the remaining configuration cycles. Our work provides key insights in how accelerator performance is affected by setup mechanisms, thereby facilitating automatic code generation for circumventing the configuration wall.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: This paper presents an FPGA-based hardware architecture for history-aware trajectory k-anonymization that improves on previous shortest-path-only methods by integrating historical trajectory data to preserve behavioral accuracy while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Previous FPGA-based trajectory anonymization relied solely on shortest-path computations, which failed to capture realistic travel behavior and reduced data utility. The authors aimed to address this limitation by incorporating historical trajectory data.

Method: Developed a novel architecture that integrates parallel history-based trajectory searches with conventional shortest-path finding, using a custom fixed-point counting module to accurately weigh contributions from historical data to prioritize behaviorally common routes.

Result: The FPGA implementation achieved real-time throughput of over 6,000 records/s, improved data retention by up to 1.2% compared to previous shortest-path-only design, and preserved major arterial roads more effectively.

Conclusion: This represents a key advancement enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under strict latency constraints of location-based services.

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [13] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner is a framework that generates module-level assertions for logic design verification by using AST-based structural information to guide LLMs, outperforming existing methods and improving error detection.


<details>
  <summary>Details</summary>
Motivation: Existing assertion generation methods only produce top-level assertions, missing verification needs at the micro-architectural module level where design errors are more frequent.

Method: Performs AST-based structural extraction to derive module call graph, I/O table, and dataflow graph, then uses this information to guide LLMs in generating module-level specifications and assertions.

Result: AssertMiner outperforms existing methods like AssertLLM and Spec2Assertion in generating high-quality module-level assertions, and when integrated with these methods, enhances structural coverage and significantly improves error detection capability.

Conclusion: AssertMiner enables more comprehensive and efficient verification by addressing the gap in module-level assertion generation through AST-guided LLM assistance.

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [14] [The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads](https://arxiv.org/abs/2511.10010)
*Shahid Amin,Syed Pervez Hussnain Shah*

Main category: cs.AR

TL;DR: This paper reviews the co-evolution of AI and computer architecture, analyzing GPU, ASIC, and FPGA paradigms for accelerating AI workloads, and explores emerging technologies like PIM and neuromorphic computing.


<details>
  <summary>Details</summary>
Motivation: The massive computational demands of growing AI models, particularly Deep Neural Networks (DNNs), have pushed traditional computer architectures to their limits, creating a need to understand and optimize the architectural landscape for AI acceleration.

Method: The paper provides a structured review by analyzing dominant architectural paradigms (GPUs, ASICs, FPGAs), their design philosophies, key features, and performance trade-offs. It examines core principles including dataflow optimization, memory hierarchies, sparsity, and quantization, and synthesizes architectural principles with quantitative performance data from industry benchmarks.

Result: The survey presents a comprehensive picture of the AI accelerator landscape, showing that different architectural paradigms offer various trade-offs in performance, energy efficiency, and flexibility for AI workloads.

Conclusion: AI and computer architecture are in a symbiotic relationship where hardware-software co-design is no longer an optimization but a necessity for future progress in computing, with emerging technologies like PIM and neuromorphic computing potentially redefining future computation paradigms.

Abstract: The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing.

</details>


### [15] [Combined power management and congestion control in High-Speed Ethernet-based Networks for Supercomputers and Data Centers](https://arxiv.org/abs/2511.10159)
*Miguel Sánchez de la Rosa,Francisco J. andújar,Jesus Escudero-Sahuquillo,José L. Sánchez,Francisco J. Alfaro-Cortés*

Main category: cs.AR

TL;DR: This paper explores network management in large-scale computing systems, focusing on congestion control to prevent performance degradation under heavy load and power management to save energy during idle periods, while examining their interactions.


<details>
  <summary>Details</summary>
Motivation: The proliferation of datacenters and supercomputers requires sophisticated network management to handle their scale and complexity, where network interconnections can become bottlenecks affecting the entire infrastructure.

Method: The work explores two key aspects of network management: congestion control mechanisms to prevent performance degradation under heavy use, and power management techniques to conserve energy during idle periods.

Result: The paper examines how these two network management approaches interact with each other in large-scale computing environments.

Conclusion: Effective network management in datacenters and supercomputers requires addressing both performance optimization through congestion control and energy efficiency through power management, while understanding their interdependencies.

Abstract: The demand for computer in our daily lives has led to the proliferation of Datacenters that power indispensable many services. On the other hand, computing has become essential for some research for various scientific fields, that require Supercomputers with vast computing capabilities to produce results in reasonable time. The scale and complexity of these systems, compared to our day-to-day devices, are like comparing a cell to a living organism. To make them work properly, we need state-of-the-art technology and engineering, not just raw resources. Interconnecting the different computer nodes that make up a whole is a delicate task, as it can become the bottleneck for the whole infrastructure. In this work, we explore two aspects of the network: how to prevent degradation under heavy use with congestion control, and how to save energy when idle with power management; and how the two may interact.

</details>


### [16] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: The paper proposes beamspace processing algorithms and VLSI architectures for massive MIMO systems that reduce data detection power consumption while maintaining high throughput.


<details>
  <summary>Details</summary>
Motivation: Massive MIMO and mmWave communication technologies face challenges with excessive baseband processing hardware cost and power consumption, which beamspace processing can address by leveraging channel sparsity at mmWave frequencies.

Method: The authors review existing beamspace data detection algorithms and propose new algorithms including complex sparsity-adaptive equalizer (CSPADE) with corresponding VLSI architectures, implementing them in a 22nm FDSOI process with both fully-parallelized and sequential MAC-based approaches.

Result: Implementation results show that CSPADE achieves up to 54% power savings compared to antenna-domain equalization in fully-parallelized form, and up to 66% power savings in sequential MAC-based architecture. The designs also achieve the highest reported throughput among existing massive MIMO data detectors with better energy and area efficiency.

Conclusion: Beamspace processing with the proposed CSPADE algorithm and VLSI architectures effectively reduces power consumption in massive MIMO systems while maintaining high performance, making them suitable for future wireless communication systems.

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [17] [Taiji: A DPU Memory Elasticity Solution for In-production Cloud Environments](https://arxiv.org/abs/2511.09936)
*Hao Zheng,Longxiang Wang,Yun Xu,Qiang Wang,Yibin Shen,Xiaoshe Dong,Bang Di,Jia Wei,Shenyu Dong,Xingjun Zhang,Weichen Chen,Zhao Han,Sanqian Zhao,Dongdong Huang,Jie Qi,Yifan Yang,Zhao Gao,Yi Wang,Jinhu Li,Xudong Ren,Min He,Hang Yang,Xiao Zheng,Haijiao Hao,Jiesheng Wu*

Main category: cs.OS

TL;DR: Taiji is a resource-elasticity architecture for Data Processing Units (DPUs) that enables memory overcommitment through hybrid virtualization and parallel memory swapping, expanding DPU memory by over 50% with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: DPUs face challenges with long hardware upgrade cycles and limited resources despite enhancing server network and storage performance in cloud computing environments.

Method: Combines hybrid virtualization with parallel memory swapping, switching the DPU's OS into a guest OS and inserting a lightweight virtualization layer to make nearly all DPU memory swappable.

Result: Expands DPU memory resources by over 50%, maintains virtualization overhead around 5%, ensures 90% of swap-ins complete within 10 microseconds, and supports hot-switch and hot-upgrade capabilities.

Conclusion: Taiji delivers an efficient, reliable, low-overhead elasticity solution for DPUs and is successfully deployed in large-scale production systems across more than 30,000 servers.

Abstract: The growth of cloud computing drives data centers toward higher density and efficiency. Data processing units (DPUs) enhance server network and storage performance but face challenges such as long hardware upgrade cycles and limited resources. To address these, we propose Taiji, a resource-elasticity architecture for DPUs. Combining hybrid virtualization with parallel memory swapping, Taiji switches the DPU's operating system (OS) into a guest OS and inserts a lightweight virtualization layer, making nearly all DPU memory swappable. It achieves memory overcommitment for the switched guest OS via high-performance memory elasticity, fully transparent to upper-layer applications, and supports hot-switch and hot-upgrade to meet in-production cloud requirements. Experiments show that Taiji expands DPU memory resources by over 50%, maintains virtualization overhead around 5%, and ensures 90% of swap-ins complete within 10 microseconds. Taiji delivers an efficient, reliable, low-overhead elasticity solution for DPUs and is deployed in large-scale production systems across more than 30,000 servers.

</details>


### [18] [Vmem: A Lightweight Hot-Upgradable Memory Management for In-production Cloud Environment](https://arxiv.org/abs/2511.09961)
*Hao Zheng,Qiang Wang,Longxiang Wang,Xishi Qiu,Yibin Shen,Xiaoshe Dong,Naixuan Guan,Jia Wei,Fudong Qiu,Xingjun Zhang,Yun Xu,Mao Zhao,Yisheng Xie,Shenglong Zhao,Min He,Yu Li,Xiao Zheng,Ben Luo,Jiesheng Wu*

Main category: cs.OS

TL;DR: Vmem is a memory management architecture for cloud environments that addresses metadata overhead and stability issues through lightweight reserved memory management, supporting online upgrades and improving memory utilization, VM boot times, and network performance.


<details>
  <summary>Details</summary>
Motivation: Traditional memory management has problems with metadata overhead, architectural complexity, and stability degradation, which are intensified in cloud environments. Existing optimizations are insufficient for cloud computing's dual demands of flexibility and low overhead.

Method: Vmem uses a lightweight reserved memory management architecture that enables flexible, efficient cloud server memory utilization and supports online upgrades to meet cloud requirements for high stability and rapid iterative evolution.

Result: Vmem increases sellable memory rate by about 2%, achieves over 3x faster boot time for VFIO-based VMs, improves network performance by about 10% for DPU-accelerated VMs, and has been deployed at large scale for seven years on over 300,000 cloud servers supporting hundreds of millions of VMs.

Conclusion: Vmem demonstrates efficiency and stability in large-scale cloud deployments, providing a solution that meets cloud computing's requirements for flexible memory management with low overhead while supporting online upgrades.

Abstract: Traditional memory management suffers from metadata overhead, architectural complexity, and stability degradation, problems intensified in cloud environments. Existing software/hardware optimizations are insufficient for cloud computing's dual demands of flexibility and low overhead. This paper presents Vmem, a memory management architecture for in-production cloud environments that enables flexible, efficient cloud server memory utilization through lightweight reserved memory management. Vmem is the first such architecture to support online upgrades, meeting cloud requirements for high stability and rapid iterative evolution. Experiments show Vmem increases sellable memory rate by about 2%, delivers extreme elasticity and performance, achieves over 3x faster boot time for VFIO-based virtual machines (VMs), and improves network performance by about 10% for DPU-accelerated VMs. Vmem has been deployed at large scale for seven years, demonstrating efficiency and stability on over 300,000 cloud servers supporting hundreds of millions of VMs.

</details>
