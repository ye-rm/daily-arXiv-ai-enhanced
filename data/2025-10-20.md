<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: This paper critiques the Mess paper (MICRO 2024 best paper runner-up) by demonstrating that its Ramulator 2.0 simulation results are incorrect and irreproducible due to configuration errors, and identifies issues with DAMOV simulation statistics and artifact reproducibility.


<details>
  <summary>Details</summary>
Motivation: To correct factual errors in the Mess paper's evaluation of memory system performance simulators, particularly regarding Ramulator 2.0, and to address concerns about the reliability of the scientific record and review processes.

Method: The authors analyzed the Mess paper's simulation configurations and usage, identified multiple human errors in simulator setup, and conducted their own experiments with correctly configured Ramulator 2.0 to compare results.

Result: Found that Ramulator 2.0's simulated memory performance actually resembles real system characteristics when properly configured, contradicting the Mess paper's claims. Also identified incorrect DAMOV simulation statistics and incomplete artifact repository in the Mess paper.

Conclusion: The Mess paper contains significant errors in simulator evaluation methodology and results. The community should correct these errors to prevent propagation of misleading results and maintain scientific integrity. Proper validation and collaboration with simulator developers are essential.

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [2] [Impact of AI-Triage on Radiologist Report Turnaround Time: Real-World Time-Savings and Insights from Model Predictions](https://arxiv.org/abs/2510.15237)
*Yee Lam Elim Thompson,Jonathan Fergus,Jonathan Chung,Jana G. Delfino,Weijie Chen,Gary M. Levine,Frank W. Samuelson*

Main category: cs.PF

TL;DR: AI triage device for pulmonary embolism in CT scans significantly reduced report turnaround time during work hours by 22.2 minutes, with minimal impact during off-hours. Workflow analysis showed computational model accurately predicted time-savings.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of workflow parameters on time-savings in report turnaround time (TAT) following deployment of an AI-triage device for prioritizing pulmonary embolism in chest CT pulmonary angiography exams.

Method: Retrospective study of 11,252 adult CTPA exams comparing pre-AI and post-AI periods. Analyzed TAT for PE-positive exams and extracted workflow parameters from 527,234 PACS records including exam inter-arrival time and radiologist read-time. Used computational model to predict time-savings.

Result: Significant time-savings during work hours (22.2 minutes, p=0.004) but not during off-hours (2.82 minutes, p=0.345). Model predictions aligned with observed results (29.6 minutes for work hours, 2.10 minutes for off-hours). Pre-AI PE-positive rate was 13.3%, post-AI was 16.2%.

Conclusion: Consideration and quantification of clinical workflow parameters are essential for accurate assessment of expected time-savings in TAT following AI-triage device deployment, with significant benefits primarily during work hours.

Abstract: Objective: To quantify the impact of workflow parameters on time-savings in
report turnaround time (TAT) due to an AI-triage device that prioritized
pulmonary embolism (PE) in chest CT pulmonary angiography (CTPA) exams.
Methods: This retrospective study analyzed 11252 adult CTPA exams conducted for
suspected PE at a single tertiary academic medical center. Data was divided
into two periods: pre-AI and post-AI. For PE-positive exams, TAT - defined as
the duration from patient scan completion to the first preliminary report
completion - was compared between the two periods. Time-savings were reported
separately for work-hour and off-hour cohorts. To characterize radiologist
workflow, 527234 records were retrieved from the PACS and workflow parameters
such as exam inter-arrival time and radiologist read-time extracted. These
parameters were input into a computational model to predict time-savings
following deployment of an AI-triage device and to study the impact of workflow
parameters. Results: The pre-AI dataset included 4694 chest CTPA exams with
13.3% being PE-positive. The post-AI dataset comprised 6558 exams with 16.2%
being PE-positive. The mean TAT for pre-AI and post-AI during work hours are
68.9 [95% CI" 55.0, 82.8] and 46.7 [38.1, 55.2] minutes respectively, and those
during off-hours are 44.8 [33.7, 55.9] and 42.0 [33.6, 50.3] minutes.
Clinically-observed time-savings during work hours (22.2 [95% CI: 5.85, 38.6]
minutes) were significant (p=0.004), while off-hour (2.82 [-11.1, 16.7]
minutes) were not (p=0.345). Observed time-savings aligned with model
predictions (29.6 [95% range: 23.2, 38.1] minutes for work hours; 2.10 [1.76,
2.58] minutes for off-hours). Discussion: Consideration and quantification of
clinical workflow contribute to an accurate assessment of the expected
time-savings in TAT following deployment of an AI-triage device.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive hash table is a high-performance GPU hash table that uses warp-cooperative techniques, packed bucket layout, and dynamic resizing to achieve high throughput and load factors up to 95%, outperforming existing GPU hash tables.


<details>
  <summary>Details</summary>
Motivation: Existing GPU hash tables struggle with concurrent updates, high load factors, and irregular memory access patterns, limiting their performance in data-intensive applications.

Method: Uses cache-aligned packed bucket layout for coalesced memory access, warp-synchronous concurrency protocols (WABC and WCME) to reduce contention, and load-factor-aware dynamic resizing with linear hashing. Employs a four-step insertion strategy with replace, claim-and-commit, bounded cuckoo eviction, and overflow-stash fallback.

Result: Achieves load factors up to 95% with 1.5-2x higher throughput than state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore). On balanced workloads, reaches 3.5 billion updates/s and nearly 4 billion lookups/s on NVIDIA RTX 4090.

Conclusion: Hive hash table demonstrates superior scalability and efficiency for GPU-accelerated data processing through its warp-cooperative design, lock-free protocols, and dynamic resizing capabilities.

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [4] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO is a blockchain execution engine that combines optimistic concurrency control with object data model to improve performance under high contention workloads, achieving up to 42% higher throughput than state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Current parallel execution frameworks using OCC or PCC suffer performance degradation under high contention workloads, making execution layer the new blockchain performance bottleneck.

Method: NEMO introduces four innovations: greedy commit rule for owned objects, refined dependency handling, static read/write hints, and priority-based scheduler favoring transactions that unblock others.

Result: Simulation experiments show NEMO significantly reduces redundant computation and achieves higher throughput - up to 42% higher than Block-STM and 61% higher than pessimistic concurrency control baseline with 16 workers.

Conclusion: NEMO effectively addresses the execution layer bottleneck in blockchains under high contention by combining OCC with object data model and novel optimization techniques.

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [5] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: A Kubernetes operator and elastic scheduler for Charm++ HPC applications that enables dynamic rescaling to maximize cloud resource utilization while minimizing response time for high-priority jobs.


<details>
  <summary>Details</summary>
Motivation: The pay-as-you-go cost model of cloud resources requires efficient utilization through dynamic rescaling capabilities, which traditional MPI lacks but Charm++ natively supports through migratable objects.

Method: Developed a Kubernetes operator for running Charm++ applications and implemented a priority-based elastic job scheduler that dynamically rescales jobs based on Kubernetes cluster state.

Result: The elastic scheduler demonstrates significant performance improvements over traditional static schedulers, with minimal overhead for rescaling HPC jobs.

Conclusion: The proposed solution effectively enables dynamic rescaling of HPC applications in cloud environments, maximizing resource utilization while prioritizing high-priority jobs.

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [6] [Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks](https://arxiv.org/abs/2510.15215)
*Zhimin Qiu,Feng Liu,Yuxiao Wang,Chenrui Hu,Ziyu Cheng,Di Wu*

Main category: cs.DC

TL;DR: Proposes a graph neural network approach for traffic prediction in distributed backend systems, combining spatial graph structures with temporal dependencies to overcome traditional model limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle to capture complex dependencies and dynamic features in distributed systems, necessitating a more sophisticated approach for accurate traffic forecasting.

Method: Abstracts system as graph with nodes/edges, uses graph convolution for spatial propagation, gated recurrent structure for temporal modeling, spatiotemporal joint modeling module, and decoder for predictions trained with mean squared error.

Result: Achieves stable performance and low error across different prediction horizons and model depths, significantly improving accuracy and robustness compared to mainstream baselines.

Conclusion: Demonstrates the effectiveness of graph neural networks for traffic prediction in distributed backend systems and verifies their potential in complex system modeling.

Abstract: This paper addresses the problem of traffic prediction in distributed backend
systems and proposes a graph neural network based modeling approach to overcome
the limitations of traditional models in capturing complex dependencies and
dynamic features. The system is abstracted as a graph with nodes and edges,
where node features represent traffic and resource states, and adjacency
relations describe service interactions. A graph convolution mechanism enables
multi order propagation and aggregation of node features, while a gated
recurrent structure models historical sequences dynamically, thus integrating
spatial structures with temporal evolution. A spatiotemporal joint modeling
module further fuses graph representation with temporal dependency, and a
decoder generates future traffic predictions. The model is trained with mean
squared error to minimize deviations from actual values. Experiments based on
public distributed system logs construct combined inputs of node features,
topology, and sequences, and compare the proposed method with mainstream
baselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method
achieves stable performance and low error across different prediction horizons
and model depths, significantly improving the accuracy and robustness of
traffic forecasting in distributed backend systems and verifying the potential
of graph neural networks in complex system modeling.

</details>


### [7] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan is a controller that enables LLM infrastructure to signal applications to adjust output length based on system load, reducing latency and energy consumption.


<details>
  <summary>Details</summary>
Motivation: LLM applications operate blindly to infrastructure load, causing latency inflation and poor user experience during congestion.

Method: beLLMan actively signals first-party LLM applications to progressively adjust output length in response to changing system load conditions.

Result: On H100 GPU testbed: 8X lower end-to-end latency, 25% energy reduction while serving 19% more requests for summarization workload during congestion.

Conclusion: beLLMan effectively manages system load by controlling output length, significantly improving latency and energy efficiency in LLM inference.

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [8] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: This paper explores the trade-offs between local and cloud-based simulation environments for embedded AI systems, focusing on scalability vs. privacy, and proposes solutions to improve trust in remote simulations.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of embedded systems and complex AI algorithms requires powerful hardware/software co-design methodology using virtual prototyping. The diversity of simulation solutions and availability of remote computing resources create new operational possibilities.

Method: The work examines the dichotomy between local and cloud-based simulation environments, analyzing how compute infrastructure setup impacts performance and data security. It discusses development workflows for embedded AI and the role of efficient simulations.

Result: The paper highlights the trade-offs between scalability and privacy in simulation environments, and how infrastructure choices affect execution performance and data security in embedded AI development.

Conclusion: The proposed solution aims to sustainably improve trust in remote simulations and facilitate the adoption of virtual prototyping practices for embedded AI systems.

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [9] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: Discrete load balancing via matchings achieves constant discrepancy of 3 in rounds matching continuous balancing bounds, improving previous works and showing discrete balancing is as efficient as continuous.


<details>
  <summary>Details</summary>
Motivation: To understand if discrete load balancing with integer tokens can achieve similar efficiency as continuous fractional load balancing, and to improve upon previous large-constant discrepancy bounds.

Method: Simple local balancing schemes using matchings where tokens are averaged between matched nodes, with random tie-breaking for odd sums. Covers three models: random matching per round, fixed periodic matching sequences, and asynchronous edge balancing.

Result: With high probability, achieves discrepancy of 3 in number of rounds that asymptotically matches spectral bounds for continuous load balancing, working for arbitrary graphs.

Conclusion: Discrete load balancing is no harder than continuous load balancing, achieving small constant discrepancy efficiently across various matching models and arbitrary graph structures.

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [10] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: UWFQ scheduler improves Spark performance by ensuring user-level fairness and reducing job response times through virtual fair queuing and dynamic task partitioning.


<details>
  <summary>Details</summary>
Motivation: Spark's built-in schedulers struggle with maintaining user-level fairness and low response times in shared applications, favoring users who submit more jobs and lacking adaptability to dynamic workloads.

Method: Developed User Weighted Fair Queuing (UWFQ) scheduler that simulates virtual fair queuing, schedules jobs based on estimated finish times with bounded fairness, and introduces runtime partitioning to address task skew and reduce priority inversions.

Result: UWFQ reduces average response time of small jobs by up to 74% compared to existing Spark schedulers and state-of-the-art fair scheduling algorithms, as demonstrated through multi-user synthetic workloads and Google cluster traces.

Conclusion: UWFQ effectively addresses Spark's scheduling limitations by providing better user-level fairness and significantly improved job performance through innovative queuing and dynamic task partitioning techniques.

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [11] [Retrofitting Service Dependency Discovery in Distributed Systems](https://arxiv.org/abs/2510.15490)
*Diogo Landau,Gijs Blanken,Jorge Barbosa,Nishant Saurabh*

Main category: cs.DC

TL;DR: XXXX is a novel runtime system that constructs process-level service dependency graphs in distributed systems, overcoming NAT obfuscation through non-disruptive TCP packet header metadata injection without requiring source code instrumentation.


<details>
  <summary>Details</summary>
Motivation: Modern distributed systems with complex service dependencies suffer from cascading failures, and existing runtime approaches fail to accurately infer service dependencies due to NAT obfuscation that hides actual service hosts.

Method: XXXX implements non-disruptive metadata injection into TCP packet headers that maintains protocol correctness across host boundaries, working without source code instrumentation and remaining resilient under complex network routing including NAT.

Result: XXXX was the only approach that performed consistently across all network configurations (NAT-free, internal-NAT, external-NAT) and achieved precision and recall values of 100% in most scenarios, outperforming three state-of-the-art systems.

Conclusion: XXXX provides an effective solution for constructing accurate service dependency graphs in complex distributed environments with NAT, offering consistent performance and high correctness without disrupting existing TCP connections.

Abstract: Modern distributed systems rely on complex networks of interconnected
services, creating direct or indirect dependencies that can propagate faults
and cause cascading failures. To localize the root cause of performance
degradation in these environments, constructing a service dependency graph is
highly beneficial. However, building an accurate service dependency graph is
impaired by complex routing techniques, such as Network Address Translation
(NAT), an essential mechanism for connecting services across networks. NAT
obfuscates the actual hosts running the services, causing existing run-time
approaches that passively observe network metadata to fail in accurately
inferring service dependencies. To this end, this paper introduces XXXX, a
novel run-time system for constructing process-level service dependency graphs.
It operates without source code instrumentation and remains resilient under
complex network routing mechanisms, including NAT. XXXX implements a
non-disruptive method of injecting metadata onto a TCP packet's header that
maintains protocol correctness across host boundaries. In other words, if no
receiving agent is present, the instrumentation leaves existing TCP connections
unaffected, ensuring non-disruptive operation when it is partially deployed
across hosts. We evaluated XXXX extensively against three state-of-the-art
systems across nine scenarios, involving three network configurations
(NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX
was the only approach that performed consistently across networking
configurations. With regards to correctness, it performed on par with, or
better than, the state-of-the-art with precision and recall values of 100% in
the majority of the scenarios.

</details>


### [12] [GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters](https://arxiv.org/abs/2510.15652)
*Ahmad Raeisi,Mahdi Dolati,Sina Darabi,Sadegh Talebi,Patrick Eugster,Ahmad Khonsari*

Main category: cs.DC

TL;DR: A learning-based system for managing ML workloads in heterogeneous clusters using two neural networks to optimize resource allocation while minimizing energy consumption and meeting performance requirements.


<details>
  <summary>Details</summary>
Motivation: The growing computational demands in ML and the impracticality of upgrading to latest hardware necessitate sustainable use of existing mixed-generation resources in heterogeneous clusters.

Method: The system uses two neural networks: one provides initial estimates of model performance on different hardware and co-location effects, while an optimizer allocates resources based on these estimates. After deployment, real performance data refines predictions via a second neural network.

Result: The approach creates an adaptive, iterative system that learns over time to make more effective resource allocation decisions, improving estimates for both allocated and unallocated hardware and unobserved co-location scenarios.

Conclusion: The proposed learning-based architecture enables efficient and sustainable resource management in heterogeneous deep learning clusters through continuous adaptation and refinement of allocation decisions.

Abstract: The growing demand for computational resources in machine learning has made
efficient resource allocation a critical challenge, especially in heterogeneous
hardware clusters where devices vary in capability, age, and energy efficiency.
Upgrading to the latest hardware is often infeasible, making sustainable use of
existing, mixed-generation resources essential. In this paper, we propose a
learning-based architecture for managing machine learning workloads in
heterogeneous clusters. The system operates online, allocating resources to
incoming training or inference requests while minimizing energy consumption and
meeting performance requirements. It uses two neural networks: the first
provides initial estimates of how well a new model will utilize different
hardware types and how it will affect co-located models. An optimizer then
allocates resources based on these estimates. After deployment, the system
monitors real performance and uses this data to refine its predictions via a
second neural network. This updated model improves estimates not only for the
current hardware but also for hardware not initially allocated and for
co-location scenarios not yet observed. The result is an adaptive, iterative
approach that learns over time to make more effective resource allocation
decisions in heterogeneous deep learning clusters.

</details>


### [13] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: This paper proves a superconstant lower bound of 2^Ω(log* n) for distributed Lovász local lemma (LLL) problems in quantum-LOCAL models, specifically for sinkless orientation, using a new lower bound technique.


<details>
  <summary>Details</summary>
Motivation: Recent advances in quantum computing have focused on distributed quantum computing, and there was a need to understand the complexity of fundamental problems like the Lovász local lemma in quantum distributed models, addressing open questions about lower bounds.

Method: The authors developed an entirely new lower bound technique to prove complexity bounds, focusing on the sinkless orientation problem as a special case of LLL, and analyzed it in the quantum-LOCAL model and the stronger randomized online-LOCAL model.

Result: They proved a lower bound of 2^Ω(log* n) for distributed LLL problems, specifically for sinkless orientation, which represents the first superconstant lower bound for these problems across various distributed computing models.

Conclusion: The work establishes fundamental complexity limitations for distributed LLL problems in quantum settings and introduces a promising new technique for proving post-quantum lower bounds that could be applicable to many important locality problems.

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [14] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky is a full-stack FPGA-aware orchestration engine that addresses FPGA limitations in cloud environments by providing virtualization, state management, and cloud-native orchestration services, achieving high performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: FPGAs face adoption barriers in cloud-native environments due to lack of virtualization, isolation, and preemption support in CPU-oriented orchestrators, leading to low scalability and flexibility.

Method: Funky provides three key contributions: FPGA virtualization for lightweight sandboxes, FPGA state management enabling task preemption and checkpointing, and FPGA-aware orchestration components following CRI/OCI specifications.

Result: Funky ports 23 OpenCL applications with only 3.4% code modification, reduces OCI image sizes by 28.7x compared to AMD's containers, incurs only 7.4% performance overhead, and demonstrates scalability and fault tolerance using Google production traces.

Conclusion: Funky successfully enables cloud-native FPGA orchestration with high performance, strong isolation, and efficient resource management, making FPGAs more accessible and practical for cloud deployments.

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [15] [Maratona Linux a tale of upgrading from Ubuntu 20.04 to 22.04](https://arxiv.org/abs/2510.15263)
*Davi Antônio da Silva Santos,Bruno César Ribas*

Main category: cs.OS

TL;DR: Maratona Linux is a development environment for ICPC's South American programming contest, built on Ubuntu LTS with IDEs, compilers, and network restrictions. It has been successfully migrated from Ubuntu 20.04 to 22.04 LTS with added improvements.


<details>
  <summary>Details</summary>
Motivation: To create and maintain a standardized development environment for the ICPC South American regional programming contest that ensures consistency, security, and proper tooling for all participants.

Method: Developed as Debian packages that modify standard Ubuntu installations, installing necessary development tools (IDEs, compilers, debuggers, interpreters), documentation, and enforcing network restrictions. The project involves systematic migration between Ubuntu LTS versions and continuous improvement of the packaging system.

Result: Successfully migrated from Ubuntu 16.04 to 20.04 and then to 22.04 LTS. Improved the system by adding static analyzers, updating package dependencies, splitting large packages, and enhancing the packaging pipeline.

Conclusion: Maratona Linux has proven to be a robust and maintainable development environment for programming competitions, with successful version migrations and continuous improvements that enhance its functionality and reliability for contest participants.

Abstract: Maratona Linux is the development environment used since 2016 on the
``Maratona de Programa\c{c}\~ao'', ICPC's South American regional contest. It
consists of Debian packages that modify a standard Ubuntu installation in order
to make it suitable for the competition, installing IDEs, documentation,
compilers, debuggers, interpreters, and enforcing network restrictions. The
project, which began based on Ubuntu 16.04, has been successfully migrated from
20.04 to 22.04, the current Long-term Support (LTS) version. The project has
also been improved by adding static analyzers, updating the package dependency
map, splitting large packages, and enhancing the packaging pipeline.

</details>
