<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems](https://arxiv.org/abs/2512.21701)
*Nan Chen,Xiaotian Dai,Tong Cheng,Alan Burns,Iain Bate,Shuai Zhao*

Main category: cs.OS

TL;DR: LEFT-RS is a lock-free fault-tolerant protocol for multicore real-time systems that enables concurrent resource access, improves fault recovery efficiency, and significantly enhances schedulability.


<details>
  <summary>Details</summary>
Motivation: Multicore embedded systems require resource sharing for real-time applications, but conventional locking protocols don't handle transient faults within critical sections, and existing fault tolerance approaches increase blocking or have coordination overhead.

Method: Proposed LEFT-RS protocol allows tasks to concurrently access and read global resources in parallel critical sections, enabling early completion for successful tasks when others experience faults, with limited overhead and enhanced fault resilience.

Result: Extensive evaluation shows LEFT-RS significantly outperforms existing approaches, achieving up to 84.5% improvement in schedulability on average, with comprehensive worst-case response time analysis ensuring timing guarantees.

Conclusion: LEFT-RS provides an effective lock-free fault-tolerant resource sharing solution for multicore real-time systems that addresses limitations of conventional approaches while improving efficiency and schedulability.

Abstract: Emerging real-time applications have driven the transition to multicore embedded systems, where tasks must share resources due to functional demands and limited availability. These resources, whether local or global, are protected within critical sections to prevent race conditions, with locking protocols ensuring both exclusive access and timing requirements. However, transient faults occurring within critical sections can disrupt execution and propagate errors across multiple tasks. Conventional locking protocols fail to address such faults, and integrating traditional fault tolerance techniques often increases blocking. Recent approaches improve fault recovery through parallel replica execution; however, challenges remain due to sequential accessing, coordination overhead, and susceptibility to common-mode faults. In this paper, we propose a Lock-frEe Fault-Tolerant Resource Sharing (LEFT-RS) protocol for multicore real-time systems. LEFT-RS allows tasks to concurrently access and read global resources while entering their critical sections in parallel. Each task can complete its access earlier upon successful execution if other tasks experience faults, thereby improving the efficiency of resource usage. Our design also limits the overhead and enhances fault resilience. We present a comprehensive worst-case response time analysis to ensure timing guarantees. Extensive evaluation results demonstrate that our method significantly outperforms existing approaches, achieving up to an 84.5% improvement in schedulability on average.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: Smart cities use data-centric architectures to improve urban services efficiency, sustainability, and resilience.


<details>
  <summary>Details</summary>
Motivation: Urban areas face challenges in service delivery, resource management, and resilience to disruptions. Traditional approaches may lack the adaptability and efficiency needed for modern urban demands.

Method: Adopting data-centric architectures that leverage data collection, analysis, and integration across urban systems and services.

Result: Enhanced operational efficiency, improved sustainability outcomes, and increased resilience of urban services through data-driven decision making.

Conclusion: Data-centric approaches are essential for transforming smart cities into more efficient, sustainable, and resilient urban environments.

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [3] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: FinDEP is a fine-grained task scheduling algorithm for disaggregated expert parallelism that improves MoE inference throughput by maximizing task overlap through computational partitioning and optimized scheduling.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-experts (MoE) architectures scale model size with sublinear computational cost but suffer from memory-intensive inference due to KV caches and sparse expert activation. Existing disaggregated expert parallelism (DEP) approaches lack support for shared experts and efficient task scheduling, limiting performance.

Method: FinDEP introduces three innovations: 1) partitioning computation and communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization problem that supports variable granularity and ordering, and 3) developing an efficient solver for the large search space of scheduling possibilities.

Result: Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE models show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

Conclusion: FinDEP effectively addresses the limitations of existing DEP approaches by introducing fine-grained task scheduling that maximizes task overlap, significantly improving MoE inference throughput while supporting shared experts and variable granularity.

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [4] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: nncase is an open-source compilation framework that addresses memory architecture heterogeneity for efficient LLM deployment through e-graph-based term rewriting, outperforming mainstream frameworks and achieving performance comparable to hand-optimized solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional compilers struggle with fragmented workflows and high adaptation costs due to memory architecture heterogeneity, hindering efficient deployment of large language models across diverse hardware targets.

Method: nncase uses an e-graph-based term rewriting engine to solve the phase ordering problem, with three key modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for cache locality. A buffer-aware Codegen phase ensures efficient kernel instantiation.

Result: nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models, and achieves performance comparable to hand-optimized llama.cpp on CPUs, demonstrating effective automated compilation for high-performance LLM deployment.

Conclusion: nncase provides a viable automated compilation solution for high-performance LLM deployment across diverse hardware targets, addressing the challenges of memory architecture heterogeneity through unified optimization and global exploration strategies.

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [5] [Embedding Samples Dispatching for Recommendation Model Training in Edge Environments](https://arxiv.org/abs/2512.21615)
*Guopeng Li,Haisheng Tan,Chi Zhang,Hongqiu Ni,Zilong Wang,Xinyue Zhang,Yang Xu,Han Tian*

Main category: cs.DC

TL;DR: ESD optimizes embedding sample dispatch to edge workers to reduce transmission costs in DLRM training, achieving up to 36.76% cost reduction and 1.74x training speedup.


<details>
  <summary>Details</summary>
Motivation: Training DLRMs on edge workers benefits privacy, latency, and personalization, but embedding table size requires parameter servers, causing high transmission costs that dominate training cycles.

Method: Developed ESD mechanism with HybridDis dispatch method that combines optimal and heuristic algorithms to balance decision quality and resource consumption when dispatching embedding samples to edge workers.

Result: ESD reduces embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training compared to state-of-the-art mechanisms on real-world workloads.

Conclusion: ESD effectively addresses edge-specific challenges like heterogeneous networks and limited resources by optimizing embedding sample dispatch, significantly reducing transmission costs and accelerating DLRM training.

Abstract: Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.

</details>


### [6] [Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference](https://arxiv.org/abs/2512.21730)
*Linyi Jiang,Yifei Zhu,Hao Yin,Bo Li*

Main category: cs.DC

TL;DR: Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD videos using vision transformers by dynamically identifying and transmitting critical patches while balancing latency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Processing Ultra-HD videos with transformer-based vision models faces computational overhead in on-device computing or transmission overhead in cloud computing, creating a bottleneck for real-time applications.

Method: Hyperion integrates three key components: 1) collaboration-aware importance scorer identifying critical regions at patch level, 2) dynamic scheduler adapting patch transmission quality to balance latency and accuracy under dynamic networks, and 3) weighted ensembler fusing edge and cloud results.

Result: Hyperion enhances frame processing rate by up to 1.61 times and improves accuracy by up to 20.2% compared to state-of-the-art baselines under various network environments.

Conclusion: Hyperion successfully addresses the computational and transmission bottlenecks of Ultra-HD vision transformers through a cloud-device collaborative approach, enabling efficient real-time inference on high-resolution video data.

Abstract: Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.

</details>


### [7] [LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices](https://arxiv.org/abs/2512.21835)
*Mingyu Sun,Xiao Zhang,Shen Qu,Yan Li,Mengbai Xiao,Yuan Yuan,Dongxiao Yu*

Main category: cs.DC

TL;DR: LIME enables lossless inference of large language models across memory-constrained edge devices using interleaved pipeline parallelism and model offloading, achieving significant speedups without accuracy degradation.


<details>
  <summary>Details</summary>
Motivation: LLMs have powerful reasoning capabilities but face challenges for edge deployment due to massive parameter scales, limited computational resources on edge devices, memory constraints, and network bandwidth bottlenecks at the edge.

Method: LIME employs interleaved pipeline parallelism with model offloading to balance computation and communication across multiple edge devices. It includes a fine-grained offline allocation scheduler and online memory adaptation strategy to optimize computing/storage resources and minimize inference latency.

Result: Deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct inference, LIME achieves 1.7× speedup under sporadic request patterns and 3.7× speedup under bursty request patterns compared to state-of-the-art baselines, with no model accuracy compromise.

Conclusion: LIME successfully addresses edge deployment challenges for large models by enabling lossless inference across memory-constrained devices through collaborative optimization of computation, communication, and memory resources.

Abstract: Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\times$ and 3.7$\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.

</details>


### [8] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: This paper presents the first systematic study of resource allocation for distributed LLM inference, focusing on block placement and request routing optimization to reduce inference time in geographically-distributed systems.


<details>
  <summary>Details</summary>
Motivation: Large language models are expensive to deploy due to GPU requirements. While distributed systems like PETALS enable LLM deployment across low-end GPUs over the Internet, their performance critically depends on optimal resource allocation, which remains an unsolved problem.

Method: The authors develop experimentally validated performance models, formulate block placement and request routing as a mixed integer linear programming problem (proving NP-hardness), create a polynomial-complexity algorithm with performance guarantees, and adapt it for online settings. They also build a CPU-only simulator for performance prediction.

Result: The proposed solution substantially reduces inference time compared to state-of-the-art methods in diverse geographically-distributed server settings. The CPU-only simulator accurately predicts GPU server performance for large deployments.

Conclusion: This work provides the first systematic approach to resource allocation in distributed LLM inference, offering practical solutions with performance guarantees that can lower deployment barriers and facilitate future research through accessible simulation tools.

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [9] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST is a GPU-accelerated BFS framework that leverages Tensor Cores for irregular graph computations through bitmap-oriented structures, graph reordering, and optimized SpMSpV operations.


<details>
  <summary>Details</summary>
Motivation: Modern GPUs have specialized Tensor Cores with high throughput for dense operations, but exploiting them for irregular, unstructured graph computations like BFS is challenging due to load imbalance, redundancy, and synchronization issues.

Method: BLEST introduces Binarised Virtual Slice Sets (BVSS) for warp-level load balancing, applies two graph reordering strategies (compression-oriented for social graphs, bandwidth-reducing for non-social graphs), develops batched SpMSpV multiplication using bitwise TC tiles, and combines kernel fusion with lazy vertex updates.

Result: BLEST achieves average speedups of 3.58× over BerryBees, 4.64× over Gunrock, and 4.9× over GSWITCH across a broad set of real-world graphs.

Conclusion: BLEST successfully demonstrates how to effectively leverage Tensor Cores for irregular graph computations like BFS through careful algorithmic design, graph reordering, and optimization techniques that address load balancing, memory efficiency, and synchronization challenges.

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [10] [Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View](https://arxiv.org/abs/2512.22035)
*Yanmeng Wang,Zhiwen Dai,Shuai Wang,Jian Zhou,Fu Xiao,Tony Q. S. Quek,Tsung-Hui Chang*

Main category: cs.DC

TL;DR: FedAuto is a novel Federated Fine-Tuning framework that addresses connection failures and data heterogeneity through adaptive aggregation, operating without prior network knowledge and providing strong convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Current Federated Fine-Tuning (FFT) methods suffer from performance degradation due to unreliable server-client connections and heterogeneous client data distributions. Existing approaches make impractical assumptions about homogeneous network conditions or require prior knowledge of connection failures, which doesn't align with real-world networks with diverse communication standards and failure patterns.

Method: FedAuto uses adaptive aggregation to mitigate combined effects of connection failures and data heterogeneity. It operates without prior knowledge of network conditions or infrastructure modifications, enabling plug-and-play deployment. The framework employs a novel per-round aggregation perspective and provides rigorous convergence guarantees without assumptions on connection failure probabilities or client selection strategies.

Result: Extensive experiments show FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (including LoRA). It even surpasses strategies that rely on complex communication resource optimization.

Conclusion: FedAuto provides a practical and theoretically sound solution for Federated Fine-Tuning in real-world networks with unreliable connections and heterogeneous data, offering strong performance improvements without requiring prior network knowledge or infrastructure changes.

Abstract: Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.

</details>


### [11] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO is a specialized communication library for MoE models that optimizes data shuffling between experts by fusing data transformation with communication, achieving significant speedups over existing solutions.


<details>
  <summary>Details</summary>
Motivation: Existing communication libraries handle MoE's expert parallelism poorly, with data shuffling overhead accounting for over half of end-to-end runtime, creating a bottleneck for efficient MoE training and inference.

Method: FUSCO addresses the layout conflict between MoE's expert-major data layout and communication operations' device-major layout through fused data transformation and communication, using fine-grained data layout capture, pipelined communication engine, and lightweight planning with load-balancing mechanisms.

Result: FUSCO achieves up to 3.84× speedup over NCCL and 2.01× over DeepEP in benchmarks, reduces training latency by 1.17-1.39× vs NCCL and 1.10-1.19× vs DeepEP, and lowers first-token generation latency by 1.09-1.25× vs NCCL and 1.06-1.16× vs DeepEP.

Conclusion: FUSCO provides an efficient and lightweight solution for MoE data shuffling by addressing the fundamental layout conflict, significantly improving both training and inference performance for large-scale MoE models.

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [12] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: PRAXIS is an LLM-driven orchestrator that uses structured traversal over service dependency graphs and program dependence graphs to diagnose code- and configuration-caused cloud incidents, achieving 3.1x better RCA accuracy with 3.8x lower token consumption than ReAct baselines.


<details>
  <summary>Details</summary>
Motivation: Cloud incidents are extremely costly (over $2M per hour), with code- and configuration-related issues being the predominant root causes. Current approaches need improvement in accurately diagnosing these incidents efficiently.

Method: PRAXIS employs an LLM-driven structured traversal over two types of graphs: (1) service dependency graph (SDG) capturing microservice-level dependencies, and (2) hammock-block program dependence graph (PDG) capturing code-level dependencies for each microservice. The LLM acts as a traversal policy moving between services and code dependencies to localize and explain failures.

Result: PRAXIS improves root cause analysis (RCA) accuracy by up to 3.1x compared to state-of-the-art ReAct baselines while reducing token consumption by 3.8x. Demonstrated on 30 comprehensive real-world incidents compiled into an RCA benchmark.

Conclusion: PRAXIS provides an effective orchestrator for diagnosing cloud incidents through structured graph traversal with LLMs, significantly improving accuracy and efficiency in root cause analysis for code- and configuration-related cloud failures.

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling](https://arxiv.org/abs/2512.22066)
*Hannah Atmer,Yuan Yao,Thiemo Voigt,Stefanos Kaxiras*

Main category: cs.AR

TL;DR: This paper analyzes how SRAM size and operating frequency affect energy efficiency and performance in LLM inference, finding optimal configurations with high frequencies (1200-1400MHz) and small buffers (32-64KB) for best energy-delay product.


<details>
  <summary>Details</summary>
Motivation: Energy consumption is critical for LLM deployment costs and environmental impact. The paper aims to understand how hardware parameters (SRAM size, frequency) affect energy efficiency and performance, particularly addressing the distinct compute-bound prefill and memory-bound decode phases.

Method: Combined simulation methodology using OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity to analyze SRAM size and frequency impacts on LLM inference energy and performance.

Result: Total energy use is dominated by SRAM size in both phases, with larger buffers increasing static energy without proportional latency benefits. High frequencies reduce prefill latency but decode latency is capped by memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by cutting execution time and static energy consumption.

Conclusion: Optimal hardware configuration is high operating frequencies (1200-1400MHz) with small local buffer size (32-64KB) for best energy-delay product. Memory bandwidth acts as performance ceiling, and increasing compute frequency only helps until workload becomes memory-bound, providing architectural insights for energy-efficient LLM accelerator design.

Abstract: Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.

</details>


### [14] [Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation](https://arxiv.org/abs/2512.21777)
*Zhenya Zang,Xingda Li,David Day Uei Li*

Main category: cs.AR

TL;DR: A simplified predictive local learning rule eliminates global backpropagation and membrane integration, using sparse binary-driven vector additions triggered only by prediction errors, integrated into ELM to reduce training complexity from O(M³) to O(M) while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To eliminate the computational overhead of global backpropagation in neural networks and membrane integration in event-based training, enabling energy-efficient online learning on low-cost edge devices.

Method: Proposes a biologically inspired predictive local learning rule where weight updates occur only on prediction errors using sparse, binary-driven vector additions. This rule is integrated into an extreme learning machine (ELM), replacing conventional matrix inversion with the simplified update mechanism.

Result: Reduces training complexity from O(M³) to O(M) for M hidden layer nodes while maintaining comparable accuracy (within 3.6% degradation on training and 2.0% on test datasets). FPGA implementation shows significant reductions in computational and memory requirements.

Conclusion: The proposed approach demonstrates strong potential for energy-efficient online learning on low-cost edge devices by eliminating computationally intensive operations while preserving accuracy.

Abstract: We propose a simplified, biologically inspired predictive local learning rule that eliminates the need for global backpropagation in conventional neural networks and membrane integration in event-based training. Weight updates are triggered only on prediction errors and are performed using sparse, binary-driven vector additions. We integrate this rule into an extreme learning machine (ELM), replacing the conventional computationally intensive matrix inversion. Compared to standard ELM, our approach reduces the complexity of the training from O(M^3) to O(M), in terms of M nodes in the hidden layer, while maintaining comparable accuracy (within 3.6% and 2.0% degradation on training and test datasets, respectively). We demonstrate an FPGA implementation and compare it with existing studies, showing significant reductions in computational and memory requirements. This design demonstrates strong potential for energy-efficient online learning on low-cost edge devices.

</details>
