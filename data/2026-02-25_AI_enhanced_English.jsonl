{"id": "2602.20471", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20471", "abs": "https://arxiv.org/abs/2602.20471", "authors": ["Da Chen", "Guangyu Hu", "Kaihong Xu", "Kaichao Liang", "Songjiang Li", "Wei Yang", "XiangYu Wen", "Mingxuan Yuan"], "title": "SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction", "comment": "4 pages, 6 figures, accpeted by ISCAS 2026", "summary": "Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.", "AI": {"tldr": "SegSEM adapts SAM2 for SEM contour extraction using few-shot fine-tuning and hybrid architecture with traditional algorithm fallback.", "motivation": "Need to extract high-fidelity 2D contours from SEM images for OPC model calibration, but adapting foundation models like SAM2 to specialized domains with scarce annotated data is challenging.", "method": "Proposes SegSEM framework with two principles: 1) data-efficient fine-tuning strategy that selectively trains only model encoders, 2) robust hybrid architecture integrating traditional algorithm as confidence-aware fallback.", "result": "Experiments using small dataset of 60 production images demonstrate methodology's viability for SEM contour extraction in few-shot setting.", "conclusion": "Primary contribution is methodology for leveraging foundation models in data-constrained industrial applications, specifically for SEM contour extraction."}}
{"id": "2602.20515", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20515", "abs": "https://arxiv.org/abs/2602.20515", "authors": ["Rakshith Jayanth", "Viktor Prasanna"], "title": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill", "comment": null, "summary": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.", "AI": {"tldr": "FAST-Prefill is an FPGA accelerator for long-context LLM inference that uses dynamic sparse attention to speed up the prefill stage, achieving 2.5\u00d7 faster TTFT and 4.5\u00d7 better energy efficiency than GPUs.", "motivation": "The prefill stage in long-context LLM inference is computationally expensive due to self-attention over complete input contexts. Sparse attention helps but creates dynamic sparsity patterns that make the problem memory-bound. GPUs have huge energy requirements for long-context inference, making FPGAs attractive alternatives for acceleration.", "method": "FAST-Prefill uses three key innovations: 1) A fused pipeline unit with memory-aware execution order to efficiently generate sparse indices and reduce irregular memory accesses, 2) A liveness-driven, dual-tier cache to reduce off-chip memory traffic for KV cache access, and 3) A hybrid Matrix Processing Unit (MPU) using DSPs and bit-plane decomposition with LUTs for high-throughput matrix multiplication.", "result": "Implemented on Alveo U280 FPGA and evaluated on Llama and Qwen models with context lengths from 4K to 128K tokens (batch size=1). Achieved average speedup of up to 2.5\u00d7 in Time To First Token (TTFT) and 4.5\u00d7 improvement in energy efficiency compared to Nvidia A5000 GPU implementation.", "conclusion": "FAST-Prefill demonstrates that FPGA-based acceleration with dynamic sparse attention is effective for long-context LLM inference, significantly improving both performance and energy efficiency compared to GPU implementations, making it a promising approach for energy-efficient long-context processing."}}
{"id": "2602.20662", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20662", "abs": "https://arxiv.org/abs/2602.20662", "authors": ["Hongyi Guan", "Yijia Zhang", "Wenqiang Wang", "Yizhao Gao", "Shijie Cao", "Chen Zhang", "Ningyi Xu"], "title": "TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence", "comment": "13 pages", "summary": "The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence.", "AI": {"tldr": "TOM is a hybrid ROM-SRAM accelerator with ternary quantization for efficient LLM deployment on edge devices, addressing memory wall challenges through extreme density and on-device tunability.", "motivation": "LLM deployment on edge devices faces memory wall challenges: limited on-device memory capacity/bandwidth constrains model size, inference speed, and on-device adaptation capabilities.", "method": "TOM combines ternary quantization with ROM for extreme density, using: 1) sparsity-aware ROM architecture synthesizing ternary weights as standard-cell logic; 2) distributed processing with ROM banks co-located with SRAM-based QLoRA adapters; 3) workload-aware dynamic power gating for ROM banks.", "result": "TOM achieves 3,306 TPS inference throughput using BitNet-2B model, demonstrating real-time, energy-efficient edge intelligence capabilities.", "conclusion": "TOM successfully addresses memory wall challenges in edge LLM deployment through a hybrid ROM-SRAM architecture with ternary quantization, balancing extreme memory density with on-device tunability for efficient edge intelligence."}}
{"id": "2602.20802", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20802", "abs": "https://arxiv.org/abs/2602.20802", "authors": ["Philippos Papaphilippou"], "title": "LUTstructions: Self-loading FPGA-based Reconfigurable Instructions", "comment": null, "summary": "General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration.", "AI": {"tldr": "A novel FPGA-based softcore architecture called LUTstruction enables reconfigurable instructions by loading instruction implementations from main memory, achieving FPGA-on-FPGA implementation with no frequency overhead.", "motivation": "General-purpose processors have limited instruction sets that cannot efficiently express arbitrary tasks, even with extensive vector extensions. There's a need for more flexible, reconfigurable instruction capabilities.", "method": "The paper proposes LUTstruction, a custom FPGA architecture designed for low-latency custom instructions and wide reconfiguration. It implements a softcore with reconfigurable areas that can load instruction implementation bitstreams from main memory, creating an FPGA-on-FPGA system.", "result": "The resulting softcore achieves FPGA-on-FPGA implementation of instruction implementations with no notable operating frequency overhead. The architecture enables seamless loading of instruction implementations from main memory.", "conclusion": "The LUTstruction architecture successfully demonstrates the feasibility of reconfigurable instructions in softcores, providing a flexible alternative to fixed instruction sets while maintaining performance through FPGA-on-FPGA implementation."}}
{"id": "2602.20341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20341", "abs": "https://arxiv.org/abs/2602.20341", "authors": ["Ignacio Amores-Sesar", "Mirza Ahad Baig", "Seth Gilbert", "Ray Neiheiser", "Michelle X. Yeo"], "title": "The Tragedy of Chain Commons", "comment": null, "summary": "Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.", "AI": {"tldr": "The paper analyzes Byzantine Fault Tolerant consensus in blockchains, focusing on the trade-offs in decoupled designs that separate transaction ordering from execution, and introduces a new \"gaslighting\" attack.", "motivation": "To address the bottleneck of transaction execution/validation in BFT consensus, modular decoupled designs separate ordering from execution, but this creates vulnerabilities where invalid transactions remain in the ledger, increasing costs and enabling strategic behavior.", "method": "The authors develop a formal framework to analyze consensus-execution interaction, identify a new \"gaslighting\" attack in decoupled designs, prove fundamental trade-offs between attack resilience and resource utilization, and propose an intermediate leader-based protocol model.", "result": "The study reveals that decoupled designs enable gaslighting attacks, demonstrates an impossibility result where deterministic resilience to gaslighting and high resource utilization cannot both be achieved, and proposes a practical intermediate model for leader-based protocols.", "conclusion": "There's a fundamental trade-off in decoupled blockchain designs between gaslighting attack resilience and resource utilization, requiring careful protocol design choices, with an intermediate leader-based model offering a practical solution balancing security and performance."}}
{"id": "2602.20826", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2602.20826", "abs": "https://arxiv.org/abs/2602.20826", "authors": ["Yuanhai Zhang", "Songyang He", "Ruizhe Gou", "Mingyue Cui", "Boyang Li", "Shuai Zhao", "Kai Huang"], "title": "Exploiting Dependency and Parallelism: Real-Time Scheduling and Analysis for GPU Tasks", "comment": null, "summary": "With the rapid advancement of Artificial Intelligence, the Graphics Processing Unit (GPU) has become increasingly essential across a growing number of safety-critical application domains. Applying a GPU is indispensable for parallel computing; however, the complex data dependencies and resource contention across kernels within a GPU task may unpredictably delay its execution time. To address these problems, this paper presents a scheduling and analysis method for Directed Acyclic Graph (DAG)-structured GPU tasks. Given a DAG representation, the proposed scheduling scales the kernel-level parallelism and establishes inter-kernel dependencies to provide a reduced and predictable DAG response time. The corresponding timing analysis yields a safe yet nonpessimistic makespan bound without any assumption on kernel priorities. The proposed method is implemented using the standard CUDA API, requiring no additional software or hardware support. Experimental results under synthetic and real-world benchmarks demonstrate that the proposed approach effectively reduces the worst-case makespan and measured task execution time compared to the existing methods up to 32.8% and 21.3%, respectively.", "AI": {"tldr": "This paper presents a scheduling and analysis method for DAG-structured GPU tasks that reduces worst-case makespan and execution time by scaling kernel-level parallelism and establishing inter-kernel dependencies.", "motivation": "GPUs are increasingly used in safety-critical domains where predictable execution time is essential. However, complex data dependencies and resource contention across GPU kernels can unpredictably delay execution times, creating a need for methods to ensure timing predictability.", "method": "The method uses Directed Acyclic Graph (DAG) representations of GPU tasks, scales kernel-level parallelism, and establishes inter-kernel dependencies to provide reduced and predictable DAG response times. It includes timing analysis that yields safe yet non-pessimistic makespan bounds without kernel priority assumptions, implemented using standard CUDA API without additional hardware/software support.", "result": "Experimental results with synthetic and real-world benchmarks show the proposed approach reduces worst-case makespan by up to 32.8% and measured task execution time by up to 21.3% compared to existing methods.", "conclusion": "The proposed scheduling and analysis method effectively addresses GPU timing unpredictability in safety-critical applications by providing predictable DAG response times with significant performance improvements over existing approaches."}}
{"id": "2602.20444", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20444", "abs": "https://arxiv.org/abs/2602.20444", "authors": ["Paul Borrill"], "title": "Circumventing the FLP Impossibility Result with Open Atomic Ethernet", "comment": "12 pages, 3 figures, 1 table", "summary": "The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.", "AI": {"tldr": "FLP impossibility result can be circumvented by Open Atomic Ethernet (OAE) using bisynchronous communication, achieving deterministic consensus without violating FLP.", "motivation": "The FLP impossibility result has constrained distributed computing for 40 years, forcing workarounds like randomization and weakened models. This paper challenges the assumption that FLP is an absolute constraint by showing it only applies to specific system models.", "method": "Introduces Open Atomic Ethernet (OAE) with bisynchronous communication - bounded-time bilateral resolution where both parties reach common knowledge at every round boundary. Uses a swap-based protocol at Layer 2 that rejects the asynchronous model assumptions underlying FLP.", "result": "OAE achieves deterministic atomic coordination without violating FLP impossibility, circumventing it by operating outside the asynchronous model constraints that FLP assumes.", "conclusion": "FLP is not a fundamental law but a theorem about specific models. By using bisynchronous communication at Layer 2, OAE provides deterministic consensus while respecting FLP's mathematical validity within its original model."}}
{"id": "2602.20450", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20450", "abs": "https://arxiv.org/abs/2602.20450", "authors": ["Nihal Balivada", "Shrey Gupta", "Shashank Shreedhar Bhatt", "Suyash Gupta"], "title": "Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.", "AI": {"tldr": "Terraform is a novel client selection methodology for federated learning that uses gradient updates and deterministic selection to choose heterogeneous clients, achieving up to 47% higher accuracy than prior works.", "motivation": "Federated learning suffers from lower accuracy compared to traditional ML due to statistical heterogeneity across clients. Existing methods use model updates (loss and bias) that don't accurately represent client heterogeneity and have non-deterministic selection methods.", "method": "Terraform introduces a bi-pronged approach: 1) uses gradient updates instead of loss/bias to better represent client heterogeneity, and 2) employs a deterministic selection algorithm to choose heterogeneous clients for retraining.", "result": "Terraform achieves up to 47% higher accuracy over prior works. Comprehensive ablation studies and training time analyses demonstrate its efficiency and robustness.", "conclusion": "Terraform effectively addresses FL's accuracy limitations by using gradient-based heterogeneity representation and deterministic client selection, providing a robust solution for federated learning with statistical heterogeneity."}}
{"id": "2602.20561", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20561", "abs": "https://arxiv.org/abs/2602.20561", "authors": ["Sana Taghipour Anvar", "David Kaeli"], "title": "A Granularity Characterization of Task Scheduling Effectiveness", "comment": null, "summary": "Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.", "AI": {"tldr": "The paper introduces a framework linking task-graph dependency topology to scheduling overhead growth, showing that dependency structure (not just problem size) determines when dynamic scheduling overhead becomes dominant and causes strong-scaling breakdowns.", "motivation": "Task-based runtime systems face strong-scaling sensitivity to task granularity, where scheduling overhead can transition from negligible to dominant as parallelism increases. There's a lack of understanding about how algorithmic structure impacts whether dynamic scheduling is always beneficial, despite empirical observations of performance drops.", "method": "The authors introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. They characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation versus when it dominates performance.", "result": "Experimental evaluation on representative parallel workloads with diverse dependency patterns shows the characterization explains both gradual and abrupt strong-scaling breakdowns. Overhead models derived from dependency topology accurately predict strong-scaling limits and enable runtime decision rules for selecting dynamic or static execution.", "conclusion": "Dependency structure governs how scheduling overhead scales with parallelism, enabling practical runtime decisions about dynamic vs. static execution without requiring exhaustive strong-scaling studies or extensive offline tuning."}}
{"id": "2602.20656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20656", "abs": "https://arxiv.org/abs/2602.20656", "authors": ["Guanbin Xu", "ZhenGuo Xu", "Yuzhe Li", "Youhui Bai", "Ping Gong", "Chaoyi Ruan", "Cheng Li"], "title": "Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training", "comment": "6 pages, 8 figures", "summary": "Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations.", "AI": {"tldr": "Lagom is a system that co-tunes communication parameters to balance computation and communication resources, reducing optimization complexity from exponential to linear and achieving significant speedups over existing solutions.", "motivation": "Overlapping communication with computation is essential for distributed large-model training, but optimizing this overlap remains challenging, especially when computation becomes the bottleneck.", "method": "Lagom introduces a unified cost model and a priority-based search algorithm to co-tune communication parameters, balancing resource usage between computation and communication.", "result": "Evaluations on high- and low-bandwidth GPU clusters show Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelization strategies.", "conclusion": "Lagom effectively optimizes communication-computation overlap for distributed training by co-tuning parameters with reduced complexity, delivering consistent performance improvements across different network conditions."}}
{"id": "2602.20887", "categories": ["cs.DC", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.20887", "abs": "https://arxiv.org/abs/2602.20887", "authors": ["David Knapp", "Johannes Albrecht Holke", "Thomas Spenke", "Carsten Burstedde"], "title": "A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement", "comment": null, "summary": "The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.", "AI": {"tldr": "This paper introduces pyramid elements as a new functional type for tree-based AMR to connect tetrahedral and hexahedral elements in 3D mixed-element meshes, resolving hanging edge issues.", "motivation": "To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes by enabling proper connection between tetrahedral and hexahedral elements without hanging edges.", "method": "Introduces pyramid elements as a new functional type, presents a well-defined space-filling curve for pyramids, resolves element and forest-level challenges of pyramidal refinement, and generalizes global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange.", "result": "The demonstrations confirm the efficiency and scalability of the complete hybrid-element dynamic AMR framework with pyramid elements.", "conclusion": "The pyramid element successfully enables dynamic adaptive mesh refinement for 3D mixed-element meshes, providing a complete and scalable framework for hybrid-element AMR."}}
{"id": "2602.21022", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.21022", "abs": "https://arxiv.org/abs/2602.21022", "authors": ["Antonio Cruciani", "Avinandan Das", "Massimo Equi", "Henrik Lievonen", "Diep Luong-Le", "Augusto Modanese", "Jukka Suomela"], "title": "Is a LOCAL algorithm computable?", "comment": "33 pages, 1 figure", "summary": "Common definitions of the \"standard\" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.\n  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $\u03a0$ with the following properties:\n  1. $\u03a0$ can be solved in $O(\\log n)$ rounds if the \\textsf{LOCAL} model is uncomputable.\n  2. $\u03a0$ can be solved in $O(\\log n)$ rounds in the computable model if we know any upper bound on $n$.\n  3. $\u03a0$ requires $\u03a9(\\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.\n  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $\u03a0$, if you have any bound on $n$, then $\u03a0$ has the same round complexity in the computable and uncomputable models.", "AI": {"tldr": "The paper shows that computability assumptions in the LOCAL model matter for LCL problems, and reveals a connection between computability and knowledge of graph size n.", "motivation": "To address the ambiguity in LOCAL model definitions about whether nodes use arbitrary or computable functions, and to show this distinction matters even for LCL problems.", "method": "Theoretical analysis of LOCAL model variants (computable vs uncomputable) and their relationship with knowledge of graph size n, constructing a specific LCL problem \u03a0 to demonstrate the differences.", "result": "Found an LCL problem \u03a0 with: (1) O(log n) rounds in uncomputable model, (2) O(log n) rounds in computable model with knowledge of n, (3) \u03a9(\u221an) rounds in computable model without knowledge of n. Showed general connection between computability and knowledge of n for all LCL problems.", "conclusion": "Computability assumptions in LOCAL model are significant for LCL problems and are directly connected to knowledge of graph size n, requiring more precise model definitions."}}
{"id": "2602.21140", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21140", "abs": "https://arxiv.org/abs/2602.21140", "authors": ["Haley Li", "Xinglu Wang", "Cong Feng", "Chunxu Zuo", "Yanan Wang", "Hei Lo", "Yufei Cui", "Bingji Wang", "Duo Cui", "Shuming Jing", "Yizhou Shan", "Ying Xiong", "Jiannan Wang", "Yong Zhang", "Zhenan Fan"], "title": "ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments", "comment": "21 pages, 6 figures", "summary": "As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.", "AI": {"tldr": "ReviveMoE enables rapid failure recovery in large-scale LLM deployments without restarting serving instances, addressing the high costs of traditional restart approaches in model-as-a-service inference.", "motivation": "As LLM deployments scale across more hardware, failure probability increases significantly. Traditional recovery by restarting LLM serving instances is costly in MaaS inference settings due to delays from reloading model weights and recompiling computation graphs.", "method": "ReviveMoE is a method for rapid failure recovery that works without restarting serving instances. It supports both traditional LLM architectures (collocating MoE and attention) and disaggregated architectures (separating MoE from attention). It's integrated into Huawei Cloud's MaaS, built on xDeepServe serving platform and XCCL communications library.", "result": "The paper presents ReviveMoE as a solution implemented in Huawei Cloud's MaaS infrastructure, though specific performance metrics aren't provided in the abstract.", "conclusion": "ReviveMoE provides a robust countermeasure for inevitable failures in large-scale LLM deployments, offering rapid recovery without the costly delays associated with traditional restart approaches in MaaS inference settings."}}
{"id": "2602.21182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21182", "abs": "https://arxiv.org/abs/2602.21182", "authors": ["Paul Borrill"], "title": "Circumventing the CAP Theorem with Open Atomic Ethernet", "comment": "23 pages, 14 figures", "summary": "The CAP theorem is routinely treated as a systems law: under network partition, a replicated service must sacrifice either consistency or availability. The theorem is correct within its standard asynchronous network model, but operational practice depends on where partition-like phenomena become observable and on how lower layers discard or preserve semantic information about message fate. This paper argues that Open Atomic Ethernet (OAE) shifts the engineering regime in which CAP tradeoffs become application-visible by (i) replacing fire-and-forget link semantics with bounded-time bilateral reconciliation of endpoint state -- the property we call bisynchrony -- and (ii) avoiding Clos funnel points via an octavalent mesh in which each node can act as the root of a locally repaired spanning tree. The result is not the elimination of hard graph cuts, but a drastic reduction in the frequency and duration of application-visible \"soft partitions\" by detecting and healing dominant fabric faults within hundreds of nanoseconds. We connect this view to Brewer's original CAP framing, the formalization by Gilbert and Lynch, the CAL theorem of Lee et al., which replaces binary partition tolerance with a quantitative measure of apparent latency, and Abadi's PACELC extension.", "AI": {"tldr": "The paper argues that Open Atomic Ethernet (OAE) changes how CAP tradeoffs manifest in practice by introducing bisynchrony (bounded-time bilateral reconciliation) and an octavalent mesh topology, drastically reducing application-visible soft partitions through nanosecond-scale fault detection and healing.", "motivation": "The CAP theorem is widely treated as a fundamental systems law requiring tradeoffs between consistency and availability during network partitions. However, the authors argue that operational practice depends on where partition-like phenomena become observable and how lower layers handle message fate semantics. The motivation is to show how OAE shifts the engineering regime where CAP tradeoffs become visible to applications.", "method": "The paper proposes Open Atomic Ethernet (OAE) with two key innovations: (1) bisynchrony - replacing fire-and-forget link semantics with bounded-time bilateral reconciliation of endpoint state, and (2) an octavalent mesh topology that avoids Clos funnel points, allowing each node to act as the root of a locally repaired spanning tree.", "result": "OAE doesn't eliminate hard graph cuts but drastically reduces the frequency and duration of application-visible \"soft partitions\" by detecting and healing dominant fabric faults within hundreds of nanoseconds. This changes how CAP tradeoffs manifest in practice.", "conclusion": "The paper concludes that OAE shifts the engineering regime for CAP tradeoffs by reducing application-visible partition phenomena through bisynchrony and mesh topology, connecting this approach to Brewer's CAP, Gilbert and Lynch's formalization, CAL theorem (quantitative latency), and PACELC extension."}}
