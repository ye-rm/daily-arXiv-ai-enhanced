<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: The paper proposes 'Pervasive Context Management' to decouple LLM initialization from inferences, reducing execution time by 72.1% and enabling opportunistic scaling on 32.8% of cluster GPUs.


<details>
  <summary>Details</summary>
Motivation: Current HPC cluster designs are inadequate for Generative AI workloads, causing long wait times on batch queues or expensive LLM startup costs when resources are preempted.

Method: Decouple LLM initialization context from actual inferences and retain the context in GPUs until no longer needed, using 'Pervasive Context Management' technique.

Result: Reduced execution time by 72.1% (from 3 hours to 48 minutes) using same GPUs, and scaled opportunistically on 32.8% of cluster GPUs to further reduce execution time to 13 minutes.

Conclusion: Pervasive Context Management effectively addresses the limitations of current HPC cluster designs for Generative AI workloads by eliminating long queues and high startup costs.

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [2] [Anonymized Network Sensing using C++26 std::execution on GPUs](https://arxiv.org/abs/2510.14050)
*Michael Mandulak,Sayan Ghosh,S M Ferdous,Mahantesh Halappanavar,George Slota*

Main category: cs.DC

TL;DR: This paper presents a composable framework using C++26 Senders model for efficient network sensing analytics on dense-GPU systems, achieving significant performance improvements over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Large-scale network sensing requires parallel methods due to growing packet data, but GPU implementations face challenges with memory management and porting complex workloads. Composable frameworks using modern C++ can mitigate these issues.

Method: Uses the C++26 Senders model for asynchronous data operation chaining, treating GPUs as first-class execution resources. Implements the Anonymized Network Sensing Graph Challenge on dense-GPU systems with a commodity library-based approach.

Result: Achieves up to 55x performance improvements on 8x NVIDIA A100 GPUs compared to the reference serial GraphBLAS baseline, demonstrating that productive programming models don't compromise performance.

Conclusion: The C++26 Senders model provides an effective approach for developing multi-GPU network analytics workloads with standardized asynchronous semantics, delivering substantial performance gains while maintaining programming productivity.

Abstract: Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.

</details>


### [3] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex is a workflow-aware serving platform for agentic workloads that uses stage isolation to provision dedicated resource pools for each workflow stage, improving performance and enabling advanced serving features.


<details>
  <summary>Details</summary>
Motivation: To address inter-stage interference in agentic workflows and improve KV cache utilization, throughput, and performance predictability.

Method: Uses stage isolation by provisioning dedicated resource pools for each distinct stage of agentic workflows, with customized resource allocation and scheduling per stage.

Result: Mitigates inter-stage interference in compute and memory, leading to better KV cache utilization, higher throughput, and more predictable performance.

Conclusion: Cortex provides a foundation for advanced agent-native serving paradigms including malleable resource management, speculative execution of workflow branches, and shared multi-tiered caching for agentic state.

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [4] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: This paper presents scalable distributed memory algorithms for computing fixed-radius near-neighbor graphs in general metric spaces using cover trees, achieving significant speedups over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Many data analysis algorithms require fixed-radius near-neighbor graphs as a first step, but existing parallel solutions focus mainly on Euclidean spaces and approximate methods, while many applications need exact solutions with non-Euclidean metrics.

Method: The authors develop a shared-memory algorithm for cover tree construction and two distributed-memory algorithms: a simple point-partitioning strategy and a spatial-partitioning strategy, both leveraging the cover tree algorithm on each node.

Result: The algorithms show excellent parallel scaling across various datasets and metrics, achieving speedups up to 678.34x with 1024 cores for graphs with 70 neighbors per vertex, and up to 1590.99x with 4096 cores for graphs with 500 neighbors per vertex.

Conclusion: The proposed scalable sparsity-aware distributed memory algorithms using cover trees effectively compute near-neighbor graphs in general metric spaces, providing significant performance improvements over existing methods.

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [5] [Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability](https://arxiv.org/abs/2510.14151)
*Saeed Moradi,Koosha Esmaeilzadeh Khorasani,Sara Rouhani*

Main category: cs.DC

TL;DR: A blockchain-agnostic framework for interoperability between permissioned and permissionless networks using cryptographic techniques and lightweight architecture with enhanced transaction anonymity.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges in bridging permissioned and permissionless blockchains due to differences in access control, architectures, and security requirements, enabling collaborative ecosystems.

Method: Leverages cryptographic techniques for secure data exchanges, lightweight architectural design for simplified implementation, and integrates Clover and Dandelion++ protocols for enhanced transaction anonymity.

Result: Performance evaluations demonstrate effectiveness in achieving secure and efficient interoperability by measuring forwarding time, throughput, availability, and collusion impact across heterogeneous blockchain ecosystems.

Conclusion: The framework successfully enables interoperability between permissioned and permissionless blockchains through secure data exchanges, simplified implementation, and enhanced transaction anonymity.

Abstract: Interoperability is essential for transforming blockchains from isolated
networks into collaborative ecosystems, unlocking their full potential. While
significant progress has been made in public blockchain interoperability,
bridging permissioned and permissionless blockchains poses unique challenges
due to differences in access control, architectures, and security requirements.
This paper introduces a blockchain-agnostic framework to enable
interoperability between permissioned and permissionless networks. Leveraging
cryptographic techniques, the framework ensures secure data exchanges. Its
lightweight architectural design simplifies implementation and maintenance,
while the integration of Clover and Dandelion++ protocols enhances transaction
anonymity. Performance evaluations demonstrate the framework's effectiveness in
achieving secure and efficient interoperability by measuring the forwarding
time, the throughput, the availability, and their collusion impact of the
system across heterogeneous blockchain ecosystems.

</details>


### [6] [Deadlock-free routing for Full-mesh networks without using Virtual Channels](https://arxiv.org/abs/2510.14730)
*Alejandro Cano,Cristóbal Camarero,Carmen Martínez,Ramón Beivide*

Main category: cs.DC

TL;DR: TERA is a novel routing algorithm that uses an embedded physical subnetwork to provide deadlock-free non-minimal paths without virtual channels, outperforming existing approaches in Full-mesh networks.


<details>
  <summary>Details</summary>
Motivation: Virtual channels (VCs) in high-radix networks introduce significant overhead in switch area, power, and complexity, limiting scalability. Existing VC-less routing through link ordering suffers from performance degradation under adversarial traffic.

Method: Proposes TERA (Topology-Embedded Routing Algorithm) which employs an embedded physical subnetwork to provide deadlock-free non-minimal paths without using virtual channels.

Result: In Full-mesh networks, TERA outperforms link ordering by 80% under adversarial traffic and up to 100% in application kernels. Reduces buffer requirements by 50% compared to VC-based approaches while maintaining comparable latency and throughput. In 2D-HyperX, achieves up to 32% performance improvement over state-of-the-art algorithms using same VCs.

Conclusion: TERA provides an effective solution for deadlock-free routing in high-radix networks without the overhead of virtual channels, offering significant performance improvements and reduced buffer requirements.

Abstract: High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh
core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in
adaptive routing. However, VCs introduce significant overhead in the switch in
terms of area, power, and design complexity, limiting the switch scalability.
This paper starts by revisiting VC-less routing through link ordering schemes
in Full-mesh networks, which offer implementation simplicity but suffer from
performance degradation under adversarial traffic. Thus, to overcome these
challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel
routing algorithm which employs an embedded physical subnetwork to provide
deadlock-free non-minimal paths without using VCs.
  In a Full-mesh network, TERA outperforms link ordering routing algorithms by
80% when dealing with adversarial traffic, and up to 100% in application
kernels. Furthermore, compared to other VC-based approaches, it reduces buffer
requirements by 50%, while maintaining comparable latency and throughput.
Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms
state-of-the-art algorithms that use the same number of VCs, achieving
performance improvements of up to 32%.

</details>


### [7] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG is an asymmetric order-fair consensus protocol that verifies fair transaction ordering without recomputing the ordering graph, achieving higher performance than symmetric approaches while maintaining fairness guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing BFT consensus protocols like Themis require every replica to re-run the leader's expensive graph-based ordering computation, creating symmetric and redundant work that limits performance.

Method: AUTIG uses an asymmetric architecture where the leader maintains a persistent Unconfirmed-Transaction Incremental Graph (UTIG) and emits verifiable proofs of fairness, while followers validate proofs without maintaining historical state. Key innovations include incremental graph maintenance, decoupled pipelines, and proof designs covering internal pairs and frontier completeness.

Result: Experiments show AUTIG achieves higher throughput and lower end-to-end latency compared to symmetric graph-based baselines while preserving gamma-batch-order-fairness under partial synchrony.

Conclusion: AUTIG demonstrates that verifying fair order doesn't require recomputing it, enabling high-performance order-fair consensus through asymmetric verification of succinct proofs rather than symmetric recomputation.

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [8] [ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains](https://arxiv.org/abs/2510.14580)
*Hyein Woo,Miryeong Kwon,Jiseon Kim,Eunjee Na,Hanjin Choi,Seonghyeon Jang,Myoungsoo Jung*

Main category: cs.DC

TL;DR: ScalePool is a novel cluster architecture that uses unified hardware interconnects (XLink and CXL) instead of traditional networking to connect accelerators, featuring explicit memory tiering and achieving significant performance improvements for LLM training and memory-intensive workloads.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional long-distance networking for accelerator interconnection and address interoperability constraints in heterogeneous clusters, while enabling scalable and coherent memory sharing across clusters.

Method: Integrates Accelerator-Centric Links (XLink) for intra-cluster low-latency communication and hierarchical CXL-based switching fabrics for inter-cluster memory sharing. Uses explicit memory tiering with tier-1 for latency-critical operations and tier-2 for high-capacity memory pooling.

Result: Accelerates LLM training by 1.22x on average and up to 1.84x compared to conventional RDMA-based environments. Reduces latency by up to 4.5x for memory-intensive workloads through tier-2 memory disaggregation.

Conclusion: ScalePool successfully addresses accelerator interconnection challenges through unified hardware interconnects and memory tiering, demonstrating significant performance improvements for modern AI workloads like LLM training.

Abstract: This paper proposes ScalePool, a novel cluster architecture designed to
interconnect numerous accelerators using unified hardware interconnects rather
than traditional long-distance networking. ScalePool integrates
Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified
XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for
intra-cluster, low-latency accelerator communication, while using hierarchical
CXL-based switching fabrics for scalable and coherent inter-cluster memory
sharing. By abstracting interfaces through CXL, ScalePool structurally resolves
interoperability constraints, enabling heterogeneous cluster operation and
composable resource disaggregation. In addition, ScalePool introduces explicit
memory tiering: the latency-critical tier-1 combines accelerator-local memory
with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs
dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable
and efficient memory pooling. Evaluation results show that ScalePool
accelerates LLM training by 1.22x on average and up to 1.84x compared to
conventional RDMA-based environments. Furthermore, the proposed tier-2 memory
disaggregation strategy reduces latency by up to 4.5x for memory-intensive
workloads.

</details>


### [9] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA extends SJA from centralized to decentralized scheduling using auction theory and online optimization for MIG-enabled GPUs, enabling adaptive resource management through bidirectional job-scheduler negotiation.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized scheduling struggles with complex, variable workloads on MIG-enabled GPUs, requiring more scalable approaches that can handle temporal variability and maintain fairness.

Method: Jobs generate and score feasible subjobs in response to scheduler-announced execution windows, while scheduler performs policy-driven clearing balancing utilization, fairness, and responsiveness through bidirectional iterative interaction.

Result: JASDA embeds feedback, calibration, and probabilistic safety directly into scheduling loop, enabling adaptive and transparent decision-making for scalable resource management.

Conclusion: JASDA provides a scalable foundation bridging theoretical scheduling models with practical deployment in modern MIG-enabled environments, particularly relevant to AI and Agriculture 4.0 applications.

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [10] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: MPI-over-CXL replaces traditional data copying in MPI implementations with direct shared memory access using CXL technology, reducing communication overhead in HPC workloads.


<details>
  <summary>Details</summary>
Motivation: Traditional MPI implementations suffer from overhead due to redundant data movement and buffer management through explicit memory-copy operations, which significantly impacts HPC workloads with intensive inter-processor communication.

Method: Developed MPI-over-CXL that leverages CXL's cache-coherent shared memory across multiple hosts, replacing data-copy methods with direct shared memory access. Implemented custom CXL 3.2 controller, FPGA-based multi-host emulation, and dedicated software stack to map shared memory regions directly into MPI processes' virtual address spaces for pointer-based communication.

Result: Evaluations using representative benchmarks demonstrated substantial performance improvements over conventional MPI systems, with significant reductions in communication latency and memory bandwidth usage.

Conclusion: MPI-over-CXL shows potential to enhance efficiency and scalability in large-scale HPC environments by eliminating redundant copying operations through direct shared memory access.

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [11] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM is an efficient LLM inference framework with a decoupled service-engine architecture that optimizes scheduling, KV cache management, and computational efficiency for high-performance enterprise serving.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high-performance, large-scale enterprise-grade LLM serving with deep optimizations for diverse AI accelerators.

Method: Builds a decoupled service-engine architecture with intelligent scheduling, workload-adaptive dynamic Prefill-Decode disaggregation, Encode-Prefill-Decode policy for multimodal inputs, distributed KV Cache management, multi-layer execution pipeline optimizations, adaptive graph mode, and xTensor memory management.

Result: Achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining average throughput of 1.7x that of MindIE with Deepseek-series models under identical TPOT constraints.

Conclusion: xLLM delivers significantly superior performance and resource efficiency for large-scale LLM inference serving and is publicly available as an open-source framework.

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [12] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: This paper analyzes an infinite balls-into-bins process with deletions using Greedy[2] allocation. It provides bounds on discrepancy and overload metrics, showing O(n) balls above average, O(log(n)) discrepancy with matching lower bound, and loglog(n)+O(1) overload.


<details>
  <summary>Details</summary>
Motivation: To understand load balancing dynamics in systems with both insertions and deletions, particularly bounding the gap between maximum load and average load under stochastic processes.

Method: Uses layered induction analysis from previous work, detailed potential analysis, and probabilistic couplings to simplify conditioning. Considers Greedy[2] allocation strategy with random deletions.

Result: Proves that at any time t: total balls above average is O(n), discrepancy is O(log(n)) with matching lower bound, and overload is loglog(n)+O(1). For 'good' insertion sequences, discrepancy reduces to loglog(n)+O(1).

Conclusion: The analysis provides tight bounds for load balancing in dynamic systems with insertions and deletions, showing logarithmic or doubly logarithmic bounds depending on the metric and insertion patterns.

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation](https://arxiv.org/abs/2510.14172)
*Yuchao Su,Srikar Chundury,Jiajia Li,Frank Mueller*

Main category: cs.AR

TL;DR: \name is a diagonal-optimized quantum simulation accelerator that exploits structured diagonal sparsity in Hamiltonian matrices to achieve significant performance improvements and energy savings over existing methods.


<details>
  <summary>Details</summary>
Motivation: Hamiltonian simulation is computationally challenging due to exponential growth of Hilbert space with qubits. Existing sparse matrix accelerators are designed for ML workloads and don't optimize for the structured diagonal patterns common in Hamiltonian simulations.

Method: \name exploits diagonal structure in problem-Hamiltonian matrices and uses a restructured systolic array dataflow to transform diagonally sparse matrices into dense computations for high utilization and performance.

Result: \name demonstrates average performance improvements of 10.26×, 33.58×, and 53.15× over SIGMA, Outer Product, and Gustavson's algorithm respectively, with peak speedups up to 127.03×. Energy consumption reduced by average 471.55× and up to 4630.58× compared to SIGMA.

Conclusion: \name is the first diagonal-optimized quantum simulation accelerator that effectively addresses the computational challenges of Hamiltonian simulation by leveraging structured diagonal sparsity patterns for significant performance and energy efficiency gains.

Abstract: Hamiltonian simulation is a key workload in quantum computing, enabling the
study of complex quantum systems and serving as a critical tool for classical
verification of quantum devices. However, it is computationally challenging
because the Hilbert space dimension grows exponentially with the number of
qubits. The growing dimensions make matrix exponentiation, the key kernel in
Hamiltonian simulations, increasingly expensive. Matrix exponentiation is
typically approximated by the Taylor series, which contains a series of matrix
multiplications. Since Hermitian operators are often sparse, sparse matrix
multiplication accelerators are essential for improving the scalability of
classical Hamiltonian simulation. Yet, existing accelerators are primarily
designed for machine learning workloads and tuned to their characteristic
sparsity patterns, which differ fundamentally from those in Hamiltonian
simulations that are often dominated by structured diagonals.
  In this work, we present \name, the first diagonal-optimized quantum
simulation accelerator. It exploits the diagonal structure commonly found in
problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic
array dataflow to transform diagonally sparse matrices into dense computations,
enabling high utilization and performance. Through detailed cycle-level
simulation of diverse benchmarks in HamLib, \name{} demonstrates average
performance improvements of $10.26\times$, $33.58\times$, and $53.15\times$
over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak
speedups up to $127.03\times$ while reducing energy consumption by an average
of $471.55\times$ and up to $4630.58\times$ compared to SIGMA.

</details>


### [14] [Computing-In-Memory Aware Model Adaption For Edge Devices](https://arxiv.org/abs/2510.14379)
*Ming-Han Lin,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: A two-stage CIM-aware model adaptation process that compresses models, reallocates resources based on layer importance, and performs quantization-aware training to overcome throughput and accuracy bottlenecks in Computing-in-Memory macros for deep learning acceleration.


<details>
  <summary>Details</summary>
Motivation: To address the throughput and accuracy limitations caused by limited macro size and ADC precision in Computing-in-Memory (CIM) macros for deep learning acceleration, while maintaining high parallel computation and low power consumption benefits.

Method: Two-stage approach: 1) Model compression and resource reallocation based on layer importance and macro size constraints to reduce weight loading latency and improve resource utilization; 2) Quantization-aware training incorporating partial sum quantization and ADC precision to mitigate quantization errors.

Result: Achieves 90% CIM array utilization, enables concurrent activation of up to 256 word lines, achieves up to 93% compression, while maintaining accuracy comparable to previous methods.

Conclusion: The proposed CIM-aware model adaptation effectively overcomes throughput and accuracy bottlenecks in CIM macros, significantly improving resource utilization and compression rates while preserving model accuracy.

Abstract: Computing-in-Memory (CIM) macros have gained popularity for deep learning
acceleration due to their highly parallel computation and low power
consumption. However, limited macro size and ADC precision introduce throughput
and accuracy bottlenecks. This paper proposes a two-stage CIM-aware model
adaptation process. The first stage compresses the model and reallocates
resources based on layer importance and macro size constraints, reducing model
weight loading latency while improving resource utilization and maintaining
accuracy. The second stage performs quantization-aware training, incorporating
partial sum quantization and ADC precision to mitigate quantization errors in
inference. The proposed approach enhances CIM array utilization to 90\%,
enables concurrent activation of up to 256 word lines, and achieves up to 93\%
compression, all while preserving accuracy comparable to previous methods.

</details>


### [15] [ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems](https://arxiv.org/abs/2510.14750)
*İsmail Emir Yüksel,Ataberk Olgun,F. Nisa Bostancı,Haocong Luo,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: Researchers discovered ColumnDisturb, a new DRAM read disturbance phenomenon where activating a single row can disturb cells across multiple subarrays through bitlines, affecting up to 3072 rows and causing bitflips within standard refresh windows.


<details>
  <summary>Details</summary>
Motivation: To identify and characterize a new widespread read disturbance phenomenon in commodity DRAM chips that affects more rows than traditional RowHammer/RowPress and has implications for DRAM reliability and refresh mechanisms.

Method: Experimental demonstration using 216 DDR4 and 4 HBM2 chips from three major manufacturers, rigorously characterizing ColumnDisturb under various operational conditions with 27 key experimental observations.

Result: ColumnDisturb affects all three manufacturers, worsens with technology scaling (5.06x reduction in time to first bitflip), causes bitflips within standard 63.6ms refresh windows, and affects up to 198x more rows than retention failures.

Conclusion: ColumnDisturb poses significant reliability challenges for current and future DRAM chips, greatly reducing the effectiveness of retention-aware refresh mechanisms and requiring new mitigation strategies as technology scales down.

Abstract: We experimentally demonstrate a new widespread read disturbance phenomenon,
ColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a
DRAM row (aggressor row) open, we show that it is possible to disturb DRAM
cells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells
sharing the same columns as the aggressor row (across multiple DRAM subarrays).
With ColumnDisturb, the activation of a single row concurrently disturbs cells
across as many as three subarrays (e.g., 3072 rows) as opposed to
RowHammer/RowPress, which affect only a few neighboring rows of the aggressor
row in a single subarray. We rigorously characterize ColumnDisturb and its
characteristics under various operational conditions using 216 DDR4 and 4 HBM2
chips from three major manufacturers. Among our 27 key experimental
observations, we highlight two major results and their implications.
  First, ColumnDisturb affects chips from all three major manufacturers and
worsens as DRAM technology scales down to smaller node sizes (e.g., the minimum
time to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We
observe that, in existing DRAM chips, ColumnDisturb induces bitflips within a
standard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict
that, as DRAM technology node size reduces, ColumnDisturb would worsen in
future DRAM chips, likely causing many more bitflips in the standard refresh
window. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows
than retention failures. Therefore, ColumnDisturb has strong implications for
retention-aware refresh mechanisms that leverage the heterogeneity in cell
retention times: our detailed analyses show that ColumnDisturb greatly reduces
the benefits of such mechanisms.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [16] [Stability and Heavy-traffic Delay Optimality of General Load Balancing Policies in Heterogeneous Service Systems](https://arxiv.org/abs/2510.14284)
*Yishun Luo,Martin Zubeldia*

Main category: cs.PF

TL;DR: The paper analyzes load balancing with periodic queue length observations and heterogeneous servers, providing stability conditions and heavy-traffic delay optimality results.


<details>
  <summary>Details</summary>
Motivation: To study load balancing systems where dispatchers can only access queue lengths periodically rather than continuously, which is more realistic for practical systems with limited communication capabilities.

Method: The authors consider a family of policies where dispatchers access queue lengths every T time units and make dispatching decisions based on scaled queue length ordering and server processing rates.

Result: The paper provides necessary and sufficient conditions for system stability and sufficient conditions for heavy-traffic delay optimality, showing that queue lengths converge to scaled deterministic vectors with exponential scaling factors.

Conclusion: The proposed policies with periodic queue observations can achieve stability and delay optimality in heavy-traffic regimes, making them suitable for practical load balancing systems with limited communication resources.

Abstract: We consider a load balancing system consisting of $n$ single-server queues
working in parallel, with heterogeneous service rates. Jobs arrive to a central
dispatcher, which has to dispatch them to one of the queues immediately upon
arrival. For this setting, we consider a broad family of policies where the
dispatcher can only access the queue lengths sporadically, every $T$ units of
time. We assume that the dispatching decisions are made based only on the order
of the scaled queue lengths at the last time that the queues were accessed, and
on the processing rate of each server. For these general policies, we provide
easily verifiable necessary and sufficient conditions for the stability of the
system, and sufficient conditions for heavy-traffic delay optimality. We also
show that, in heavy-traffic, the queue length converges in distribution to a
scaled deterministic vector, where the scaling factor is an exponential random
variable.

</details>
