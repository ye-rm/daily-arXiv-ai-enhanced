<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System](https://arxiv.org/abs/2602.09174)
*Marzieh Barkhordar,Alireza Tabatabaeian,Mohammad Sadrosadati,Christina Giannoula,Juan Gomez Luna,Izzat El Hajj,Onur Mutlu,Alaa R. Alameldeen*

Main category: cs.DC

TL;DR: This paper explores graph algorithm performance on UPMEM's real-world PIM system, identifying bottlenecks and comparing against CPU/GPU baselines to guide future PIM hardware design.


<details>
  <summary>Details</summary>
Motivation: Traditional CPU/GPU architectures face memory bottlenecks in graph processing due to extensive data movement. PIM offers promise by integrating computation near memory, but previous studies haven't leveraged real-world PIM systems.

Method: Implemented representative graph algorithms on UPMEM's general-purpose PIM architecture, characterized performance, identified bottlenecks, compared against CPU/GPU baselines, and derived insights for future hardware design.

Result: Found that optimal data partitioning across PIM cores is crucial for performance. Identified critical hardware limitations in current PIM architectures and specific areas needing enhancement across computation, memory, and communication subsystems.

Conclusion: PIM shows promise for accelerating graph workloads but requires hardware improvements including increased instruction-level parallelism, better DMA engines with non-blocking capabilities, and direct interconnection networks among PIM cores to reduce data transfer overheads.

Abstract: Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.
  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.
  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.

</details>


### [2] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: LLM-CoOpt is an algorithm-hardware co-design framework that improves LLM inference throughput by 13.43% and reduces latency by 16.79% through KV cache optimization, grouped-query attention, and paged attention for long sequences.


<details>
  <summary>Details</summary>
Motivation: Address memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing during LLM inference.

Method: Three key strategies: (1) Opt-KV - Key-Value Cache Optimization with FP8 quantization, (2) Opt-GQA - Grouped-Query Attention restructuring, (3) Opt-Pa - Paged Attention with sequence segmentation and lazy memory mapping.

Result: On LLaMa-13B-GPTQ model: throughput increased by up to 13.43%, latency reduced by up to 16.79%, while maintaining model accuracy.

Conclusion: LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [3] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: The paper establishes a fundamental criterion for when coordination is intrinsically required in distributed systems based on monotonicity with respect to history extension.


<details>
  <summary>Details</summary>
Motivation: To determine when coordination is fundamentally necessary in distributed specifications rather than being an artifact of specific protocols or implementations, providing a general theoretical foundation for understanding coordination requirements.

Method: Uses an asynchronous message-passing model and analyzes specifications over Lamport histories (partially ordered executions under happens-before). The Coordination Criterion states that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.

Result: Establishes a sharp boundary between specifications that can be implemented without coordination and those requiring coordination. The criterion provides a uniform explanation for various classical distributed systems results including CAP impossibility, CALM coordination-freedom, agreement tasks, snapshot tasks, transactional isolation levels, and invariant confluence.

Conclusion: Coordination requirements in distributed systems are fundamentally determined by monotonicity properties of specifications with respect to history extension, revealing that many classical results are instances of the same underlying semantic phenomenon.

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [4] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: This paper explores high-performance portability for quantum state-vector simulations using vector-length agnostic (VLA) designs on ARM SVE and RISC-V RVV architectures, achieving significant speedups across multiple processors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand whether high-performance portability can be achieved in vector-length agnostic (VLA) designs for emerging vector architectures (ARM SVE and RISC-V RVV), using quantum state-vector simulations as an important workload for quantum computing.

Method: The authors propose a VLA design with optimization techniques including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. They implement this in Google's Qsim and evaluate on three ARM processors (NVIDIA Grace, AWS Graviton3, Fujitsu A64FX) with five quantum circuits up to 36 qubits.

Result: The single-source VLA implementation achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton. The authors also define new metrics and PMU events to quantify vectorization activities.

Conclusion: The work demonstrates that high-performance portability can be achieved with VLA designs for quantum simulations, and provides generic insights for future VLA designs through new metrics and performance analysis across different vector architectures.

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [5] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: AFD (Attention-FFN Disaggregation) has performance limitations compared to standard Expert Parallelism due to interconnect bandwidth constraints and scaling imbalances, but can be beneficial with high-bandwidth hardware and specific model characteristics.


<details>
  <summary>Details</summary>
Motivation: To understand the performance boundaries of Attention-FFN Disaggregation (AFD) compared to standard large-scale Expert Parallelism (EP) for deploying large-scale MoE models, addressing challenges in memory capacity and bandwidth for expert activation.

Method: Systematic analysis of AFD by extending the roofline model to communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Examining how increasing FFN instance count affects performance and comparing AFD's discrete node-level scaling with EP's continuous batch adjustment.

Result: Reveals a "dead zone" on standard clusters where increasing FFN instances doesn't improve HFU due to computational workload being capped by scale-out bandwidth. AFD's discrete scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Limitations diminish with Superpod-class hardware (abundant interconnect bandwidth) and models with coarse-grained experts and lower sparsity.

Conclusion: AFD is a promising approach for specific hardware-model combinations rather than a universal solution. It works best with high-bandwidth hardware and specific model characteristics, but has fundamental limitations on standard clusters due to bandwidth constraints and scaling imbalances.

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>


### [6] [Efficient Remote Prefix Fetching with GPU-native Media ASICs](https://arxiv.org/abs/2602.09725)
*Liang Mi,Weijun Wang,Jinghan Chen,Ting Cao,Haipeng Dai,Yunxin Liu*

Main category: cs.DC

TL;DR: KVFetcher uses GPU-native video codecs for efficient remote KV cache reuse, achieving up to 3.51Ã— faster time-to-first-token while maintaining lossless accuracy.


<details>
  <summary>Details</summary>
Motivation: Remote KV cache reuse accelerates LLM inference by fetching precomputed KV caches, but performance degrades in bandwidth-limited networks. Existing compression solutions have heavyweight decompression that counteracts benefits.

Method: KVFetcher employs GPU-native video codecs with two techniques: 1) codec-friendly tensor layout for compact video format compression, and 2) efficient KV fetcher that orchestrates transmission, decoding, and restoration in a pipelined manner to eliminate resource contention and mask network fluctuations.

Result: KVFetcher reduces time-to-first-token (TTFT) by up to 3.51 times compared to state-of-the-art methods while maintaining lossless accuracy, and works across diverse GPU hardware from high- to low-end.

Conclusion: KVFetcher provides an efficient and widely deployable solution for remote KV cache reuse that effectively addresses bandwidth limitations through GPU-native video codec compression and optimized pipelining.

Abstract: Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [AgentCgroup: Understanding and Controlling OS Resources of AI Agents](https://arxiv.org/abs/2602.09345)
*Yusheng Zheng,Jiakun Fan,Quanzhi Fu,Yiwei Yang,Wei Zhang,Andi Quinn*

Main category: cs.OS

TL;DR: AgentCgroup: eBPF-based resource controller for AI agents in cloud environments that addresses OS-level resource management mismatches through hierarchical cgroup structures and in-kernel enforcement.


<details>
  <summary>Details</summary>
Motivation: AI agents in multi-tenant cloud environments execute diverse tool calls with distinct resource demands and rapid fluctuations, but existing resource controls have granularity, responsiveness, and adaptability mismatches with AI agent execution patterns.

Method: Systematic characterization of OS-level resource dynamics in sandboxed AI coding agents using 144 SWE-rebench tasks across two LLM models, then comparing against serverless, microservice, and batch workloads to identify mismatches. Proposed AgentCgroup uses eBPF-based hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies.

Result: Measurements show: (1) OS-level execution accounts for 56-74% of end-to-end task latency; (2) memory is the concurrency bottleneck; (3) memory spikes are tool-call-driven with up to 15.4x peak-to-average ratio; (4) resource demands are highly unpredictable across tasks, runs, and models. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.

Conclusion: Existing resource controls are mismatched for AI agent workloads, requiring new approaches like AgentCgroup that provide fine-grained, responsive, and adaptive resource management through hierarchical cgroup structures and in-kernel enforcement mechanisms.

Abstract: AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup , an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies driven by in-kernel monitoring. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design](https://arxiv.org/abs/2602.09410)
*Yuchao Liao,Tosiron Adegbija,Roman Lysecky*

Main category: cs.AR

TL;DR: LLMs accelerate FPGA accelerator design for post-quantum cryptography, achieving 2.6x speedup for FALCON signature scheme kernels compared to conventional HLS approaches.


<details>
  <summary>Details</summary>
Motivation: Post-quantum cryptography algorithms are computationally complex and difficult to implement efficiently on hardware, requiring accelerated hardware-software co-design processes.

Method: Novel framework leveraging LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation, with human-in-the-loop optimization.

Result: LLM-generated accelerators achieve up to 2.6x speedup in kernel execution time with shorter critical paths compared to conventional HLS-based approaches, though with trade-offs in resource utilization and power consumption.

Conclusion: LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising direction for rapid and adaptive PQC accelerator design on FPGAs.

Abstract: Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.

</details>


### [9] [Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems](https://arxiv.org/abs/2602.09554)
*Thomas Benz*

Main category: cs.AR

TL;DR: The paper discusses the computing and communication challenges in ACES (Autonomy, Connectivity, Electrification, Shared mobility) domains, highlighting the need for high-performance, energy-efficient heterogeneous systems with specialized accelerators to overcome limitations of Moore's Law and handle mixed-criticality workloads.


<details>
  <summary>Details</summary>
Motivation: Industrial domains like automotive, robotics, and aerospace are evolving toward ACES (Autonomy, Connectivity, Electrification, Shared mobility), which dramatically increases requirements for onboard computing performance and communication infrastructure. Traditional scaling approaches (Moore's Law, Dennard Scaling) are slowing down, necessitating new approaches to meet performance and energy-efficiency demands for resource-bound applications.

Method: The paper proposes a co-design approach that integrates memory systems with use cases, compute units, and accelerators. This involves moving toward heterogeneous systems with application-specific hardware accelerators, addressing memory bandwidth/capacity challenges, and managing mixed-criticality workloads through careful interconnect design to minimize contention between different criticality levels.

Result: The analysis identifies key challenges: the need for substantial compute at high energy-efficiency, growing memory bandwidth/capacity demands for complex irregular datasets, pressure on interconnect systems from sensor data influx, and the complexity of combining real-time critical with general compute tasks on shared platforms.

Conclusion: Fulfilling ACES requirements demands a holistic co-design approach that carefully integrates memory systems with application use cases, compute units, and accelerators, while addressing the unique challenges of heterogeneous mixed-criticality systems with minimal interconnect contention for predictability.

Abstract: Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.

</details>
