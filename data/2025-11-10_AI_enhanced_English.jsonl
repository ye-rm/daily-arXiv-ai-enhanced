{"id": "2511.04928", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2511.04928", "abs": "https://arxiv.org/abs/2511.04928", "authors": ["Mahek Desai", "Apoorva Rumale", "Marjan Asadinia", "Sherrene Bogle"], "title": "WIRE: Write Energy Reduction via Encoding in Phase Change Main Memories (PCM)", "comment": null, "summary": "Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic\nRandom-Access Memory (DRAM) in terms of scalability and standby energy\nefficiency. Altering a PCM cell's state during writes demands substantial\nenergy, posing a significant challenge to PCM's role as the primary main\nmemory. Prior research has explored methods to reduce write energy consumption,\nincluding the elimination of redundant writes, minimizing cell writes, and\nemploying compact row buffers for filtering PCM main memory accesses. However,\nthese techniques had certain drawbacks like bit-wise comparison of the stored\nvalues, preemptive updates increasing write cycles, and poor endurance. In this\npaper, we propose WIRE, a new coding mechanism through which most write\noperations force a maximum of one-bit flip. In this coding-based data storage\nmethod, we look at the frequent value stack and assign a code word to the most\nfrequent values such that they have a hamming distance of one. In most of the\nwrite accesses, writing a value needs one or fewer bit flips which can save\nconsiderable write energy. This technique can be augmented with a wear-leveling\nmechanism at the block level, and rotating the difference bit in the assigned\ncodes, increasing the lifetime of the PCM array at a low cost. Using a\nfull-system evaluation of our method and comparing it to the existing\nmechanisms, our experimental results for multi-threaded and multi-programmed\nworkloads revealed considerable improvement in lifetime and write energy as\nwell as bit flip reduction.", "AI": {"tldr": "WIRE is a new coding mechanism for Phase Change Memory (PCM) that reduces write energy by ensuring most write operations require only one-bit flip, achieved through assigning codewords with hamming distance of one to frequent values.", "motivation": "PCM faces significant energy challenges during write operations despite its advantages in scalability and standby energy efficiency over DRAM. Existing techniques for reducing write energy have limitations including bit-wise comparison overheads, increased write cycles, and poor endurance.", "method": "The WIRE coding mechanism assigns codewords to frequent values such that they have a hamming distance of one, enabling most write operations to require only one-bit flip. This is augmented with block-level wear-leveling and rotating difference bits to improve PCM lifetime.", "result": "Experimental evaluation with multi-threaded and multi-programmed workloads showed significant improvements in PCM lifetime, write energy reduction, and bit flip reduction compared to existing mechanisms.", "conclusion": "WIRE coding mechanism effectively addresses PCM's write energy challenges by minimizing bit flips during writes, while also improving memory lifetime through wear-leveling techniques, making it a promising solution for PCM main memory applications."}}
{"id": "2511.04798", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04798", "abs": "https://arxiv.org/abs/2511.04798", "authors": ["Matheus Farias", "Wanghley Martins", "H. T. Kung"], "title": "MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars", "comment": "5 pages, 6 figures", "summary": "Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)\nweight mapping technique for memristive bit-sliced compute-in-memory (CIM)\ncrossbars that reduces parasitic resistance (PR) nonidealities.\n  PR limits crossbar efficiency by mapping DNN matrices into small crossbar\ntiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring\ndigital synchronization before the next layer. At this granularity, designers\neither deploy many small crossbars in parallel or reuse a few sequentially-both\nincreasing analog-to-digital conversions, latency, I/O pressure, and chip area.\n  MDM alleviates PR effects by optimizing active-memristor placement.\nExploiting bit-level structured sparsity, it feeds activations from the denser\nlow-order side and reorders rows according to the Manhattan distance,\nrelocating active cells toward regions less affected by PR and thus lowering\nthe nonideality factor (NF).\n  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and\nimproves accuracy under analog distortion by an average of 3.6% in ResNets.\nOverall, it provides a lightweight, spatially informed method for scaling CIM\nDNN accelerators.", "AI": {"tldr": "Manhattan Distance Mapping (MDM) is a post-training weight mapping technique for memristive CIM crossbars that reduces parasitic resistance effects by optimizing active-memristor placement, improving accuracy by 3.6% on average in ResNets.", "motivation": "Parasitic resistance limits crossbar efficiency by forcing DNN matrices to be mapped into small tiles, which reduces CIM speedup and increases digital synchronization overhead, ADC conversions, latency, I/O pressure, and chip area.", "method": "MDM exploits bit-level structured sparsity by feeding activations from the denser low-order side and reordering rows according to Manhattan distance to relocate active cells toward regions less affected by parasitic resistance.", "result": "Applied to DNN models on ImageNet-1k, MDM reduces nonideality factor by up to 46% and improves accuracy under analog distortion by an average of 3.6% in ResNets.", "conclusion": "MDM provides a lightweight, spatially informed method for scaling CIM DNN accelerators by effectively mitigating parasitic resistance nonidealities."}}
{"id": "2511.04853", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04853", "abs": "https://arxiv.org/abs/2511.04853", "authors": ["Nuno dos Santos Fernandes", "Pedro Tom\u00e1s", "Nuno Roma", "Frank Winklmeier", "Patricia Conde-Mu\u00ed\u00f1o"], "title": "Marionette: Data Structure Description and Management for Heterogeneous Computing", "comment": "5 pages, 2 figures. To be published as a short paper accepted by the\n  24th International Symposium on Parallel and Distributed Computing (ISPDC)", "summary": "Adapting large, object-oriented C++ codebases for hardware acceleration might\nbe extremely challenging, particularly when targeting heterogeneous platforms\nsuch as GPUs. Marionette is a C++17 library designed to address this by\nenabling flexible, efficient, and portable data structure definitions. It\ndecouples data layout from the description of the interface, supports multiple\nmemory management strategies, and provides efficient data transfers and\nconversions across devices, all of this with minimal runtime overhead due to\nthe compile-time nature of its abstractions. By allowing interfaces to be\naugmented with arbitrary functions, Marionette maintains compatibility with\nexisting code and offers a streamlined interface that supports both\nstraightforward and advanced use cases. This paper outlines its design, usage,\nand performance, including a CUDA-based case study demonstrating its efficiency\nand flexibility.", "AI": {"tldr": "Marionette is a C++17 library that enables flexible, efficient, and portable data structure definitions for hardware acceleration on heterogeneous platforms like GPUs, with minimal runtime overhead through compile-time abstractions.", "motivation": "Adapting large, object-oriented C++ codebases for hardware acceleration is extremely challenging, especially for heterogeneous platforms such as GPUs.", "method": "Decouples data layout from interface description, supports multiple memory management strategies, provides efficient data transfers and conversions across devices, and allows interfaces to be augmented with arbitrary functions using compile-time abstractions.", "result": "Enables flexible, efficient, and portable data structure definitions with minimal runtime overhead, maintains compatibility with existing code, and supports both straightforward and advanced use cases.", "conclusion": "Marionette provides an effective solution for adapting C++ codebases to heterogeneous platforms, as demonstrated by a CUDA-based case study showing its efficiency and flexibility."}}
{"id": "2511.04682", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04682", "abs": "https://arxiv.org/abs/2511.04682", "authors": ["Eleni Bougioukou", "Theodore Antonakopoulos"], "title": "Efficient Deployment of CNN Models on Multiple In-Memory Computing Units", "comment": "5 pages, 4 figures, 2025 14th International Conference on Modern\n  Circuits and Systems Technologies (MOCAST)", "summary": "In-Memory Computing (IMC) represents a paradigm shift in deep learning\nacceleration by mitigating data movement bottlenecks and leveraging the\ninherent parallelism of memory-based computations. The efficient deployment of\nConvolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use\nof advanced task allocation strategies for achieving maximum computational\nefficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple\nProcessing Units (PUs) for investigating how the deployment of a CNN model in a\nmulti-processing system affects its performance, in terms of processing rate\nand latency. For that purpose, we introduce the Load-Balance-Longest-Path\n(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE\nPUs, for maximizing the processing rate and minimizing latency due to efficient\nresources utilization. We are benchmarking LBLP against other alternative\nscheduling strategies for a number of CNN models and experimental results\ndemonstrate the effectiveness of the proposed algorithm.", "AI": {"tldr": "The paper introduces the Load-Balance-Longest-Path (LBLP) algorithm for optimizing CNN deployment on In-Memory Computing hardware with multiple Processing Units, achieving improved processing rates and reduced latency through dynamic task allocation.", "motivation": "To address data movement bottlenecks in deep learning acceleration and leverage the parallelism of In-Memory Computing (IMC) for efficient CNN deployment on multi-processing systems.", "method": "Developed the LBLP algorithm that dynamically assigns CNN nodes to available IMC Processing Units, using an IMC Emulator (IMCE) with multiple PUs to investigate performance impacts.", "result": "Experimental benchmarking against alternative scheduling strategies demonstrated that LBLP effectively maximizes processing rate and minimizes latency through efficient resource utilization across various CNN models.", "conclusion": "The proposed LBLP algorithm provides an effective dynamic task allocation strategy for optimizing CNN performance on IMC-based hardware systems."}}
{"id": "2511.05053", "categories": ["cs.DC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05053", "abs": "https://arxiv.org/abs/2511.05053", "authors": ["Wakuto Matsumi", "Riaz-Ul-Haque Mian"], "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs", "comment": null, "summary": "Machine learning based on neural networks has advanced rapidly, but the high\nenergy consumption required for training and inference remains a major\nchallenge. Hyperdimensional Computing (HDC) offers a lightweight,\nbrain-inspired alternative that enables high parallelism but often suffers from\nlower accuracy on complex visual tasks. To overcome this, hybrid accelerators\ncombining HDC and Convolutional Neural Networks (CNNs) have been proposed,\nthough their adoption is limited by poor generalizability and programmability.\nThe rise of open-source RISC-V architectures has created new opportunities for\ndomain-specific GPU design. Unlike traditional proprietary GPUs, emerging\nRISC-V-based GPUs provide flexible, programmable platforms suitable for custom\ncomputation models such as HDC. In this study, we design and implement custom\nGPU instructions optimized for HDC operations, enabling efficient processing\nfor hybrid HDC-CNN workloads. Experimental results using four types of custom\nHDC instructions show a performance improvement of up to 56.2 times in\nmicrobenchmark tests, demonstrating the potential of RISC-V GPUs for\nenergy-efficient, high-performance computing.", "AI": {"tldr": "This paper proposes custom GPU instructions for RISC-V GPUs to accelerate hybrid HDC-CNN computing, achieving up to 56.2x performance improvement in microbenchmarks.", "motivation": "Neural networks have high energy consumption, while Hyperdimensional Computing (HDC) offers lightweight alternatives but suffers from lower accuracy. Hybrid HDC-CNN accelerators face limitations in generalizability and programmability, creating an opportunity for RISC-V GPU customization.", "method": "Design and implementation of custom GPU instructions optimized for HDC operations on RISC-V-based GPUs, enabling efficient processing of hybrid HDC-CNN workloads with four types of custom instructions.", "result": "Experimental results show performance improvement of up to 56.2 times in microbenchmark tests using the custom HDC instructions.", "conclusion": "RISC-V GPUs with custom instructions demonstrate significant potential for energy-efficient, high-performance computing of hybrid HDC-CNN workloads."}}
{"id": "2511.04684", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04684", "abs": "https://arxiv.org/abs/2511.04684", "authors": ["Yuchao Qin", "Anjunyi Fan", "Bonan Yan"], "title": "RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression", "comment": "5 pages, 4 figures", "summary": "Data centers handle vast volumes of data that require efficient lossless\ncompression, yet emerging probabilistic models based methods are often\ncomputationally slow. To address this, we introduce RAS, the Range Asymmetric\nNumeral System Acceleration System, a hardware architecture that integrates the\nrANS algorithm into a lossless compression pipeline and eliminates key\nbottlenecks. RAS couples an rANS core with a probabilistic generator, storing\ndistributions in BF16 format and converting them once into a fixed-point domain\nshared by a unified division/modulo datapath. A two-stage rANS update with\nbyte-level re-normalization reduces logic cost and memory traffic, while a\nprediction-guided decoding path speculatively narrows the cumulative\ndistribution function (CDF) search window and safely falls back to maintain\nbit-exactness. A multi-lane organization scales throughput and enables\nfine-grained clock gating for efficient scheduling. On image workloads, our\nRTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a\nPython rANS baseline, reducing average decoder binary-search steps from 7.00 to\n3.15 (approximately 55% fewer). When paired with neural probability models, RAS\nsustains higher compression ratios than classical codecs and outperforms\nCPU/GPU rANS implementations, offering a practical approach to fast neural\nlossless compression.", "AI": {"tldr": "RAS is a hardware acceleration system for rANS algorithm that achieves significant speedups (121.2x encode, 70.9x decode) over software implementations while maintaining high compression ratios with neural probability models.", "motivation": "Data centers need efficient lossless compression but current probabilistic model-based methods are computationally slow, creating a need for hardware acceleration.", "method": "Hardware architecture integrating rANS algorithm with BF16 format distributions, unified division/modulo datapath, two-stage rANS update with byte-level re-normalization, prediction-guided decoding with speculative CDF search, and multi-lane organization with clock gating.", "result": "Achieved 121.2x encode and 70.9x decode speedups over Python baseline, reduced decoder binary-search steps from 7.00 to 3.15 (55% fewer), sustained higher compression ratios than classical codecs, and outperformed CPU/GPU implementations.", "conclusion": "RAS provides a practical approach to fast neural lossless compression by eliminating bottlenecks in rANS algorithm through specialized hardware architecture."}}
{"id": "2511.05067", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05067", "abs": "https://arxiv.org/abs/2511.05067", "authors": ["Giuseppe Esposito", "Juan-David Guerrero-Balaguera", "Josie Esteban Rodriguez Condia", "Matteo Sonza Reorda", "Marco Barbiero", "Rossella Fortuna"], "title": "GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters", "comment": null, "summary": "Graphics Processing Units (GPUs) are specialized accelerators in data centers\nand high-performance computing (HPC) systems, enabling the fast execution of\ncompute-intensive applications, such as Convolutional Neural Networks (CNNs).\nHowever, sustained workloads can impose significant stress on GPU components,\nraising reliability concerns due to potential faults that corrupt the\nintermediate application computations, leading to incorrect results. Estimating\nthe stress induced by an application is thus crucial to predict reliability\n(with\\,special\\,emphasis\\,on\\,aging\\,effects). In this work, we combine online\ntelemetry parameters and hardware performance counters to assess GPU stress\ninduced by different applications. The experimental results indicate the stress\ninduced by a parallel workload can be estimated by combining telemetry data and\nPerformance Counters that reveal the efficiency in the resource usage of the\ntarget workload. For this purpose the selected performance counters focus on\nmeasuring the i) throughput, ii) amount of issued instructions and iii) stall\nevents.", "AI": {"tldr": "This paper proposes a method to estimate GPU stress using online telemetry parameters and hardware performance counters, focusing on measuring throughput, issued instructions, and stall events to predict reliability concerns from sustained workloads.", "motivation": "GPUs in data centers and HPC systems face reliability issues due to sustained workloads that stress components and cause faults, potentially corrupting computations and leading to incorrect results. Estimating this stress is crucial for predicting reliability, especially aging effects.", "method": "The researchers combine online telemetry parameters and hardware performance counters to assess GPU stress. They focus on performance counters that measure throughput, amount of issued instructions, and stall events to evaluate resource usage efficiency.", "result": "Experimental results show that stress induced by parallel workloads can be estimated by combining telemetry data and performance counters that reveal resource usage efficiency of the target workload.", "conclusion": "The combination of telemetry parameters and specific performance counters (throughput, issued instructions, stall events) provides an effective approach for estimating GPU stress and predicting reliability concerns in sustained workload scenarios."}}
{"id": "2511.04687", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04687", "abs": "https://arxiv.org/abs/2511.04687", "authors": ["Teona Bagashvili", "Tarikul Islam Papon", "Subhadeep Sarkar", "Manos Athanassoulis"], "title": "Eliminating the Hidden Cost of Zone Management in ZNS SSDs", "comment": null, "summary": "Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput\nand low-latency storage by eliminating device-side garbage collection. They\nexpose storage as append-only zones that give the host applications direct\ncontrol over data placement. However, current ZNS implementations suffer from\n(a) device-level write amplification (DLWA), (b) increased wear, and (c)\ninterference with host I/O due to zone mapping and management. We identify two\nprimary design decisions as the main cause: (i) fixed physical zones and (ii)\nfull-zone operations that lead to excessive physical writes. We propose\nSilentZNS, a new zone mapping and management approach that addresses the\naforementioned limitations by on-the-fly allocating available resources to\nzones, while minimizing wear, maintaining parallelism, and avoiding unnecessary\nwrites at the device-level. SilentZNS is a flexible zone allocation scheme that\ndeparts from the traditional logical-to-physical zone mapping and allows for\narbitrary collections of blocks to be assigned to a zone. We add the necessary\nconstraints to ensure wear-leveling and state-of-the-art read performance, and\nuse only the required blocks to avoid dummy writes during zone reset. We\nimplement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that\nit eliminates the undue burden of dummy writes by up to 20x, leading to lower\nDLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up\nto 3.7x faster workload execution.", "AI": {"tldr": "SilentZNS is a new zone mapping approach for ZNS SSDs that eliminates device-level write amplification and wear by dynamically allocating blocks to zones instead of using fixed physical zones, reducing dummy writes by up to 20x and improving performance by 3.7x.", "motivation": "Current ZNS SSD implementations suffer from device-level write amplification, increased wear, and interference with host I/O due to fixed physical zones and full-zone operations that cause excessive physical writes.", "method": "Proposes SilentZNS - a flexible zone allocation scheme that dynamically allocates available blocks to zones, departing from traditional logical-to-physical zone mapping, while maintaining wear-leveling and read performance constraints.", "result": "Eliminates dummy writes by up to 20x, reduces device-level write amplification by 86% at 10% zone occupancy, decreases overall wear by up to 76.9%, and accelerates workload execution by up to 3.7x.", "conclusion": "SilentZNS effectively addresses the limitations of current ZNS implementations by providing flexible zone allocation that minimizes unnecessary writes and wear while maintaining performance."}}
{"id": "2511.05321", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.05321", "abs": "https://arxiv.org/abs/2511.05321", "authors": ["Maximilian Kirschner", "Konstantin Dudzik", "Ben Krusekamp", "J\u00fcrgen Becker"], "title": "MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference", "comment": null, "summary": "Real-time systems, particularly those used in domains like automated driving,\nare increasingly adopting neural networks. From this trend arises the need for\nhigh-performance hardware exhibiting predictable timing behavior. While\nstate-of-the-art real-time hardware often suffers from limited memory and\ncompute resources, modern AI accelerators typically lack the crucial\npredictability due to memory interference.\n  We present a new hardware architecture to bridge this gap between performance\nand predictability. The architecture features a multi-core vector processor\nwith predictable cores, each equipped with local scratchpad memories. A central\nmanagement core orchestrates access to shared external memory following a\nstatically determined schedule.\n  To evaluate the proposed hardware architecture, we analyze different variants\nof our parameterized design. We compare these variants to a baseline\narchitecture consisting of a single-core vector processor with large vector\nregisters. We find that configurations with a larger number of smaller cores\nachieve better performance due to increased effective memory bandwidth and\nhigher clock frequencies. Crucially for real-time systems, execution time\nfluctuation remains very low, demonstrating the platform's time predictability.", "AI": {"tldr": "A new multi-core vector processor architecture with predictable cores and local scratchpad memories addresses the gap between performance and predictability in real-time systems using neural networks, outperforming single-core designs while maintaining low execution time fluctuations.", "motivation": "Real-time systems using neural networks need high-performance hardware with predictable timing, but current solutions either have limited resources or lack predictability due to memory interference in AI accelerators.", "method": "Proposed a multi-core vector processor with predictable cores featuring local scratchpad memories, orchestrated by a central management core that follows a statically determined schedule for shared external memory access.", "result": "Configurations with more smaller cores achieved better performance due to increased effective memory bandwidth and higher clock frequencies, while maintaining very low execution time fluctuations.", "conclusion": "The architecture successfully bridges the performance-predictability gap for real-time neural network systems, demonstrating that multi-core designs with proper memory management can provide both high performance and timing predictability."}}
