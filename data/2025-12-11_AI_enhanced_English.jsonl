{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "METRO is a novel token-routing algorithm for expert-parallel MoE serving that balances activated experts per GPU instead of tokens, reducing decode latency by 11-22% and improving throughput in memory-bound regimes.", "motivation": "Existing approaches to load balancing in Expert Parallelism focus on balancing token counts across GPUs, but this degrades performance in memory-bound regimes common in MoE serving, especially during decode phase, by increasing activated experts and exacerbating memory pressure.", "method": "Proposes Minimum Expert Token ROuting (METRO) algorithm that balances number of activated experts per GPU rather than token counts. Uses novel allGather scheme to gather global top-k knowledge with minimal overhead compared to conventional allToAll, achieving near-optimal routing quality with minimal computational overhead.", "result": "Evaluation on real systems (vLLM over 8 A100 GPUs) and proprietary simulator (8-16 B200 GPUs) shows METRO reduces decode latency by 11-22%, improves total token throughput by 3-21% for Qwen3 and DeepSeek-V3 serving. Can improve decode throughput by up to 4.11x over EPLB at fixed decode SLO by trading latency headroom for throughput.", "conclusion": "METRO addresses the fundamental limitation of existing token-balancing approaches in memory-bound MoE serving by focusing on expert activation balance, significantly improving performance metrics across latency and throughput in practical serving scenarios."}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "A distributed hierarchical offloading framework for Vision Transformers that enhances privacy by partitioning visual data across multiple cloud servers, preventing any single server from reconstructing complete images, with final aggregation done only on trusted edge devices.", "motivation": "Visual intelligence tools have high computational requirements that exceed mobile/wearable device capabilities, but cloud offloading introduces significant privacy vulnerabilities during transmission and server-side computation.", "method": "Proposes a distributed hierarchical offloading framework using a local trusted edge device as orchestrator, which partitions visual data into smaller portions distributed across multiple independent cloud servers, with final merging and aggregation exclusively on the user's trusted edge device.", "result": "Applied to Segment Anything Model (SAM) as case study, the framework maintains near-baseline segmentation performance while substantially reducing risk of content reconstruction and user data exposure compared to traditional cloud approaches.", "conclusion": "The framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum by design, preventing comprehensive data reconstruction by any single external server."}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN is a distributed disk-based approximate nearest neighbor system that achieves near-linear throughput scaling across multiple servers while maintaining logarithmic search efficiency through a novel query state transfer approach.", "motivation": "As vector datasets scale to billions of vectors and beyond single-server capacity, there's a need for distributed disk-based vector search systems that can handle future datasets too large for any single server while maintaining efficient search performance.", "method": "BatANN uses a distributed disk-based ANN system with a single global graph structure. The core innovation is that when a query needs to access a neighborhood stored on another machine, the full state of the query is sent to that machine to continue execution locally, improving locality and reducing network overhead.", "result": "On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of scatter-gather baselines respectively, while maintaining mean latency below 6 ms, all using standard TCP networking.", "conclusion": "BatANN successfully demonstrates a distributed disk-based vector search system that scales near-linearly with servers while maintaining logarithmic search efficiency, representing the first open-source distributed disk-based vector search system operating over a single global graph."}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe is a multi-LLM serving system that uses universal GPU workers with workload-aware prewarming to improve time-to-first-token (TTFT) by up to 50.8\u00d7 while serving 2.5\u00d7 more requests compared to existing systems.", "motivation": "Existing multi-LLM serving systems optimize GPU utilization but degrade inference performance (especially TTFT) due to their unawareness of future workload characteristics. Real-world traces show LLM serving workloads have high periodicity and long-term predictability that can be leveraged.", "method": "Design WarmServe with universal GPU workers for one-for-many GPU prewarming. Three key techniques: (1) evict-aware model placement to mitigate cluster-wide prewarming interference, (2) proactive prewarming to prepare universal GPU workers in advance, and (3) zero-overhead memory switching mechanism for GPU memory management.", "result": "Evaluation on real-world datasets shows WarmServe improves TTFT by up to 50.8\u00d7 compared to state-of-the-art autoscaling-based systems, while serving up to 2.5\u00d7 more requests compared to GPU-sharing systems.", "conclusion": "WarmServe demonstrates that leveraging workload predictability through universal GPU workers with intelligent prewarming can significantly improve both inference performance (TTFT) and resource efficiency in multi-LLM serving systems."}}
{"id": "2512.09304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09304", "abs": "https://arxiv.org/abs/2512.09304", "authors": ["Siyuan Ma", "Jiajun Hu", "Jeeho Ryoo", "Aman Arora", "Lizy Kurian John"], "title": "RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference", "comment": null, "summary": "In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.", "AI": {"tldr": "RACAM is a novel in-DRAM bit-serial processing-in-memory architecture with locality buffers and workload mapping that achieves significant speedups for LLM inference compared to GPUs and state-of-the-art PIM systems.", "motivation": "Existing DRAM-PIM architectures have limitations: lack of data reuse, redundant data transfers, and insufficient workload mapping support, which hinder efficiency for memory-intensive workloads like LLM inference.", "method": "Proposed RACAM architecture with dedicated locality buffers, bit-serial processing elements, popcount reduction units, and broadcast units to enable data reuse and reduce data transfers. Also developed a workload mapping mechanism to exploit DRAM parallelism.", "result": "RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm\u00b2 compared to state-of-the-art Proteus PIM system for GPT3 inference.", "conclusion": "RACAM effectively addresses key limitations of existing DRAM-PIM architectures through innovative hardware components and workload mapping, demonstrating significant performance improvements for LLM inference workloads."}}
{"id": "2512.09300", "categories": ["cs.OS", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09300", "abs": "https://arxiv.org/abs/2512.09300", "authors": ["Guangxian Zou", "Isaac Zhang", "Ryan Zarick", "Kelvin Wong", "Thomas Kim", "Daniel L. -K. Wong", "Saeid Yazdinejad", "Dan Boneh"], "title": "ZeroOS: A Universal Modular Library OS for zkVMs", "comment": null, "summary": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.", "AI": {"tldr": "ZeroOS is a modular library OS for zkVM unikernels that enables vApp developers to compile only needed Linux ABI subsets using standard toolchains, reducing trusted computing base and unifying the zkVM ecosystem.", "motivation": "Current zkVMs require maintaining forks of language-specific runtimes and statically linking them into applications, creating version hell and burdening vApps with unnecessarily large trusted computing bases.", "method": "ZeroOS provides a modular library operating system (libOS) that allows vApp developers to use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can leverage ZeroOS by writing a bootloader for their platform.", "result": "ZeroOS reduces maintenance burden, unifies the zkVM ecosystem with consolidated development and audit resources, and enables vApps to have smaller trusted computing bases.", "conclusion": "ZeroOS solves the compatibility and maintenance problems in zkVMs by providing a modular libOS approach that allows selective Linux ABI usage, making verifiable computation more practical and ecosystem-friendly."}}
{"id": "2512.09502", "categories": ["cs.DC", "cs.NE", "physics.comp-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.09502", "abs": "https://arxiv.org/abs/2512.09502", "authors": ["Bruno Golosio", "Gianmarco Tiddia", "Jos\u00e9 Villamar", "Luca Pontisso", "Luca Sergi", "Francesco Simula", "Pooja Babu", "Elena Pastorelli", "Abigail Morrison", "Markus Diesmann", "Alessandro Lonardo", "Pier Stanislao Paolucci", "Johanna Senk"], "title": "Scalable Construction of Spiking Neural Networks using up to thousands of GPUs", "comment": null, "summary": "Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\\mathcal{O}(10^{10})$ neurons, each forming $\\mathcal{O}(10^{3})$--$\\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.", "AI": {"tldr": "A novel MPI-based network construction method for simulating large-scale spiking neural networks on multi-GPU clusters and exascale systems, with demonstrated scaling performance on cortical models.", "motivation": "Large-scale simulation of spiking neural networks for computational neuroscience research requires efficient management of communication and memory on high-performance computing clusters, inspired by the human cerebral cortex with its sparse connectivity and spike-based communication.", "method": "A novel network construction method using MPI where each process builds its local connectivity and prepares data structures for efficient spike exchange across the cluster during state propagation, with both point-to-point and collective communication approaches.", "result": "Demonstrated scaling performance of two cortical models using different communication strategies (point-to-point and collective communication).", "conclusion": "The presented MPI-based network construction method enables efficient large-scale simulation of spiking neural networks on multi-GPU clusters and upcoming exascale supercomputers for computational neuroscience research."}}
{"id": "2512.09427", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09427", "abs": "https://arxiv.org/abs/2512.09427", "authors": ["Guoqiang Zou", "Wanyu Wang", "Hao Zheng", "Longxiang Yin", "Yinhe Han"], "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators", "comment": "10 pages, 5 figures", "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.", "AI": {"tldr": "ODMA is an on-demand memory allocation framework for accelerators with poor random-access bandwidth that improves memory utilization and serving performance for LLMs.", "motivation": "Current memory managers are inefficient for serving LLMs on accelerators with poor random-access bandwidth (like LPDDR5-based systems). Static pre-allocation wastes memory, while fine-grained paging is unsuitable due to high random-access costs. Existing solutions don't address the specific characteristics of random-access-constrained memory accelerators.", "method": "ODMA uses a lightweight length predictor coupled with dynamic bucket partitioning and a large-bucket safeguard to handle distribution drift and heavy-tailed requests. It periodically updates boundaries from live traces to maximize memory utilization.", "result": "On Alpaca and Google-NQ datasets, ODMA improves prediction accuracy from 82.68% to 93.36%. Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA increases memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines.", "conclusion": "Hardware-aware memory allocation like ODMA enables efficient LLM serving on random-access-constrained memory platforms by significantly improving memory utilization and serving performance."}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "PHWSOA is a Pareto-based hybrid optimization algorithm combining Whale and Seagull algorithms to optimize makespan, VM load balancing, and cost in cloud task scheduling, achieving significant performance improvements over baseline methods.", "motivation": "Existing cloud task scheduling solutions mainly optimize single or limited metrics, lacking comprehensive multi-objective optimization. There's a need for algorithms that can simultaneously optimize multiple conflicting objectives like makespan, resource utilization, and cost.", "method": "Proposes PHWSOA (Pareto-based Hybrid Whale-Seagull Optimization Algorithm) that combines WOA and SOA strengths while mitigating their weaknesses. Key features: Halton sequence initialization for population diversity, Pareto-guided mutation to prevent premature convergence, parallel processing for faster convergence, and dynamic VM load redistribution mechanism.", "result": "Extensive CloudSim experiments with real-world NASA-iPSC and HPC2N workloads show PHWSOA achieves: 72.1% reduction in makespan, 36.8% improvement in VM load balancing, and 23.5% cost savings. Outperforms baseline methods including WOA, GA, PEWOA, and GCWOA.", "conclusion": "PHWSOA demonstrates strong potential for efficient resource management in practical cloud environments by effectively addressing multi-objective optimization in task scheduling through hybrid algorithm design and Pareto dominance principles."}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix is a high-performance synthetic image generator for Particle Image Velocimetry (PIV) implemented in JAX, offering massively parallel image generation for training data-hungry ML methods and accelerating development of real-time flow estimation systems.", "motivation": "The paper addresses the need for high-throughput synthetic image generation to support training of data-intensive reinforcement learning methods for flow estimation and to reduce development iteration times for real-time PIV feedback systems in active fluids control studies.", "method": "SynthPix is implemented in JAX, leveraging accelerator parallelism to achieve orders of magnitude higher throughput in image-pair generation compared to existing tools, while maintaining compatibility with standard PIV configuration parameters.", "result": "SynthPix achieves significantly higher throughput (several orders of magnitude) in image-pair generation per second compared to existing synthetic image generators for PIV applications.", "conclusion": "SynthPix provides a valuable tool for the fluid dynamics community, enabling efficient training of ML-based flow estimation methods and accelerating development of real-time PIV systems through high-performance synthetic image generation."}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "STAR is a straggler-tolerant DL training system that addresses CPU/bandwidth imbalances causing stragglers, proposing new synchronization modes and resource management to reduce Time-To-Accuracy by 48-84% compared to state-of-the-art systems.", "motivation": "Stragglers remain widespread in homogeneous GPU-based DL training due to CPU and bandwidth usage imbalances, and existing mitigation methods (switching from SSGD to ASGD) may not improve Time-To-Accuracy and can even generate more stragglers due to higher resource consumption.", "method": "STAR includes new synchronization modes that group workers for parameter updating, uses heuristic and ML methods to choose optimal synchronization mode for minimizing TTA, reallocates resources to support selected mode while minimizing impact on co-located jobs, and proactively prevents stragglers by avoiding overloading CPU/bandwidth resources in PS allocation and gradient transmission.", "result": "Trace-driven evaluation on AWS shows STAR generates 48-84% lower TTA than state-of-the-art systems in PS architecture and 51-70% lower TTA in all-reduce architecture, while maintaining the converged accuracy of SSGD.", "conclusion": "STAR effectively addresses straggler problems in homogeneous GPU-based DL training through intelligent synchronization mode selection and resource management, significantly reducing Time-To-Accuracy while maintaining model accuracy."}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "First transformation that adds both lock-freedom and recoverability to lock-based implementations by replacing lock operations with recoverable, lock-free alternatives while supporting nested locks.", "motivation": "Existing systems lack transformations that simultaneously provide lock-freedom (non-blocking progress) and recoverability (fault tolerance) for concurrent data structures. Current approaches typically address one property but not both together.", "method": "Transform lock-based implementations by substituting lock acquire and release operations with recoverable, lock-free alternatives. The transformation supports nested locks for generality and maintains correctness of the original lock-based implementation.", "result": "Successfully creates the first transformation that introduces both lock-freedom and recoverability to concurrent data structures, enabling fault-tolerant, non-blocking progress while preserving correctness.", "conclusion": "The transformation bridges the gap between lock-based and lock-free recoverable systems, providing a practical approach to enhance concurrent data structures with both non-blocking progress and fault tolerance properties."}}
