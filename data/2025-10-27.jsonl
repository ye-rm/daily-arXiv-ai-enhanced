{"id": "2510.20981", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20981", "abs": "https://arxiv.org/abs/2510.20981", "authors": ["Stefan Abi-Karam", "Rishov Sarkar", "Suhail Basalama", "Jason Cong", "Callie Hao"], "title": "FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs", "comment": "Accepted and to be presented at ASP-DAC 2026", "summary": "Dataflow hardware designs enable efficient FPGA implementations via\nhigh-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO)\nchannel buffers remains challenging. FIFO sizes are user-defined and balance\nlatency and area-undersized FIFOs cause stalls and potential deadlocks, while\noversized ones waste memory. Determining optimal sizes is non-trivial: existing\nmethods rely on restrictive assumptions, conservative over-allocation, or slow\nRTL simulations. We emphasize that runtime-based analyses (i.e., simulation)\nare the only reliable way to ensure deadlock-free FIFO optimization for\ndata-dependent designs.\n  We present FIFOAdvisor, a framework that automatically determines FIFO sizes\nin HLS designs. It leverages LightningSim, a 99.9\\% cycle-accurate simulator\nsupporting millisecond-scale incremental runs with new FIFO configurations.\nFIFO sizing is formulated as a dual-objective black-box optimization problem,\nand we explore heuristic and search-based methods to characterize the\nlatency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a\nframework for optimizing affine dataflow designs lowered from C++, MLIR, or\nPyTorch, enabling deeper optimization of FIFOs in these workloads.\n  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear\nalgebra and deep learning workloads. Our results reveal Pareto-optimal\nlatency-memory frontiers across optimization strategies. Compared to baseline\ndesigns, FIFOAdvisor achieves much lower memory usage with minimal delay\noverhead. Additionally, it delivers significant runtime speedups over\ntraditional HLS/RTL co-simulation, making it practical for rapid design space\nexploration. We further demonstrate its capability on a complex accelerator\nwith data-dependent control flow.\n  Code and results: https://github.com/sharc-lab/fifo-advisor"}
{"id": "2510.21533", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.21533", "abs": "https://arxiv.org/abs/2510.21533", "authors": ["Misaki Kida", "Shimpei Sato"], "title": "Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs", "comment": "5 pages, 5 figures", "summary": "As IoT and edge inference proliferate,there is a growing need to\nsimultaneously optimize area and delay in lookup-table (LUT)-based multipliers\nthat implement large numbers of low-bitwidth operations in parallel. This paper\nproposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx\n7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the\nlogic functions mapped to the LUTs, the proposed method reduces the LUT count\nby one compared with the prior 12-LUT design while also shortening the critical\npath. Evaluation confirms that the circuit attains minimal resource usage and a\ncritical-path delay of 2.750 ns."}
{"id": "2510.21547", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.21547", "abs": "https://arxiv.org/abs/2510.21547", "authors": ["Hangyu Zhang", "Sachin S. Sapatnekar"], "title": "Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation", "comment": "ASPDAC 2025", "summary": "Global placement is essential for high-quality and efficient circuit\nplacement for complex modern VLSI designs. Recent advancements, such as\nelectrostatics-based analytic placement, have improved scalability and solution\nquality. This work demonstrates that using an accelerated FFT technique,\nAccFFT, for electric field computation significantly reduces runtime.\nExperimental results on standard benchmarks show significant improvements when\nincorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup\nin FFT computation and a 32% total runtime improvement against ePlace-MS, with\n1.0% reduction of scaled half-perimeter wirelength after detailed placement."}
{"id": "2510.20931", "categories": ["cs.DC", "cs.AR", "C.1.4; C.4"], "pdf": "https://arxiv.org/pdf/2510.20931", "abs": "https://arxiv.org/abs/2510.20931", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Jeremy Kepner"], "title": "Lincoln AI Computing Survey (LAICS) and Trends", "comment": "12 pages, 7 figures, 2025 IEEE High Performance Extreme Computing\n  (HPEC) conference, September 2025", "summary": "In the past year, generative AI (GenAI) models have received a tremendous\namount of attention, which in turn has increased attention to computing systems\nfor training and inference for GenAI. Hence, an update to this survey is due.\nThis paper is an update of the survey of AI accelerators and processors from\npast seven years, which is called the Lincoln AI Computing Survey -- LAICS\n(pronounced \"lace\"). This multi-year survey collects and summarizes the current\ncommercial accelerators that have been publicly announced with peak performance\nand peak power consumption numbers. In the same tradition of past papers of\nthis survey, the performance and power values are plotted on a scatter graph,\nand a number of dimensions and observations from the trends on this plot are\nagain discussed and analyzed. Market segments are highlighted on the scatter\nplot, and zoomed plots of each segment are also included. A brief description\nof each of the new accelerators that have been added in the survey this year is\nincluded, and this update features a new categorization of computing\narchitectures that implement each of the accelerators."}
{"id": "2510.21048", "categories": ["cs.PF", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21048", "abs": "https://arxiv.org/abs/2510.21048", "authors": ["Jiabo Shi", "Dimitrios Pezaros", "Yehia Elkhatib"], "title": "xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads", "comment": null, "summary": "The global scarcity of GPUs necessitates more sophisticated strategies for\nDeep Learning jobs in shared cluster environments. Accurate estimation of how\nmuch GPU memory a job will require is fundamental to enabling advanced\nscheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and\nresource underutilization. However, existing estimation methods have\nlimitations. Approaches relying on static analysis or historical data with\nmachine learning often fail to accurately capture runtime dynamics.\nFurthermore, direct GPU analysis consumes scarce resources, and some techniques\nrequire intrusive code modifications. Thus, the key challenge lies in precisely\nestimating dynamic memory requirements, including memory allocator nuances,\nwithout consuming GPU resources and non-intrusive code changes. To address this\nchallenge, we propose xMem, a novel framework that leverages CPU-only dynamic\nanalysis to accurately estimate peak GPU memory requirements a priori. We\nconducted a thorough evaluation of xMem against state-of-the-art solutions\nusing workloads from 25 different models, including architectures like\nConvolutional Neural Networks and Transformers. The analysis of 5209 runs,\nwhich includes ANOVA and Monte Carlo results, highlights xMem's benefits: it\ndecreases the median relative error by 91% and significantly reduces the\nprobability of estimation failure as safe OOM thresholds by 75%, meaning that\nthe estimated value can often be used directly without causing OOM. Ultimately,\nthese improvements lead to a 368% increase in memory conservation potential\nover current solutions."}
{"id": "2510.20931", "categories": ["cs.DC", "cs.AR", "C.1.4; C.4"], "pdf": "https://arxiv.org/pdf/2510.20931", "abs": "https://arxiv.org/abs/2510.20931", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Jeremy Kepner"], "title": "Lincoln AI Computing Survey (LAICS) and Trends", "comment": "12 pages, 7 figures, 2025 IEEE High Performance Extreme Computing\n  (HPEC) conference, September 2025", "summary": "In the past year, generative AI (GenAI) models have received a tremendous\namount of attention, which in turn has increased attention to computing systems\nfor training and inference for GenAI. Hence, an update to this survey is due.\nThis paper is an update of the survey of AI accelerators and processors from\npast seven years, which is called the Lincoln AI Computing Survey -- LAICS\n(pronounced \"lace\"). This multi-year survey collects and summarizes the current\ncommercial accelerators that have been publicly announced with peak performance\nand peak power consumption numbers. In the same tradition of past papers of\nthis survey, the performance and power values are plotted on a scatter graph,\nand a number of dimensions and observations from the trends on this plot are\nagain discussed and analyzed. Market segments are highlighted on the scatter\nplot, and zoomed plots of each segment are also included. A brief description\nof each of the new accelerators that have been added in the survey this year is\nincluded, and this update features a new categorization of computing\narchitectures that implement each of the accelerators."}
{"id": "2510.21155", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21155", "abs": "https://arxiv.org/abs/2510.21155", "authors": ["Dandan Liang", "Jianing Zhang", "Evan Chen", "Zhe Li", "Rui Li", "Haibo Yang"], "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach", "comment": null, "summary": "Split Federated Learning (SFL) enables scalable training on edge devices by\ncombining the parallelism of Federated Learning (FL) with the computational\noffloading of Split Learning (SL). Despite its great success, SFL suffers\nsignificantly from the well-known straggler issue in distributed learning\nsystems. This problem is exacerbated by the dependency between Split Server and\nclients: the Split Server side model update relies on receiving activations\nfrom clients. Such synchronization requirement introduces significant time\nlatency, making straggler a critical bottleneck to the scalability and\nefficiency of the system. To mitigate this problem, we propose MU-SplitFed, a\nstraggler-resilient SFL algorithm in zeroth-order optimization that decouples\ntraining progress from straggler delays via a simple yet effective unbalanced\nupdate mechanism.\n  By enabling the server to perform $\\tau$ local updates per client round,\nMU-SplitFed achieves a convergence rate of $O(\\sqrt{d/(\\tau T)})$ for\nnon-convex objectives, demonstrating a linear speedup of $\\tau$ in\ncommunication rounds. Experiments demonstrate that MU-SplitFed consistently\noutperforms baseline methods with the presence of stragglers and effectively\nmitigates their impact through adaptive tuning of $\\tau$. The code for this\nproject is available at https://github.com/Johnny-Zip/MU-SplitFed."}
{"id": "2510.21173", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21173", "abs": "https://arxiv.org/abs/2510.21173", "authors": ["Víctor Rampérez", "Javier Soriano", "David Lizcano", "Shadi Aljawarneh", "Juan A. Lara"], "title": "From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker", "comment": null, "summary": "Cloud computing has been consolidated as a support for the vast majority of\ncurrent and emerging technologies. However, there are some barriers that\nprevent the exploitation of the full potential of this technology. First, the\nmajor cloud providers currently put the onus of implementing the mechanisms\nthat ensure compliance with the desired service levels on cloud consumers.\nHowever, consumers do not have the required expertise. Since each cloud\nprovider exports a different set of low-level metrics, the strategies defined\nto ensure compliance with the established service-level agreement (SLA) are\nbound to a particular cloud provider. This fosters provider lock-in and\nprevents consumers from benefiting from the advantages of multi-cloud\nenvironments. This paper presents a solution to the problem of automatically\ntranslating SLAs into objectives expressed as metrics that can be measured\nacross multiple cloud providers. First, we propose an intelligent\nknowledge-based system capable of automatically translating high-level SLAs\ndefined by cloud consumers into a set of conditions expressed as vendor-neutral\nmetrics, providing feedback to cloud consumers (intelligent tutoring system).\nSecondly, we present the set of vendor-neutral metrics and explain how they can\nbe measured for the different cloud providers. Finally, we report a validation\nbased on two use cases (IaaS and PaaS) in a multi-cloud environment formed by\nleading cloud providers. This evaluation has demonstrated that, thanks to the\ncomplementarity of the two solutions, cloud consumers can automatically and\ntransparently exploit the multi-cloud in many application domains, as endorsed\nby the cloud experts consulted in the course of this study."}
{"id": "2510.21183", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21183", "abs": "https://arxiv.org/abs/2510.21183", "authors": ["Anwesha Mukherjee", "Rajkumar Buyya"], "title": "Generative Federated Learning for Smart Prediction and Recommendation Applications", "comment": null, "summary": "This paper proposes a generative adversarial network and federated\nlearning-based model to address various challenges of the smart prediction and\nrecommendation applications, such as high response time, compromised data\nprivacy, and data scarcity. The integration of the generative adversarial\nnetwork and federated learning is referred to as Generative Federated Learning\n(GFL). As a case study of the proposed model, a heart health monitoring\napplication is considered. The realistic synthetic datasets are generated using\nthe generated adversarial network-based proposed algorithm for improving data\ndiversity, data quality, and data augmentation, and remove the data scarcity\nand class imbalance issues. In this paper, we implement the centralized and\ndecentralized federated learning approaches in an edge computing paradigm. In\ncentralized federated learning, the edge nodes communicate with the central\nserver to build the global and personalized local models in a collaborative\nmanner. In the decentralized federated learning approach, the edge nodes\ncommunicate among themselves to exchange model updates for collaborative\ntraining. The comparative study shows that the proposed framework outperforms\nthe existing heart health monitoring applications. The results show that using\nthe proposed framework (i) the prediction accuracy is improved by 12% than the\nconventional framework, and (ii) the response time is reduced by 73% than the\nconventional cloud-only system."}
{"id": "2510.21304", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21304", "abs": "https://arxiv.org/abs/2510.21304", "authors": ["Hagit Attiya", "Constantin Enea", "Enrique Román-Calvo"], "title": "Arbitration-Free Consistency is Available (and Vice Versa)", "comment": null, "summary": "The fundamental tension between \\emph{availability} and \\emph{consistency}\nshapes the design of distributed storage systems. Classical results capture\nextreme points of this trade-off: the CAP theorem shows that strong models like\nlinearizability preclude availability under partitions, while weak models like\ncausal consistency remain implementable without coordination. These theorems\napply to simple read-write interfaces, leaving open a precise explanation of\nthe combinations of object semantics and consistency models that admit\navailable implementations.\n  This paper develops a general semantic framework in which storage\nspecifications combine operation semantics and consistency models. The\nframework encompasses a broad range of objects (key-value stores, counters,\nsets, CRDTs, and transactional databases) and consistency models (from causal\nconsistency and sequential consistency to snapshot isolation and transactional\nand non-transactional SQL).\n  Within this framework, we prove the \\emph{Arbitration-Free Consistency} (AFC)\ntheorem, showing that an object specification within a consistency model admits\nan available implementation if and only if it is \\emph{arbitration-free}, that\nis, it does not require a total arbitration order to resolve visibility or read\ndependencies.\n  The AFC theorem unifies and generalizes previous results, revealing\narbitration-freedom as the fundamental property that delineates\ncoordination-free consistency from inherently synchronized behavior."}
{"id": "2510.21348", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21348", "abs": "https://arxiv.org/abs/2510.21348", "authors": ["João A. Silva", "Hervé Paulino", "João M. Lourenço"], "title": "Parsley's Group Size Study", "comment": null, "summary": "Parsley is a resilient group-based Distributed Hash Table that incorporates a\npreemptive peer relocation technique and a dynamic data sharding mechanism to\nenhance robustness and balance. In addition to the hard limits on group size,\ndefined by minimum and maximum thresholds, Parsley introduces two soft limits\nthat define a target interval for maintaining stable group sizes. These soft\nboundaries allow the overlay to take proactive measures to prevent violations\nof the hard limits, improving system stability under churn. This work provides\nan in-depth analysis of the rationale behind the parameter values adopted for\nParsley's evaluation. Unlike related systems, which specify group size limits\nwithout justification, we conduct a systematic overlay characterization study\nto understand the effects of these parameters on performance and scalability.\nThe study examines topology operations, the behavior of large groups, and the\noverall trade-offs observed, offering a grounded explanation for the chosen\nconfiguration values."}
{"id": "2510.21373", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21373", "abs": "https://arxiv.org/abs/2510.21373", "authors": ["Sankalpa Timilsina", "Susmit Shannigrahi"], "title": "LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science", "comment": null, "summary": "Scientific communities are increasingly using geographically distributed\ncomputing platforms. The current methods of compute placement predominantly use\nlogically centralized controllers such as Kubernetes (K8s) to match tasks to\navailable resources. However, this centralized approach is unsuitable in\nmulti-organizational collaborations. Furthermore, workflows often need to use\nmanual configurations tailored for a single platform and cannot adapt to\ndynamic changes across infrastructure. Our work introduces a decentralized\ncontrol plane for placing computations on geographically dispersed compute\nclusters using semantic names. We assign semantic names to computations to\nmatch requests with named Kubernetes (K8s) service endpoints. We show that this\napproach provides multiple benefits. First, it allows placement of\ncomputational jobs to be independent of location, enabling any cluster with\nsufficient resources to execute the computation. Second, it facilitates dynamic\ncompute placement without requiring prior knowledge of cluster locations or\npredefined configurations."}
{"id": "2510.21419", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21419", "abs": "https://arxiv.org/abs/2510.21419", "authors": ["Sankalpa Timilsina", "Susmit Shannigrahi"], "title": "Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads", "comment": null, "summary": "Distributed cloud environments hosting data-intensive applications often\nexperience slowdowns due to network congestion, asymmetric bandwidth, and\ninter-node data shuffling. These factors are typically not captured by\ntraditional host-level metrics like CPU or memory. Scheduling without\naccounting for these conditions can lead to poor placement decisions, longer\ndata transfers, and suboptimal job performance. We present a network-aware job\nscheduler that uses supervised learning to predict the completion time of\ncandidate jobs. Our system introduces a prediction-and-ranking mechanism that\ncollects real-time telemetry from all nodes, uses a trained supervised model to\nestimate job duration per node, and ranks them to select the best placement. We\nevaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the\nFABRIC testbed by running network-intensive Spark workloads. Compared to the\ndefault Kubernetes scheduler, which makes placement decisions based on current\nresource availability alone, our proposed supervised scheduler achieved 34-54%\nhigher accuracy in selecting optimal nodes for job placement. The novelty of\nour work lies in the demonstration of supervised learning for real-time,\nnetwork-aware job scheduling on a multi-site cluster."}
{"id": "2510.21493", "categories": ["cs.DC", "94-08", "D.2.2"], "pdf": "https://arxiv.org/pdf/2510.21493", "abs": "https://arxiv.org/abs/2510.21493", "authors": ["Rüdiger Valk", "Daniel Moldt"], "title": "On Reduction and Synthesis of Petri's Cycloids", "comment": null, "summary": "Cycloids are particular Petri nets for modelling processes of actions and\nevents, belonging to the fundaments of Petri's general systems theory. Defined\nby four parameters they provide an algebraic formalism to describe strongly\nsynchronized sequential processes. To further investigate their structure,\nreduction systems of cycloids are defined in the style of rewriting systems and\nproperties of irreducible cycloids are proved. In particular the synthesis of\ncycloid parameters from their Petri net structure is derived, leading to an\nefficient method for a decision procedure for cycloid isomorphism."}
{"id": "2510.21549", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.21549", "abs": "https://arxiv.org/abs/2510.21549", "authors": ["Marc Fuchs", "Fabian Kuhn"], "title": "Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence", "comment": null, "summary": "The distributed coloring problem is arguably one of the key problems studied\nin the area of distributed graph algorithms. The most standard variant of the\nproblem asks for a proper vertex coloring of a graph with $\\Delta+1$ colors,\nwhere $\\Delta$ is the maximum degree of the graph. Despite an immense amount of\nwork on distributed coloring problems in the distributed setting, determining\nthe deterministic complexity of $(\\Delta+1)$-coloring in the standard message\npassing model remains one of the most important open questions of the area. In\nthis paper, we aim to improve our understanding of the deterministic complexity\nof $(\\Delta+1)$-coloring as a function of $\\Delta$ in a special family of\ngraphs for which significantly faster algorithms are already known. The\nneighborhood independence $\\theta$ of a graph is the maximum number of pairwise\nnon-adjacent neighbors of some node of the graph. In general, in graphs of\nneighborhood independence $\\theta=O(1)$ (e.g., line graphs), it is known that\n$(\\Delta+1)$-coloring can be solved in $2^{O(\\sqrt{\\log\\Delta})}+O(\\log^* n)$\nrounds. In the present paper, we significantly improve this result, and we show\nthat in graphs of neighborhood independence $\\theta$, a $(\\Delta+1)$-coloring\ncan be computed in $(\\theta\\cdot\\log\\Delta)^{O(\\log\\log\\Delta /\n\\log\\log\\log\\Delta)}+O(\\log^* n)$ rounds and thus in quasipolylogarithmic time\nin $\\Delta$ as long as $\\theta$ is at most polylogarithmic in $\\Delta$. We also\nshow that the known approach that leads to a polylogarithmic in $\\Delta$\nalgorithm for $(2\\Delta-1)$-edge coloring already fails for edge colorings of\nhypergraphs of rank at least $3$."}
{"id": "2510.21048", "categories": ["cs.PF", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21048", "abs": "https://arxiv.org/abs/2510.21048", "authors": ["Jiabo Shi", "Dimitrios Pezaros", "Yehia Elkhatib"], "title": "xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads", "comment": null, "summary": "The global scarcity of GPUs necessitates more sophisticated strategies for\nDeep Learning jobs in shared cluster environments. Accurate estimation of how\nmuch GPU memory a job will require is fundamental to enabling advanced\nscheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and\nresource underutilization. However, existing estimation methods have\nlimitations. Approaches relying on static analysis or historical data with\nmachine learning often fail to accurately capture runtime dynamics.\nFurthermore, direct GPU analysis consumes scarce resources, and some techniques\nrequire intrusive code modifications. Thus, the key challenge lies in precisely\nestimating dynamic memory requirements, including memory allocator nuances,\nwithout consuming GPU resources and non-intrusive code changes. To address this\nchallenge, we propose xMem, a novel framework that leverages CPU-only dynamic\nanalysis to accurately estimate peak GPU memory requirements a priori. We\nconducted a thorough evaluation of xMem against state-of-the-art solutions\nusing workloads from 25 different models, including architectures like\nConvolutional Neural Networks and Transformers. The analysis of 5209 runs,\nwhich includes ANOVA and Monte Carlo results, highlights xMem's benefits: it\ndecreases the median relative error by 91% and significantly reduces the\nprobability of estimation failure as safe OOM thresholds by 75%, meaning that\nthe estimated value can often be used directly without causing OOM. Ultimately,\nthese improvements lead to a 368% increase in memory conservation potential\nover current solutions."}
