<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 8]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication](https://arxiv.org/abs/2511.21910)
*Haoxuan Shan,Cong Guo,Chiyue Wei,Feng Cheng,Junyao Zhang,Hai,Li,Yiran Chen*

Main category: cs.AR

TL;DR: Platinum is a lightweight ASIC accelerator for ultra-low-bit neural networks using lookup tables, achieving significant speedups and energy efficiency over existing solutions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for more efficient hardware for large language models, particularly for ultra-low-bit quantization where existing lookup table methods suffer from computation/hardware overheads and suboptimal ternary-weight network support.

Method: Proposes Platinum ASIC accelerator with offline-generated LUT construction paths to reduce overhead, and adaptive path switching between general bit-serial and optimized ternary-weight execution for mixed-precision matrix multiplication.

Result: On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), with energy reductions of 32.4x, 3.23x, and 20.9x in 0.96mm² chip area.

Conclusion: Platinum demonstrates LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms, offering significant performance and energy improvements.

Abstract: The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.

</details>


### [2] [CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing](https://arxiv.org/abs/2511.22166)
*Shuai Dong,Junyi Yang,Ye Ke,Hongyang Shang,Arindam Basu*

Main category: cs.AR

TL;DR: CADC introduces dendritic nonlinearity into crossbar-based IMC for CNNs, dramatically reducing partial sums by 54-88% while maintaining accuracy, achieving significant speedup and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Crossbar-based IMC architectures for CNN acceleration face significant overhead from partial sums generated when partitioning large convolutional layers across multiple crossbars, requiring additional buffer, transfer, and accumulation operations.

Method: Proposes crossbar-aware dendritic convolution (CADC) that embeds a nonlinear dendritic function (zeroing negative values) directly within crossbar computations to increase sparsity in partial sums.

Result: CADC reduces partial sums by 80% (LeNet-5), 54% (ResNet-18), 66% (VGG-16), and up to 88% (SNNs). Enables zero-compression/skipping (29.3% buffer/transfer reduction, 47.9% accumulation reduction) with minimal accuracy degradation (0.01-0.9%). Achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 with 11x-18x speedup and 1.9x-22.9x energy efficiency improvement.

Conclusion: CADC effectively addresses partial sum overhead in crossbar-based IMC for CNNs through biologically-inspired dendritic nonlinearity, achieving substantial reductions in computational overhead while maintaining accuracy, resulting in significant performance and energy efficiency gains.

Abstract: Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.

</details>


### [3] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas is a hardware-software co-design framework for RISC-V ASIPs that combines burst DMA memory access, HLS optimizations, and an e-graph based retargetable compiler to achieve up to 9.27x speedup on real-world workloads.


<details>
  <summary>Details</summary>
Motivation: Existing RISC-V ASIP frameworks suffer from limited performance due to restricted hardware synthesis capabilities and rigid compiler support, creating a need for a more holistic co-design approach.

Method: Aquas uses MLIR-based hardware-software co-design with: 1) burst DMA engine for fast memory access, 2) advanced HLS optimizations for hardware synthesis, and 3) e-graph based retargetable compiler with novel matching engine for efficient instruction matching.

Result: The framework achieves up to 9.27x speedup on real-world workloads including point cloud processing and LLM inference, demonstrating significant performance improvements over existing approaches.

Conclusion: Aquas provides an effective holistic hardware-software co-design solution for RISC-V ASIPs that addresses the limitations of existing frameworks and delivers substantial performance gains for diverse applications.

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [4] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff is a gradient-based optimization framework that automatically finds optimal intra-layer mapping and inter-layer fusion strategies to accelerate DNN inference on tensor accelerators.


<details>
  <summary>Details</summary>
Motivation: Efficient deployment of DNNs/LLMs on tensor accelerators is challenging due to the enormous design space created by the interaction between intra-layer mapping and inter-layer fusion strategies.

Method: Constructs a unified differentiable analytical cost model to predict energy/latency for single-layer mappings and layer fusion strategies, then uses gradient-based optimization with discrete constraints encoded in loss function to explore design space.

Result: Experimental results show FADiff achieves better optimization in terms of energy and latency compared to existing methods.

Conclusion: FADiff successfully addresses the challenge of optimizing DNN deployment on tensor accelerators through gradient-based joint optimization of mapping and fusion strategies.

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [5] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 3RSeT reduces read disturbance errors in STT-MRAM caches by 71.8% through selective tag comparison, improving MTTF by 3.6x and reducing energy by 62.1% with minimal area overhead.


<details>
  <summary>Details</summary>
Motivation: STT-MRAM is promising for on-chip caches due to its advantages over SRAM, but suffers from read disturbance errors during parallel tag comparisons, which significantly increases error rates and reliability challenges.

Method: Proposes 3RSeT (Read Disturbance Rate Reduction by Selective Tag Comparison) which proactively disables tags with no chance of hit using low significant bits of tags on each access request, eliminating unnecessary tag reads.

Result: 3RSeT reduces read disturbance rate in tag array by 71.8%, improves Mean Time To Failure by 3.6x, reduces energy consumption by 62.1% with no performance compromise and less than 0.4% area overhead.

Conclusion: 3RSeT effectively addresses the read disturbance error problem in STT-MRAM caches through selective tag comparison, significantly improving reliability and energy efficiency with minimal hardware cost.

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [6] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: ITA proposes treating LLM weights as physical circuit topology in ASICs instead of mutable data, eliminating memory hierarchy to overcome the "Memory Wall" bottleneck on edge devices.


<details>
  <summary>Details</summary>
Motivation: The "Memory Wall" problem - prohibitive bandwidth and energy costs of fetching LLM weights from DRAM for every token generated - throttles LLM deployment on consumer edge devices. Current architectures treat weights as mutable software data, incurring massive energy penalties for general-purpose programmability.

Method: The Immutable Tensor Architecture (ITA) treats model weights as physical circuit topology rather than data. Parameters are encoded directly into metal interconnects and logic of mature-node ASICs (28nm/40nm), eliminating memory hierarchy. A "Split-Brain" system design uses a host CPU for dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

Result: The approach fundamentally changes how LLM weights are stored and accessed, potentially enabling energy-efficient LLM inference on edge devices by eliminating DRAM access costs for weight fetching.

Conclusion: ITA represents a paradigm shift from treating model weights as mutable software data to encoding them as physical circuit topology, offering a promising solution to the "Memory Wall" problem for edge LLM deployment through specialized ASIC design.

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [7] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: Cohet is the first CXL-driven coherent heterogeneous computing framework that decouples compute and memory resources into CPU/XPU pools sharing a unified coherent memory pool, with a full-system simulator SimCXL showing significant performance improvements over PCIe.


<details>
  <summary>Details</summary>
Motivation: Conventional PCIe-based heterogeneous computing suffers from inefficient fine-grained host-device interactions and complex programming models. CXL-based coherent heterogeneous computing has potential but lacks available platforms, immature ecosystems, and unclear application prospects.

Method: Cohet framework decouples compute and memory resources into unbiased CPU and XPU pools sharing a single unified coherent memory pool. It exposes standard malloc/mmap interfaces to both CPU and XPU threads, with OS handling smart memory allocation. SimCXL simulator models all CXL sub-protocols and device types, calibrated against real CXL testbeds.

Result: CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. CXL-NIC achieves 5.5-40.2x speedup for RAO offloading and 1.86x average speedup for RPC (de)serialization offloading compared to PCIe-NIC.

Conclusion: Cohet demonstrates the viability and benefits of CXL-driven coherent heterogeneous computing, with significant performance improvements over traditional PCIe-based approaches, validated through the SimCXL simulator and real-world applications like RAO and RPC.

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [8] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: GAVINA is a novel DNN accelerator combining undervolting with bit-serial computation for flexible mixed-precision approximation, achieving up to 89 TOP/sW energy efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Voltage overscaling (undervolting) offers quadratic power savings but has high error rates, and existing undervolting accelerators using 8-bit arithmetic can't compete with modern low-precision (<8b) architectures.

Method: Proposed Guarded Aggressive underVolting (GAV) combines undervolting with bit-serial computation, selectively lowering supply voltage on least significant bit combinations. Implemented as GAVINA architecture supporting arbitrary mixed precision and flexible undervolting.

Result: GAVINA achieves up to 89 TOP/sW energy efficiency in most aggressive configuration. GAV provides 20% energy efficiency boost via undervolting with negligible accuracy degradation on ResNet-18.

Conclusion: GAVINA successfully addresses undervolting's high error rate limitations by combining it with bit-serial computation, enabling flexible mixed-precision approximation with significant energy efficiency gains and minimal accuracy impact.

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: A novel framework combining high-performance cluster computing with intelligent algorithms in a blockchain infrastructure, featuring evolved proof-of-work consensus, dynamic trust ratings, and statistical draw system for inclusive participation.


<details>
  <summary>Details</summary>
Motivation: Address the high energy consumption and exclusion of less powerful systems in high-performance computing and intelligent algorithms by creating a more sustainable, scalable, and inclusive approach that promotes efficiency and broad participation.

Method: Proposes a blockchain-based framework integrating high-performance cluster computing with intelligent algorithms. Key features include: 1) Evolved proof-of-work consensus linking computational efforts to block rewards, 2) Dynamic trust rating system based on accurate block validation history, 3) Statistical draw system to give less powerful nodes opportunities for block creation.

Result: The framework promotes optimal resource utilization, wide participation across computational capacities, and creates a merit-based system that rewards genuine contributions while ensuring sustainability and inclusivity in intelligent algorithm development.

Conclusion: The proposed strategy offers a comprehensive solution to sustainability challenges in high-performance computing and intelligent algorithms by combining blockchain technology with evolved consensus mechanisms, trust-based incentives, and inclusive participation systems.

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [10] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: The paper shows that for n > 2f, asynchronous message-passing with crash failures (AMP_f) and Heard-Of model with message omissions (HO_f) are equivalent for colorless tasks, but only equivalent for colored tasks when f = 1 (and n > 2).


<details>
  <summary>Details</summary>
Motivation: To understand the precise relationship between two fundamental distributed computing models (AMP_f and HO_f) and determine where round-based abstractions can capture asynchronous computation.

Method: Bidirectional simulations between AMP_f and HO_f via an intermediate model that captures the notion of silencing, with proofs for both deterministic and randomized protocols against non-adaptive adversaries.

Result: For n > 2f: equivalence for colorless tasks; for colored tasks: equivalence only when f = 1 (and n > 2). Separation for larger f due to silenced processes in HO_f leading to incompatible decisions.

Conclusion: The results precisely delineate where round-based abstractions (HO model) capture asynchronous computation and where they do not, showing that expressive limits are structural rather than probabilistic.

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [11] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: A latency-constraint disaggregated architecture for LLM serving that separates resources into latency-strict and latency-relaxed pools to handle co-located online/offline workloads, with bottleneck-based scheduling and fast preemption to maintain SLOs.


<details>
  <summary>Details</summary>
Motivation: LLMs are deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads improves resource utilization but causes severe load imbalance in Prefill/Decode disaggregated systems due to fluctuating request mixes altering the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot handle bursty traffic patterns.

Method: 1) Latency-constraint disaggregated architecture separating cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. 2) Bottleneck-based scheduler guided by Roofline-based performance model for performance bottleneck based scheduling. 3) Fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.

Result: Experiments on real-world traces show the method improves offline throughput by up to 3x compared to existing offline system approaches, while maintaining online request SLOs.

Conclusion: The proposed latency-constraint disaggregated architecture with bottleneck-based scheduling and fast preemption effectively handles co-located online/offline LLM workloads, solving P/D imbalance while preserving online performance and significantly improving resource utilization.

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [12] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+ is a novel cache replacement algorithm designed specifically for metadata caches that addresses correlated references, outperforming state-of-the-art algorithms like S3-FIFO by up to 28.5% lower miss ratio on metadata traces.


<details>
  <summary>Details</summary>
Motivation: Metadata caches inherently exhibit correlated references even when data accesses don't, which reduces effectiveness of existing cache replacement algorithms because these correlated references are often mistakenly categorized as hot blocks.

Method: Clock2Q+ uses three queues similar to S3-FIFO but introduces a correlation window in the Small FIFO queue where blocks in this window do not set the reference bit, allowing it to better handle correlated references.

Result: Clock2Q+ achieves up to 28.5% lower miss ratio on metadata traces compared to S3-FIFO, performs well on data traces too, has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is easy to tune and implement.

Conclusion: Clock2Q+ is an effective cache replacement algorithm specifically designed for metadata caches that addresses correlated references, outperforming state-of-the-art algorithms while maintaining properties essential for large-scale storage systems.

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [13] [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)
*Matteo Bjornsson,Taylor Hardin,Taylor Heinecke,Marcin Furtak,David L. Millman,Mike P. Wittie*

Main category: cs.DC

TL;DR: ZipperChain is a distributed ledger technology that achieves transaction immutability, agreement, and availability without distributed consensus by using a pipeline of specialized services on fast data center networks, achieving high throughput and fast finality without native tokens.


<details>
  <summary>Details</summary>
Motivation: To overcome performance limitations of distributed consensus mechanisms in DLTs, which suffer from network communication bottlenecks between participating nodes.

Method: Uses a construction process that transfers trust from third-party services to ZipperChain's correctness guarantees. Blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by fast data center networks, avoiding distributed consensus.

Result: Achieves transaction throughput approaching network line speeds with block finality around 500 ms, while maintaining immutability, agreement, and availability of transaction data.

Conclusion: ZipperChain provides a high-performance DLT alternative that doesn't require distributed consensus or native tokens, offering fast transaction processing through centralized block creation infrastructure.

Abstract: Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.

</details>


### [14] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: CDF-based RDL integration outperforms FFI in multilingual replicated data systems for software quality, latency, memory, and throughput.


<details>
  <summary>Details</summary>
Motivation: Modern distributed systems use multiple languages across replicas, but existing RDLs typically support single languages, requiring special integration code with unknown quality and performance characteristics.

Method: Empirical study comparing two RDL integration strategies: foreign-function interface (FFI) and common data format (CDF), measuring software metrics and performance.

Result: CDF offers superior software quality, lower latency, better memory consumption, and higher throughput than FFI. A CDF-based RDL was created and enhanced with plug-in extensibility.

Conclusion: CDF-based integration provides better design insights for RDLs in multilingual replicated data systems, addressing modern distributed system needs.

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [15] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT introduces prefix-aware attention kernels for LLM decoding that exploit shared prefixes across requests to reduce KV cache memory accesses and improve performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM serving suffers from memory-bound decode attention due to repeated loading of shared KV cache prefixes across requests. Existing attention implementations fail to exploit hierarchical prefix sharing (system prompts, templates, RAG), wasting memory bandwidth and resources.

Method: PAT uses a pack-forward-merge paradigm: packs queries by shared prefix to reduce memory accesses, runs customized multi-tile kernels for resource efficiency, applies multi-stream forwarding and KV splitting to reduce bubbles, and performs online softmax merging with low overhead.

Result: PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% compared to state-of-the-art attention kernels on real-world and synthetic workloads.

Conclusion: Prefix-aware attention kernels like PAT effectively exploit shared prefixes in LLM workloads to significantly reduce memory bandwidth pressure and improve decoding performance.

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [16] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: This paper studies optimal fault-tolerant Simultaneous Agreement protocols for crash failures using limited information exchanges, introducing a new exchange that achieves near-optimal performance with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Previous work on optimal fault-tolerant Agreement protocols focused on full information exchange, which is costly in message size. The authors aim to develop optimal protocols using limited information exchanges to reduce communication overhead while maintaining fault tolerance.

Method: The paper analyzes Simultaneous Agreement for crash failures using various limited information exchanges from literature: FloodSet protocol, its variant with failure counting, and another variant with agent-value association. A new information exchange is introduced that enables decisions at worst one round later than optimal but with lower computation and space requirements. Protocols are derived by implementing knowledge-based programs.

Result: The paper derives protocols that are optimal among protocols for each of these information exchanges. The new information exchange achieves decisions at worst one round later than the optimal protocol of Dwork and Moses (1988) but with significantly lower computation cost and space requirements.

Conclusion: Limited information exchanges can provide efficient alternatives to full information approaches for fault-tolerant agreement protocols, with the newly introduced exchange offering a favorable trade-off between decision latency and computational overhead.

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [17] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer is a unified system-level acceleration framework for LLM serving that optimizes expert placement, cache compression, and scheduling to maximize end-to-end efficiency.


<details>
  <summary>Details</summary>
Motivation: Large Language Models impose substantial challenges on serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks, requiring system-level optimization.

Method: OmniInfer integrates three components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling, built atop vLLM with adaptive resource disaggregation and global coordination.

Result: Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, reduces TPOT by 36%, and with OmniProxy further reduces TTFT by 38%.

Conclusion: OmniInfer provides a comprehensive system-level acceleration framework that significantly improves LLM serving efficiency through fine-grained optimization and is open-sourced for community use.

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [18] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge is a distributed context management system for LLM edge services that stores user context as token sequences rather than raw text, improving response times and reducing synchronization overhead.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of LLMs benefits latency-sensitive and privacy-aware applications, but managing user context across distributed edge nodes is challenging due to LLMs' stateless nature. Existing client-side context storage solutions introduce network latency and bandwidth overhead, undermining edge deployment advantages.

Method: DisCEdge stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, the system avoids redundant computation and enables efficient data replication. The authors implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware.

Result: DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to raw-text-based systems. It reduces client request sizes by a median of 90% compared to client-side context management while guaranteeing data consistency.

Conclusion: DisCEdge effectively addresses the challenge of managing user context in distributed LLM edge deployments by using tokenized context storage, providing significant performance improvements in response times, synchronization overhead, and bandwidth usage while maintaining data consistency.

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [19] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC accelerates mesh-based Monte Carlo light simulation using GPU ray-tracing cores for 1.5-4.5x speedup while eliminating complex mesh generation.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh-based Monte Carlo (MMC) methods for light-tissue interaction modeling are computationally expensive due to frequent ray-boundary intersection tests, despite GPU acceleration. The need for complex tetrahedral mesh generation and limited performance improvements motivate leveraging modern GPU hardware capabilities.

Method: RT-MMC uses NVIDIA's OptiX platform to leverage hardware-accelerated ray-tracing cores (RT-cores) on modern GPUs. It extends graphics ray-tracing pipelines to volumetric ray-tracing in turbid media, eliminating tetrahedral mesh generation requirements while supporting wide-field sources without mesh retesselation.

Result: RT-MMC shows excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. The performance gains significantly enhance MMC practicality for routine simulations.

Conclusion: Migrating from software-based to hardware-based ray-tracing simplifies MMC workflows and provides significant speedups expected to increase with wider ray-tracing hardware adoption. This approach enables leveraging emerging hardware resources for various biophotonics applications.

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [20] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe is a dynamic adapter placement and routing framework that addresses performance skew in multi-tenant LoRA serving by accounting for rank variability, achieving up to 2× higher throughput and 9× lower TTFT with 50% fewer GPUs.


<details>
  <summary>Details</summary>
Motivation: Current LoRA serving systems co-batch heterogeneous adapters without considering rank (size) variability, causing severe performance skew and requiring more GPUs to meet SLOs, while existing optimizations ignore this heterogeneity and underutilize GPU resources.

Method: LoRAServe uses workload-aware dynamic adapter placement and routing, dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access to handle rank diversity in LoRA serving.

Result: Evaluations on production traces show LoRAServe achieves up to 2× higher throughput, up to 9× lower Time To First Token (TTFT), and uses up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

Conclusion: LoRAServe effectively addresses rank diversity challenges in multi-tenant LoRA serving, maximizing throughput and minimizing tail latency under real-world workload drift while significantly reducing GPU requirements.

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [21] [Areon: Latency-Friendly and Resilient Multi-Proposer Consensus](https://arxiv.org/abs/2511.23025)
*Álvaro Castro-Castilla,Marcin Pawlowski,Hong-Sheng Zhou*

Main category: cs.DC

TL;DR: Areon is a latency-friendly, stake-weighted, multi-proposer PoS consensus protocol using DAG structure with sliding window references and CCA-local fork choice for robust consensus under partial synchrony.


<details>
  <summary>Details</summary>
Motivation: To create a robust proof-of-stake consensus protocol that achieves bounded-latency finality with lower reorganization frequency and depth compared to chain-based approaches, particularly under partial synchrony and adversarial conditions.

Method: Uses multiple proposers per slot organizing blocks into a DAG with sliding window references. Implements CCA-local, window-filtered fork choice comparing subDAG weights. Formalizes Areon-Ideal (abstract) and Areon-Base (practical with VRF-based eligibility, bounded references, validity checks).

Result: Proves backbone-style (k,ε)-finality theorem with confirmation depth as function of window length and tail probability. Simulation shows Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth than Ouroboros Praos across various adversarial stakes and network delays.

Conclusion: Areon's DAG-based approach with multiple proposers per slot and sliding window references provides robust consensus with bounded-latency finality and improved reorganization characteristics compared to chain-based protocols under partial synchrony.

Abstract: We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.
  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.
  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.

</details>


### [22] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: C²P²SL applies pipeline parallelism to split learning in wireless networks, overlapping communication and computation to reduce training time by over 38% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Split learning offloads computation to base stations while preserving data privacy, but suffers from sequential computation and communication processes that limit system efficiency.

Method: Proposes communication-computation pipeline parallel split learning (C²P²SL) that treats UE and BS communication/computation as an overall pipeline, parallelizing micro-batches. Formulates joint optimization of task split (cutting layer) and resource allocation using alternating optimization.

Result: Experimental results show C²P²SL reduces system training time by over 38% while maintaining convergence accuracy under different communication conditions.

Conclusion: Pipeline parallelism effectively addresses the sequential bottleneck in split learning, significantly improving training efficiency in wireless networks while preserving privacy benefits.

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>


### [23] [Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election](https://arxiv.org/abs/2511.23297)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: The paper studies leader election in content-oblivious communication model, showing that while general function computation is impossible, leader election becomes possible with topology knowledge in certain graph families, with specific impossibility results and algorithms provided.


<details>
  <summary>Details</summary>
Motivation: To explore whether leader election is possible in the extremely weak content-oblivious communication model where nodes can only send asynchronous, content-less pulses, despite previous results showing no non-constant functions can be computed over a single edge.

Method: Analyzes leader election in content-oblivious model with topology knowledge, proving impossibility results for graphs symmetric about an edge, and designing algorithms for trees that are not symmetric about any edge and even-diameter trees with diameter knowledge.

Result: Shows leader election is possible in certain graph families with topology knowledge: trees not symmetric about any edge admit O(n²) message algorithm; even-diameter trees with diameter knowledge admit O(nr) algorithm. Proves impossibility for graphs symmetric about an edge and necessity of exact topology knowledge.

Conclusion: Topology knowledge enables leader election in content-oblivious model despite its extreme communication limitations, with exact knowledge being necessary in some cases and specific graph properties determining feasibility.

Abstract: The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.
  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.
  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.
  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.
  Necessity of topology knowledge: In the family of graphs $\mathcal{G} = \{P_3, P_5\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\mathcal{G}$, then terminating leader election is impossible.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [24] [Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends](https://arxiv.org/abs/2511.22334)
*Pablo Prieto,Pablo Abad*

Main category: cs.PF

TL;DR: This study evaluates hardware platforms (CPUs, GPUs, NPUs) for running Small Language Models on edge devices, finding NPUs achieve highest performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Edge computing needs AI inference but edge devices have strict resource constraints. Small Language Models offer lightweight alternatives, but selecting optimal hardware balancing performance and efficiency is challenging.

Method: Evaluated inference performance and energy efficiency of commercial CPUs (Intel/ARM), GPUs (NVIDIA), and NPUs (RaiderChip) using common execution framework and state-of-the-art SLMs. Analyzed maximum performance, processing efficiency, and energy efficiency with bandwidth normalization for fair cross-architecture comparison.

Result: Specialized backends outperform general-purpose CPUs, with NPUs achieving highest performance by wide margin. Low-power ARM processors deliver competitive energy efficiency, but metrics combining performance and power (EDP) highlight NPUs as dominant architecture.

Conclusion: Hardware designs optimized for both efficiency and performance offer clear advantage for edge workloads, with NPUs emerging as superior choice for SLM inference on resource-constrained edge devices.

Abstract: Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.

</details>


### [25] [What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$](https://arxiv.org/abs/2511.22442)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: The paper analyzes Fβ scores for ranking classification models, showing they provide meaningful tradeoffs between precision and recall rankings, but F1 is suboptimal; it provides tools to find optimal β values.


<details>
  <summary>Details</summary>
Motivation: Ranking classification models is challenging because performance is multidimensional (precision and recall), and existing Fβ scores are widely used but their ranking properties and optimality as tradeoffs haven't been properly analyzed.

Method: The authors analyze Fβ-induced rankings mathematically, frame the tradeoff problem as optimization using Kendall rank correlations, and provide theoretical tools with closed-form expressions to find optimal β values for any performance distribution.

Result: Fβ rankings are meaningful and define shortest paths between precision- and recall-induced rankings; F1 and its skew-insensitive version are far from optimal tradeoffs; tools are provided to compute optimal β values, demonstrated on six case studies.

Conclusion: While Fβ scores provide meaningful rankings between precision and recall, the commonly used F1 is suboptimal; the paper provides theoretical foundations and practical tools to find better tradeoffs through optimal β selection.

Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_β$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_β$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_β$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $β$ for any distribution or set of performances, and we illustrate their use on six case studies.

</details>


### [26] [Motion-to-Motion Latency Measurement Framework for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2511.22467)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.PF

TL;DR: This paper presents a framework for measuring Motion-to-Motion (M2M) latency in CAV teleoperation, addressing the gap in standard measurement methods for delays between operator steering input and vehicle steering response.


<details>
  <summary>Details</summary>
Motivation: Latency is critical for CAV teleoperation performance, affecting operator response times. Existing work focuses on Glass-to-Glass (video) latency, but there's no standard method for measuring Motion-to-Motion latency between operator steering input and vehicle steering response.

Method: Developed an M2M latency measurement framework using Hall-effect sensors and two synchronized Raspberry Pi 5 devices. The system records interrupt-based timestamps on both operator and vehicle sides to estimate latency independently of teleoperation architecture.

Result: Precision tests show 10-15 ms accuracy. Field results reveal actuator delays dominate M2M latency, with median values exceeding 750 ms.

Conclusion: The paper successfully presents a standardized framework for measuring M2M latency in CAV teleoperation, revealing that actuator delays are the primary contributor to overall latency, with implications for system design and performance optimization.

Abstract: Latency is a key performance factor for the teleoperation of Connected and Autonomous Vehicles (CAVs). It affects how quickly an operator can perceive changes in the driving environment and apply corrective actions. Most existing work focuses on Glass-to-Glass (G2G) latency, which captures delays only in the video pipeline. However, there is no standard method for measuring Motion-to-Motion (M2M) latency, defined as the delay between the physical steering movement of the remote operator and the corresponding steering motion in the vehicle. This paper presents an M2M latency measurement framework that uses Hall-effect sensors and two synchronized Raspberry Pi~5 devices. The system records interrupt-based timestamps on both sides to estimate M2M latency, independently of the underlying teleoperation architecture. Precision tests show an accuracy of 10--15~ms, while field results indicate that actuator delays dominate M2M latency, with median values above 750~ms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [27] [Sneak Path Current Modeling in Memristor Crossbar Arrays for Analog In-Memory Computing](https://arxiv.org/abs/2511.21796)
*Shah Zayed Riam,Zhenlin Pei,Kyle Mooney,Chenyun Pan,Na Gong,Jinhui Wang*

Main category: cs.ET

TL;DR: This paper presents an analytical framework for estimating sneak path currents in memristor crossbar arrays using IMEC A14 technology, achieving <10.9% error while being up to 4784× faster than SPICE simulations.


<details>
  <summary>Details</summary>
Motivation: Memristor crossbar arrays are promising for next-gen computing but suffer from sneak path currents that degrade performance and reliability, creating a need for accurate and efficient modeling tools.

Method: Developed a closed-form analytical model based on IMEC A14 (1.4 nm) technology that captures interdependence of array size, ON/OFF ratio, read voltage, and interconnect conditions through mathematically derived relationships.

Result: The model achieves <10.9% error compared to SPICE simulations while being up to 4784 times faster, and supports various practical configurations including different data patterns and connection strategies.

Conclusion: The analytical framework provides a powerful tool for quantitative assessment and pre-design/real-time optimization of memristor-based analog in-memory computing architectures, addressing key scaling trade-offs.

Abstract: Memristor crossbar arrays have emerged as a key component for next-generation non-volatile memories, artificial neural networks, and analog in-memory computing (IMC) systems. By minimizing data transfer between the processor and memory, they offer substantial energy savings. However, a major design challenge in memristor crossbar arrays is the presence of sneak path currents, which degrade electrical performance, reduce noise margins, and limit reliable operations. This work presents a closed-form analytical framework based on IMEC A14 (1.4 nm) Technology for accurately estimating sneak path currents in memristor crossbar arrays. The proposed model captures the interdependence of key design parameters in memristor crossbar arrays, including array size, ON/OFF ratio of memristors, read voltage, and interconnect conditions, through mathematically derived relationships. It supports various practical configurations, such as different data patterns and connection strategies, enabling rapid and comprehensive sneak path current modeling. The sensitivity analysis includes how design parameters influence sneak path current and noise margin loss, underscoring the trade-offs involved in scaling crossbar arrays. Validation through SPICE simulations shows that the model achieves an error of less than 10.9% while being up to 4784 times faster than full circuit simulations. This analytical framework offers a powerful tool for quantitative assessment and pre-design/real-time optimization of memristor-based analog in-memory computing (IMC) architectures.

</details>


### [28] [Quantifying Geometry Effects on Low-Cost Intelligent Reflecting Surfaces](https://arxiv.org/abs/2511.22408)
*Yizhi He,Sayed Amir Hoseini,Mahbub Hassan*

Main category: cs.ET

TL;DR: Analysis of cost-saving IRS architectures: column-wise grouping and 1-bit phase quantization reduce median SNR gain by ~4 dB each (~8 dB combined) compared to ideal continuous-phase control, but still provide double-digit SNR gains over no-IRS baseline.


<details>
  <summary>Details</summary>
Motivation: To quantify the performance impact of practical cost-saving measures (column-wise element grouping and 1-bit phase quantization) in IRS deployments compared to ideal fully-controlled continuous-phase systems, addressing the need to balance hardware complexity and control overhead with performance requirements.

Method: Simulated a single-input single-output link at 26 GHz (mmWave) across three deployment geometries varying relative heights of access point, IRS, and user equipment. Compared performance of column-wise grouping and 1-bit phase quantization against ideal continuous-phase baseline.

Result: Switching from continuous to binary phase control reduces median SNR gain by ~4 dB; column-wise grouping introduces similar penalty; combining both constraints incurs ~8 dB loss under height-offset deployments. When nodes share same height, column-wise degradation becomes negligible. A 32x32 column-wise binary IRS still delivers double-digit SNR gains over no-IRS baseline.

Conclusion: Simplified IRS architectures with column-wise grouping and binary phase control remain viable for cost-constrained scenarios, providing quantitative guidelines for when such architectures can meet link-budget targets versus when full element-wise control is justified.

Abstract: Intelligent Reflecting Surfaces (IRS) promise low-power coverage extension, yet practical deployments must curb hardware complexity and control overhead. This paper quantifies the performance impact of two cost-saving measures, column-wise element grouping and 1-bit (binary) phase quantization, relative to the ideal fully-controlled, continuous-phase baseline. A single-input single-output link is simulated at 26 GHz (mmWave) across three deployment geometries that vary the relative heights of access point, IRS and user equipment. Results show that switching from continuous to binary phase control reduces median SNR gain by approximately 4 dB, while adopting column-wise grouping introduces a similar penalty; combining both constraints incurs approximately 8 dB loss under height-offset deployments. When all nodes share the same height, the degradation from column-wise control becomes negligible, indicating deployment geometry can offset control-granularity limits. Despite the losses, a 32 x 32 column-wise binary IRS still delivers double-digit SNR gains over the no-IRS baseline in most positions, confirming its viability for cost-constrained scenarios. The study provides quantitative guidelines on when simplified IRS architectures can meet link-budget targets and where full element-wise control remains justified.

</details>
