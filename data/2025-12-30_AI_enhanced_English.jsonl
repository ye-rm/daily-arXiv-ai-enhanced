{"id": "2512.22131", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22131", "abs": "https://arxiv.org/abs/2512.22131", "authors": ["Sheng Lu", "Qianhou Qu", "Sungyong Jung", "Qilian Liang", "Chenyun Pan"], "title": "An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator", "comment": null, "summary": "Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.", "AI": {"tldr": "RFET-based stochastic computing neural network accelerator reduces hardware complexity and improves efficiency over FinFET designs.", "motivation": "Stochastic computing neural networks (SCNNs) suffer from high resource usage due to components like stochastic number generators (SNGs) and accumulative parallel counters (APCs), limiting performance despite SC's potential for hardware complexity reduction.", "method": "Introduces novel SCNN architecture based on reconfigurable field-effect transistors (RFETs) that leverage device-level reconfigurability to design efficient SNGs, APCs, and core modules, plus a dedicated SCNN accelerator architecture for system-level simulation.", "result": "Experimental results using open-source standard cell libraries show the RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to FinFET-based design at same technology node.", "conclusion": "RFET technology enables more efficient SCNN accelerators by reducing hardware complexity through device-level reconfigurability, offering significant improvements over conventional FinFET designs."}}
{"id": "2512.22435", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22435", "abs": "https://arxiv.org/abs/2512.22435", "authors": ["Zining Wang", "Jian Gao", "Weimin Fu", "Xiaolong Guo", "Xuan Zhang"], "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience", "comment": null, "summary": "Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\\times$ overall pass rate, a 48$\\times$ Pass@1, and a 4$\\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.", "AI": {"tldr": "AnalogSAGE: A self-evolving multi-agent framework for analog circuit design that uses three-stage agent explorations with stratified memory layers to achieve 10x higher pass rates than existing methods.", "motivation": "Analog circuit design is knowledge-intensive and relies heavily on human intuition. Existing LLM-based approaches are limited by prompt-driven netlist generation or predefined templates, making them unable to handle complex specification requirements effectively.", "method": "AnalogSAGE uses a multi-agent framework with three-stage agent explorations coordinated through four stratified memory layers. It enables iterative refinement with simulation-grounded feedback and is implemented with open-source SKY130 PDK using ngspice.", "result": "Achieves 10x overall pass rate, 48x Pass@1 improvement, and 4x reduction in parameter search space compared to existing frameworks across ten operational amplifier design problems of varying difficulty.", "conclusion": "Stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice, demonstrating the effectiveness of the self-evolving multi-agent approach."}}
{"id": "2512.23062", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.23062", "abs": "https://arxiv.org/abs/2512.23062", "authors": ["Soham Pramanik", "Vimal William", "Arnab Raha", "Debayan Das", "Amitava Mukherjee", "Janet L. Paluh"], "title": "TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators", "comment": null, "summary": "The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.", "AI": {"tldr": "TYTAN is a Taylor-series based non-linear activation engine that accelerates AI inference at the edge with 2x performance improvement, 56% power reduction, and 35x smaller area compared to NVDLA baseline.", "motivation": "The need for domain-specific architectures to enhance acceleration and energy efficiency of AI inference at the edge, addressing resource constraints like computational cost and energy consumption from intensive operations like GEMMs and activation functions.", "method": "Proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine with a Generalized Non-linear Approximation Engine (G-NAE). Integrates re-configurable hardware design with specialized algorithm that dynamically estimates necessary approximation for each activation function to minimize deviation from baseline accuracy.", "result": "System-level simulations using Silvaco's FreePDK45 process node show TYTAN operates at >950 MHz clock frequency, achieving ~2 times performance improvement, ~56% power reduction, and ~35 times lower area compared to baseline NVIDIA Deep Learning Accelerator (NVDLA) implementation.", "conclusion": "TYTAN effectively supports accelerated, energy-efficient AI inference at the edge by optimizing non-linear activation functions through Taylor-series approximation and re-configurable hardware design."}}
{"id": "2512.22139", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22139", "abs": "https://arxiv.org/abs/2512.22139", "authors": ["Amur Saqib Pal", "Muhammad Mohsin Ghaffar", "Faisal Shafait", "Christian Weis", "Norbert Wehn"], "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA", "comment": "Accepted for publication by 25th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS 2025)", "summary": "Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.", "AI": {"tldr": "HLS4PC is an FPGA acceleration framework for 3D point cloud models that reduces complexity and improves throughput through hardware-aware optimizations.", "motivation": "Traditional point-based 3D point cloud models require server-grade GPUs but suffer from GPU under-utilization due to sparse, unstructured data, hindering real-time performance in safety-critical applications.", "method": "Developed HLS4PC, a parameterizable HLS framework for FPGA acceleration that leverages FPGA parallelization and algorithmic optimizations. Applied hardware-aware compression techniques to PointMLP-Elite model including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning to create PointMLP-Lite.", "result": "Created PointMLP-Lite with 4x less complexity and only 2% accuracy drop on ModelNet40. FPGA acceleration achieved 3.56x higher throughput than previous works, 2.3x higher than GPU, and 22x higher than CPU implementations.", "conclusion": "HLS4PC enables efficient FPGA acceleration of 3D point cloud models, significantly improving throughput while maintaining accuracy, making real-time performance feasible for safety-critical applications."}}
{"id": "2512.22125", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22125", "abs": "https://arxiv.org/abs/2512.22125", "authors": ["Jithin VG", "Ditto PS"], "title": "GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems", "comment": null, "summary": "The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.", "AI": {"tldr": "GPU-Virt-Bench is a benchmarking framework for evaluating GPU virtualization systems across 56 metrics in 10 categories, enabling comparison between software solutions and hardware MIG technology.", "motivation": "The growing demand for GPU-accelerated AI/LLM workloads requires efficient GPU resource sharing in cloud environments. While NVIDIA's MIG provides hardware isolation, it's only available on high-end datacenter GPUs, and existing software solutions lack standardized evaluation methods.", "method": "Developed GPU-Virt-Bench, a comprehensive benchmarking framework that measures 56 performance metrics across 10 categories including overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery.", "result": "The framework enables systematic comparison between software virtualization approaches (HAMi-core, BUD-FCSP) and ideal MIG behavior, revealing critical performance characteristics for production deployment decisions.", "conclusion": "GPU-Virt-Bench provides a standardized evaluation methodology for GPU virtualization systems, offering actionable insights for practitioners deploying GPU resources in multi-tenant environments and addressing the gap in systematic benchmarking for software-based GPU virtualization solutions."}}
{"id": "2512.22381", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22381", "abs": "https://arxiv.org/abs/2512.22381", "authors": ["Mohammad Zakaria Haider", "Amit Kumar Podder", "Prabin Mali", "Aranya Chakrabortty", "Sumit Paudyal", "Mohammad Ashiqur Rahman"], "title": "PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System", "comment": null, "summary": "The rapid deployment of electric vehicle charging stations (EVCS) within distribution networks necessitates intelligent and adaptive control to maintain the grid's resilience and reliability. In this work, we propose PHANTOM, a physics-aware adversarial network that is trained and optimized through a multi-agent reinforcement learning model. PHANTOM integrates a physics-informed neural network (PINN) enabled by federated learning (FL) that functions as a digital twin of EVCS-integrated systems, ensuring physically consistent modeling of operational dynamics and constraints. Building on this digital twin, we construct a multi-agent RL environment that utilizes deep Q-networks (DQN) and soft actor-critic (SAC) methods to derive adversarial false data injection (FDI) strategies capable of bypassing conventional detection mechanisms. To examine the broader grid-level consequences, a transmission and distribution (T and D) dual simulation platform is developed, allowing us to capture cascading interactions between EVCS disturbances at the distribution level and the operations of the bulk transmission system. Results demonstrate how learned attack policies disrupt load balancing and induce voltage instabilities that propagate across T and D boundaries. These findings highlight the critical need for physics-aware cybersecurity to ensure the resilience of large-scale vehicle-grid integration.", "AI": {"tldr": "PHANTOM is a physics-aware adversarial network using multi-agent RL to generate stealthy false data injection attacks on EV charging stations, demonstrating vulnerabilities in vehicle-grid integration.", "motivation": "The rapid deployment of EV charging stations creates cybersecurity vulnerabilities that could threaten grid resilience and reliability, requiring intelligent adaptive control and physics-aware security approaches.", "method": "PHANTOM integrates physics-informed neural networks with federated learning as a digital twin, then uses multi-agent RL (DQN and SAC methods) to develop adversarial false data injection strategies that bypass conventional detection.", "result": "The learned attack policies successfully disrupt load balancing and induce voltage instabilities that propagate across transmission and distribution boundaries, demonstrating significant grid vulnerabilities.", "conclusion": "The findings highlight the critical need for physics-aware cybersecurity measures to ensure resilience in large-scale vehicle-grid integration against sophisticated adversarial attacks."}}
{"id": "2512.22135", "categories": ["cs.DC", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22135", "abs": "https://arxiv.org/abs/2512.22135", "authors": ["Zicai Cui", "Zhouyuan Jian", "Weiwen Liu", "Weinan Zhang"], "title": "SoDA: An Efficient Interaction Paradigm for the Agentic Web", "comment": null, "summary": "As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.", "AI": {"tldr": "SoDA introduces a user sovereignty paradigm for the Agentic Web, decoupling memory from apps to eliminate data lock-in and using intent alignment to reduce cognitive load, achieving 27-35% token savings and 72-88% cognitive load reduction.", "motivation": "The paper addresses challenges in the transition from mobile App-dominated Attention Economy to Agentic Web: data lock-in by platforms and cognitive overload from complex manual interactions. Current interaction modes fail to support user sovereignty in this new era.", "method": "Proposes Sovereign Digital Avatar (SoDA) with orthogonal decoupling of storage, computation, and interaction. Uses Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing (Sensitivity Coefficient and Strictness Parameter) for risk governance in zero-trust environments.", "result": "27-35% reduction in token consumption during cross-platform service migration and complex task execution. 72% cognitive load reduction vs standard RAG, 88% vs manual workflows. Significant boost in Information Signal-to-Noise Ratio (SNR).", "conclusion": "SoDA establishes essential interaction infrastructure for efficient, low-friction, decentralized Agentic Web by shifting from killing time to saving time through user sovereignty, breaking platform monopoly on user memory."}}
{"id": "2512.22722", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.22722", "abs": "https://arxiv.org/abs/2512.22722", "authors": ["Yue Zhou", "Shaan Shah", "Tamal Dey", "Yucheng Zhou", "Ashwani Kumar", "Sashank Sriram", "Siyou Guo", "Siddharth Kumar", "Ranjan Kumar Patel", "Eva Y. Andrei", "Ertugrul Cubukcu", "Shriram Ramanathan", "Duygu Kuzum"], "title": "Protonic Nickelate Device Networks for Spatiotemporal Neuromorphic Computing", "comment": null, "summary": "Computation in biological neural circuits arises from the interplay of nonlinear temporal responses and spatially distributed dynamic network interactions. Replicating this richness in hardware has remained challenging, as most neuromorphic devices emulate only isolated neuron- or synapse-like functions. In this work, we introduce an integrated neuromorphic computing platform in which both nonlinear spatiotemporal processing and programmable memory are realized within a single perovskite nickelate material system. By engineering symmetric and asymmetric hydrogenated NdNiO3 junction devices on the same wafer, we combine ultrafast, proton-mediated transient dynamics with stable multilevel resistance states. Networks of symmetric NdNiO3 junctions exhibit emergent spatial interactions mediated by proton redistribution, while each node simultaneously provides short-term temporal memory, enabling nanoseconds scale operation with an energy cost of 0.2 nJ per input. When interfaced with asymmetric output units serving as reconfigurable long-term weights, these networks allow both feature transformation and linear classification in the same material system. Leveraging these emergent interactions, the platform enables real-time pattern recognition and achieves high accuracy in spoken-digit classification and early seizure detection, outperforming temporal-only or uncoupled architectures. These results position protonic nickelates as a compact, energy-efficient, CMOS-compatible platform that integrates processing and memory for scalable intelligent hardware.", "AI": {"tldr": "Researchers developed an integrated neuromorphic computing platform using perovskite nickelate materials that combines both nonlinear spatiotemporal processing and programmable memory in a single material system, enabling energy-efficient pattern recognition and classification tasks.", "motivation": "Biological neural circuits exhibit rich nonlinear temporal responses and spatially distributed dynamic interactions that are challenging to replicate in hardware, as most neuromorphic devices only emulate isolated neuron- or synapse-like functions separately.", "method": "Engineered symmetric and asymmetric hydrogenated NdNiO3 junction devices on the same wafer, combining ultrafast proton-mediated transient dynamics with stable multilevel resistance states. Networks of symmetric junctions exhibit emergent spatial interactions via proton redistribution while providing short-term temporal memory.", "result": "The platform achieved nanosecond-scale operation with 0.2 nJ energy per input, enabled real-time pattern recognition, and outperformed temporal-only or uncoupled architectures in spoken-digit classification and early seizure detection tasks with high accuracy.", "conclusion": "Protonic nickelates represent a compact, energy-efficient, CMOS-compatible platform that integrates both processing and memory capabilities for scalable intelligent hardware, bridging the gap between biological neural richness and hardware implementation."}}
{"id": "2512.22147", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.22147", "abs": "https://arxiv.org/abs/2512.22147", "authors": ["Ruifan Chu", "Anbang Wang", "Xiuxiu Bai", "Shuai Liu", "Xiaoshe Dong"], "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs", "comment": null, "summary": "In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.", "AI": {"tldr": "An LLM-based framework optimizes GPU kernels without full application builds by creating Minimal Executable Programs, using iterative optimization with error repair and pattern inheritance, achieving significant speedups across platforms.", "motivation": "Manual GPU kernel tuning is expensive and non-portable, while existing LLM methods assume cheap compilation/execution - unrealistic for large applications where full builds/runs are costly.", "method": "Extract hotspot kernels, complete them into Minimal Executable Programs (MEPs), perform multi-round iterative optimization with Automatic Error Repair and Performance Pattern Inheritance to fix faults and reuse effective strategies, then reintegrate optimized variants.", "result": "Achieved average speedups: 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels from supercomputing applications, surpassing direct LLM optimization.", "conclusion": "The framework enables practical, low-cost GPU kernel optimization without full-source dependencies, offers cross-platform portability, and outperforms direct LLM approaches while preserving correctness."}}
{"id": "2512.22136", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22136", "abs": "https://arxiv.org/abs/2512.22136", "authors": ["Mahadev Sunil Kumar", "Arnab Raha", "Debayan Das", "Gopakumar G", "Amitava Mukherjee"], "title": "SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware", "comment": null, "summary": "Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.", "AI": {"tldr": "A method for efficiently deploying distributed deep neural networks on edge devices through structured pruning and multi-objective optimization, demonstrated on MVCNN for 3D object recognition.", "motivation": "Deep neural networks are computationally expensive and have large parameter counts, making deployment on resource-constrained edge devices challenging while maintaining task performance.", "method": "Integrates structured model pruning with multi-objective optimization to tailor network capacity to heterogeneous device constraints. Specifically quantifies contribution of individual views in MVCNN to classification accuracy and allocates pruning budgets accordingly.", "result": "Resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by 1.2x to 5.0x across diverse hardware platforms.", "conclusion": "Performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments."}}
{"id": "2512.22784", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.22784", "abs": "https://arxiv.org/abs/2512.22784", "authors": ["Mikhail Erementchouk", "Aditya Shukla", "Pinaki Mazumder"], "title": "Relaxation-based dynamical Ising machines for discrete tomography", "comment": "21 pages; 6 figures", "summary": "Dynamical Ising machines are continuous dynamical systems that evolve from a generic initial state to a state strongly related to the ground state of the classical Ising model. We show that such a machine driven by the V${}_2$ dynamical model can solve exactly discrete tomography problems about reconstructing a binary image from the pixel sums along a discrete set of rays. In contrast to usual applications of Ising machines, targeting approximate solutions to optimization problems, the randomly initialized V${}_2$ model converges with high probability ($P_{\\mathrm{succ}} \\approx 1$) to an image precisely satisfying the tomographic data. For the problems with at most two rays intersecting at each pixel, the V${}_2$ model converges in internal machine time that depends only weakly on the image size. Our consideration is an example of how specific dynamical systems can produce exact solutions to highly non-trivial data processing tasks. Crucially, this solving capability arises from the dynamical features of the V${}_2$ model itself, in particular its equations of motion that enable non-local transitions of the discrete component of the relaxed spin beyond Hamming-neighborhood constraints, rather than from merely recasting the tomography problem in spin form.", "AI": {"tldr": "The V\u2082 dynamical Ising machine can solve discrete tomography problems exactly, reconstructing binary images from pixel sums along rays with high probability of convergence to precise solutions.", "motivation": "To demonstrate that dynamical Ising machines can produce exact solutions to complex data processing tasks (discrete tomography) rather than just approximate solutions to optimization problems, showing how specific dynamical systems can solve non-trivial problems.", "method": "Using the V\u2082 dynamical model, a continuous dynamical system that evolves from random initial states to solve discrete tomography problems. The model's equations of motion enable non-local transitions beyond Hamming-neighborhood constraints, allowing exact reconstruction of binary images from ray sums.", "result": "The V\u2082 model converges with high probability (P_succ \u2248 1) to images precisely satisfying tomographic data. For problems with at most two rays intersecting at each pixel, convergence time depends only weakly on image size.", "conclusion": "The V\u2082 dynamical Ising machine can solve discrete tomography problems exactly, demonstrating that specific dynamical systems can produce precise solutions to complex data processing tasks through their inherent dynamical features rather than just recasting problems in spin form."}}
{"id": "2512.23434", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23434", "abs": "https://arxiv.org/abs/2512.23434", "authors": ["Yongjie Guan"], "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates", "comment": "14 pages, 10 figures. Includes appendices", "summary": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.", "AI": {"tldr": "LRH (Local Rendezvous Hashing) improves consistent hashing by restricting HRW selection to a cache-local window of C neighboring nodes, achieving better load balance with lower lookup cost and minimal churn.", "motivation": "Traditional consistent hashing methods have limitations: ring-based schemes need many virtual nodes for good load balance, while multi-probe methods improve balance but suffer from scattered memory accesses and higher lookup costs.", "method": "LRH preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. It uses one binary search to locate a key, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted).", "result": "In benchmarks with N=5000 nodes, V=256 virtual nodes, K=50M keys and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697).", "conclusion": "LRH provides a practical solution that combines the benefits of ring-based consistent hashing with improved load balance and performance, significantly outperforming multi-probe methods while maintaining low churn during topology changes."}}
{"id": "2512.22137", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22137", "abs": "https://arxiv.org/abs/2512.22137", "authors": ["Jiangwen Dong", "Jiayu Li", "Wanyu Lin"], "title": "HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration", "comment": null, "summary": "Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.", "AI": {"tldr": "HybridFlow is a resource-adaptive framework for edge-cloud LLM collaboration that uses fine-grained task decomposition and adaptive routing to reduce inference latency and token usage.", "motivation": "LLMs have high inference latency and token consumption, making real-time deployment on edge devices challenging. Existing edge-cloud collaboration methods use coarse-grained task allocation, leading to redundant computation and inefficient resource utilization.", "method": "Two-stage approach: (1) dynamic decomposition of complex queries into interdependent subtasks that execute as soon as dependencies are resolved, and (2) resource-aware subtask routing using a learned router that assigns subtasks to edge or cloud based on predicted utility gains and real-time budget states.", "result": "Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning show effective reduction in end-to-end inference time and overall token usage while maintaining competitive accuracy.", "conclusion": "HybridFlow enables fast and token-efficient collaborative reasoning between edge and cloud LLMs through fine-grained parallelism and adaptive resource allocation, addressing limitations of existing coarse-grained edge-cloud approaches."}}
{"id": "2512.23212", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.23212", "abs": "https://arxiv.org/abs/2512.23212", "authors": ["Amod Holla", "Sumedh Chatterjee", "Sutanu Sen", "Anushka Mukherjee", "Fernando Garcia-Redondo", "Dwaipayan Biswas", "Francesca Iacopi", "Kaushik Roy"], "title": "LIMO: Low-Power In-Memory-Annealer and Matrix-Multiplication Primitive for Edge Computing", "comment": "26 pages, 12 figures; under review", "summary": "Combinatorial optimization (CO) underpins applications in science and engineering, ranging from logistics to electronic design automation. A classic example is the NP-complete Traveling Salesman Problem (TSP). Finding exact solutions for large-scale TSP instances remains computationally intractable; on von Neumann architectures, such solvers are constrained by the memory wall, incurring compute-memory traffic that grows with instance size. Metaheuristics, such as simulated annealing implemented on compute-in-memory (CiM) architectures, offer a way to mitigate the von Neumann bottleneck. This is accomplished by performing in-memory optimization cycles to rapidly find approximate solutions for TSP instances. Yet this approach suffers from degrading solution quality as instance size increases, owing to inefficient state-space exploration. To address this, we present LIMO, a mixed-signal computational macro that implements an in-memory annealing algorithm with reduced search-space complexity. The annealing process is aided by the stochastic switching of spin-transfer-torque magnetic-tunnel-junctions (STT-MTJs) to escape local minima. For large instances, our macro co-design is complemented by a refinement-based divide-and-conquer algorithm amenable to parallel optimization in a spatial architecture. Consequently, our system comprising several LIMO macros achieves superior solution quality and faster time-to-solution on instances up to 85,900 cities compared to prior hardware annealers. The modularity of our annealing peripherals allows the LIMO macro to be reused for other applications, such as vector-matrix multiplications (VMMs). This enables our architecture to support neural network inference. As an illustration, we show image classification and face detection with software-comparable accuracy, while achieving lower latency and energy consumption than baseline CiM architectures.", "AI": {"tldr": "LIMO is a mixed-signal computational macro for in-memory annealing that solves large-scale TSP problems with superior quality and speed, while also supporting neural network inference.", "motivation": "Traditional von Neumann architectures face memory wall limitations for solving large-scale combinatorial optimization problems like TSP, while existing compute-in-memory annealing approaches suffer from degrading solution quality with increasing problem size.", "method": "LIMO implements in-memory annealing with reduced search-space complexity using stochastic switching of STT-MTJs to escape local minima, complemented by a refinement-based divide-and-conquer algorithm for parallel optimization in spatial architectures.", "result": "The system achieves superior solution quality and faster time-to-solution on TSP instances up to 85,900 cities compared to prior hardware annealers, and also enables neural network inference with software-comparable accuracy, lower latency, and energy consumption.", "conclusion": "LIMO provides an effective hardware solution for large-scale combinatorial optimization while maintaining modularity for other applications like neural network inference, addressing both the von Neumann bottleneck and scalability limitations of existing approaches."}}
{"id": "2512.23494", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23494", "abs": "https://arxiv.org/abs/2512.23494", "authors": ["Eddy Truyen", "Wouter Joosen"], "title": "Optimal Configuration of API Resources in Cloud Native Computing", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.", "AI": {"tldr": "A framework for optimizing CPU/memory resource allocation during DevOps Release phase, applied to microservices with factor screening to reduce search space vs. Bayesian optimization for near-optimal configs.", "motivation": "Resource allocation optimization during DevOps Release phase is largely unexplored; most research focuses on Ops stage scheduling/autoscaling. Current approaches may leave containers with inappropriate memory allocation if no upfront fine-tuning is done before deployment.", "method": "Apply existing offline performance optimization framework to microservice applications during Release phase. Evaluate using TeaStore microservice app, statistically compare different optimization algorithms with factor screening to reduce search space vs. Bayesian optimization without screening.", "result": "Factor screening helps find optimal resource configuration with affordable sampling budget. For statistical algorithm comparison, screening is necessary to make data collection feasible. For near-optimal configurations, Bayesian optimization without screening performs better.", "conclusion": "Upfront resource optimization during Release phase is valuable; factor screening reduces search space for optimal configs, while Bayesian optimization without screening works better for near-optimal solutions. Different approaches suit different optimization goals."}}
{"id": "2512.23383", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2512.23383", "abs": "https://arxiv.org/abs/2512.23383", "authors": ["Hakan Yildiz", "Axel K\u00fcpper"], "title": "interID - An Ecosystem-agnostic Verifier Application for Self-sovereign Identity", "comment": null, "summary": "Self-Sovereign Identity is a transformative paradigm in digital identity management, empowering individuals with full control over their credentials. However, the coexistence of diverse SSI ecosystems, such as the European Digital Identity and the European Blockchain Services Infrastructure, poses significant challenges for cross-ecosystem interoperability due to technological and trust framework differences. This paper introduces \\textit{interID}, a modular credential verification application that addresses this fragmentation by orchestrating ecosystem-specific verifier services. Our key contributions include: (1) an ecosystem-agnostic orchestration layer that interfaces with multiple SSI verification services, (2) a unified API that abstracts underlying protocol complexities for service providers, and (3) a practical implementation that bridges three major SSI ecosystems: Hyperledger Indy/Aries, EBSI, and EUDI. Evaluation results demonstrate that interID successfully verifies credentials across all tested wallets with minimal performance overhead, while maintaining a flexible architecture that can be extended to accept credentials from additional SSI ecosystems. This work offers both a technical solution and architectural pattern for achieving interoperability in SSI verifier implementations.", "AI": {"tldr": "interID is a modular credential verification application that enables cross-ecosystem interoperability between different Self-Sovereign Identity (SSI) systems by orchestrating ecosystem-specific verifier services.", "motivation": "The coexistence of diverse SSI ecosystems (like European Digital Identity and European Blockchain Services Infrastructure) creates fragmentation and interoperability challenges due to technological and trust framework differences, preventing seamless credential verification across systems.", "method": "The paper introduces interID with three key components: (1) an ecosystem-agnostic orchestration layer that interfaces with multiple SSI verification services, (2) a unified API that abstracts underlying protocol complexities for service providers, and (3) a practical implementation bridging three major SSI ecosystems: Hyperledger Indy/Aries, EBSI, and EUDI.", "result": "Evaluation shows interID successfully verifies credentials across all tested wallets with minimal performance overhead, while maintaining a flexible architecture that can be extended to support additional SSI ecosystems.", "conclusion": "interID provides both a technical solution and architectural pattern for achieving interoperability in SSI verifier implementations, addressing the fragmentation problem in the SSI landscape."}}
{"id": "2512.22142", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22142", "abs": "https://arxiv.org/abs/2512.22142", "authors": ["Leyang Xue", "Meghana Madhyastha", "Myungjin Lee", "Amos Storkey", "Randal Burns", "Mahesh K. Marina"], "title": "On Harnessing Idle Compute at the Edge for Foundation Model Training", "comment": "Extended abstract version of this paper appeared in ACM MobiCom 2025", "summary": "The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.\n  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.\n  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.", "AI": {"tldr": "Cleave enables efficient decentralized foundation model training across edge devices by using selective hybrid tensor parallelism and cost optimization, matching cloud performance while handling device heterogeneity and failures.", "motivation": "Current foundation model training is centralized in cloud data centers, which is costly and limits accessibility. Decentralized training across edge devices offers a democratized alternative but faces challenges with performance, scalability, memory constraints, communication overhead, and device heterogeneity.", "method": "Cleave introduces selective hybrid tensor parallelism to finely partition training operations, combined with a parameter server centric framework to manage memory limits and avoid communication bottlenecks. It also uses a cost optimization model to guide device selection and workload distribution for handling device heterogeneity and churn.", "result": "Cleave matches cloud-based GPU training performance, scales efficiently to larger models and thousands of devices (supporting 8x more devices than baselines), achieves up to 10x faster per-batch training than state-of-the-art edge methods, and handles device failures with at least 100x faster recovery.", "conclusion": "Cleave successfully enables efficient decentralized foundation model training on edge devices, overcoming key limitations of existing approaches and providing a viable alternative to centralized cloud training while maintaining performance parity."}}
{"id": "2512.22149", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22149", "abs": "https://arxiv.org/abs/2512.22149", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan"], "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments", "comment": "7 pages, 2 figures", "summary": "Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.", "AI": {"tldr": "Adaptive GPU resource allocation framework for multi-agent LLM systems reduces latency by 85% vs round-robin while maintaining throughput, using O(N) algorithm for real-time adaptation on serverless platforms.", "motivation": "Multi-agent LLM systems face resource allocation challenges on serverless GPU platforms due to heterogeneous agent workloads, varying computational demands, and need for cost-effective scaling.", "method": "Dynamic GPU resource allocation framework based on workload characteristics, agent priorities, and minimum resource requirements with O(N) complexity algorithm for real-time adaptation.", "result": "85% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation; outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics.", "conclusion": "The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure, addressing heterogeneous computational demands, dynamic workload fluctuations, and capacity constraints."}}
{"id": "2512.22168", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22168", "abs": "https://arxiv.org/abs/2512.22168", "authors": ["Wei Li", "Zhenyu Bai", "Heru Wang", "Pranav Dangi", "Zhiqiang Zhang", "Cheng Tan", "Huiying Lan", "Weng-Fai Wong", "Tulika Mitra"], "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures", "comment": null, "summary": "Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.", "AI": {"tldr": "TL is an end-to-end compiler framework that compiles tile-based programs onto spatial dataflow accelerators, addressing the challenge of distributing tile instances across cores and optimizing data reuse through on-chip networks.", "motivation": "Spatial dataflow accelerators offer higher throughput and efficiency than traditional architectures by reducing memory bottlenecks, but their performance heavily depends on workload mapping. Limited programmability and reliance on hand-tuned vendor libraries hinder wider adoption.", "method": "TL proposes a hardware representation capturing interconnect topology, memory hierarchy, and compute capabilities. Built on MLIR ecosystem, it defines generic entry points for different front-ends and endpoints for back-ends, focusing on distributing tile instances across spatially distributed cores and exploiting on-chip networks.", "result": "The paper presents TL framework that enables both specialized architecture-specific optimizations and support for diverse spatial dataflow targets, addressing the central challenge of tile distribution and data reuse optimization.", "conclusion": "TL provides an end-to-end compilation solution for spatial dataflow accelerators that improves programmability while optimizing data movement and reuse across distributed cores, potentially enabling wider adoption of these high-performance architectures."}}
{"id": "2512.22173", "categories": ["cs.DC", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.22173", "abs": "https://arxiv.org/abs/2512.22173", "authors": ["Aliaksandr V. Yakutovich", "Jusong Yu", "Daniel Hollas", "Edan Bainglass", "Corsin Battaglia", "Miki Bonacci", "Lucas Fernandez Vilanova", "Stephan Henne", "Anders Kaestner", "Michel Kenzelmann", "Graham Kimbell", "Jakob Lass", "Fabio Lopes", "Daniel G. Mazzone", "Andres Ortega-Guerrero", "Xing Wang", "Nicola Marzari", "Carlo A. Pignedoli", "Giovanni Pizzi"], "title": "AiiDAlab: on the route to accelerate science", "comment": null, "summary": "With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).", "AI": {"tldr": "AiiDAlab is a web-based platform that simplifies complex computational workflows, enabling scientists across multiple disciplines to focus on research rather than technical computational details while ensuring reproducibility through automated provenance tracking.", "motivation": "The need for robust, automated research workflows to handle large-scale interdependent simulations on supercomputers, while reducing the technical expertise required for setup, execution, and interpretation of computational results.", "method": "Development of the AiiDAlab platform with an intuitive web browser interface that abstracts computational complexity, integrates with the AiiDA engine for automatic provenance tracking, and supports multiple scientific disciplines through user-driven improvements.", "result": "Successful expansion from computational materials science to quantum chemistry, atmospheric modeling, battery research, experimental data analysis, and educational settings, with improved user onboarding, resource access, and large dataset handling.", "conclusion": "AiiDAlab has matured into a powerful cross-disciplinary platform that accelerates scientific discovery by allowing researchers to focus on science rather than computational details, while ensuring reproducibility and supporting FAIR principles through integration with electronic laboratory notebooks."}}
{"id": "2512.22174", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22174", "abs": "https://arxiv.org/abs/2512.22174", "authors": ["Muhammad Zeeshan Karamat", "Sadman Saif", "Christiana Chamon Garcia"], "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs", "comment": null, "summary": "Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.", "AI": {"tldr": "BitFlipScope is a software framework for detecting and localizing bit-flip faults in LLMs, enabling fault diagnosis and lightweight recovery without fine-tuning.", "motivation": "LLMs in safety-critical settings are vulnerable to bit-flip faults from hardware degradation, cosmic radiation, or attacks like Rowhammer, which can cause unpredictable/dangerous behavior. Localizing these corruptions is essential for diagnosis, targeted correction, and restoring functionality without costly retraining.", "method": "Two deployment scenarios: 1) With clean reference model: differential analysis of outputs, hidden states, and internal activations to detect anomalous behavior and pinpoint faults. 2) Without reference model: residual-path perturbation and loss-sensitivity profiling to infer fault-impacted region directly from corrupted model.", "result": "The framework enables effective fault diagnosis and supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models.", "conclusion": "BitFlipScope represents an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments."}}
{"id": "2512.22180", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22180", "abs": "https://arxiv.org/abs/2512.22180", "authors": ["Alexander K. Chen"], "title": "iOS as Acceleration", "comment": "7 pages main text, 7 pages appendix. Presented at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.", "AI": {"tldr": "This paper explores using iOS mobile phones as distributed compute resources to enhance local machine learning capabilities at zero cost, overcoming barriers like privacy concerns, physical environment constraints, and cloud costs.", "motivation": "Large-scale ML requires powerful compute, creating barriers for restricted system environments. Cloud computing isn't always viable due to private/sensitive data, unavailable physical environments, or high costs, necessitating local computing solutions.", "method": "The paper presents a proof-of-concept system using distributed pipeline parallelism to harness iOS devices' powerful processors, addressing limitations like memory constraints, thermal throttling, and OS sandboxing.", "result": "The system demonstrates significant benefits in weaker compute environments by accelerating modest model training, batch inference, and agentic LRM tool-usage through iOS device utilization.", "conclusion": "Mobile devices have untapped potential to contribute to machine learning, with practical use-cases identified alongside limitations and future work directions for improving commonplace mobile device utilization."}}
{"id": "2512.22195", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22195", "abs": "https://arxiv.org/abs/2512.22195", "authors": ["Kun-Woo Shin", "Jay H. Park", "Moonwook Oh", "Yohan Jo", "Jaeyoung Do", "Sang-Won Lee"], "title": "MatKV: Trading Compute for Flash Storage in LLM Inference", "comment": "Accepted for publication in ICDE 2026", "summary": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.", "AI": {"tldr": "MatKV precomputes and stores key-value vectors for RAG objects in flash storage, reducing inference time and power consumption by half compared to GPU recomputation.", "motivation": "Two trends drive the need for efficient RAG inference: (1) inference costs surpassing training costs, and (2) widespread adoption of retrieval augmented generation. The prefill phase in RAG is particularly energy-intensive and slow when processing long inputs on GPUs.", "method": "MatKV precomputes key-value vectors of RAG objects (documents) and materializes them in inexpensive, fast, power-efficient flash storage. These precomputed KVs are then reused at inference time instead of being recomputed on GPUs.", "result": "Experiments show MatKV reduces both inference time and power consumption by half for RAG workloads compared to full KV computation on GPUs, with minimal impact on question-answering accuracy. Additional optimizations include overlapping KV loading with text decoding and enabling use of low-end GPUs for decoding.", "conclusion": "MatKV makes large-scale generative AI applications more cost-effective, power-efficient, and accessible across diverse hardware environments by addressing the energy-intensive prefill phase in RAG inference."}}
{"id": "2512.22215", "categories": ["cs.DC", "cs.MS", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.22215", "abs": "https://arxiv.org/abs/2512.22215", "authors": ["Simone Bn\u00e0", "Giuseppe Giaquinto", "Ettore Fadiga", "Tommaso Zanelli", "Francesco Bottau"], "title": "SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM", "comment": "43 pages", "summary": "High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.", "AI": {"tldr": "SPUMA is a full GPU port of OpenFOAM for NVIDIA and AMD GPUs using portable programming models and unified memory management, achieving up to 85% weak scaling efficiency and 82% energy reduction compared to CPUs.", "motivation": "While HPC on hybrid clusters offers significant opportunities for CFD, programmability remains challenging for GPUs in open-source contexts like OpenFOAM, despite widespread GPU adoption.", "method": "SPUMA implements a portable programming model with a memory pool manager leveraging modern GPU unified memory features, tested on LUMI (AMD MI250X) and Leonardo (NVIDIA A100) pre-exascale clusters using the DrivAer industrial test case.", "result": "Achieved 65% strong scaling efficiency with 8M cells/GPU, 75-85% weak scaling on 20 GPUs (improving to \u226590% with NVIDIA AmgX), one A100 equivalent to 200-300 CPU cores with sufficient oversubscription, and up to 82% energy reduction vs CPUs.", "conclusion": "SPUMA successfully demonstrates efficient GPU porting of OpenFOAM with portable programming models, achieving high performance and significant energy savings on both NVIDIA and AMD GPU architectures."}}
{"id": "2512.22231", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22231", "abs": "https://arxiv.org/abs/2512.22231", "authors": ["Nachiappan Chockalingam", "Akshay Deshpande", "Lokesh Butra", "Ram Sekhar Bodala", "Nitin Saksena", "Adithya Parthasarathy", "Balakrishna Pothineni", "Akash Kumar Agarwal"], "title": "Scalable Cloud-Native Architectures for Intelligent PMU Data Processing", "comment": null, "summary": "Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.", "AI": {"tldr": "A cloud-native architecture combining AI, edge/cloud computing, and distributed stream processing for scalable, low-latency PMU data analytics in smart grids.", "motivation": "Traditional centralized PMU data processing struggles with latency, scalability, and reliability issues due to increasing PMU deployments and dynamic grid conditions.", "method": "Cloud-native architecture integrating AI with edge/cloud computing, using distributed stream processing, containerized microservices, elastic resource orchestration, and ML models for time-series analysis.", "result": "Achieves sub-second response times while scaling to large PMU deployments, with analytical models showing improved latency, throughput, and reliability.", "conclusion": "Provides a robust, flexible foundation for next-generation smart grid analytics with embedded security/privacy mechanisms for critical infrastructure."}}
{"id": "2512.22402", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22402", "abs": "https://arxiv.org/abs/2512.22402", "authors": ["Bhanu Prakash Vangala", "Tanu Malik"], "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models", "comment": null, "summary": "Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.", "AI": {"tldr": "Pick and Spin is a Kubernetes-based framework for scalable, cost-effective self-hosting of large language models, featuring unified deployment, adaptive scaling, and intelligent routing that improves success rates by 21.6%, reduces latency by 30%, and cuts GPU costs by 33% compared to static deployments.", "motivation": "Organizations want to self-host LLMs for privacy, cost control, and customization, but face challenges with GPU utilization, workload routing, and reliability in deployment and maintenance.", "method": "Built on Kubernetes with unified Helm-based deployment, adaptive scale-to-zero automation, and hybrid routing that balances cost, latency, and accuracy using keyword heuristics and a lightweight DistilBERT classifier.", "result": "Evaluated four models (Llama-3 90B, Gemma-3 27B, Qwen-3 235B, DeepSeek-R1 685B) across eight benchmarks with five inference strategies and two routing variants (31,019 prompts, 163,720 inference runs). Achieved up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query vs static deployments.", "conclusion": "Pick and Spin provides a practical solution for scalable and economical self-hosted LLM orchestration, addressing key deployment challenges while significantly improving performance and reducing costs."}}
{"id": "2512.22420", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22420", "abs": "https://arxiv.org/abs/2512.22420", "authors": ["Rui Li", "Zhaoning Zhang", "Libo Zhang", "Huaimin Wang", "Xiang Fu", "Zhiquan Lai"], "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving", "comment": "6 pages, 11 figures", "summary": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.", "AI": {"tldr": "Nightjar is a learning-based adaptive speculative decoding algorithm that dynamically adjusts speculative length based on request load, improving throughput by up to 14.8% and reducing latency by 20.2% compared to fixed-length speculative decoding.", "motivation": "Current speculative decoding implementations use fixed speculative lengths, which fail to adapt to dynamic request rates and create performance bottlenecks in real-world serving scenarios, especially degrading performance in high-load, compute-bound environments.", "method": "Nightjar uses a learning-based algorithm to dynamically select optimal speculative length for different batch sizes, and can even disable speculative decoding when it provides no benefit, adapting to request load in real-time.", "result": "Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.", "conclusion": "Adaptive speculative inference through learning-based algorithms like Nightjar effectively addresses the limitations of fixed-length speculative decoding, providing significant performance improvements for LLM serving in dynamic real-world scenarios."}}
{"id": "2512.22492", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22492", "abs": "https://arxiv.org/abs/2512.22492", "authors": ["Zhenqian Chen", "Baoquan Zhong", "Xiang Li", "Qing Dai", "Xinkui Zhao", "Miao Ye", "Ren Cheng", "Lufei Zhang", "Jianwei Yin"], "title": "Role-Based Fault Tolerance System for LLM RL Post-Training", "comment": "16 pages, 19 figures", "summary": "RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.\n  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \\textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \\textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \\textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\\% failure injection frequency, RobustRL can achieve an ETTR of over 80\\% compared with the 60\\% in ByteRobust and achieves 8.4\\%-17.4\\% faster in end-to-end training time.", "AI": {"tldr": "RobustRL is a fault-tolerant system for RL post-training of LLMs that isolates failures by role, enabling partial recovery instead of full restarts, improving effective training time by 20%+.", "motivation": "RL post-training for LLMs combines training and inference workloads, exposing systems to faults from both sides. Existing fault tolerance frameworks target either training or inference separately, missing optimization opportunities in RL's asynchronous execution.", "method": "Uses role-based fault isolation (trainer, rollout, management roles) with three components: (1) Role-aware monitoring for accurate failure detection, (2) Non-disruptive recovery for trainers using rollout warm standbys and isolated machine replacement for rollouts, (3) Dynamic UCX-based point-to-point communication for weight synchronization between recovered roles.", "result": "Achieves over 80% Effective Training Time Ratio (ETTR) vs 60% in ByteRobust, with 8.4%-17.4% faster end-to-end training time on 256-GPU cluster with Qwen3-8B-Math workload under 10% failure injection frequency.", "conclusion": "RobustRL provides comprehensive fault tolerance for RL post-training by isolating failures by role, enabling partial recovery instead of full restarts, significantly improving training efficiency and robustness in distributed RL systems."}}
{"id": "2512.22560", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22560", "abs": "https://arxiv.org/abs/2512.22560", "authors": ["Wei Gao", "Yuheng Zhao", "Tianyuan Wu", "Shaopan Xiong", "Weixun Wang", "Dakai An", "Lunxi Cao", "Dilxat Muhtar", "Zichen Liu", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Yongbin Li", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng", "Wei Wang"], "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure", "comment": "17 pages, 17 figures", "summary": "Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.\n  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\\(\\times\\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.", "AI": {"tldr": "RollArc is a distributed system that optimizes agentic reinforcement learning training on disaggregated infrastructure by using hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation to achieve 1.35-2.05\u00d7 training speedup.", "motivation": "Agentic RL workloads are highly heterogeneous with compute-intensive prefill, bandwidth-bound decoding, and CPU-heavy environment simulations, requiring disaggregated infrastructure for specialized hardware. However, naive disaggregation causes synchronization overhead and resource underutilization due to complex dependencies between stages.", "method": "RollArc uses three core principles: (1) hardware-affinity workload mapping to route compute-bound and bandwidth-bound tasks to best-fit GPU devices, (2) fine-grained asynchrony managing execution at trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation that offloads stateless components to serverless infrastructure for elastic scaling.", "result": "RollArc achieves 1.35-2.05\u00d7 end-to-end training time reduction compared to monolithic and synchronous baselines. It successfully trained a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with over 3,000 GPUs, demonstrating scalability and robustness.", "conclusion": "RollArc effectively addresses the challenges of agentic RL training on disaggregated infrastructure by optimizing workload distribution and resource utilization, significantly improving training throughput while maintaining scalability for large-scale deployments."}}
{"id": "2512.22695", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22695", "abs": "https://arxiv.org/abs/2512.22695", "authors": ["Mona Moghadampanah", "Adib Rezaei Shahmirzadi", "Farhana Amin", "Dimitrios S. Nikolopoulos"], "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference", "comment": null, "summary": "Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.", "AI": {"tldr": "MLLMs introduce significant energy overhead (17-94%) compared to text-only models due to modality inflation, with bottlenecks varying by architecture and substantial GPU underutilization during multimodal execution.", "motivation": "While multimodal LLMs expand capabilities, their energy trade-offs across modalities remain poorly understood, as prior work focuses on text-only models. The paper aims to analyze the energy inefficiency introduced by multimodal inputs.", "method": "Conducted first detailed stage-level analysis of MLLM energy consumption, breaking pipeline into vision encoding, prefill, and decoding stages. Evaluated four representative MLLMs on NVIDIA A100 GPU, quantifying additional energy for multimodal vs text-only inference. Examined GPU power traces and implemented stage-wise dynamic voltage and frequency scaling (DVFS) optimization.", "result": "Found 17-94% energy overhead for multimodal inference across models. Identified two main bottlenecks: compute-heavy vision encoders and downstream impact of large visual token sequences during prefill. Discovered substantial GPU underutilization during multimodal execution and varying energy scaling behaviors across models based on input complexity. Demonstrated DVFS optimization enables energy savings with modest performance impact.", "conclusion": "Modality inflation is a key source of inefficiency in MLLMs, with energy bottlenecks varying by architecture. Stage-wise DVFS offers effective optimization. Findings provide practical insights for designing more energy-efficient multimodal LLM serving systems."}}
{"id": "2512.22743", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.22743", "abs": "https://arxiv.org/abs/2512.22743", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads", "comment": "15 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.\n  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).\n  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.", "AI": {"tldr": "OptiNIC is a domain-specific RDMA transport for distributed ML that eliminates traditional reliability guarantees (retransmissions, in-order delivery) in favor of best-effort transport, leveraging ML's tolerance for partial/missing data to reduce tail latency and improve performance.", "motivation": "As distributed ML scales to thousands of GPUs, tail latency in collective communication becomes a major bottleneck. Traditional RDMA transports enforce strict reliability and in-order delivery, introducing complexity and latency that scale poorly for ML workloads where rare packet delays can stall entire model pipelines.", "method": "OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling best-effort, out-of-order RDMA transport. It introduces adaptive timeouts to trigger forward progress when data may be lost/delayed, retains standard congestion control, and shifts loss recovery to the ML pipeline itself using techniques like Hadamard Transform and Erasure Coding.", "result": "OptiNIC improves time-to-accuracy by 2x, increases throughput by 1.6x for training/inference, lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults across two public clouds (Hyperstack and CloudLab).", "conclusion": "OptiNIC delivers a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads by revisiting traditional reliability guarantees based on ML's tolerance for partial/missing data, significantly improving performance and reducing tail latency."}}
{"id": "2512.22925", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22925", "abs": "https://arxiv.org/abs/2512.22925", "authors": ["Panlong Wu", "Yifei Zhong", "Danyang Chen", "Ting Wang", "Fangxin Wang"], "title": "Argus: Token Aware Distributed LLM Inference Optimization", "comment": null, "summary": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.", "AI": {"tldr": "Argus is a token-aware distributed edge-cloud LLM inference framework that optimizes task offloading by predicting output token lengths and using Lyapunov optimization to handle dynamic, heterogeneous environments.", "motivation": "LLMs have significant inference time variability in autoregressive architectures, especially in heterogeneous edge-cloud systems. Existing solutions neglect dynamic, stochastic environments and ignore variable output token lengths and device diversity.", "method": "Argus features: 1) Length-Aware Semantics (LAS) module that predicts output token lengths using a fine-tuned language model with token-length-sensitive feature modulation; 2) Lyapunov-guided Offloading Optimization (LOO) module that formulates long-term Quality-of-Experience optimization considering both LLM prefilling and decoding costs; 3) Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to solve the integer nonlinear programming problem.", "result": "Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.", "conclusion": "Argus is the first token-aware distributed edge-cloud LLM inference framework that effectively addresses the challenges of inference time variability in dynamic, heterogeneous environments through precise token length prediction and optimized task offloading."}}
{"id": "2512.23495", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23495", "abs": "https://arxiv.org/abs/2512.23495", "authors": ["Eddy Truyen"], "title": "Decoupling Adaptive Control in TeaStore", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.", "AI": {"tldr": "The paper analyzes different approaches for implementing self-adaptation in microservices using the TeaStore case study, examining trade-offs between fine-grained adaptation and system-wide control, and proposing a multi-tiered architecture combining various techniques.", "motivation": "To provide guidance on implementing self-adaptation in microservices by examining how different approaches address key properties: system-wide consistency, planning, and modularity, using the TeaStore specification as a case study.", "method": "Examines three approaches: software architectural methods, cloud-native Operator pattern, and legacy programming language techniques for decoupling self-adaptive control logic from the TeaStore application. Analyzes trade-offs between fine-grained expressive adaptation and system-wide control.", "result": "Analysis suggests these approaches are not mutually exclusive and can be combined into a multi-tiered architecture for self-adaptive microservices. Highlights when reuse of adaptation strategies is most effective.", "conclusion": "A multi-tiered architecture combining software architectural methods, Operator patterns, and programming techniques provides a comprehensive approach to implementing self-adaptation in microservices, balancing fine-grained control with system-wide consistency."}}
